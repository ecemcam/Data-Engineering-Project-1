[0m19:12:48.582296 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ab0de86c740>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ab0dd71cb30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ab0dcab90d0>]}


============================== 19:12:48.586217 | fa067ef7-5672-470b-abec-755e204c35cb ==============================
[0m19:12:48.586217 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m19:12:48.587159 [debug] [MainThread]: running dbt with arguments {'write_json': 'True', 'send_anonymous_usage_stats': 'True', 'debug': 'False', 'target_path': 'None', 'quiet': 'False', 'indirect_selection': 'eager', 'invocation_command': 'dbt debug', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'partial_parse': 'True', 'cache_selected_only': 'False', 'log_format': 'default', 'introspect': 'True', 'fail_fast': 'False', 'printer_width': '80', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'static_parser': 'True', 'profiles_dir': '/home/ecem/.dbt', 'use_experimental_parser': 'False', 'log_cache_events': 'False', 'empty': 'None'}
[0m19:12:48.598455 [info ] [MainThread]: dbt version: 1.11.0-b3
[0m19:12:48.599089 [info ] [MainThread]: python version: 3.12.3
[0m19:12:48.600055 [info ] [MainThread]: python path: /home/ecem/Desktop/data-pipeline-project/venv/bin/python3
[0m19:12:48.600811 [info ] [MainThread]: os info: Linux-6.14.0-33-generic-x86_64-with-glibc2.39
[0m19:12:51.618621 [info ] [MainThread]: Using profiles dir at /home/ecem/.dbt
[0m19:12:51.620388 [info ] [MainThread]: Using profiles.yml file at /home/ecem/.dbt/profiles.yml
[0m19:12:51.623344 [info ] [MainThread]: Using dbt_project.yml file at /home/ecem/Desktop/data-pipeline-project/dbt/dbt_project.yml
[0m19:12:51.624163 [info ] [MainThread]: adapter type: bigquery
[0m19:12:51.624757 [info ] [MainThread]: adapter version: 1.10.2
[0m19:12:51.788144 [info ] [MainThread]: Configuration:
[0m19:12:51.790497 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m19:12:51.794931 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m19:12:51.796788 [info ] [MainThread]: Required dependencies:
[0m19:12:51.798769 [debug] [MainThread]: Executing "git --help"
[0m19:12:51.838051 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m19:12:51.838846 [debug] [MainThread]: STDERR: "b''"
[0m19:12:51.839347 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m19:12:51.840196 [info ] [MainThread]: Connection:
[0m19:12:51.841065 [info ] [MainThread]:   method: service-account-json
[0m19:12:51.841559 [info ] [MainThread]:   database: data-pipeline-project-474812
[0m19:12:51.842549 [info ] [MainThread]:   execution_project: data-pipeline-project-474812
[0m19:12:51.843305 [info ] [MainThread]:   schema: analytics_staging
[0m19:12:51.844081 [info ] [MainThread]:   location: US
[0m19:12:51.845455 [info ] [MainThread]:   priority: interactive
[0m19:12:51.847955 [info ] [MainThread]:   maximum_bytes_billed: None
[0m19:12:51.848475 [info ] [MainThread]:   impersonate_service_account: None
[0m19:12:51.849027 [info ] [MainThread]:   job_retry_deadline_seconds: None
[0m19:12:51.849803 [info ] [MainThread]:   job_retries: 1
[0m19:12:51.850231 [info ] [MainThread]:   job_creation_timeout_seconds: None
[0m19:12:51.850866 [info ] [MainThread]:   job_execution_timeout_seconds: 300
[0m19:12:51.851639 [info ] [MainThread]:   timeout_seconds: 300
[0m19:12:51.852565 [info ] [MainThread]:   client_id: None
[0m19:12:51.853731 [info ] [MainThread]:   token_uri: None
[0m19:12:51.854627 [info ] [MainThread]:   compute_region: None
[0m19:12:51.855346 [info ] [MainThread]:   dataproc_cluster_name: None
[0m19:12:51.856502 [info ] [MainThread]:   gcs_bucket: None
[0m19:12:51.857454 [info ] [MainThread]:   dataproc_batch: None
[0m19:12:51.858935 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m19:12:52.172326 [debug] [MainThread]: Acquiring new bigquery connection 'debug'
[0m19:12:52.173705 [debug] [MainThread]: On debug: select 1 as id
[0m19:12:52.174536 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:12:52.175120 [debug] [MainThread]: BigQuery adapter: Got an error when attempting to create a bigquery " "client: ''NoneType' object has no attribute 'keys''
[0m19:12:52.176141 [debug] [MainThread]: BigQuery adapter: Unhandled error while running:
select 1 as id
[0m19:12:52.177106 [debug] [MainThread]: BigQuery adapter: Database Error
  'NoneType' object has no attribute 'keys'
[0m19:12:52.178344 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m19:12:52.179084 [info ] [MainThread]: [31m1 check failed:[0m
[0m19:12:52.180021 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >'NoneType' object has no attribute 'close'

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m19:12:52.181247 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": false, "command_wall_clock_time": 3.6643167, "process_in_blocks": "56", "process_kernel_time": 1.297968, "process_mem_max_rss": "365828", "process_out_blocks": "24", "process_user_time": 4.070901}
[0m19:12:52.182220 [debug] [MainThread]: Command `dbt debug` failed at 19:12:52.182084 after 3.67 seconds
[0m19:12:52.183231 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ab0dd2cd6a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ab0dd8a5340>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ab0bd4541a0>]}
[0m19:12:52.184135 [debug] [MainThread]: Flushing usage events
[0m19:12:53.203879 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:00:52.504601 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f30f00db7a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f30f01bb2c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f30f01b9250>]}


============================== 20:00:52.521312 | ebcf1012-55ef-47b4-81dc-4a5d4f6ecc0a ==============================
[0m20:00:52.521312 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m20:00:52.526768 [debug] [MainThread]: running dbt with arguments {'log_format': 'default', 'write_json': 'True', 'target_path': 'None', 'no_print': 'None', 'quiet': 'False', 'profiles_dir': '/home/ecem/.dbt', 'empty': 'None', 'version_check': 'True', 'send_anonymous_usage_stats': 'True', 'fail_fast': 'False', 'partial_parse': 'True', 'debug': 'False', 'introspect': 'True', 'warn_error': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'use_colors': 'True', 'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'cache_selected_only': 'False', 'use_experimental_parser': 'False', 'static_parser': 'True', 'invocation_command': 'dbt debug'}
[0m20:00:52.596429 [info ] [MainThread]: dbt version: 1.11.0-b3
[0m20:00:52.607410 [info ] [MainThread]: python version: 3.12.3
[0m20:00:52.609625 [info ] [MainThread]: python path: /home/ecem/Desktop/data-pipeline-project/venv/bin/python3
[0m20:00:52.613626 [info ] [MainThread]: os info: Linux-6.14.0-33-generic-x86_64-with-glibc2.39
[0m20:00:56.157285 [info ] [MainThread]: Using profiles dir at /home/ecem/.dbt
[0m20:00:56.160367 [info ] [MainThread]: Using profiles.yml file at /home/ecem/.dbt/profiles.yml
[0m20:00:56.161150 [info ] [MainThread]: Using dbt_project.yml file at /home/ecem/Desktop/data-pipeline-project/dbt/dbt_project.yml
[0m20:00:56.161960 [info ] [MainThread]: adapter type: bigquery
[0m20:00:56.162621 [info ] [MainThread]: adapter version: 1.10.2
[0m20:00:56.433007 [info ] [MainThread]: Configuration:
[0m20:00:56.434118 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m20:00:56.435093 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m20:00:56.435668 [info ] [MainThread]: Required dependencies:
[0m20:00:56.436700 [debug] [MainThread]: Executing "git --help"
[0m20:00:56.441920 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m20:00:56.442690 [debug] [MainThread]: STDERR: "b''"
[0m20:00:56.443514 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m20:00:56.444268 [info ] [MainThread]: Connection:
[0m20:00:56.444956 [info ] [MainThread]:   method: service-account-json
[0m20:00:56.445612 [info ] [MainThread]:   database: data-pipeline-project-474812
[0m20:00:56.446513 [info ] [MainThread]:   execution_project: data-pipeline-project-474812
[0m20:00:56.447335 [info ] [MainThread]:   schema: analytics_staging
[0m20:00:56.448459 [info ] [MainThread]:   location: US
[0m20:00:56.449434 [info ] [MainThread]:   priority: interactive
[0m20:00:56.451658 [info ] [MainThread]:   maximum_bytes_billed: None
[0m20:00:56.453255 [info ] [MainThread]:   impersonate_service_account: None
[0m20:00:56.453823 [info ] [MainThread]:   job_retry_deadline_seconds: None
[0m20:00:56.454473 [info ] [MainThread]:   job_retries: 1
[0m20:00:56.455155 [info ] [MainThread]:   job_creation_timeout_seconds: None
[0m20:00:56.455615 [info ] [MainThread]:   job_execution_timeout_seconds: 300
[0m20:00:56.456216 [info ] [MainThread]:   timeout_seconds: 300
[0m20:00:56.456646 [info ] [MainThread]:   client_id: None
[0m20:00:56.457618 [info ] [MainThread]:   token_uri: None
[0m20:00:56.462819 [info ] [MainThread]:   compute_region: None
[0m20:00:56.464130 [info ] [MainThread]:   dataproc_cluster_name: None
[0m20:00:56.464914 [info ] [MainThread]:   gcs_bucket: None
[0m20:00:56.465619 [info ] [MainThread]:   dataproc_batch: None
[0m20:00:56.466676 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m20:00:56.813858 [debug] [MainThread]: Acquiring new bigquery connection 'debug'
[0m20:00:56.816183 [debug] [MainThread]: On debug: select 1 as id
[0m20:00:56.818130 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:00:56.820499 [debug] [MainThread]: BigQuery adapter: Got an error when attempting to create a bigquery " "client: ''NoneType' object has no attribute 'keys''
[0m20:00:56.822105 [debug] [MainThread]: BigQuery adapter: Unhandled error while running:
select 1 as id
[0m20:00:56.823412 [debug] [MainThread]: BigQuery adapter: Database Error
  'NoneType' object has no attribute 'keys'
[0m20:00:56.824181 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m20:00:56.825605 [info ] [MainThread]: [31m1 check failed:[0m
[0m20:00:56.827379 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >'NoneType' object has no attribute 'close'

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m20:00:56.830829 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": false, "command_wall_clock_time": 4.5310764, "process_in_blocks": "0", "process_kernel_time": 1.155368, "process_mem_max_rss": "365328", "process_out_blocks": "16", "process_user_time": 5.325698}
[0m20:00:56.834833 [debug] [MainThread]: Command `dbt debug` failed at 20:00:56.834293 after 4.54 seconds
[0m20:00:56.836424 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f30f24e6fc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f30d05c10a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f30d07829c0>]}
[0m20:00:56.836992 [debug] [MainThread]: Flushing usage events
[0m20:00:57.848785 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:10:10.955963 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e34d09f3440>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e34d0c509e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e34d064d9a0>]}


============================== 20:10:10.959888 | 1027a6b4-012f-4380-9a09-72eb9c4ca911 ==============================
[0m20:10:10.959888 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m20:10:10.961544 [debug] [MainThread]: running dbt with arguments {'send_anonymous_usage_stats': 'True', 'no_print': 'None', 'fail_fast': 'False', 'printer_width': '80', 'target_path': 'None', 'profiles_dir': '/home/ecem/.dbt', 'indirect_selection': 'eager', 'cache_selected_only': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'introspect': 'True', 'static_parser': 'True', 'use_colors': 'True', 'debug': 'False', 'write_json': 'True', 'log_cache_events': 'False', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'use_experimental_parser': 'False', 'version_check': 'True', 'partial_parse': 'True', 'warn_error': 'None', 'invocation_command': 'dbt debug', 'log_format': 'default', 'quiet': 'False', 'empty': 'None'}
[0m20:10:10.971561 [info ] [MainThread]: dbt version: 1.11.0-b3
[0m20:10:10.974497 [info ] [MainThread]: python version: 3.12.3
[0m20:10:10.975409 [info ] [MainThread]: python path: /home/ecem/Desktop/data-pipeline-project/venv/bin/python3
[0m20:10:10.976485 [info ] [MainThread]: os info: Linux-6.14.0-33-generic-x86_64-with-glibc2.39
[0m20:10:13.454819 [info ] [MainThread]: Using profiles dir at /home/ecem/.dbt
[0m20:10:13.455649 [info ] [MainThread]: Using profiles.yml file at /home/ecem/.dbt/profiles.yml
[0m20:10:13.460733 [info ] [MainThread]: Using dbt_project.yml file at /home/ecem/Desktop/data-pipeline-project/dbt/dbt_project.yml
[0m20:10:13.463395 [info ] [MainThread]: adapter type: bigquery
[0m20:10:13.465428 [info ] [MainThread]: adapter version: 1.10.2
[0m20:10:13.631900 [info ] [MainThread]: Configuration:
[0m20:10:13.633088 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m20:10:13.635296 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m20:10:13.636490 [info ] [MainThread]: Required dependencies:
[0m20:10:13.637712 [debug] [MainThread]: Executing "git --help"
[0m20:10:13.642323 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m20:10:13.643131 [debug] [MainThread]: STDERR: "b''"
[0m20:10:13.645184 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m20:10:13.645712 [info ] [MainThread]: Connection:
[0m20:10:13.646454 [info ] [MainThread]:   method: service-account-json
[0m20:10:13.646949 [info ] [MainThread]:   database: data-pipeline-project-474812
[0m20:10:13.647774 [info ] [MainThread]:   execution_project: data-pipeline-project-474812
[0m20:10:13.648380 [info ] [MainThread]:   schema: analytics_staging
[0m20:10:13.648910 [info ] [MainThread]:   location: US
[0m20:10:13.649578 [info ] [MainThread]:   priority: interactive
[0m20:10:13.650034 [info ] [MainThread]:   maximum_bytes_billed: None
[0m20:10:13.650660 [info ] [MainThread]:   impersonate_service_account: None
[0m20:10:13.651130 [info ] [MainThread]:   job_retry_deadline_seconds: None
[0m20:10:13.651855 [info ] [MainThread]:   job_retries: 1
[0m20:10:13.652621 [info ] [MainThread]:   job_creation_timeout_seconds: None
[0m20:10:13.653499 [info ] [MainThread]:   job_execution_timeout_seconds: 300
[0m20:10:13.654405 [info ] [MainThread]:   timeout_seconds: 300
[0m20:10:13.654928 [info ] [MainThread]:   client_id: None
[0m20:10:13.655649 [info ] [MainThread]:   token_uri: None
[0m20:10:13.656224 [info ] [MainThread]:   compute_region: None
[0m20:10:13.656725 [info ] [MainThread]:   dataproc_cluster_name: None
[0m20:10:13.657362 [info ] [MainThread]:   gcs_bucket: None
[0m20:10:13.657833 [info ] [MainThread]:   dataproc_batch: None
[0m20:10:13.658715 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m20:10:13.974330 [debug] [MainThread]: Acquiring new bigquery connection 'debug'
[0m20:10:13.975008 [debug] [MainThread]: On debug: select 1 as id
[0m20:10:13.976454 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:10:13.977315 [debug] [MainThread]: BigQuery adapter: Got an error when attempting to create a bigquery " "client: ''NoneType' object has no attribute 'keys''
[0m20:10:13.978107 [debug] [MainThread]: BigQuery adapter: Unhandled error while running:
select 1 as id
[0m20:10:13.978558 [debug] [MainThread]: BigQuery adapter: Database Error
  'NoneType' object has no attribute 'keys'
[0m20:10:13.979395 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m20:10:13.979984 [info ] [MainThread]: [31m1 check failed:[0m
[0m20:10:13.980617 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >'NoneType' object has no attribute 'close'

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m20:10:13.982435 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": false, "command_wall_clock_time": 3.204516, "process_in_blocks": "0", "process_kernel_time": 1.35919, "process_mem_max_rss": "365660", "process_out_blocks": "16", "process_user_time": 3.946496}
[0m20:10:13.986042 [debug] [MainThread]: Command `dbt debug` failed at 20:10:13.985436 after 3.21 seconds
[0m20:10:13.987437 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e34cff1fa70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e34b0b55f40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e34b08b2960>]}
[0m20:10:13.988886 [debug] [MainThread]: Flushing usage events
[0m20:10:15.008211 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:24:59.111414 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75484f071490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75484f1ceae0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75484f071400>]}


============================== 20:24:59.126460 | 6cce7f86-e4f6-45b5-a2d1-91f1255363c3 ==============================
[0m20:24:59.126460 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m20:24:59.132532 [debug] [MainThread]: running dbt with arguments {'introspect': 'True', 'static_parser': 'True', 'indirect_selection': 'eager', 'target_path': 'None', 'invocation_command': 'dbt debug', 'quiet': 'False', 'version_check': 'True', 'use_colors': 'True', 'partial_parse': 'True', 'printer_width': '80', 'no_print': 'None', 'use_experimental_parser': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'debug': 'False', 'cache_selected_only': 'False', 'send_anonymous_usage_stats': 'True', 'profiles_dir': '/home/ecem/.dbt', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'empty': 'None', 'log_cache_events': 'False', 'fail_fast': 'False', 'log_format': 'default', 'write_json': 'True', 'warn_error': 'None'}
[0m20:24:59.143528 [info ] [MainThread]: dbt version: 1.11.0-b3
[0m20:24:59.145510 [info ] [MainThread]: python version: 3.12.3
[0m20:24:59.149797 [info ] [MainThread]: python path: /home/ecem/Desktop/data-pipeline-project/venv/bin/python3
[0m20:24:59.151228 [info ] [MainThread]: os info: Linux-6.14.0-33-generic-x86_64-with-glibc2.39
[0m20:25:01.485110 [info ] [MainThread]: Using profiles dir at /home/ecem/.dbt
[0m20:25:01.486140 [info ] [MainThread]: Using profiles.yml file at /home/ecem/.dbt/profiles.yml
[0m20:25:01.487407 [info ] [MainThread]: Using dbt_project.yml file at /home/ecem/Desktop/data-pipeline-project/dbt/dbt_project.yml
[0m20:25:01.488322 [info ] [MainThread]: adapter type: bigquery
[0m20:25:01.488996 [info ] [MainThread]: adapter version: 1.10.2
[0m20:25:01.619369 [info ] [MainThread]: Configuration:
[0m20:25:01.620368 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m20:25:01.621450 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m20:25:01.622155 [info ] [MainThread]: Required dependencies:
[0m20:25:01.623228 [debug] [MainThread]: Executing "git --help"
[0m20:25:01.651302 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m20:25:01.651919 [debug] [MainThread]: STDERR: "b''"
[0m20:25:01.653339 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m20:25:01.657264 [info ] [MainThread]: Connection:
[0m20:25:01.661657 [info ] [MainThread]:   method: service-account-json
[0m20:25:01.666478 [info ] [MainThread]:   database: data-pipeline-project-474812
[0m20:25:01.667137 [info ] [MainThread]:   execution_project: data-pipeline-project-474812
[0m20:25:01.667526 [info ] [MainThread]:   schema: analytics_staging
[0m20:25:01.668338 [info ] [MainThread]:   location: US
[0m20:25:01.669632 [info ] [MainThread]:   priority: interactive
[0m20:25:01.672320 [info ] [MainThread]:   maximum_bytes_billed: None
[0m20:25:01.674894 [info ] [MainThread]:   impersonate_service_account: None
[0m20:25:01.679241 [info ] [MainThread]:   job_retry_deadline_seconds: None
[0m20:25:01.679785 [info ] [MainThread]:   job_retries: 1
[0m20:25:01.680161 [info ] [MainThread]:   job_creation_timeout_seconds: None
[0m20:25:01.680709 [info ] [MainThread]:   job_execution_timeout_seconds: 300
[0m20:25:01.681492 [info ] [MainThread]:   timeout_seconds: 300
[0m20:25:01.682039 [info ] [MainThread]:   client_id: None
[0m20:25:01.682689 [info ] [MainThread]:   token_uri: None
[0m20:25:01.683496 [info ] [MainThread]:   compute_region: None
[0m20:25:01.683939 [info ] [MainThread]:   dataproc_cluster_name: None
[0m20:25:01.684454 [info ] [MainThread]:   gcs_bucket: None
[0m20:25:01.684908 [info ] [MainThread]:   dataproc_batch: None
[0m20:25:01.685660 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m20:25:01.939650 [debug] [MainThread]: Acquiring new bigquery connection 'debug'
[0m20:25:01.940193 [debug] [MainThread]: On debug: select 1 as id
[0m20:25:01.940855 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:25:01.941502 [debug] [MainThread]: BigQuery adapter: Got an error when attempting to create a bigquery " "client: ''NoneType' object has no attribute 'keys''
[0m20:25:01.942091 [debug] [MainThread]: BigQuery adapter: Unhandled error while running:
select 1 as id
[0m20:25:01.942610 [debug] [MainThread]: BigQuery adapter: Database Error
  'NoneType' object has no attribute 'keys'
[0m20:25:01.943181 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m20:25:01.943814 [info ] [MainThread]: [31m1 check failed:[0m
[0m20:25:01.944214 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >'NoneType' object has no attribute 'close'

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m20:25:01.946029 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": false, "command_wall_clock_time": 2.907825, "process_in_blocks": "0", "process_kernel_time": 1.446799, "process_mem_max_rss": "365992", "process_out_blocks": "16", "process_user_time": 3.113894}
[0m20:25:01.947199 [debug] [MainThread]: Command `dbt debug` failed at 20:25:01.947013 after 2.91 seconds
[0m20:25:01.947903 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75484f141850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75482f446150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75484e86e570>]}
[0m20:25:01.948798 [debug] [MainThread]: Flushing usage events
[0m20:25:02.860363 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:28:26.336801 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x791c30c289e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x791c3115e600>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x791c31825430>]}


============================== 20:28:26.342080 | 68f29113-1f43-4941-b938-09a940602227 ==============================
[0m20:28:26.342080 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m20:28:26.350533 [debug] [MainThread]: running dbt with arguments {'target_path': 'None', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt debug', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'log_format': 'default', 'cache_selected_only': 'False', 'empty': 'None', 'send_anonymous_usage_stats': 'True', 'no_print': 'None', 'profiles_dir': '/home/ecem/.dbt', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'debug': 'False', 'use_experimental_parser': 'False', 'write_json': 'True', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'quiet': 'False', 'version_check': 'True', 'printer_width': '80', 'warn_error': 'None', 'use_colors': 'True', 'partial_parse': 'True'}
[0m20:28:26.385701 [info ] [MainThread]: dbt version: 1.11.0-b3
[0m20:28:26.389186 [info ] [MainThread]: python version: 3.12.3
[0m20:28:26.391312 [info ] [MainThread]: python path: /home/ecem/Desktop/data-pipeline-project/venv/bin/python3
[0m20:28:26.395839 [info ] [MainThread]: os info: Linux-6.14.0-33-generic-x86_64-with-glibc2.39
[0m20:28:28.619208 [info ] [MainThread]: Using profiles dir at /home/ecem/.dbt
[0m20:28:28.620580 [info ] [MainThread]: Using profiles.yml file at /home/ecem/.dbt/profiles.yml
[0m20:28:28.622391 [info ] [MainThread]: Using dbt_project.yml file at /home/ecem/Desktop/data-pipeline-project/dbt/dbt_project.yml
[0m20:28:28.624248 [info ] [MainThread]: adapter type: bigquery
[0m20:28:28.624791 [info ] [MainThread]: adapter version: 1.10.2
[0m20:28:28.760693 [info ] [MainThread]: Configuration:
[0m20:28:28.762151 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m20:28:28.763782 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m20:28:28.766323 [info ] [MainThread]: Required dependencies:
[0m20:28:28.767909 [debug] [MainThread]: Executing "git --help"
[0m20:28:28.773248 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m20:28:28.773790 [debug] [MainThread]: STDERR: "b''"
[0m20:28:28.774157 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m20:28:28.775150 [info ] [MainThread]: Connection:
[0m20:28:28.775711 [info ] [MainThread]:   method: service-account
[0m20:28:28.776432 [info ] [MainThread]:   database: data-pipeline-project-474812
[0m20:28:28.777186 [info ] [MainThread]:   execution_project: data-pipeline-project-474812
[0m20:28:28.777770 [info ] [MainThread]:   schema: analytics_staging
[0m20:28:28.778476 [info ] [MainThread]:   location: US
[0m20:28:28.779220 [info ] [MainThread]:   priority: interactive
[0m20:28:28.779704 [info ] [MainThread]:   maximum_bytes_billed: None
[0m20:28:28.782088 [info ] [MainThread]:   impersonate_service_account: None
[0m20:28:28.782529 [info ] [MainThread]:   job_retry_deadline_seconds: None
[0m20:28:28.783319 [info ] [MainThread]:   job_retries: 1
[0m20:28:28.783866 [info ] [MainThread]:   job_creation_timeout_seconds: None
[0m20:28:28.784553 [info ] [MainThread]:   job_execution_timeout_seconds: 300
[0m20:28:28.785275 [info ] [MainThread]:   timeout_seconds: 300
[0m20:28:28.786292 [info ] [MainThread]:   client_id: None
[0m20:28:28.786868 [info ] [MainThread]:   token_uri: None
[0m20:28:28.787519 [info ] [MainThread]:   compute_region: None
[0m20:28:28.787921 [info ] [MainThread]:   dataproc_cluster_name: None
[0m20:28:28.788278 [info ] [MainThread]:   gcs_bucket: None
[0m20:28:28.788618 [info ] [MainThread]:   dataproc_batch: None
[0m20:28:28.789099 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m20:28:29.053487 [debug] [MainThread]: Acquiring new bigquery connection 'debug'
[0m20:28:29.054262 [debug] [MainThread]: On debug: select 1 as id
[0m20:28:29.055972 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:28:31.148136 [debug] [MainThread]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:849fe2e3-1bcf-4fb0-9f06-9f392037eac3&page=queryresults
[0m20:28:31.918238 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m20:28:31.919438 [info ] [MainThread]: [32mAll checks passed![0m
[0m20:28:31.920660 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": true, "command_wall_clock_time": 5.6520734, "process_in_blocks": "0", "process_kernel_time": 1.156262, "process_mem_max_rss": "371152", "process_out_blocks": "16", "process_user_time": 3.492453}
[0m20:28:31.921437 [debug] [MainThread]: Command `dbt debug` succeeded at 20:28:31.921323 after 5.65 seconds
[0m20:28:31.921882 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m20:28:31.922490 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x791c30030560>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x791c0fb6b0e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x791c0f739d00>]}
[0m20:28:31.922989 [debug] [MainThread]: Flushing usage events
[0m20:28:32.906479 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:38:46.535988 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73bb9ce7d4f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73bb9c728650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73bb9bf1d0d0>]}


============================== 14:38:46.563015 | e2e2c7cb-4682-42a6-9130-448406c6aa82 ==============================
[0m14:38:46.563015 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m14:38:46.567910 [debug] [MainThread]: running dbt with arguments {'partial_parse': 'True', 'profiles_dir': '/home/ecem/.dbt', 'empty': 'None', 'target_path': 'None', 'introspect': 'True', 'static_parser': 'True', 'warn_error': 'None', 'send_anonymous_usage_stats': 'True', 'write_json': 'True', 'log_cache_events': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'printer_width': '80', 'invocation_command': 'dbt debug', 'version_check': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'quiet': 'False', 'log_format': 'default', 'use_colors': 'True'}
[0m14:38:46.652244 [info ] [MainThread]: dbt version: 1.11.0-b3
[0m14:38:46.654413 [info ] [MainThread]: python version: 3.12.3
[0m14:38:46.660131 [info ] [MainThread]: python path: /home/ecem/Desktop/data-pipeline-project/venv/bin/python3
[0m14:38:46.662729 [info ] [MainThread]: os info: Linux-6.14.0-33-generic-x86_64-with-glibc2.39
[0m14:38:50.277987 [info ] [MainThread]: Using profiles dir at /home/ecem/.dbt
[0m14:38:50.278898 [info ] [MainThread]: Using profiles.yml file at /home/ecem/.dbt/profiles.yml
[0m14:38:50.279295 [info ] [MainThread]: Using dbt_project.yml file at /home/ecem/Desktop/data-pipeline-project/dbt/dbt_project.yml
[0m14:38:50.279661 [info ] [MainThread]: adapter type: bigquery
[0m14:38:50.280120 [info ] [MainThread]: adapter version: 1.10.2
[0m14:38:50.589600 [info ] [MainThread]: Configuration:
[0m14:38:50.590994 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m14:38:50.592909 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m14:38:50.594897 [info ] [MainThread]: Required dependencies:
[0m14:38:50.595642 [debug] [MainThread]: Executing "git --help"
[0m14:38:50.601602 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m14:38:50.602482 [debug] [MainThread]: STDERR: "b''"
[0m14:38:50.602874 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m14:38:50.603411 [info ] [MainThread]: Connection:
[0m14:38:50.603863 [info ] [MainThread]:   method: service-account
[0m14:38:50.604282 [info ] [MainThread]:   database: data-pipeline-project-474812
[0m14:38:50.605182 [info ] [MainThread]:   execution_project: data-pipeline-project-474812
[0m14:38:50.606399 [info ] [MainThread]:   schema: analytics_staging
[0m14:38:50.606865 [info ] [MainThread]:   location: US
[0m14:38:50.608088 [info ] [MainThread]:   priority: interactive
[0m14:38:50.608584 [info ] [MainThread]:   maximum_bytes_billed: None
[0m14:38:50.609551 [info ] [MainThread]:   impersonate_service_account: None
[0m14:38:50.610698 [info ] [MainThread]:   job_retry_deadline_seconds: None
[0m14:38:50.611324 [info ] [MainThread]:   job_retries: 1
[0m14:38:50.612191 [info ] [MainThread]:   job_creation_timeout_seconds: None
[0m14:38:50.612939 [info ] [MainThread]:   job_execution_timeout_seconds: 300
[0m14:38:50.615110 [info ] [MainThread]:   timeout_seconds: 300
[0m14:38:50.615690 [info ] [MainThread]:   client_id: None
[0m14:38:50.616509 [info ] [MainThread]:   token_uri: None
[0m14:38:50.617342 [info ] [MainThread]:   compute_region: None
[0m14:38:50.617894 [info ] [MainThread]:   dataproc_cluster_name: None
[0m14:38:50.618472 [info ] [MainThread]:   gcs_bucket: None
[0m14:38:50.619065 [info ] [MainThread]:   dataproc_batch: None
[0m14:38:50.619964 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m14:38:50.916876 [debug] [MainThread]: Acquiring new bigquery connection 'debug'
[0m14:38:50.917525 [debug] [MainThread]: On debug: select 1 as id
[0m14:38:50.918055 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:38:52.492775 [debug] [MainThread]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:6a77afa4-e021-4ab8-86a2-ee855347e6cd&page=queryresults
[0m14:38:53.224179 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m14:38:53.224968 [info ] [MainThread]: [32mAll checks passed![0m
[0m14:38:53.228161 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": true, "command_wall_clock_time": 6.7577887, "process_in_blocks": "0", "process_kernel_time": 1.909404, "process_mem_max_rss": "371428", "process_out_blocks": "16", "process_user_time": 4.589613}
[0m14:38:53.229260 [debug] [MainThread]: Command `dbt debug` succeeded at 14:38:53.229129 after 6.76 seconds
[0m14:38:53.229826 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m14:38:53.231273 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73bb9be77440>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73bb7c37ca40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73bb9c729070>]}
[0m14:38:53.232413 [debug] [MainThread]: Flushing usage events
[0m14:38:54.244307 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:39:15.998914 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d0e33b0d4f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d0e3409fb90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d0e33878950>]}


============================== 14:39:16.004189 | fc8c22d6-2c39-4258-92d5-ddb593e515b8 ==============================
[0m14:39:16.004189 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m14:39:16.005514 [debug] [MainThread]: running dbt with arguments {'warn_error': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True', 'fail_fast': 'False', 'empty': 'None', 'no_print': 'None', 'profiles_dir': '/home/ecem/.dbt', 'use_colors': 'True', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'partial_parse': 'True', 'indirect_selection': 'eager', 'printer_width': '80', 'use_experimental_parser': 'False', 'cache_selected_only': 'False', 'target_path': 'None', 'invocation_command': 'dbt source freshness', 'debug': 'False', 'static_parser': 'True', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_cache_events': 'False', 'log_format': 'default', 'write_json': 'True', 'version_check': 'True'}
[0m14:39:18.564086 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'fc8c22d6-2c39-4258-92d5-ddb593e515b8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d0e13c34770>]}
[0m14:39:18.630107 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'fc8c22d6-2c39-4258-92d5-ddb593e515b8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d0e13ed2b40>]}
[0m14:39:18.631320 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m14:39:18.899727 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m14:39:18.901607 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m14:39:18.902360 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'fc8c22d6-2c39-4258-92d5-ddb593e515b8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d0e13f9ddf0>]}
[0m14:39:19.885435 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.data_pipeline_project.marts
- models.data_pipeline_project.staging
- models.data_pipeline_project
[0m14:39:19.896196 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fc8c22d6-2c39-4258-92d5-ddb593e515b8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d0e13dc89b0>]}
[0m14:39:19.978432 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m14:39:19.980694 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m14:39:19.999610 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fc8c22d6-2c39-4258-92d5-ddb593e515b8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d0e331ed9d0>]}
[0m14:39:20.000133 [info ] [MainThread]: Found 508 macros
[0m14:39:20.000668 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fc8c22d6-2c39-4258-92d5-ddb593e515b8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d0e139cc920>]}
[0m14:39:20.002419 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m14:39:20.037855 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m14:39:20.042111 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m14:39:20.047491 [debug] [MainThread]: Wrote artifact FreshnessResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/sources.json
[0m14:39:20.048395 [info ] [MainThread]: Done.
[0m14:39:20.050139 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_success": true, "command_wall_clock_time": 4.115184, "process_in_blocks": "4552", "process_kernel_time": 1.205022, "process_mem_max_rss": "369488", "process_out_blocks": "3040", "process_user_time": 4.434811}
[0m14:39:20.053334 [debug] [MainThread]: Command `dbt source freshness` succeeded at 14:39:20.051599 after 4.12 seconds
[0m14:39:20.054006 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d0e338acfe0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d0e139145f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d0e13d3ac90>]}
[0m14:39:20.072355 [debug] [MainThread]: Flushing usage events
[0m14:39:20.928643 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:44:55.760469 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x766443b2af00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x766442fe9f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76644298a330>]}


============================== 15:44:55.766295 | 376f2c27-41fe-4e72-a961-b255c30c737a ==============================
[0m15:44:55.766295 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m15:44:55.766970 [debug] [MainThread]: running dbt with arguments {'use_colors': 'True', 'static_parser': 'True', 'version_check': 'True', 'warn_error': 'None', 'target_path': 'None', 'send_anonymous_usage_stats': 'True', 'partial_parse': 'True', 'profiles_dir': '/home/ecem/.dbt', 'no_print': 'None', 'log_format': 'default', 'cache_selected_only': 'False', 'log_cache_events': 'False', 'empty': 'None', 'fail_fast': 'False', 'printer_width': '80', 'use_experimental_parser': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'write_json': 'True', 'introspect': 'True', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'quiet': 'False', 'invocation_command': 'dbt source freshness', 'debug': 'False', 'indirect_selection': 'eager'}
[0m15:44:58.734685 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '376f2c27-41fe-4e72-a961-b255c30c737a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7664231c3b90>]}
[0m15:44:58.807072 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '376f2c27-41fe-4e72-a961-b255c30c737a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76642311af90>]}
[0m15:44:58.808096 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m15:44:59.170468 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m15:44:59.794984 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
[0m15:44:59.797836 [debug] [MainThread]: Partial parsing: added file: data_pipeline_project://models/staging/raw_sources.yml
[0m15:45:00.274647 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.data_pipeline_project.staging
- models.data_pipeline_project
- models.data_pipeline_project.marts
[0m15:45:00.286447 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '376f2c27-41fe-4e72-a961-b255c30c737a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76642382d8e0>]}
[0m15:45:00.397832 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m15:45:00.407537 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m15:45:00.493092 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '376f2c27-41fe-4e72-a961-b255c30c737a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x766422a74140>]}
[0m15:45:00.494067 [info ] [MainThread]: Found 10 sources, 508 macros
[0m15:45:00.494819 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '376f2c27-41fe-4e72-a961-b255c30c737a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x766425fced80>]}
[0m15:45:00.498139 [info ] [MainThread]: 
[0m15:45:00.499963 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:45:00.500909 [info ] [MainThread]: 
[0m15:45:00.502026 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m15:45:00.503368 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '376f2c27-41fe-4e72-a961-b255c30c737a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x766422a60d10>]}
[0m15:45:00.504587 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:45:00.518879 [debug] [Thread-1 (]: Began running node source.data_pipeline_project.raw_data.orders
[0m15:45:00.520620 [debug] [Thread-2 (]: Began running node source.data_pipeline_project.raw_data.shipments
[0m15:45:00.519602 [info ] [Thread-1 (]: 1 of 4 START freshness of raw_data.orders ...................................... [RUN]
[0m15:45:00.522934 [debug] [Thread-3 (]: Began running node source.data_pipeline_project.raw_data.subscriptions
[0m15:45:00.524935 [debug] [Thread-1 (]: Acquiring new bigquery connection 'source.data_pipeline_project.raw_data.orders'
[0m15:45:00.522188 [info ] [Thread-2 (]: 2 of 4 START freshness of raw_data.shipments ................................... [RUN]
[0m15:45:00.525715 [debug] [Thread-4 (]: Began running node source.data_pipeline_project.raw_data.users
[0m15:45:00.526883 [info ] [Thread-3 (]: 3 of 4 START freshness of raw_data.subscriptions ............................... [RUN]
[0m15:45:00.527734 [debug] [Thread-1 (]: Began compiling node source.data_pipeline_project.raw_data.orders
[0m15:45:00.529051 [debug] [Thread-2 (]: Acquiring new bigquery connection 'source.data_pipeline_project.raw_data.shipments'
[0m15:45:00.535530 [debug] [Thread-3 (]: Acquiring new bigquery connection 'source.data_pipeline_project.raw_data.subscriptions'
[0m15:45:00.531163 [info ] [Thread-4 (]: 4 of 4 START freshness of raw_data.users ....................................... [RUN]
[0m15:45:00.539545 [debug] [Thread-1 (]: Began executing node source.data_pipeline_project.raw_data.orders
[0m15:45:00.541511 [debug] [Thread-2 (]: Began compiling node source.data_pipeline_project.raw_data.shipments
[0m15:45:00.553978 [debug] [Thread-2 (]: Began executing node source.data_pipeline_project.raw_data.shipments
[0m15:45:00.544943 [debug] [Thread-4 (]: Acquiring new bigquery connection 'source.data_pipeline_project.raw_data.users'
[0m15:45:00.542311 [debug] [Thread-3 (]: Began compiling node source.data_pipeline_project.raw_data.subscriptions
[0m15:45:00.574984 [debug] [Thread-2 (]: On source.data_pipeline_project.raw_data.shipments: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "source.data_pipeline_project.raw_data.shipments"} */
select
      max(createdAt) as max_loaded_at,
      current_timestamp() as snapshotted_at
    from `data-pipeline-project-474812`.`raw_data`.`shipments`
    
  
[0m15:45:00.576957 [debug] [Thread-4 (]: Began compiling node source.data_pipeline_project.raw_data.users
[0m15:45:00.578601 [debug] [Thread-1 (]: On source.data_pipeline_project.raw_data.orders: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "source.data_pipeline_project.raw_data.orders"} */
select
      max(createdAt) as max_loaded_at,
      current_timestamp() as snapshotted_at
    from `data-pipeline-project-474812`.`raw_data`.`orders`
    
  
[0m15:45:00.581638 [debug] [Thread-3 (]: Began executing node source.data_pipeline_project.raw_data.subscriptions
[0m15:45:00.583287 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m15:45:00.584250 [debug] [Thread-4 (]: Began executing node source.data_pipeline_project.raw_data.users
[0m15:45:00.585248 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m15:45:00.588325 [debug] [Thread-3 (]: On source.data_pipeline_project.raw_data.subscriptions: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "source.data_pipeline_project.raw_data.subscriptions"} */
select
      max(createdAt) as max_loaded_at,
      current_timestamp() as snapshotted_at
    from `data-pipeline-project-474812`.`raw_data`.`subscriptions`
    
  
[0m15:45:00.650856 [debug] [Thread-4 (]: On source.data_pipeline_project.raw_data.users: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "source.data_pipeline_project.raw_data.users"} */
select
      max(createdAt) as max_loaded_at,
      current_timestamp() as snapshotted_at
    from `data-pipeline-project-474812`.`raw_data`.`users`
    
  
[0m15:45:00.703290 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m15:45:00.704463 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m15:45:02.085275 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:80e0770a-14e7-4eab-a0e2-26dcf72e5ca9&page=queryresults
[0m15:45:02.090740 [error] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:80e0770a-14e7-4eab-a0e2-26dcf72e5ca9&page=queryresults
[0m15:45:02.099954 [debug] [Thread-2 (]: BigQuery adapter: Unhandled error while running:
macro collect_freshness
[0m15:45:02.101447 [debug] [Thread-2 (]: BigQuery adapter: Database Error
  Unrecognized name: createdAt at [3:11]
[0m15:45:02.168266 [debug] [Thread-2 (]: Database Error in source shipments (models/staging/raw_sources.yml)
  Unrecognized name: createdAt at [3:11]
[0m15:45:02.169341 [error] [Thread-2 (]: 2 of 4 ERROR freshness of raw_data.shipments ................................... [[31mERROR[0m in 1.64s]
[0m15:45:02.172148 [debug] [Thread-2 (]: Finished running node source.data_pipeline_project.raw_data.shipments
[0m15:45:02.354384 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:cebb10e1-dba7-44ea-84eb-9ce1a677a2c8&page=queryresults
[0m15:45:02.382832 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:51f7f331-0a98-4704-a012-39c383a78450&page=queryresults
[0m15:45:02.461325 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:aba22851-b59d-4057-a570-7e1d37fb9985&page=queryresults
[0m15:45:03.367647 [error] [Thread-4 (]: 4 of 4 ERROR STALE freshness of raw_data.users ................................. [[31mERROR STALE[0m in 2.82s]
[0m15:45:03.379403 [debug] [Thread-4 (]: Finished running node source.data_pipeline_project.raw_data.users
[0m15:45:03.388167 [debug] [Thread-7 (]: Marking all children of 'source.data_pipeline_project.raw_data.users' to be skipped because of status 'error'. 
[0m15:45:03.400957 [error] [Thread-1 (]: 1 of 4 ERROR STALE freshness of raw_data.orders ................................ [[31mERROR STALE[0m in 2.88s]
[0m15:45:03.404319 [debug] [Thread-1 (]: Finished running node source.data_pipeline_project.raw_data.orders
[0m15:45:03.406375 [debug] [Thread-7 (]: Marking all children of 'source.data_pipeline_project.raw_data.orders' to be skipped because of status 'error'. 
[0m15:45:03.416729 [error] [Thread-3 (]: 3 of 4 ERROR STALE freshness of raw_data.subscriptions ......................... [[31mERROR STALE[0m in 2.88s]
[0m15:45:03.418746 [debug] [Thread-3 (]: Finished running node source.data_pipeline_project.raw_data.subscriptions
[0m15:45:03.420072 [debug] [Thread-7 (]: Marking all children of 'source.data_pipeline_project.raw_data.subscriptions' to be skipped because of status 'error'. 
[0m15:45:03.424594 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:45:03.427939 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:45:03.428457 [debug] [MainThread]: Connection 'source.data_pipeline_project.raw_data.orders' was properly closed.
[0m15:45:03.431736 [debug] [MainThread]: Connection 'source.data_pipeline_project.raw_data.shipments' was properly closed.
[0m15:45:03.433040 [debug] [MainThread]: Connection 'source.data_pipeline_project.raw_data.subscriptions' was properly closed.
[0m15:45:03.434034 [debug] [MainThread]: Connection 'source.data_pipeline_project.raw_data.users' was properly closed.
[0m15:45:03.435401 [info ] [MainThread]: 
[0m15:45:03.436555 [info ] [MainThread]: Finished running 4 sources in 0 hours 0 minutes and 2.93 seconds (2.93s).
[0m15:45:03.483980 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m15:45:03.491123 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m15:45:03.507892 [debug] [MainThread]: Wrote artifact FreshnessResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/sources.json
[0m15:45:03.508916 [info ] [MainThread]: 
[0m15:45:03.509625 [error] [MainThread]:   Database Error in source shipments (models/staging/raw_sources.yml)
  Unrecognized name: createdAt at [3:11]
[0m15:45:03.510587 [info ] [MainThread]: 
[0m15:45:03.511306 [error] [MainThread]: [31mFailure in source users (models/staging/raw_sources.yml)[0m
[0m15:45:03.511980 [error] [MainThread]:   Status: error
[0m15:45:03.512449 [info ] [MainThread]: 
[0m15:45:03.512959 [error] [MainThread]: [31mFailure in source orders (models/staging/raw_sources.yml)[0m
[0m15:45:03.513481 [error] [MainThread]:   Status: error
[0m15:45:03.515258 [info ] [MainThread]: 
[0m15:45:03.517205 [error] [MainThread]: [31mFailure in source subscriptions (models/staging/raw_sources.yml)[0m
[0m15:45:03.517902 [error] [MainThread]:   Status: error
[0m15:45:03.518897 [info ] [MainThread]: Done.
[0m15:45:03.520545 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_success": false, "command_wall_clock_time": 7.8328643, "process_in_blocks": "80", "process_kernel_time": 1.481895, "process_mem_max_rss": "386924", "process_out_blocks": "3160", "process_user_time": 6.171002}
[0m15:45:03.521407 [debug] [MainThread]: Command `dbt source freshness` failed at 15:45:03.521290 after 7.83 seconds
[0m15:45:03.521924 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x766442c3f4a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x766422bd9850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x766422a361e0>]}
[0m15:45:03.522424 [debug] [MainThread]: Flushing usage events
[0m15:45:04.563501 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:48:10.107525 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7552547418e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7552545f5ee0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x755253e36120>]}


============================== 15:48:10.112124 | 365d4d43-16ed-4b21-b850-d50135170a89 ==============================
[0m15:48:10.112124 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m15:48:10.114589 [debug] [MainThread]: running dbt with arguments {'write_json': 'True', 'log_format': 'default', 'use_colors': 'True', 'invocation_command': 'dbt source freshness', 'no_print': 'None', 'fail_fast': 'False', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'printer_width': '80', 'indirect_selection': 'eager', 'version_check': 'True', 'send_anonymous_usage_stats': 'True', 'cache_selected_only': 'False', 'target_path': 'None', 'warn_error': 'None', 'empty': 'None', 'log_cache_events': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'profiles_dir': '/home/ecem/.dbt', 'debug': 'False', 'static_parser': 'True', 'partial_parse': 'True', 'introspect': 'True', 'use_experimental_parser': 'False', 'quiet': 'False'}
[0m15:48:13.014803 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '365d4d43-16ed-4b21-b850-d50135170a89', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75525369fec0>]}
[0m15:48:13.078623 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '365d4d43-16ed-4b21-b850-d50135170a89', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7552342fa1e0>]}
[0m15:48:13.079517 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m15:48:13.417972 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m15:48:13.640064 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:48:13.641310 [debug] [MainThread]: Partial parsing: updated file: data_pipeline_project://models/staging/raw_sources.yml
[0m15:48:13.804280 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.data_pipeline_project
- models.data_pipeline_project.staging
- models.data_pipeline_project.marts
[0m15:48:13.823143 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '365d4d43-16ed-4b21-b850-d50135170a89', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x755234133c20>]}
[0m15:48:13.893294 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m15:48:13.900700 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m15:48:13.931293 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '365d4d43-16ed-4b21-b850-d50135170a89', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x755233b89d60>]}
[0m15:48:13.934087 [info ] [MainThread]: Found 10 sources, 508 macros
[0m15:48:13.936315 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '365d4d43-16ed-4b21-b850-d50135170a89', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x755255da2000>]}
[0m15:48:13.944581 [info ] [MainThread]: 
[0m15:48:13.946033 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:48:13.947172 [info ] [MainThread]: 
[0m15:48:13.948154 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m15:48:13.951216 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '365d4d43-16ed-4b21-b850-d50135170a89', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x755233be2990>]}
[0m15:48:13.952193 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:48:13.973703 [debug] [Thread-1 (]: Began running node source.data_pipeline_project.raw_data.orders
[0m15:48:13.974361 [debug] [Thread-2 (]: Began running node source.data_pipeline_project.raw_data.shipments
[0m15:48:13.976969 [info ] [Thread-1 (]: 1 of 4 START freshness of raw_data.orders ...................................... [RUN]
[0m15:48:13.976033 [debug] [Thread-4 (]: Began running node source.data_pipeline_project.raw_data.users
[0m15:48:13.975016 [debug] [Thread-3 (]: Began running node source.data_pipeline_project.raw_data.subscriptions
[0m15:48:13.987905 [debug] [Thread-1 (]: Acquiring new bigquery connection 'source.data_pipeline_project.raw_data.orders'
[0m15:48:13.991186 [debug] [Thread-1 (]: Began compiling node source.data_pipeline_project.raw_data.orders
[0m15:48:13.991895 [debug] [Thread-1 (]: Began executing node source.data_pipeline_project.raw_data.orders
[0m15:48:13.986808 [info ] [Thread-2 (]: 2 of 4 START freshness of raw_data.shipments ................................... [RUN]
[0m15:48:14.000023 [debug] [Thread-2 (]: Acquiring new bigquery connection 'source.data_pipeline_project.raw_data.shipments'
[0m15:48:13.989503 [info ] [Thread-4 (]: 4 of 4 START freshness of raw_data.users ....................................... [RUN]
[0m15:48:14.013346 [debug] [Thread-1 (]: On source.data_pipeline_project.raw_data.orders: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "source.data_pipeline_project.raw_data.orders"} */
select
      max(createdAt) as max_loaded_at,
      current_timestamp() as snapshotted_at
    from `data-pipeline-project-474812`.`raw_data`.`orders`
    
  
[0m15:48:13.990392 [info ] [Thread-3 (]: 3 of 4 START freshness of raw_data.subscriptions ............................... [RUN]
[0m15:48:14.017638 [debug] [Thread-2 (]: Began compiling node source.data_pipeline_project.raw_data.shipments
[0m15:48:14.020086 [debug] [Thread-4 (]: Acquiring new bigquery connection 'source.data_pipeline_project.raw_data.users'
[0m15:48:14.022982 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m15:48:14.024242 [debug] [Thread-3 (]: Acquiring new bigquery connection 'source.data_pipeline_project.raw_data.subscriptions'
[0m15:48:14.077400 [debug] [Thread-3 (]: Began compiling node source.data_pipeline_project.raw_data.subscriptions
[0m15:48:14.025818 [debug] [Thread-4 (]: Began compiling node source.data_pipeline_project.raw_data.users
[0m15:48:14.025174 [debug] [Thread-2 (]: Began executing node source.data_pipeline_project.raw_data.shipments
[0m15:48:14.078802 [debug] [Thread-3 (]: Began executing node source.data_pipeline_project.raw_data.subscriptions
[0m15:48:14.079982 [debug] [Thread-4 (]: Began executing node source.data_pipeline_project.raw_data.users
[0m15:48:14.083199 [debug] [Thread-2 (]: On source.data_pipeline_project.raw_data.shipments: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "source.data_pipeline_project.raw_data.shipments"} */
select
      max(collectDate) as max_loaded_at,
      current_timestamp() as snapshotted_at
    from `data-pipeline-project-474812`.`raw_data`.`shipments`
    
  
[0m15:48:14.087374 [debug] [Thread-3 (]: On source.data_pipeline_project.raw_data.subscriptions: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "source.data_pipeline_project.raw_data.subscriptions"} */
select
      max(createdAt) as max_loaded_at,
      current_timestamp() as snapshotted_at
    from `data-pipeline-project-474812`.`raw_data`.`subscriptions`
    
  
[0m15:48:14.094934 [debug] [Thread-4 (]: On source.data_pipeline_project.raw_data.users: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "source.data_pipeline_project.raw_data.users"} */
select
      max(createdAt) as max_loaded_at,
      current_timestamp() as snapshotted_at
    from `data-pipeline-project-474812`.`raw_data`.`users`
    
  
[0m15:48:14.098160 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m15:48:14.097226 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m15:48:14.096405 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m15:48:15.686321 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:a0a1e47c-a563-4004-852d-b67eebebb733&page=queryresults
[0m15:48:15.713227 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:9d0322ae-2a2a-4f06-b14d-2063e1fb5cd0&page=queryresults
[0m15:48:16.078882 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:f384e412-1c6a-487b-b206-096bf743bc25&page=queryresults
[0m15:48:16.081617 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:2e9e9846-32a6-45cd-a298-137e3925901e&page=queryresults
[0m15:48:16.807120 [error] [Thread-3 (]: 3 of 4 ERROR STALE freshness of raw_data.subscriptions ......................... [[31mERROR STALE[0m in 2.78s]
[0m15:48:16.815188 [debug] [Thread-3 (]: Finished running node source.data_pipeline_project.raw_data.subscriptions
[0m15:48:16.818220 [debug] [Thread-7 (]: Marking all children of 'source.data_pipeline_project.raw_data.subscriptions' to be skipped because of status 'error'. 
[0m15:48:16.797212 [error] [Thread-4 (]: 4 of 4 ERROR STALE freshness of raw_data.users ................................. [[31mERROR STALE[0m in 2.78s]
[0m15:48:16.822170 [debug] [Thread-4 (]: Finished running node source.data_pipeline_project.raw_data.users
[0m15:48:16.824890 [debug] [Thread-7 (]: Marking all children of 'source.data_pipeline_project.raw_data.users' to be skipped because of status 'error'. 
[0m15:48:17.019889 [error] [Thread-2 (]: 2 of 4 ERROR STALE freshness of raw_data.shipments ............................. [[31mERROR STALE[0m in 3.02s]
[0m15:48:17.023664 [debug] [Thread-2 (]: Finished running node source.data_pipeline_project.raw_data.shipments
[0m15:48:17.025316 [debug] [Thread-7 (]: Marking all children of 'source.data_pipeline_project.raw_data.shipments' to be skipped because of status 'error'. 
[0m15:48:17.127074 [error] [Thread-1 (]: 1 of 4 ERROR STALE freshness of raw_data.orders ................................ [[31mERROR STALE[0m in 3.14s]
[0m15:48:17.128580 [debug] [Thread-1 (]: Finished running node source.data_pipeline_project.raw_data.orders
[0m15:48:17.129557 [debug] [Thread-7 (]: Marking all children of 'source.data_pipeline_project.raw_data.orders' to be skipped because of status 'error'. 
[0m15:48:17.136011 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:48:17.140083 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:48:17.140609 [debug] [MainThread]: Connection 'source.data_pipeline_project.raw_data.orders' was properly closed.
[0m15:48:17.141164 [debug] [MainThread]: Connection 'source.data_pipeline_project.raw_data.shipments' was properly closed.
[0m15:48:17.141692 [debug] [MainThread]: Connection 'source.data_pipeline_project.raw_data.users' was properly closed.
[0m15:48:17.142284 [debug] [MainThread]: Connection 'source.data_pipeline_project.raw_data.subscriptions' was properly closed.
[0m15:48:17.142957 [info ] [MainThread]: 
[0m15:48:17.144319 [info ] [MainThread]: Finished running 4 sources in 0 hours 0 minutes and 3.19 seconds (3.19s).
[0m15:48:17.190125 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m15:48:17.197881 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m15:48:17.210433 [debug] [MainThread]: Wrote artifact FreshnessResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/sources.json
[0m15:48:17.211899 [info ] [MainThread]: 
[0m15:48:17.214651 [error] [MainThread]: [31mFailure in source subscriptions (models/staging/raw_sources.yml)[0m
[0m15:48:17.216022 [error] [MainThread]:   Status: error
[0m15:48:17.217996 [info ] [MainThread]: 
[0m15:48:17.219305 [error] [MainThread]: [31mFailure in source users (models/staging/raw_sources.yml)[0m
[0m15:48:17.221373 [error] [MainThread]:   Status: error
[0m15:48:17.222367 [info ] [MainThread]: 
[0m15:48:17.223931 [error] [MainThread]: [31mFailure in source shipments (models/staging/raw_sources.yml)[0m
[0m15:48:17.225199 [error] [MainThread]:   Status: error
[0m15:48:17.226702 [info ] [MainThread]: 
[0m15:48:17.228252 [error] [MainThread]: [31mFailure in source orders (models/staging/raw_sources.yml)[0m
[0m15:48:17.229376 [error] [MainThread]:   Status: error
[0m15:48:17.232921 [info ] [MainThread]: Done.
[0m15:48:17.234188 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_success": false, "command_wall_clock_time": 7.1812153, "process_in_blocks": "0", "process_kernel_time": 2.09747, "process_mem_max_rss": "386560", "process_out_blocks": "3168", "process_user_time": 4.77487}
[0m15:48:17.235879 [debug] [MainThread]: Command `dbt source freshness` failed at 15:48:17.235706 after 7.18 seconds
[0m15:48:17.237103 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x755253ebfb00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x755233cf2720>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x755213d0c2f0>]}
[0m15:48:17.238004 [debug] [MainThread]: Flushing usage events
[0m15:48:18.289379 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:54:44.539133 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72ddb1970860>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72ddb0c6be00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72ddb235e330>]}


============================== 15:54:44.550936 | b2921fe2-d4e2-4c8c-b192-9fc4c672a647 ==============================
[0m15:54:44.550936 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m15:54:44.553305 [debug] [MainThread]: running dbt with arguments {'log_cache_events': 'False', 'partial_parse': 'True', 'profiles_dir': '/home/ecem/.dbt', 'no_print': 'None', 'static_parser': 'True', 'indirect_selection': 'eager', 'printer_width': '80', 'version_check': 'True', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'write_json': 'True', 'use_colors': 'True', 'fail_fast': 'False', 'warn_error': 'None', 'introspect': 'True', 'cache_selected_only': 'False', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_experimental_parser': 'False', 'quiet': 'False', 'log_format': 'default', 'debug': 'False', 'send_anonymous_usage_stats': 'True', 'empty': 'None', 'invocation_command': 'dbt source freshness'}
[0m15:54:46.903033 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b2921fe2-d4e2-4c8c-b192-9fc4c672a647', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72ddb2223a40>]}
[0m15:54:46.986614 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b2921fe2-d4e2-4c8c-b192-9fc4c672a647', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72dd93151580>]}
[0m15:54:46.992058 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m15:54:47.278468 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m15:54:47.512159 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:54:47.512951 [debug] [MainThread]: Partial parsing: updated file: data_pipeline_project://models/staging/raw_sources.yml
[0m15:54:47.593575 [error] [MainThread]: Encountered an error:
Parsing Error
  Invalid sources config given in models/staging/raw_sources.yml @ sources: {'name': 'raw_data', 'description': 'Source data from the ingestion layer (raw CSV files)', 'database': 'data-pipeline-project-474812', 'schema': 'raw_data', 'tables': [{'name': 'users', 'description': 'Raw users table', 'loaded_at_field': 'createdAt', 'freshness': {'warn_after': {'count': 90, 'period': 'days'}, 'error_after': {'count': 180, 'period': 'days'}}, 'loaded_at_field_present': True}, {'name': 'orders', 'description': 'Raw orders table', 'loaded_at_field': 'createdAt', 'freshness': {'warn_after': {'count': 90, 'period': 'days'}, 'error_after': {'count': 180, 'period': 'days'}}, 'loaded_at_field_present': True}, {'name': 'subscriptions', 'description': 'Raw subscriptions table', 'loaded_at_field': 'createdAt', 'freshness': {'warn_after': {'count': 90, 'period': 'days'}, 'error_after': {'count': 180, 'period': 'days'}}, 'loaded_at_field_present': True}, {'name': 'shipments', 'description': 'Raw shipments table', 'loaded_at_field': 'collectDate', 'freshness': {'warn_after': {'count': 90, 'period': 'days'}, 'error_after': {'count': 180, 'period': 'days'}}, 'loaded_at_field_present': True}, {'name': 'marketing_spend', 'description': 'Raw marketing spend data (wide format - requires unpivoting)'}, {'name': 'addresses', 'description': 'Raw addresses table'}, {'name': 'countries', 'description': 'Raw countries reference table'}, {'name': 'states', 'description': 'Raw states reference table'}, {'name': 'cities', 'description': 'Raw cities reference table'}, {'name': 'neighborhoods', 'description': 'Raw neighborhoods reference table'}]} - at path ['tables'][0]['freshness']: {'warn_after': {'count': 90, 'period': 'days'}, 'error_after': {'count': 180, 'period': 'days'}} is not valid under any of the given schemas
[0m15:54:47.595111 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_success": false, "command_wall_clock_time": 3.124485, "process_in_blocks": "0", "process_kernel_time": 1.128413, "process_mem_max_rss": "369948", "process_out_blocks": "16", "process_user_time": 3.662022}
[0m15:54:47.595718 [debug] [MainThread]: Command `dbt source freshness` failed at 15:54:47.595615 after 3.13 seconds
[0m15:54:47.596191 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72dd8ffc8920>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72ddafa1fd40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72ddafa1f800>]}
[0m15:54:47.596657 [debug] [MainThread]: Flushing usage events
[0m15:54:48.644467 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:57:50.646706 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73da896f0ec0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73da8b0af050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73da88f9f6e0>]}


============================== 15:57:50.650291 | 25e9ca6f-f268-45ef-a798-593845789b81 ==============================
[0m15:57:50.650291 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m15:57:50.651183 [debug] [MainThread]: running dbt with arguments {'log_format': 'default', 'invocation_command': 'dbt source freshness', 'use_experimental_parser': 'False', 'empty': 'None', 'fail_fast': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'target_path': 'None', 'cache_selected_only': 'False', 'introspect': 'True', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'warn_error': 'None', 'log_cache_events': 'False', 'indirect_selection': 'eager', 'write_json': 'True', 'quiet': 'False', 'debug': 'False', 'version_check': 'True', 'static_parser': 'True', 'use_colors': 'True', 'profiles_dir': '/home/ecem/.dbt'}
[0m15:57:52.989586 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '25e9ca6f-f268-45ef-a798-593845789b81', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73da693f9ac0>]}
[0m15:57:53.065331 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '25e9ca6f-f268-45ef-a798-593845789b81', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73da69f29520>]}
[0m15:57:53.066557 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m15:57:53.313378 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m15:57:53.546593 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:57:53.548415 [debug] [MainThread]: Partial parsing: updated file: data_pipeline_project://models/staging/raw_sources.yml
[0m15:57:53.727880 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.data_pipeline_project.marts
- models.data_pipeline_project.staging
- models.data_pipeline_project
[0m15:57:53.739215 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '25e9ca6f-f268-45ef-a798-593845789b81', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73da68aacaa0>]}
[0m15:57:53.801322 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m15:57:53.805531 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m15:57:53.824142 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '25e9ca6f-f268-45ef-a798-593845789b81', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73da68b46450>]}
[0m15:57:53.825068 [info ] [MainThread]: Found 10 sources, 508 macros
[0m15:57:53.826217 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '25e9ca6f-f268-45ef-a798-593845789b81', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73da68eaf890>]}
[0m15:57:53.828667 [info ] [MainThread]: 
[0m15:57:53.829561 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:57:53.832068 [info ] [MainThread]: 
[0m15:57:53.833137 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m15:57:53.834193 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '25e9ca6f-f268-45ef-a798-593845789b81', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73da69228b30>]}
[0m15:57:53.835963 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:57:53.853536 [debug] [Thread-1 (]: Began running node source.data_pipeline_project.raw_data.orders
[0m15:57:53.856167 [debug] [Thread-2 (]: Began running node source.data_pipeline_project.raw_data.shipments
[0m15:57:53.859524 [debug] [Thread-3 (]: Began running node source.data_pipeline_project.raw_data.subscriptions
[0m15:57:53.863315 [debug] [Thread-4 (]: Began running node source.data_pipeline_project.raw_data.users
[0m15:57:53.862594 [info ] [Thread-2 (]: 2 of 4 START freshness of raw_data.shipments ................................... [RUN]
[0m15:57:53.867446 [debug] [Thread-2 (]: Acquiring new bigquery connection 'source.data_pipeline_project.raw_data.shipments'
[0m15:57:53.867921 [debug] [Thread-2 (]: Began compiling node source.data_pipeline_project.raw_data.shipments
[0m15:57:53.868432 [debug] [Thread-2 (]: Began executing node source.data_pipeline_project.raw_data.shipments
[0m15:57:53.862023 [info ] [Thread-1 (]: 1 of 4 START freshness of raw_data.orders ...................................... [RUN]
[0m15:57:53.880080 [debug] [Thread-1 (]: Acquiring new bigquery connection 'source.data_pipeline_project.raw_data.orders'
[0m15:57:53.880922 [debug] [Thread-1 (]: Began compiling node source.data_pipeline_project.raw_data.orders
[0m15:57:53.881455 [debug] [Thread-1 (]: Began executing node source.data_pipeline_project.raw_data.orders
[0m15:57:53.864925 [info ] [Thread-3 (]: 3 of 4 START freshness of raw_data.subscriptions ............................... [RUN]
[0m15:57:53.907660 [debug] [Thread-3 (]: Acquiring new bigquery connection 'source.data_pipeline_project.raw_data.subscriptions'
[0m15:57:53.923623 [debug] [Thread-3 (]: Began compiling node source.data_pipeline_project.raw_data.subscriptions
[0m15:57:53.866117 [info ] [Thread-4 (]: 4 of 4 START freshness of raw_data.users ....................................... [RUN]
[0m15:57:53.931435 [debug] [Thread-2 (]: On source.data_pipeline_project.raw_data.shipments: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "source.data_pipeline_project.raw_data.shipments"} */
select
      max(collectDate) as max_loaded_at,
      current_timestamp() as snapshotted_at
    from `data-pipeline-project-474812`.`raw_data`.`shipments`
    
  
[0m15:57:53.932401 [debug] [Thread-1 (]: On source.data_pipeline_project.raw_data.orders: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "source.data_pipeline_project.raw_data.orders"} */
select
      max(createdAt) as max_loaded_at,
      current_timestamp() as snapshotted_at
    from `data-pipeline-project-474812`.`raw_data`.`orders`
    
  
[0m15:57:53.934580 [debug] [Thread-4 (]: Acquiring new bigquery connection 'source.data_pipeline_project.raw_data.users'
[0m15:57:53.933391 [debug] [Thread-3 (]: Began executing node source.data_pipeline_project.raw_data.subscriptions
[0m15:57:53.935591 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m15:57:53.936533 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m15:57:53.937742 [debug] [Thread-4 (]: Began compiling node source.data_pipeline_project.raw_data.users
[0m15:57:54.070147 [debug] [Thread-4 (]: Began executing node source.data_pipeline_project.raw_data.users
[0m15:57:53.954430 [debug] [Thread-3 (]: On source.data_pipeline_project.raw_data.subscriptions: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "source.data_pipeline_project.raw_data.subscriptions"} */
select
      max(createdAt) as max_loaded_at,
      current_timestamp() as snapshotted_at
    from `data-pipeline-project-474812`.`raw_data`.`subscriptions`
    
  
[0m15:57:54.075525 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m15:57:54.074630 [debug] [Thread-4 (]: On source.data_pipeline_project.raw_data.users: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "source.data_pipeline_project.raw_data.users"} */
select
      max(createdAt) as max_loaded_at,
      current_timestamp() as snapshotted_at
    from `data-pipeline-project-474812`.`raw_data`.`users`
    
  
[0m15:57:54.078792 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m15:57:55.581223 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:c388f5d4-8ab9-43ba-a71a-f39e0b284c93&page=queryresults
[0m15:57:55.598443 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:77c88daa-182e-4d70-889a-c7cc86766990&page=queryresults
[0m15:57:55.700502 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:3c83124e-c042-4fc7-8466-082db1bacab9&page=queryresults
[0m15:57:55.792936 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:5e88d041-b426-4417-a5ef-12eac2d23579&page=queryresults
[0m15:57:56.601520 [info ] [Thread-3 (]: 3 of 4 PASS freshness of raw_data.subscriptions ................................ [[32mPASS[0m in 2.69s]
[0m15:57:56.606391 [debug] [Thread-3 (]: Finished running node source.data_pipeline_project.raw_data.subscriptions
[0m15:57:56.651879 [info ] [Thread-4 (]: 4 of 4 PASS freshness of raw_data.users ........................................ [[32mPASS[0m in 2.72s]
[0m15:57:56.652821 [debug] [Thread-4 (]: Finished running node source.data_pipeline_project.raw_data.users
[0m15:57:56.690980 [info ] [Thread-1 (]: 1 of 4 PASS freshness of raw_data.orders ....................................... [[32mPASS[0m in 2.81s]
[0m15:57:56.691992 [debug] [Thread-1 (]: Finished running node source.data_pipeline_project.raw_data.orders
[0m15:57:56.967052 [info ] [Thread-2 (]: 2 of 4 PASS freshness of raw_data.shipments .................................... [[32mPASS[0m in 3.10s]
[0m15:57:56.975592 [debug] [Thread-2 (]: Finished running node source.data_pipeline_project.raw_data.shipments
[0m15:57:57.007944 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:57:57.011039 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:57:57.012002 [debug] [MainThread]: Connection 'source.data_pipeline_project.raw_data.shipments' was properly closed.
[0m15:57:57.012356 [debug] [MainThread]: Connection 'source.data_pipeline_project.raw_data.orders' was properly closed.
[0m15:57:57.012714 [debug] [MainThread]: Connection 'source.data_pipeline_project.raw_data.subscriptions' was properly closed.
[0m15:57:57.013058 [debug] [MainThread]: Connection 'source.data_pipeline_project.raw_data.users' was properly closed.
[0m15:57:57.013463 [info ] [MainThread]: 
[0m15:57:57.013894 [info ] [MainThread]: Finished running 4 sources in 0 hours 0 minutes and 3.18 seconds (3.18s).
[0m15:57:57.044003 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m15:57:57.046108 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m15:57:57.052619 [debug] [MainThread]: Wrote artifact FreshnessResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/sources.json
[0m15:57:57.053490 [info ] [MainThread]: Done.
[0m15:57:57.055473 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_success": true, "command_wall_clock_time": 6.4735827, "process_in_blocks": "0", "process_kernel_time": 1.246292, "process_mem_max_rss": "386352", "process_out_blocks": "3168", "process_user_time": 5.226071}
[0m15:57:57.056293 [debug] [MainThread]: Command `dbt source freshness` succeeded at 15:57:57.056183 after 6.47 seconds
[0m15:57:57.056789 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73da68c939e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73da88d91070>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73da88d91c70>]}
[0m15:57:57.057521 [debug] [MainThread]: Flushing usage events
[0m15:57:58.068882 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:58:48.112416 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d7541f1e570>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d7541c6d5e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d75423ccb00>]}


============================== 15:58:48.120689 | e9c10ce7-d66f-40a4-8c30-085cfae64ceb ==============================
[0m15:58:48.120689 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m15:58:48.122274 [debug] [MainThread]: running dbt with arguments {'write_json': 'True', 'partial_parse': 'True', 'warn_error': 'None', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'introspect': 'True', 'printer_width': '80', 'version_check': 'True', 'debug': 'False', 'quiet': 'False', 'log_cache_events': 'False', 'cache_selected_only': 'False', 'use_colors': 'True', 'empty': 'None', 'fail_fast': 'False', 'profiles_dir': '/home/ecem/.dbt', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True', 'log_format': 'default', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'invocation_command': 'dbt source freshness', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'no_print': 'None'}
[0m15:58:51.169067 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e9c10ce7-d66f-40a4-8c30-085cfae64ceb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d7521e18440>]}
[0m15:58:51.235067 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e9c10ce7-d66f-40a4-8c30-085cfae64ceb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d75423f6480>]}
[0m15:58:51.237761 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m15:58:51.569170 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m15:58:51.791474 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:58:51.793069 [debug] [MainThread]: Partial parsing: updated file: data_pipeline_project://models/staging/raw_sources.yml
[0m15:58:52.222215 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.data_pipeline_project
- models.data_pipeline_project.staging
- models.data_pipeline_project.marts
[0m15:58:52.252949 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e9c10ce7-d66f-40a4-8c30-085cfae64ceb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d7521e1a5a0>]}
[0m15:58:52.414946 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m15:58:52.429224 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m15:58:52.478568 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e9c10ce7-d66f-40a4-8c30-085cfae64ceb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d7521976120>]}
[0m15:58:52.481803 [info ] [MainThread]: Found 10 sources, 508 macros
[0m15:58:52.485300 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e9c10ce7-d66f-40a4-8c30-085cfae64ceb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d75219d3950>]}
[0m15:58:52.490105 [info ] [MainThread]: 
[0m15:58:52.491290 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:58:52.492431 [info ] [MainThread]: 
[0m15:58:52.494316 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m15:58:52.496048 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e9c10ce7-d66f-40a4-8c30-085cfae64ceb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d7521976450>]}
[0m15:58:52.498047 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:58:52.509613 [debug] [Thread-1 (]: Began running node source.data_pipeline_project.raw_data.orders
[0m15:58:52.510578 [debug] [Thread-2 (]: Began running node source.data_pipeline_project.raw_data.shipments
[0m15:58:52.517602 [debug] [Thread-3 (]: Began running node source.data_pipeline_project.raw_data.subscriptions
[0m15:58:52.516428 [info ] [Thread-1 (]: 1 of 4 START freshness of raw_data.orders ...................................... [RUN]
[0m15:58:52.529360 [debug] [Thread-1 (]: Acquiring new bigquery connection 'source.data_pipeline_project.raw_data.orders'
[0m15:58:52.521947 [debug] [Thread-4 (]: Began running node source.data_pipeline_project.raw_data.users
[0m15:58:52.525727 [info ] [Thread-2 (]: 2 of 4 START freshness of raw_data.shipments ................................... [RUN]
[0m15:58:52.530290 [debug] [Thread-1 (]: Began compiling node source.data_pipeline_project.raw_data.orders
[0m15:58:52.528100 [info ] [Thread-3 (]: 3 of 4 START freshness of raw_data.subscriptions ............................... [RUN]
[0m15:58:52.538314 [debug] [Thread-2 (]: Acquiring new bigquery connection 'source.data_pipeline_project.raw_data.shipments'
[0m15:58:52.534533 [info ] [Thread-4 (]: 4 of 4 START freshness of raw_data.users ....................................... [RUN]
[0m15:58:52.541301 [debug] [Thread-1 (]: Began executing node source.data_pipeline_project.raw_data.orders
[0m15:58:52.564284 [debug] [Thread-3 (]: Acquiring new bigquery connection 'source.data_pipeline_project.raw_data.subscriptions'
[0m15:58:52.565904 [debug] [Thread-4 (]: Acquiring new bigquery connection 'source.data_pipeline_project.raw_data.users'
[0m15:58:52.552751 [debug] [Thread-2 (]: Began compiling node source.data_pipeline_project.raw_data.shipments
[0m15:58:52.580165 [debug] [Thread-3 (]: Began compiling node source.data_pipeline_project.raw_data.subscriptions
[0m15:58:52.596336 [debug] [Thread-1 (]: On source.data_pipeline_project.raw_data.orders: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "source.data_pipeline_project.raw_data.orders"} */
select
      max(createdAt) as max_loaded_at,
      current_timestamp() as snapshotted_at
    from `data-pipeline-project-474812`.`raw_data`.`orders`
    
  
[0m15:58:52.597452 [debug] [Thread-4 (]: Began compiling node source.data_pipeline_project.raw_data.users
[0m15:58:52.598697 [debug] [Thread-2 (]: Began executing node source.data_pipeline_project.raw_data.shipments
[0m15:58:52.599809 [debug] [Thread-3 (]: Began executing node source.data_pipeline_project.raw_data.subscriptions
[0m15:58:52.601086 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m15:58:52.602143 [debug] [Thread-4 (]: Began executing node source.data_pipeline_project.raw_data.users
[0m15:58:52.606263 [debug] [Thread-2 (]: On source.data_pipeline_project.raw_data.shipments: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "source.data_pipeline_project.raw_data.shipments"} */
select
      max(createdAt) as max_loaded_at,
      current_timestamp() as snapshotted_at
    from `data-pipeline-project-474812`.`raw_data`.`shipments`
    
  
[0m15:58:52.611139 [debug] [Thread-3 (]: On source.data_pipeline_project.raw_data.subscriptions: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "source.data_pipeline_project.raw_data.subscriptions"} */
select
      max(createdAt) as max_loaded_at,
      current_timestamp() as snapshotted_at
    from `data-pipeline-project-474812`.`raw_data`.`subscriptions`
    
  
[0m15:58:52.685930 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m15:58:52.684573 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m15:58:52.683283 [debug] [Thread-4 (]: On source.data_pipeline_project.raw_data.users: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "source.data_pipeline_project.raw_data.users"} */
select
      max(createdAt) as max_loaded_at,
      current_timestamp() as snapshotted_at
    from `data-pipeline-project-474812`.`raw_data`.`users`
    
  
[0m15:58:52.777521 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m15:58:54.062545 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:e92c7434-e30b-4bce-aa1d-af61276fc120&page=queryresults
[0m15:58:54.224592 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:72fb2a12-c974-4834-8ea3-7a0309ed5d03&page=queryresults
[0m15:58:54.226287 [error] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:72fb2a12-c974-4834-8ea3-7a0309ed5d03&page=queryresults
[0m15:58:54.228665 [debug] [Thread-2 (]: BigQuery adapter: Unhandled error while running:
macro collect_freshness
[0m15:58:54.229637 [debug] [Thread-2 (]: BigQuery adapter: Database Error
  Unrecognized name: createdAt at [3:11]
[0m15:58:54.243747 [debug] [Thread-2 (]: Database Error in source shipments (models/staging/raw_sources.yml)
  Unrecognized name: createdAt at [3:11]
[0m15:58:54.244577 [error] [Thread-2 (]: 2 of 4 ERROR freshness of raw_data.shipments ................................... [[31mERROR[0m in 1.71s]
[0m15:58:54.245275 [debug] [Thread-2 (]: Finished running node source.data_pipeline_project.raw_data.shipments
[0m15:58:54.382812 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:a7d2cb73-f9d4-4bc1-8e9f-b27dfc48aa9b&page=queryresults
[0m15:58:54.558457 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:cad31a1d-93d2-4aa0-a2cc-c15ec625dc44&page=queryresults
[0m15:58:55.189188 [error] [Thread-1 (]: 1 of 4 ERROR STALE freshness of raw_data.orders ................................ [[31mERROR STALE[0m in 2.66s]
[0m15:58:55.191216 [debug] [Thread-1 (]: Finished running node source.data_pipeline_project.raw_data.orders
[0m15:58:55.193256 [debug] [Thread-7 (]: Marking all children of 'source.data_pipeline_project.raw_data.orders' to be skipped because of status 'error'. 
[0m15:58:55.312768 [error] [Thread-3 (]: 3 of 4 ERROR STALE freshness of raw_data.subscriptions ......................... [[31mERROR STALE[0m in 2.76s]
[0m15:58:55.321271 [debug] [Thread-3 (]: Finished running node source.data_pipeline_project.raw_data.subscriptions
[0m15:58:55.327410 [debug] [Thread-7 (]: Marking all children of 'source.data_pipeline_project.raw_data.subscriptions' to be skipped because of status 'error'. 
[0m15:58:55.457267 [error] [Thread-4 (]: 4 of 4 ERROR STALE freshness of raw_data.users ................................. [[31mERROR STALE[0m in 2.89s]
[0m15:58:55.460819 [debug] [Thread-4 (]: Finished running node source.data_pipeline_project.raw_data.users
[0m15:58:55.463651 [debug] [Thread-7 (]: Marking all children of 'source.data_pipeline_project.raw_data.users' to be skipped because of status 'error'. 
[0m15:58:55.479905 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:58:55.483752 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:58:55.486112 [debug] [MainThread]: Connection 'source.data_pipeline_project.raw_data.orders' was properly closed.
[0m15:58:55.487100 [debug] [MainThread]: Connection 'source.data_pipeline_project.raw_data.shipments' was properly closed.
[0m15:58:55.488328 [debug] [MainThread]: Connection 'source.data_pipeline_project.raw_data.subscriptions' was properly closed.
[0m15:58:55.493065 [debug] [MainThread]: Connection 'source.data_pipeline_project.raw_data.users' was properly closed.
[0m15:58:55.493717 [info ] [MainThread]: 
[0m15:58:55.494185 [info ] [MainThread]: Finished running 4 sources in 0 hours 0 minutes and 3.00 seconds (3.00s).
[0m15:58:55.578943 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m15:58:55.581779 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m15:58:55.596157 [debug] [MainThread]: Wrote artifact FreshnessResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/sources.json
[0m15:58:55.597184 [info ] [MainThread]: 
[0m15:58:55.599017 [error] [MainThread]:   Database Error in source shipments (models/staging/raw_sources.yml)
  Unrecognized name: createdAt at [3:11]
[0m15:58:55.600974 [info ] [MainThread]: 
[0m15:58:55.602890 [error] [MainThread]: [31mFailure in source orders (models/staging/raw_sources.yml)[0m
[0m15:58:55.603699 [error] [MainThread]:   Status: error
[0m15:58:55.605869 [info ] [MainThread]: 
[0m15:58:55.607700 [error] [MainThread]: [31mFailure in source subscriptions (models/staging/raw_sources.yml)[0m
[0m15:58:55.609147 [error] [MainThread]:   Status: error
[0m15:58:55.610410 [info ] [MainThread]: 
[0m15:58:55.611797 [error] [MainThread]: [31mFailure in source users (models/staging/raw_sources.yml)[0m
[0m15:58:55.612925 [error] [MainThread]:   Status: error
[0m15:58:55.613933 [info ] [MainThread]: Done.
[0m15:58:55.615755 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_success": false, "command_wall_clock_time": 7.571863, "process_in_blocks": "0", "process_kernel_time": 1.245099, "process_mem_max_rss": "387236", "process_out_blocks": "3168", "process_user_time": 6.57898}
[0m15:58:55.616730 [debug] [MainThread]: Command `dbt source freshness` failed at 15:58:55.616614 after 7.57 seconds
[0m15:58:55.621517 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d7541af56a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d7522059b80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d7521fb11f0>]}
[0m15:58:55.622269 [debug] [MainThread]: Flushing usage events
[0m15:58:56.680127 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m17:10:03.726232 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7899d276d910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7899d250b9b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7899d339b920>]}


============================== 17:10:03.729381 | ef8327e1-6553-420e-bd16-93a97f803392 ==============================
[0m17:10:03.729381 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m17:10:03.731533 [debug] [MainThread]: running dbt with arguments {'write_json': 'True', 'partial_parse': 'True', 'quiet': 'False', 'fail_fast': 'False', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'log_cache_events': 'False', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_experimental_parser': 'False', 'printer_width': '80', 'indirect_selection': 'eager', 'invocation_command': 'dbt run --models stg_users', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'debug': 'False', 'cache_selected_only': 'False', 'no_print': 'None', 'version_check': 'True', 'use_colors': 'True', 'profiles_dir': '/home/ecem/.dbt', 'send_anonymous_usage_stats': 'True', 'empty': 'False', 'warn_error': 'None'}
[0m17:10:03.733782 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality
Usage of `--models`, `--model`, and `-m` is deprecated in favor of `--select` or
`-s`.
[0m17:10:03.734819 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': 'ef8327e1-6553-420e-bd16-93a97f803392', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7899d1c164b0>]}
[0m17:10:06.238219 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ef8327e1-6553-420e-bd16-93a97f803392', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7899d3f1bce0>]}
[0m17:10:06.302147 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ef8327e1-6553-420e-bd16-93a97f803392', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7899b2b29820>]}
[0m17:10:06.306422 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m17:10:06.606073 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m17:10:06.827252 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
[0m17:10:06.832960 [debug] [MainThread]: Partial parsing: added file: data_pipeline_project://models/staging/stg_users.sql
[0m17:10:07.124008 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.data_pipeline_project.marts
[0m17:10:07.144011 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ef8327e1-6553-420e-bd16-93a97f803392', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7899b1beac60>]}
[0m17:10:07.249696 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m17:10:07.255412 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m17:10:07.271211 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ef8327e1-6553-420e-bd16-93a97f803392', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7899b1a695e0>]}
[0m17:10:07.272112 [info ] [MainThread]: Found 1 model, 10 sources, 508 macros
[0m17:10:07.272775 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ef8327e1-6553-420e-bd16-93a97f803392', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7899d16fa5a0>]}
[0m17:10:07.275585 [info ] [MainThread]: 
[0m17:10:07.276600 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m17:10:07.277525 [info ] [MainThread]: 
[0m17:10:07.278342 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m17:10:07.280284 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812'
[0m17:10:07.281244 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:10:08.515779 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812, now create_data-pipeline-project-474812_analytics_staging_staging)
[0m17:10:08.516919 [debug] [ThreadPool]: Creating schema "database: "data-pipeline-project-474812"
schema: "analytics_staging_staging"
"
[0m17:10:08.525705 [debug] [ThreadPool]: On create_data-pipeline-project-474812_analytics_staging_staging: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "connection_name": "create_data-pipeline-project-474812_analytics_staging_staging"} */
create schema if not exists `data-pipeline-project-474812`.`analytics_staging_staging`
  
[0m17:10:08.526563 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:10:09.713624 [debug] [ThreadPool]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:e6d62c98-19b6-4669-b4ae-0e034b4cea22&page=queryresults
[0m17:10:10.920340 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_data-pipeline-project-474812_analytics_staging_staging, now list_data-pipeline-project-474812_analytics_staging_staging)
[0m17:10:10.921248 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:10:11.789523 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ef8327e1-6553-420e-bd16-93a97f803392', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7899b1beac60>]}
[0m17:10:11.790376 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:10:11.799625 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.stg_users
[0m17:10:11.800632 [info ] [Thread-1 (]: 1 of 1 START sql view model analytics_staging_staging.stg_users ................ [RUN]
[0m17:10:11.801562 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now model.data_pipeline_project.stg_users)
[0m17:10:11.802478 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.stg_users
[0m17:10:11.810137 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.stg_users"
[0m17:10:11.811927 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.stg_users
[0m17:10:11.845474 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.stg_users"
[0m17:10:11.846708 [debug] [Thread-1 (]: On model.data_pipeline_project.stg_users: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.stg_users"} */


  create or replace view `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
  OPTIONS()
  as with source as (

    select * from `data-pipeline-project-474812`.`raw_data`.`users`

),

renamed_and_cleaned as (

    select

        -- Primary Key: Standardized to user_id
        _id as user_id,

        -- Standardized timestamp: We agreed to use 'created_at' for the primary timestamp
        cast(createdAt as timestamp) as created_at,
        
        -- Deduplication logic: Assign a row number based on the creation time
        -- We want the latest record (highest created_at) if duplicates exist.
        row_number() over (partition by _id order by createdAt desc) as rn

    from source
    -- Simple cleaning: ensure we only select records with a valid ID
    where _id is not null
)

select
    user_id,
    created_at

from renamed_and_cleaned
-- Final selection: only include the latest, unique instance of a user_id
where rn = 1;


[0m17:10:11.847270 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m17:10:13.127120 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:36563861-95da-41c9-bb7b-021529371468&page=queryresults
[0m17:10:13.810465 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ef8327e1-6553-420e-bd16-93a97f803392', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7899b4e488f0>]}
[0m17:10:13.813090 [info ] [Thread-1 (]: 1 of 1 OK created sql view model analytics_staging_staging.stg_users ........... [[32mCREATE VIEW (0 processed)[0m in 2.01s]
[0m17:10:13.814393 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.stg_users
[0m17:10:13.817749 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m17:10:13.819850 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:10:13.820312 [debug] [MainThread]: Connection 'model.data_pipeline_project.stg_users' was properly closed.
[0m17:10:13.821227 [info ] [MainThread]: 
[0m17:10:13.822595 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 6.54 seconds (6.54s).
[0m17:10:13.823781 [debug] [MainThread]: Command end result
[0m17:10:13.876269 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m17:10:13.879045 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m17:10:13.889892 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m17:10:13.890391 [info ] [MainThread]: 
[0m17:10:13.891124 [info ] [MainThread]: [32mCompleted successfully[0m
[0m17:10:13.892957 [info ] [MainThread]: 
[0m17:10:13.894100 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m17:10:13.895145 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- ModelParamUsageDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m17:10:13.896677 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 10.22382, "process_in_blocks": "208", "process_kernel_time": 1.561127, "process_mem_max_rss": "382592", "process_out_blocks": "3208", "process_user_time": 4.612728}
[0m17:10:13.897473 [debug] [MainThread]: Command `dbt run` succeeded at 17:10:13.897360 after 10.22 seconds
[0m17:10:13.898591 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7899d2211af0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7899b1cd1220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7899b1c2b110>]}
[0m17:10:13.899273 [debug] [MainThread]: Flushing usage events
[0m17:10:14.886799 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m17:10:35.463606 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7966957d8f50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7966971e6e40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79669503cb00>]}


============================== 17:10:35.469041 | e6827066-e0ce-44d1-9a36-2478ee6b2573 ==============================
[0m17:10:35.469041 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m17:10:35.476175 [debug] [MainThread]: running dbt with arguments {'fail_fast': 'False', 'introspect': 'True', 'debug': 'False', 'use_colors': 'True', 'indirect_selection': 'eager', 'cache_selected_only': 'False', 'target_path': 'None', 'no_print': 'None', 'quiet': 'False', 'warn_error': 'None', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_format': 'default', 'partial_parse': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'invocation_command': 'dbt run --select stg_users', 'profiles_dir': '/home/ecem/.dbt', 'log_cache_events': 'False', 'write_json': 'True', 'version_check': 'True', 'send_anonymous_usage_stats': 'True', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'printer_width': '80'}
[0m17:10:37.843407 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e6827066-e0ce-44d1-9a36-2478ee6b2573', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79667513c200>]}
[0m17:10:37.914003 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e6827066-e0ce-44d1-9a36-2478ee6b2573', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x796676923b30>]}
[0m17:10:37.914983 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m17:10:38.166918 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m17:10:38.403553 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:10:38.404561 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:10:38.413603 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.data_pipeline_project.marts
[0m17:10:38.440068 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e6827066-e0ce-44d1-9a36-2478ee6b2573', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79669471ab70>]}
[0m17:10:38.548406 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m17:10:38.552665 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m17:10:38.571988 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e6827066-e0ce-44d1-9a36-2478ee6b2573', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x796674fe2660>]}
[0m17:10:38.573010 [info ] [MainThread]: Found 1 model, 10 sources, 508 macros
[0m17:10:38.573744 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e6827066-e0ce-44d1-9a36-2478ee6b2573', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79667530ecf0>]}
[0m17:10:38.576045 [info ] [MainThread]: 
[0m17:10:38.576899 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m17:10:38.577426 [info ] [MainThread]: 
[0m17:10:38.578583 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m17:10:38.580185 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812'
[0m17:10:38.580686 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:10:39.513039 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_staging'
[0m17:10:39.513976 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:10:40.624366 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e6827066-e0ce-44d1-9a36-2478ee6b2573', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x796675002a50>]}
[0m17:10:40.626190 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:10:40.636565 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.stg_users
[0m17:10:40.637663 [info ] [Thread-1 (]: 1 of 1 START sql view model analytics_staging_staging.stg_users ................ [RUN]
[0m17:10:40.638619 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now model.data_pipeline_project.stg_users)
[0m17:10:40.639306 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.stg_users
[0m17:10:40.650691 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.stg_users"
[0m17:10:40.652112 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.stg_users
[0m17:10:40.706314 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.stg_users"
[0m17:10:40.707314 [debug] [Thread-1 (]: On model.data_pipeline_project.stg_users: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.stg_users"} */


  create or replace view `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
  OPTIONS()
  as with source as (

    select * from `data-pipeline-project-474812`.`raw_data`.`users`

),

renamed_and_cleaned as (

    select

        -- Primary Key: Standardized to user_id
        _id as user_id,

        -- Standardized timestamp: We agreed to use 'created_at' for the primary timestamp
        cast(createdAt as timestamp) as created_at,
        
        -- Deduplication logic: Assign a row number based on the creation time
        -- We want the latest record (highest created_at) if duplicates exist.
        row_number() over (partition by _id order by createdAt desc) as rn

    from source
    -- Simple cleaning: ensure we only select records with a valid ID
    where _id is not null
)

select
    user_id,
    created_at

from renamed_and_cleaned
-- Final selection: only include the latest, unique instance of a user_id
where rn = 1;


[0m17:10:40.708062 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m17:10:41.988118 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:25d40f0c-0eaa-4ea5-91c9-2496860e9d1b&page=queryresults
[0m17:10:42.816827 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e6827066-e0ce-44d1-9a36-2478ee6b2573', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x796677f487d0>]}
[0m17:10:42.818419 [info ] [Thread-1 (]: 1 of 1 OK created sql view model analytics_staging_staging.stg_users ........... [[32mCREATE VIEW (0 processed)[0m in 2.18s]
[0m17:10:42.820365 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.stg_users
[0m17:10:42.826305 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m17:10:42.830497 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:10:42.833657 [debug] [MainThread]: Connection 'list_data-pipeline-project-474812' was properly closed.
[0m17:10:42.834514 [debug] [MainThread]: Connection 'model.data_pipeline_project.stg_users' was properly closed.
[0m17:10:42.835016 [info ] [MainThread]: 
[0m17:10:42.836165 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 4.26 seconds (4.26s).
[0m17:10:42.837607 [debug] [MainThread]: Command end result
[0m17:10:42.886662 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m17:10:42.890610 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m17:10:42.899024 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m17:10:42.899719 [info ] [MainThread]: 
[0m17:10:42.900328 [info ] [MainThread]: [32mCompleted successfully[0m
[0m17:10:42.901008 [info ] [MainThread]: 
[0m17:10:42.901625 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m17:10:42.903009 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 7.511116, "process_in_blocks": "0", "process_kernel_time": 1.234302, "process_mem_max_rss": "381184", "process_out_blocks": "2144", "process_user_time": 4.249121}
[0m17:10:42.903621 [debug] [MainThread]: Command `dbt run` succeeded at 17:10:42.903492 after 7.51 seconds
[0m17:10:42.904098 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x796694e7b1d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7966741942c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79667415b1a0>]}
[0m17:10:42.904594 [debug] [MainThread]: Flushing usage events
[0m17:10:43.578328 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m18:31:54.266712 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71ca5dc95c40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71ca5f7e6f30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71ca5f892780>]}


============================== 18:31:54.270538 | 438f3623-99a2-48ad-ba70-f099ad298aeb ==============================
[0m18:31:54.270538 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m18:31:54.271448 [debug] [MainThread]: running dbt with arguments {'write_json': 'True', 'use_colors': 'True', 'target_path': 'None', 'printer_width': '80', 'debug': 'False', 'use_experimental_parser': 'False', 'no_print': 'None', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'warn_error': 'None', 'invocation_command': 'dbt run --select stg_orders', 'empty': 'False', 'version_check': 'True', 'static_parser': 'True', 'fail_fast': 'False', 'log_format': 'default', 'profiles_dir': '/home/ecem/.dbt', 'indirect_selection': 'eager', 'cache_selected_only': 'False', 'quiet': 'False', 'send_anonymous_usage_stats': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_cache_events': 'False', 'introspect': 'True', 'partial_parse': 'True'}
[0m18:31:56.786182 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '438f3623-99a2-48ad-ba70-f099ad298aeb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71ca3d7502c0>]}
[0m18:31:56.851987 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '438f3623-99a2-48ad-ba70-f099ad298aeb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71ca3da7e1b0>]}
[0m18:31:56.853458 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m18:31:57.114017 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m18:31:57.391655 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
[0m18:31:57.392727 [debug] [MainThread]: Partial parsing: added file: data_pipeline_project://models/staging/stg_orders.sql
[0m18:31:57.685474 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.data_pipeline_project.marts
[0m18:31:57.700726 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '438f3623-99a2-48ad-ba70-f099ad298aeb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71ca3d228800>]}
[0m18:31:57.810025 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m18:31:57.815296 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m18:31:57.833449 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '438f3623-99a2-48ad-ba70-f099ad298aeb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71ca3c960650>]}
[0m18:31:57.835300 [info ] [MainThread]: Found 2 models, 10 sources, 508 macros
[0m18:31:57.837354 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '438f3623-99a2-48ad-ba70-f099ad298aeb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71ca3d261be0>]}
[0m18:31:57.842989 [info ] [MainThread]: 
[0m18:31:57.845492 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m18:31:57.846474 [info ] [MainThread]: 
[0m18:31:57.849007 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m18:31:57.851215 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812'
[0m18:31:57.852580 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:31:59.037816 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812, now list_data-pipeline-project-474812_analytics_staging_staging)
[0m18:31:59.040241 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:31:59.905099 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '438f3623-99a2-48ad-ba70-f099ad298aeb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71ca3d3e8d40>]}
[0m18:31:59.906447 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:31:59.920794 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.stg_orders
[0m18:31:59.922430 [info ] [Thread-1 (]: 1 of 1 START sql view model analytics_staging_staging.stg_orders ............... [RUN]
[0m18:31:59.924280 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now model.data_pipeline_project.stg_orders)
[0m18:31:59.925295 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.stg_orders
[0m18:31:59.937521 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.stg_orders"
[0m18:31:59.939032 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.stg_orders
[0m18:31:59.987304 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.stg_orders"
[0m18:31:59.990206 [debug] [Thread-1 (]: On model.data_pipeline_project.stg_orders: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.stg_orders"} */


  create or replace view `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_orders`
  OPTIONS()
  as with source as (

    select * from `data-pipeline-project-474812`.`raw_data`.`orders`

),

renamed_and_cleaned as (

    select

        -- Deduplication logic: Prioritize the first record if an ID appears more than once
        row_number() over (partition by _id order by createdAt asc) as rn,

        -- Primary Key
        _id as order_id,

        -- Foreign Keys
        _user as user_id,
        _deliveryAddress as delivery_address_id,
        invoiceAddress as invoice_address_id,

        -- Standardized Timestamps
        cast(createdAt as timestamp) as created_at,

        -- Metrics: JSON Extraction
        -- Extract the final charged amount (in TRY, as confirmed)
        cast(json_extract_scalar(price, '$.chargedAmount') as numeric) as charged_amount_try,
        
        -- Attributes
        status as order_status,

        -- Complex/JSON Fields (Selected as raw strings)
        price as price_data,
        oneTimePurchase as one_time_purchase_data,
        subscription as subscription_data

    from source
    where _id is not null
    -- Simple cleaning: Filter out records where we cannot extract a charged amount
    and json_extract_scalar(price, '$.chargedAmount') is not null
    and cast(json_extract_scalar(price, '$.chargedAmount') as numeric) > 0

)

select 
    order_id,
    user_id,
    delivery_address_id,
    invoice_address_id,
    created_at,
    charged_amount_try,
    order_status,
    price_data,
    one_time_purchase_data,
    subscription_data
from renamed_and_cleaned
-- Final selection: ONLY select the single, canonical record (rn = 1)
where rn = 1;


[0m18:31:59.992460 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m18:32:01.236343 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:200ca145-2ebd-44c9-b034-73682dd1017f&page=queryresults
[0m18:32:01.572109 [error] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:200ca145-2ebd-44c9-b034-73682dd1017f&page=queryresults
[0m18:32:01.605064 [debug] [Thread-1 (]: Database Error in model stg_orders (models/staging/stg_orders.sql)
  Unrecognized name: invoiceAddress; Did you mean _invoiceAddress? at [25:9]
  compiled code at target/run/data_pipeline_project/models/staging/stg_orders.sql
[0m18:32:01.613303 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '438f3623-99a2-48ad-ba70-f099ad298aeb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71ca5dc97890>]}
[0m18:32:01.615114 [error] [Thread-1 (]: 1 of 1 ERROR creating sql view model analytics_staging_staging.stg_orders ...... [[31mERROR[0m in 1.69s]
[0m18:32:01.616207 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.stg_orders
[0m18:32:01.617415 [debug] [Thread-7 (]: Marking all children of 'model.data_pipeline_project.stg_orders' to be skipped because of status 'error'.  Reason: Database Error in model stg_orders (models/staging/stg_orders.sql)
  Unrecognized name: invoiceAddress; Did you mean _invoiceAddress? at [25:9]
  compiled code at target/run/data_pipeline_project/models/staging/stg_orders.sql.
[0m18:32:01.623913 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m18:32:01.626722 [debug] [MainThread]: Connection 'master' was properly closed.
[0m18:32:01.629138 [debug] [MainThread]: Connection 'model.data_pipeline_project.stg_orders' was properly closed.
[0m18:32:01.630200 [info ] [MainThread]: 
[0m18:32:01.632465 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 3.78 seconds (3.78s).
[0m18:32:01.634198 [debug] [MainThread]: Command end result
[0m18:32:01.665896 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m18:32:01.671736 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m18:32:01.695242 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m18:32:01.696109 [info ] [MainThread]: 
[0m18:32:01.698399 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m18:32:01.699060 [info ] [MainThread]: 
[0m18:32:01.700171 [error] [MainThread]: [31mFailure in model stg_orders (models/staging/stg_orders.sql)[0m
[0m18:32:01.701017 [error] [MainThread]:   Database Error in model stg_orders (models/staging/stg_orders.sql)
  Unrecognized name: invoiceAddress; Did you mean _invoiceAddress? at [25:9]
  compiled code at target/run/data_pipeline_project/models/staging/stg_orders.sql
[0m18:32:01.701432 [info ] [MainThread]: 
[0m18:32:01.701857 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_project/models/staging/stg_orders.sql
[0m18:32:01.702600 [info ] [MainThread]: 
[0m18:32:01.703658 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m18:32:01.705337 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 7.500306, "process_in_blocks": "0", "process_kernel_time": 1.15355, "process_mem_max_rss": "378196", "process_out_blocks": "3232", "process_user_time": 4.958877}
[0m18:32:01.705864 [debug] [MainThread]: Command `dbt run` failed at 18:32:01.705765 after 7.50 seconds
[0m18:32:01.706591 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71ca3c960500>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71ca3d261700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71ca3d261850>]}
[0m18:32:01.707279 [debug] [MainThread]: Flushing usage events
[0m18:32:02.836096 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m18:34:24.209214 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x758f6d5c1dc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x758f6f3eaf60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x758f6f2ce7e0>]}


============================== 18:34:24.213132 | a88d0095-fb9e-4572-89ab-7578d09ebb53 ==============================
[0m18:34:24.213132 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m18:34:24.214064 [debug] [MainThread]: running dbt with arguments {'profiles_dir': '/home/ecem/.dbt', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'invocation_command': 'dbt run --select stg_orders', 'log_cache_events': 'False', 'debug': 'False', 'target_path': 'None', 'log_format': 'default', 'partial_parse': 'True', 'introspect': 'True', 'version_check': 'True', 'printer_width': '80', 'warn_error': 'None', 'static_parser': 'True', 'write_json': 'True', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'quiet': 'False', 'use_experimental_parser': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'use_colors': 'True', 'fail_fast': 'False'}
[0m18:34:26.810473 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a88d0095-fb9e-4572-89ab-7578d09ebb53', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x758f4d47bb30>]}
[0m18:34:26.877403 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a88d0095-fb9e-4572-89ab-7578d09ebb53', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x758f4d632b40>]}
[0m18:34:26.878866 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m18:34:27.132417 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m18:34:27.392749 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m18:34:27.394028 [debug] [MainThread]: Partial parsing: updated file: data_pipeline_project://models/staging/stg_orders.sql
[0m18:34:27.668987 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.data_pipeline_project.marts
[0m18:34:27.685059 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a88d0095-fb9e-4572-89ab-7578d09ebb53', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x758f4ce59820>]}
[0m18:34:27.792227 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m18:34:27.796099 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m18:34:27.817576 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a88d0095-fb9e-4572-89ab-7578d09ebb53', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x758f4c528530>]}
[0m18:34:27.821905 [info ] [MainThread]: Found 2 models, 10 sources, 508 macros
[0m18:34:27.823107 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a88d0095-fb9e-4572-89ab-7578d09ebb53', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x758f4cef9af0>]}
[0m18:34:27.825085 [info ] [MainThread]: 
[0m18:34:27.826047 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m18:34:27.826835 [info ] [MainThread]: 
[0m18:34:27.827444 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m18:34:27.830144 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812'
[0m18:34:27.830914 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:34:28.999940 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812, now list_data-pipeline-project-474812_analytics_staging_staging)
[0m18:34:29.007247 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:34:29.896976 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a88d0095-fb9e-4572-89ab-7578d09ebb53', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x758f4cefa660>]}
[0m18:34:29.898112 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:34:29.908630 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.stg_orders
[0m18:34:29.909940 [info ] [Thread-1 (]: 1 of 1 START sql view model analytics_staging_staging.stg_orders ............... [RUN]
[0m18:34:29.911114 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now model.data_pipeline_project.stg_orders)
[0m18:34:29.911763 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.stg_orders
[0m18:34:29.922998 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.stg_orders"
[0m18:34:29.924699 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.stg_orders
[0m18:34:29.964480 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.stg_orders"
[0m18:34:29.965706 [debug] [Thread-1 (]: On model.data_pipeline_project.stg_orders: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.stg_orders"} */


  create or replace view `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_orders`
  OPTIONS()
  as with source as (

    select * from `data-pipeline-project-474812`.`raw_data`.`orders`

),

renamed_and_cleaned as (

    select

        -- Deduplication logic: Prioritize the first record if an ID appears more than once
        row_number() over (partition by _id order by createdAt asc) as rn,

        -- Primary Key
        _id as order_id,

        -- Foreign Keys
        _user as user_id,
        _deliveryAddress as delivery_address_id,
        _invoiceAddress as invoice_address_id,

        -- Standardized Timestamps
        cast(createdAt as timestamp) as created_at,

        -- Metrics: JSON Extraction
        -- Extract the final charged amount (in TRY, as confirmed)
        cast(json_extract_scalar(price, '$.chargedAmount') as numeric) as charged_amount_try,
        
        -- Attributes
        status as order_status,

        -- Complex/JSON Fields (Selected as raw strings)
        price as price_data,
        oneTimePurchase as one_time_purchase_data,
        subscription as subscription_data

    from source
    where _id is not null
    -- Simple cleaning: Filter out records where we cannot extract a charged amount
    and json_extract_scalar(price, '$.chargedAmount') is not null
    and cast(json_extract_scalar(price, '$.chargedAmount') as numeric) > 0

)

select 
    order_id,
    user_id,
    delivery_address_id,
    invoice_address_id,
    created_at,
    charged_amount_try,
    order_status,
    price_data,
    one_time_purchase_data,
    subscription_data
from renamed_and_cleaned
-- Final selection: ONLY select the single, canonical record (rn = 1)
where rn = 1;


[0m18:34:29.966955 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m18:34:31.773366 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:191a67e8-099d-4d7d-aec7-807b8b602196&page=queryresults
[0m18:34:32.140345 [error] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:191a67e8-099d-4d7d-aec7-807b8b602196&page=queryresults
[0m18:34:32.164787 [debug] [Thread-1 (]: Database Error in model stg_orders (models/staging/stg_orders.sql)
  Unrecognized name: subscription; Did you mean subscriptions? at [40:9]
  compiled code at target/run/data_pipeline_project/models/staging/stg_orders.sql
[0m18:34:32.168227 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a88d0095-fb9e-4572-89ab-7578d09ebb53', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x758f4c56c5f0>]}
[0m18:34:32.170285 [error] [Thread-1 (]: 1 of 1 ERROR creating sql view model analytics_staging_staging.stg_orders ...... [[31mERROR[0m in 2.26s]
[0m18:34:32.172001 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.stg_orders
[0m18:34:32.175052 [debug] [Thread-7 (]: Marking all children of 'model.data_pipeline_project.stg_orders' to be skipped because of status 'error'.  Reason: Database Error in model stg_orders (models/staging/stg_orders.sql)
  Unrecognized name: subscription; Did you mean subscriptions? at [40:9]
  compiled code at target/run/data_pipeline_project/models/staging/stg_orders.sql.
[0m18:34:32.180418 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m18:34:32.182188 [debug] [MainThread]: Connection 'master' was properly closed.
[0m18:34:32.182637 [debug] [MainThread]: Connection 'model.data_pipeline_project.stg_orders' was properly closed.
[0m18:34:32.183090 [info ] [MainThread]: 
[0m18:34:32.183548 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 4.36 seconds (4.36s).
[0m18:34:32.184329 [debug] [MainThread]: Command end result
[0m18:34:32.211121 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m18:34:32.213234 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m18:34:32.221541 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m18:34:32.222290 [info ] [MainThread]: 
[0m18:34:32.226042 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m18:34:32.227031 [info ] [MainThread]: 
[0m18:34:32.227877 [error] [MainThread]: [31mFailure in model stg_orders (models/staging/stg_orders.sql)[0m
[0m18:34:32.228643 [error] [MainThread]:   Database Error in model stg_orders (models/staging/stg_orders.sql)
  Unrecognized name: subscription; Did you mean subscriptions? at [40:9]
  compiled code at target/run/data_pipeline_project/models/staging/stg_orders.sql
[0m18:34:32.229603 [info ] [MainThread]: 
[0m18:34:32.232169 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_project/models/staging/stg_orders.sql
[0m18:34:32.236408 [info ] [MainThread]: 
[0m18:34:32.237925 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m18:34:32.241590 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 8.09967, "process_in_blocks": "0", "process_kernel_time": 1.070054, "process_mem_max_rss": "378056", "process_out_blocks": "3232", "process_user_time": 5.030213}
[0m18:34:32.243064 [debug] [MainThread]: Command `dbt run` failed at 18:34:32.242909 after 8.10 seconds
[0m18:34:32.245042 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x758f6ebc0d70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x758f4d36dfd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x758f4d22c1d0>]}
[0m18:34:32.246761 [debug] [MainThread]: Flushing usage events
[0m18:34:33.251592 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m18:35:39.375291 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa40349a1b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa4042a2cf0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa403519f40>]}


============================== 18:35:39.381947 | 666b5ec8-5b4f-4a4a-a45b-7b1121273f75 ==============================
[0m18:35:39.381947 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m18:35:39.382725 [debug] [MainThread]: running dbt with arguments {'indirect_selection': 'eager', 'invocation_command': 'dbt run --select stg_orders', 'partial_parse': 'True', 'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'no_print': 'None', 'empty': 'False', 'write_json': 'True', 'printer_width': '80', 'use_colors': 'True', 'warn_error': 'None', 'log_format': 'default', 'profiles_dir': '/home/ecem/.dbt', 'log_cache_events': 'False', 'send_anonymous_usage_stats': 'True', 'cache_selected_only': 'False', 'static_parser': 'True', 'quiet': 'False', 'target_path': 'None', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'debug': 'False', 'use_experimental_parser': 'False', 'version_check': 'True', 'introspect': 'True'}
[0m18:35:41.782391 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '666b5ec8-5b4f-4a4a-a45b-7b1121273f75', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa404e3b830>]}
[0m18:35:41.860028 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '666b5ec8-5b4f-4a4a-a45b-7b1121273f75', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa404d11ee0>]}
[0m18:35:41.862406 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m18:35:42.099413 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m18:35:42.340027 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m18:35:42.345139 [debug] [MainThread]: Partial parsing: updated file: data_pipeline_project://models/staging/stg_orders.sql
[0m18:35:42.609103 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.data_pipeline_project.marts
[0m18:35:42.624522 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '666b5ec8-5b4f-4a4a-a45b-7b1121273f75', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa3e2ae4c50>]}
[0m18:35:42.728104 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m18:35:42.731880 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m18:35:42.751728 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '666b5ec8-5b4f-4a4a-a45b-7b1121273f75', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa3e215cdd0>]}
[0m18:35:42.752727 [info ] [MainThread]: Found 2 models, 10 sources, 508 macros
[0m18:35:42.753636 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '666b5ec8-5b4f-4a4a-a45b-7b1121273f75', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa3e212eab0>]}
[0m18:35:42.755345 [info ] [MainThread]: 
[0m18:35:42.756135 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m18:35:42.757009 [info ] [MainThread]: 
[0m18:35:42.758443 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m18:35:42.760161 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812'
[0m18:35:42.761314 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:35:44.406516 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812, now list_data-pipeline-project-474812_analytics_staging_staging)
[0m18:35:44.407527 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:35:45.296382 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '666b5ec8-5b4f-4a4a-a45b-7b1121273f75', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa3e2a32f00>]}
[0m18:35:45.297193 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:35:45.306691 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.stg_orders
[0m18:35:45.307604 [info ] [Thread-1 (]: 1 of 1 START sql view model analytics_staging_staging.stg_orders ............... [RUN]
[0m18:35:45.308705 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now model.data_pipeline_project.stg_orders)
[0m18:35:45.309187 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.stg_orders
[0m18:35:45.319021 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.stg_orders"
[0m18:35:45.320930 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.stg_orders
[0m18:35:45.357580 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.stg_orders"
[0m18:35:45.359443 [debug] [Thread-1 (]: On model.data_pipeline_project.stg_orders: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.stg_orders"} */


  create or replace view `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_orders`
  OPTIONS()
  as with source as (

    select * from `data-pipeline-project-474812`.`raw_data`.`orders`

),

renamed_and_cleaned as (

    select

        -- Deduplication logic: Prioritize the first record if an ID appears more than once
        row_number() over (partition by _id order by createdAt asc) as rn,

        -- Primary Key
        _id as order_id,

        -- Foreign Keys
        _user as user_id,
        _deliveryAddress as delivery_address_id,
        _invoiceAddress as invoice_address_id,

        -- Standardized Timestamps
        cast(createdAt as timestamp) as created_at,

        -- Metrics: JSON Extraction
        -- Extract the final charged amount (in TRY, as confirmed)
        cast(json_extract_scalar(price, '$.chargedAmount') as numeric) as charged_amount_try,
        
        -- Attributes
        status as order_status,

        -- Complex/JSON Fields (Selected as raw strings)
        price as price_data,
        oneTimePurchase as one_time_purchase_data,
        subscriptions as subscription_data

    from source
    where _id is not null
    -- Simple cleaning: Filter out records where we cannot extract a charged amount
    and json_extract_scalar(price, '$.chargedAmount') is not null
    and cast(json_extract_scalar(price, '$.chargedAmount') as numeric) > 0

)

select 
    order_id,
    user_id,
    delivery_address_id,
    invoice_address_id,
    created_at,
    charged_amount_try,
    order_status,
    price_data,
    one_time_purchase_data,
    subscription_data
from renamed_and_cleaned
-- Final selection: ONLY select the single, canonical record (rn = 1)
where rn = 1;


[0m18:35:45.363224 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m18:35:46.895382 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:caa7b3f3-3341-4889-bbdc-30bcec220043&page=queryresults
[0m18:35:47.585469 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '666b5ec8-5b4f-4a4a-a45b-7b1121273f75', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa3e5d48920>]}
[0m18:35:47.586426 [info ] [Thread-1 (]: 1 of 1 OK created sql view model analytics_staging_staging.stg_orders .......... [[32mCREATE VIEW (0 processed)[0m in 2.27s]
[0m18:35:47.587770 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.stg_orders
[0m18:35:47.591610 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m18:35:47.593701 [debug] [MainThread]: Connection 'master' was properly closed.
[0m18:35:47.594507 [debug] [MainThread]: Connection 'model.data_pipeline_project.stg_orders' was properly closed.
[0m18:35:47.595279 [info ] [MainThread]: 
[0m18:35:47.596206 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 4.84 seconds (4.84s).
[0m18:35:47.597435 [debug] [MainThread]: Command end result
[0m18:35:47.633159 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m18:35:47.635921 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m18:35:47.648503 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m18:35:47.651556 [info ] [MainThread]: 
[0m18:35:47.653686 [info ] [MainThread]: [32mCompleted successfully[0m
[0m18:35:47.657449 [info ] [MainThread]: 
[0m18:35:47.662732 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m18:35:47.667703 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 8.361789, "process_in_blocks": "0", "process_kernel_time": 1.338467, "process_mem_max_rss": "382032", "process_out_blocks": "3232", "process_user_time": 4.557078}
[0m18:35:47.673426 [debug] [MainThread]: Command `dbt run` succeeded at 18:35:47.672378 after 8.37 seconds
[0m18:35:47.676262 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa402cd2ed0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa4034999d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa3e2bb77d0>]}
[0m18:35:47.681177 [debug] [MainThread]: Flushing usage events
[0m18:35:48.748991 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m18:49:54.299058 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71da249c3dd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71da24d2f470>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71da24fe6030>]}


============================== 18:49:54.318132 | 740021d0-0001-413c-8948-33a4d434cea1 ==============================
[0m18:49:54.318132 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m18:49:54.321866 [debug] [MainThread]: running dbt with arguments {'log_format': 'default', 'fail_fast': 'False', 'invocation_command': 'dbt run --select stg_subscriptions', 'target_path': 'None', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'send_anonymous_usage_stats': 'True', 'warn_error': 'None', 'empty': 'False', 'use_colors': 'True', 'partial_parse': 'True', 'indirect_selection': 'eager', 'no_print': 'None', 'profiles_dir': '/home/ecem/.dbt', 'introspect': 'True', 'version_check': 'True', 'cache_selected_only': 'False', 'printer_width': '80', 'debug': 'False', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_cache_events': 'False', 'use_experimental_parser': 'False', 'write_json': 'True', 'static_parser': 'True'}
[0m18:49:56.737484 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '740021d0-0001-413c-8948-33a4d434cea1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71da04b4fb90>]}
[0m18:49:56.812234 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '740021d0-0001-413c-8948-33a4d434cea1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71da04c5aa20>]}
[0m18:49:56.813029 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m18:49:57.065530 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m18:49:57.322134 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
[0m18:49:57.323112 [debug] [MainThread]: Partial parsing: added file: data_pipeline_project://models/staging/stg_subscriptions.sql
[0m18:49:57.581100 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.data_pipeline_project.marts
[0m18:49:57.596996 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '740021d0-0001-413c-8948-33a4d434cea1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71da045f8c20>]}
[0m18:49:57.715264 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m18:49:57.719633 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m18:49:57.739579 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '740021d0-0001-413c-8948-33a4d434cea1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71da045e86e0>]}
[0m18:49:57.740457 [info ] [MainThread]: Found 3 models, 10 sources, 508 macros
[0m18:49:57.741293 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '740021d0-0001-413c-8948-33a4d434cea1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71da25d344a0>]}
[0m18:49:57.743113 [info ] [MainThread]: 
[0m18:49:57.744020 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m18:49:57.744591 [info ] [MainThread]: 
[0m18:49:57.745391 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m18:49:57.746715 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812'
[0m18:49:57.748062 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:49:58.941357 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812, now list_data-pipeline-project-474812_analytics_staging_staging)
[0m18:49:58.943225 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:49:59.942221 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '740021d0-0001-413c-8948-33a4d434cea1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71da24062690>]}
[0m18:49:59.943106 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:49:59.951640 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.stg_subscriptions
[0m18:49:59.952772 [info ] [Thread-1 (]: 1 of 1 START sql view model analytics_staging_staging.stg_subscriptions ........ [RUN]
[0m18:49:59.954022 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now model.data_pipeline_project.stg_subscriptions)
[0m18:49:59.954554 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.stg_subscriptions
[0m18:49:59.962061 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.stg_subscriptions"
[0m18:49:59.964290 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.stg_subscriptions
[0m18:50:00.007120 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.stg_subscriptions"
[0m18:50:00.011116 [debug] [Thread-1 (]: On model.data_pipeline_project.stg_subscriptions: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.stg_subscriptions"} */


  create or replace view `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_subscriptions`
  OPTIONS()
  as with source as (

    select * from `data-pipeline-project-474812`.`raw_data`.`subscriptions`

),

renamed_and_cleaned as (

    select

        -- Deduplication logic: Prioritize the latest record (newest created_at)
        row_number() over (partition by _id order by createdAt desc) as rn,

        -- Primary Key
        _id as subscription_id,

        -- Foreign Key
        _user as user_id,

        -- Standardized Timestamps
        cast(createdAt as timestamp) as created_at,
        cast(nextOrderDate as timestamp) as next_order_date,
        cast(startDate as timestamp) as start_date,

        -- Metrics
        cast(totalQuantity as integer) as total_quantity,

        -- Flags (Casting to BOOLEAN for clean usage)
        cast(isActive as boolean) as is_active,
        cast(isSkip as boolean) as is_skip,

        -- Complex/JSON Field (Selected as raw string for later parsing)
        products as products_data

    from source
    where _id is not null
    -- Simple cleaning: Ensure start_date and quantity are valid
    and startDate is not null
    and totalQuantity is not null and totalQuantity >= 0
)

select
    subscription_id,
    user_id,
    created_at,
    next_order_date,
    start_date,
    total_quantity,
    is_active,
    is_skip,
    products_json_data
from renamed_and_cleaned
-- Final selection: only include the latest, unique instance
where rn = 1;


[0m18:50:00.014506 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m18:50:01.160758 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:cfd34f38-bed8-4640-a643-c603c0631f7f&page=queryresults
[0m18:50:01.471615 [error] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:cfd34f38-bed8-4640-a643-c603c0631f7f&page=queryresults
[0m18:50:01.481202 [debug] [Thread-1 (]: Database Error in model stg_subscriptions (models/staging/stg_subscriptions.sql)
  Unrecognized name: products_json_data; Did you mean products_data? at [56:5]
  compiled code at target/run/data_pipeline_project/models/staging/stg_subscriptions.sql
[0m18:50:01.483524 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '740021d0-0001-413c-8948-33a4d434cea1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71da03cc3c80>]}
[0m18:50:01.484675 [error] [Thread-1 (]: 1 of 1 ERROR creating sql view model analytics_staging_staging.stg_subscriptions  [[31mERROR[0m in 1.53s]
[0m18:50:01.485426 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.stg_subscriptions
[0m18:50:01.486027 [debug] [Thread-7 (]: Marking all children of 'model.data_pipeline_project.stg_subscriptions' to be skipped because of status 'error'.  Reason: Database Error in model stg_subscriptions (models/staging/stg_subscriptions.sql)
  Unrecognized name: products_json_data; Did you mean products_data? at [56:5]
  compiled code at target/run/data_pipeline_project/models/staging/stg_subscriptions.sql.
[0m18:50:01.492137 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m18:50:01.494790 [debug] [MainThread]: Connection 'master' was properly closed.
[0m18:50:01.495301 [debug] [MainThread]: Connection 'model.data_pipeline_project.stg_subscriptions' was properly closed.
[0m18:50:01.496007 [info ] [MainThread]: 
[0m18:50:01.497238 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 3.75 seconds (3.75s).
[0m18:50:01.501398 [debug] [MainThread]: Command end result
[0m18:50:01.528894 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m18:50:01.536703 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m18:50:01.550461 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m18:50:01.551273 [info ] [MainThread]: 
[0m18:50:01.556996 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m18:50:01.558018 [info ] [MainThread]: 
[0m18:50:01.561140 [error] [MainThread]: [31mFailure in model stg_subscriptions (models/staging/stg_subscriptions.sql)[0m
[0m18:50:01.561978 [error] [MainThread]:   Database Error in model stg_subscriptions (models/staging/stg_subscriptions.sql)
  Unrecognized name: products_json_data; Did you mean products_data? at [56:5]
  compiled code at target/run/data_pipeline_project/models/staging/stg_subscriptions.sql
[0m18:50:01.562510 [info ] [MainThread]: 
[0m18:50:01.563503 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_project/models/staging/stg_subscriptions.sql
[0m18:50:01.564521 [info ] [MainThread]: 
[0m18:50:01.565442 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m18:50:01.568272 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 7.326636, "process_in_blocks": "0", "process_kernel_time": 1.225698, "process_mem_max_rss": "378068", "process_out_blocks": "3264", "process_user_time": 4.616929}
[0m18:50:01.569390 [debug] [MainThread]: Command `dbt run` failed at 18:50:01.569249 after 7.33 seconds
[0m18:50:01.570381 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71da048b8470>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71da046d3050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71da24f6a360>]}
[0m18:50:01.571662 [debug] [MainThread]: Flushing usage events
[0m18:50:02.741405 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m18:50:26.105736 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0f02117530>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0f02a7b830>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0f0244dd00>]}


============================== 18:50:26.108823 | bbfa8a7d-cf2c-49a8-9b2e-ed88592612ba ==============================
[0m18:50:26.108823 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m18:50:26.110110 [debug] [MainThread]: running dbt with arguments {'introspect': 'True', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'target_path': 'None', 'partial_parse': 'True', 'no_print': 'None', 'empty': 'False', 'warn_error': 'None', 'cache_selected_only': 'False', 'log_cache_events': 'False', 'invocation_command': 'dbt run --select stg_subscriptions', 'quiet': 'False', 'indirect_selection': 'eager', 'debug': 'False', 'static_parser': 'True', 'use_colors': 'True', 'log_format': 'default', 'fail_fast': 'False', 'write_json': 'True', 'version_check': 'True', 'printer_width': '80', 'profiles_dir': '/home/ecem/.dbt', 'send_anonymous_usage_stats': 'True', 'use_experimental_parser': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])'}
[0m18:50:28.513025 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'bbfa8a7d-cf2c-49a8-9b2e-ed88592612ba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0f045abc80>]}
[0m18:50:28.588193 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'bbfa8a7d-cf2c-49a8-9b2e-ed88592612ba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0ee3499a00>]}
[0m18:50:28.590323 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m18:50:28.848625 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m18:50:29.108209 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m18:50:29.109767 [debug] [MainThread]: Partial parsing: updated file: data_pipeline_project://models/staging/stg_subscriptions.sql
[0m18:50:29.374683 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.data_pipeline_project.marts
[0m18:50:29.393873 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'bbfa8a7d-cf2c-49a8-9b2e-ed88592612ba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0ee21f4e00>]}
[0m18:50:29.512303 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m18:50:29.515092 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m18:50:29.533775 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'bbfa8a7d-cf2c-49a8-9b2e-ed88592612ba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0ee1850770>]}
[0m18:50:29.534932 [info ] [MainThread]: Found 3 models, 10 sources, 508 macros
[0m18:50:29.535521 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bbfa8a7d-cf2c-49a8-9b2e-ed88592612ba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0ee22ccf20>]}
[0m18:50:29.537297 [info ] [MainThread]: 
[0m18:50:29.538172 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m18:50:29.539123 [info ] [MainThread]: 
[0m18:50:29.539786 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m18:50:29.541651 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812'
[0m18:50:29.542472 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:50:30.718696 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812, now list_data-pipeline-project-474812_analytics_staging_staging)
[0m18:50:30.719352 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:50:31.624369 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bbfa8a7d-cf2c-49a8-9b2e-ed88592612ba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0ee266bd70>]}
[0m18:50:31.625138 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:50:31.636258 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.stg_subscriptions
[0m18:50:31.637823 [info ] [Thread-1 (]: 1 of 1 START sql view model analytics_staging_staging.stg_subscriptions ........ [RUN]
[0m18:50:31.639374 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now model.data_pipeline_project.stg_subscriptions)
[0m18:50:31.640057 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.stg_subscriptions
[0m18:50:31.652132 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.stg_subscriptions"
[0m18:50:31.653769 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.stg_subscriptions
[0m18:50:31.697573 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.stg_subscriptions"
[0m18:50:31.699378 [debug] [Thread-1 (]: On model.data_pipeline_project.stg_subscriptions: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.stg_subscriptions"} */


  create or replace view `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_subscriptions`
  OPTIONS()
  as with source as (

    select * from `data-pipeline-project-474812`.`raw_data`.`subscriptions`

),

renamed_and_cleaned as (

    select

        -- Deduplication logic: Prioritize the latest record (newest created_at)
        row_number() over (partition by _id order by createdAt desc) as rn,

        -- Primary Key
        _id as subscription_id,

        -- Foreign Key
        _user as user_id,

        -- Standardized Timestamps
        cast(createdAt as timestamp) as created_at,
        cast(nextOrderDate as timestamp) as next_order_date,
        cast(startDate as timestamp) as start_date,

        -- Metrics
        cast(totalQuantity as integer) as total_quantity,

        -- Flags (Casting to BOOLEAN for clean usage)
        cast(isActive as boolean) as is_active,
        cast(isSkip as boolean) as is_skip,

        -- Complex/JSON Field (Selected as raw string for later parsing)
        products as products_data

    from source
    where _id is not null
    -- Simple cleaning: Ensure start_date and quantity are valid
    and startDate is not null
    and totalQuantity is not null and totalQuantity >= 0
)

select
    subscription_id,
    user_id,
    created_at,
    next_order_date,
    start_date,
    total_quantity,
    is_active,
    is_skip,
    products_data
from renamed_and_cleaned
-- Final selection: only include the latest, unique instance
where rn = 1;


[0m18:50:31.700334 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m18:50:32.878476 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:ff3ea85a-aa56-43ed-a027-74e252cc47bf&page=queryresults
[0m18:50:33.838024 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bbfa8a7d-cf2c-49a8-9b2e-ed88592612ba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0f02b87ad0>]}
[0m18:50:33.840146 [info ] [Thread-1 (]: 1 of 1 OK created sql view model analytics_staging_staging.stg_subscriptions ... [[32mCREATE VIEW (0 processed)[0m in 2.20s]
[0m18:50:33.843314 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.stg_subscriptions
[0m18:50:33.846714 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m18:50:33.848711 [debug] [MainThread]: Connection 'master' was properly closed.
[0m18:50:33.849160 [debug] [MainThread]: Connection 'model.data_pipeline_project.stg_subscriptions' was properly closed.
[0m18:50:33.849919 [info ] [MainThread]: 
[0m18:50:33.850399 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 4.31 seconds (4.31s).
[0m18:50:33.851553 [debug] [MainThread]: Command end result
[0m18:50:33.885466 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m18:50:33.888312 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m18:50:33.896109 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m18:50:33.897229 [info ] [MainThread]: 
[0m18:50:33.898025 [info ] [MainThread]: [32mCompleted successfully[0m
[0m18:50:33.899452 [info ] [MainThread]: 
[0m18:50:33.900449 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m18:50:33.901893 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 7.851034, "process_in_blocks": "0", "process_kernel_time": 1.14111, "process_mem_max_rss": "381952", "process_out_blocks": "3256", "process_user_time": 4.797264}
[0m18:50:33.902947 [debug] [MainThread]: Command `dbt run` succeeded at 18:50:33.902833 after 7.85 seconds
[0m18:50:33.903476 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0f02079d00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0ee22ff080>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0ee21f4b60>]}
[0m18:50:33.904147 [debug] [MainThread]: Flushing usage events
[0m18:50:35.829790 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m19:05:33.160040 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x786e9e832de0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x786ea0b925a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x786e9efee000>]}


============================== 19:05:33.166730 | 07bba43c-05ed-4011-b630-093d06382e48 ==============================
[0m19:05:33.166730 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m19:05:33.169016 [debug] [MainThread]: running dbt with arguments {'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'write_json': 'True', 'send_anonymous_usage_stats': 'True', 'profiles_dir': '/home/ecem/.dbt', 'use_colors': 'True', 'log_cache_events': 'False', 'version_check': 'True', 'quiet': 'False', 'partial_parse': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'fail_fast': 'False', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt run --select stg_shipments', 'indirect_selection': 'eager', 'warn_error': 'None', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'target_path': 'None', 'no_print': 'None'}
[0m19:05:35.703698 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '07bba43c-05ed-4011-b630-093d06382e48', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x786e80b8e480>]}
[0m19:05:35.770127 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '07bba43c-05ed-4011-b630-093d06382e48', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x786e7ea06000>]}
[0m19:05:35.771898 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m19:05:36.026441 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m19:05:36.284700 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
[0m19:05:36.285427 [debug] [MainThread]: Partial parsing: added file: data_pipeline_project://models/staging/stg_shipments.sql
[0m19:05:36.555098 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.data_pipeline_project.marts
[0m19:05:36.574017 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '07bba43c-05ed-4011-b630-093d06382e48', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x786e7e4f17f0>]}
[0m19:05:36.688017 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m19:05:36.693511 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m19:05:36.715077 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '07bba43c-05ed-4011-b630-093d06382e48', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x786e7db3c350>]}
[0m19:05:36.716216 [info ] [MainThread]: Found 4 models, 10 sources, 508 macros
[0m19:05:36.716926 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '07bba43c-05ed-4011-b630-093d06382e48', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x786e7e44f7a0>]}
[0m19:05:36.718460 [info ] [MainThread]: 
[0m19:05:36.719255 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m19:05:36.720066 [info ] [MainThread]: 
[0m19:05:36.721058 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m19:05:36.723237 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812'
[0m19:05:36.724328 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:05:37.892196 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812, now list_data-pipeline-project-474812_analytics_staging_staging)
[0m19:05:37.894130 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:05:38.769338 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '07bba43c-05ed-4011-b630-093d06382e48', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x786e7e5f96d0>]}
[0m19:05:38.770908 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:05:38.787527 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.stg_shipments
[0m19:05:38.789064 [info ] [Thread-1 (]: 1 of 1 START sql view model analytics_staging_staging.stg_shipments ............ [RUN]
[0m19:05:38.790132 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now model.data_pipeline_project.stg_shipments)
[0m19:05:38.790974 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.stg_shipments
[0m19:05:38.799079 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.stg_shipments"
[0m19:05:38.800225 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.stg_shipments
[0m19:05:38.838347 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.stg_shipments"
[0m19:05:38.841399 [debug] [Thread-1 (]: On model.data_pipeline_project.stg_shipments: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.stg_shipments"} */


  create or replace view `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_shipments`
  OPTIONS()
  as with source as (

    select * from `data-pipeline-project-474812`.`raw_data`.`shipments`

),

renamed_and_cleaned as (

    select

        -- Deduplication logic: Prioritize the latest record (newest created_at, if available, otherwise just use a distinct ID)
        -- Since this table is likely event-based, we prioritize the latest row for any given ID.
        row_number() over (partition by _id order by createdAt desc) as rn,

        -- Primary Key
        _id as shipment_id,

        -- Foreign Keys
        _order as order_id,
        _user as user_id,

        -- Standardized Timestamps
        -- Assuming 'createdAt' still exists as a standard column for the record's creation time
        cast(createdAt as timestamp) as created_at,

        -- Complex/JSON Fields (Selected as raw strings for later parsing in Intermediate models)
        details as details_json,
        label as label_json

    from source
    where _id is not null
    and _order is not null -- Ensure the shipment links to a valid order
)

select
    shipment_id,
    order_id,
    user_id,
    created_at,
    details_json,
    label_json
from renamed_and_cleaned
-- Final selection: only include the latest, unique instance
where rn = 1;


[0m19:05:38.842199 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m19:05:40.136599 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:c27f0dcd-5e80-4dc9-8a22-ac06de50f91d&page=queryresults
[0m19:05:40.524509 [error] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:c27f0dcd-5e80-4dc9-8a22-ac06de50f91d&page=queryresults
[0m19:05:40.532587 [debug] [Thread-1 (]: Database Error in model stg_shipments (models/staging/stg_shipments.sql)
  Unrecognized name: createdAt at [18:54]
  compiled code at target/run/data_pipeline_project/models/staging/stg_shipments.sql
[0m19:05:40.535807 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '07bba43c-05ed-4011-b630-093d06382e48', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x786e9ef4f920>]}
[0m19:05:40.537322 [error] [Thread-1 (]: 1 of 1 ERROR creating sql view model analytics_staging_staging.stg_shipments ... [[31mERROR[0m in 1.74s]
[0m19:05:40.538744 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.stg_shipments
[0m19:05:40.540087 [debug] [Thread-7 (]: Marking all children of 'model.data_pipeline_project.stg_shipments' to be skipped because of status 'error'.  Reason: Database Error in model stg_shipments (models/staging/stg_shipments.sql)
  Unrecognized name: createdAt at [18:54]
  compiled code at target/run/data_pipeline_project/models/staging/stg_shipments.sql.
[0m19:05:40.545886 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m19:05:40.548101 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:05:40.548713 [debug] [MainThread]: Connection 'model.data_pipeline_project.stg_shipments' was properly closed.
[0m19:05:40.549156 [info ] [MainThread]: 
[0m19:05:40.549735 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 3.83 seconds (3.83s).
[0m19:05:40.551148 [debug] [MainThread]: Command end result
[0m19:05:40.608479 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m19:05:40.612293 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m19:05:40.625978 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m19:05:40.627001 [info ] [MainThread]: 
[0m19:05:40.631506 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m19:05:40.632254 [info ] [MainThread]: 
[0m19:05:40.633244 [error] [MainThread]: [31mFailure in model stg_shipments (models/staging/stg_shipments.sql)[0m
[0m19:05:40.634057 [error] [MainThread]:   Database Error in model stg_shipments (models/staging/stg_shipments.sql)
  Unrecognized name: createdAt at [18:54]
  compiled code at target/run/data_pipeline_project/models/staging/stg_shipments.sql
[0m19:05:40.634624 [info ] [MainThread]: 
[0m19:05:40.636852 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_project/models/staging/stg_shipments.sql
[0m19:05:40.638425 [info ] [MainThread]: 
[0m19:05:40.640452 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m19:05:40.644203 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 7.551565, "process_in_blocks": "0", "process_kernel_time": 0.990126, "process_mem_max_rss": "378332", "process_out_blocks": "3296", "process_user_time": 5.073372}
[0m19:05:40.645116 [debug] [MainThread]: Command `dbt run` failed at 19:05:40.644967 after 7.55 seconds
[0m19:05:40.645721 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x786e9ebf8920>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x786e7db3fc50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x786ea079e510>]}
[0m19:05:40.646637 [debug] [MainThread]: Flushing usage events
[0m19:05:41.750494 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m19:08:29.756376 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdcd0189370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdcd00a1640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdcd08388c0>]}


============================== 19:08:29.760132 | 7ad5a064-8cb9-4893-a36e-e53aad03a57d ==============================
[0m19:08:29.760132 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m19:08:29.761494 [debug] [MainThread]: running dbt with arguments {'introspect': 'True', 'empty': 'False', 'fail_fast': 'False', 'send_anonymous_usage_stats': 'True', 'log_format': 'default', 'target_path': 'None', 'profiles_dir': '/home/ecem/.dbt', 'write_json': 'True', 'use_colors': 'True', 'printer_width': '80', 'cache_selected_only': 'False', 'static_parser': 'True', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'version_check': 'True', 'no_print': 'None', 'quiet': 'False', 'warn_error': 'None', 'invocation_command': 'dbt run --select stg_shipments', 'partial_parse': 'True', 'indirect_selection': 'eager', 'debug': 'False', 'log_cache_events': 'False', 'use_experimental_parser': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])'}
[0m19:08:32.133863 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7ad5a064-8cb9-4893-a36e-e53aad03a57d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdcb0727b30>]}
[0m19:08:32.200257 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7ad5a064-8cb9-4893-a36e-e53aad03a57d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdcb09cf860>]}
[0m19:08:32.201358 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m19:08:32.465577 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m19:08:32.711429 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m19:08:32.712831 [debug] [MainThread]: Partial parsing: updated file: data_pipeline_project://models/staging/stg_shipments.sql
[0m19:08:32.987103 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.data_pipeline_project.marts
[0m19:08:33.003001 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7ad5a064-8cb9-4893-a36e-e53aad03a57d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdcafee96a0>]}
[0m19:08:33.113622 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m19:08:33.117415 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m19:08:33.133539 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7ad5a064-8cb9-4893-a36e-e53aad03a57d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdcafeea8d0>]}
[0m19:08:33.134437 [info ] [MainThread]: Found 4 models, 10 sources, 508 macros
[0m19:08:33.135176 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7ad5a064-8cb9-4893-a36e-e53aad03a57d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdcb0202570>]}
[0m19:08:33.136896 [info ] [MainThread]: 
[0m19:08:33.137692 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m19:08:33.138607 [info ] [MainThread]: 
[0m19:08:33.139425 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m19:08:33.141159 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812'
[0m19:08:33.141864 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:08:34.107868 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812, now list_data-pipeline-project-474812_analytics_staging_staging)
[0m19:08:34.109427 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:08:36.388595 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7ad5a064-8cb9-4893-a36e-e53aad03a57d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdcafe2dbb0>]}
[0m19:08:36.391121 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:08:36.404814 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.stg_shipments
[0m19:08:36.406192 [info ] [Thread-1 (]: 1 of 1 START sql view model analytics_staging_staging.stg_shipments ............ [RUN]
[0m19:08:36.407100 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now model.data_pipeline_project.stg_shipments)
[0m19:08:36.408054 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.stg_shipments
[0m19:08:36.416971 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.stg_shipments"
[0m19:08:36.418010 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.stg_shipments
[0m19:08:36.474471 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.stg_shipments"
[0m19:08:36.478231 [debug] [Thread-1 (]: On model.data_pipeline_project.stg_shipments: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.stg_shipments"} */


  create or replace view `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_shipments`
  OPTIONS()
  as with source as (

    select * from `data-pipeline-project-474812`.`raw_data`.`shipments`

),

renamed_and_cleaned as (

    select

        -- Deduplication logic: Prioritize the latest record (newest created_at, if available, otherwise just use a distinct ID)
        -- Since this table is likely event-based, we prioritize the latest row for any given ID.
        row_number() over (partition by _id order by collectDate desc) as rn,

        -- Primary Key
        _id as shipment_id,

        -- Foreign Keys
        _order as order_id,
        _user as user_id,

        -- Standardized Timestamps
        -- Assuming 'createdAt' still exists as a standard column for the record's creation time
        cast(collectDate as timestamp) as collected_at,

        -- Complex/JSON Fields (Selected as raw strings for later parsing in Intermediate models)
        details as details_json,
        label as label_json

    from source
    where _id is not null
    and _order is not null -- Ensure the shipment links to a valid order
)

select
    shipment_id,
    order_id,
    user_id,
    collected_at,
    details_json,
    label_json
from renamed_and_cleaned
-- Final selection: only include the latest, unique instance
where rn = 1;


[0m19:08:36.479302 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m19:08:37.961407 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:045087dc-6dfd-47b9-8ffd-b477552296db&page=queryresults
[0m19:08:38.673318 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7ad5a064-8cb9-4893-a36e-e53aad03a57d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdcb3148830>]}
[0m19:08:38.677018 [info ] [Thread-1 (]: 1 of 1 OK created sql view model analytics_staging_staging.stg_shipments ....... [[32mCREATE VIEW (0 processed)[0m in 2.26s]
[0m19:08:38.679239 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.stg_shipments
[0m19:08:38.682176 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m19:08:38.684290 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:08:38.685038 [debug] [MainThread]: Connection 'model.data_pipeline_project.stg_shipments' was properly closed.
[0m19:08:38.685558 [info ] [MainThread]: 
[0m19:08:38.686208 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 5.55 seconds (5.55s).
[0m19:08:38.687291 [debug] [MainThread]: Command end result
[0m19:08:38.717000 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m19:08:38.719149 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m19:08:38.725343 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m19:08:38.725886 [info ] [MainThread]: 
[0m19:08:38.726496 [info ] [MainThread]: [32mCompleted successfully[0m
[0m19:08:38.726885 [info ] [MainThread]: 
[0m19:08:38.727310 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m19:08:38.729483 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 9.02989, "process_in_blocks": "0", "process_kernel_time": 1.137145, "process_mem_max_rss": "381960", "process_out_blocks": "3296", "process_user_time": 4.738104}
[0m19:08:38.731110 [debug] [MainThread]: Command `dbt run` succeeded at 19:08:38.730962 after 9.03 seconds
[0m19:08:38.731789 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdcafee9a00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdccf9bb7a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdccf9bb560>]}
[0m19:08:38.733068 [debug] [MainThread]: Flushing usage events
[0m19:08:46.189192 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m19:35:15.388643 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d9aa41b92e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d9aa43b39e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d9aa43b12b0>]}


============================== 19:35:15.393259 | 20f5033c-239b-4d94-b9b4-7c5d83ad5ff1 ==============================
[0m19:35:15.393259 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m19:35:15.394122 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'empty': 'False', 'printer_width': '80', 'partial_parse': 'True', 'indirect_selection': 'eager', 'cache_selected_only': 'False', 'use_colors': 'True', 'send_anonymous_usage_stats': 'True', 'introspect': 'True', 'profiles_dir': '/home/ecem/.dbt', 'debug': 'False', 'target_path': 'None', 'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'quiet': 'False', 'warn_error': 'None', 'no_print': 'None', 'static_parser': 'True', 'log_format': 'default', 'log_cache_events': 'False', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'write_json': 'True', 'use_experimental_parser': 'False', 'invocation_command': 'dbt run --select stg_marketing_spend'}
[0m19:35:17.897222 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '20f5033c-239b-4d94-b9b4-7c5d83ad5ff1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d9aa49226f0>]}
[0m19:35:17.980659 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '20f5033c-239b-4d94-b9b4-7c5d83ad5ff1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d9a8465c170>]}
[0m19:35:17.986030 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m19:35:18.194636 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m19:35:18.483928 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
[0m19:35:18.488248 [debug] [MainThread]: Partial parsing: added file: data_pipeline_project://models/staging/stg_marketing_spend.sql
[0m19:35:18.755005 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.data_pipeline_project.marts
[0m19:35:18.771581 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '20f5033c-239b-4d94-b9b4-7c5d83ad5ff1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d9a83eedc10>]}
[0m19:35:18.904359 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m19:35:18.911439 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m19:35:18.927233 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '20f5033c-239b-4d94-b9b4-7c5d83ad5ff1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d9a84548cb0>]}
[0m19:35:18.927978 [info ] [MainThread]: Found 5 models, 10 sources, 508 macros
[0m19:35:18.928732 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '20f5033c-239b-4d94-b9b4-7c5d83ad5ff1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d9aa5813b90>]}
[0m19:35:18.931045 [info ] [MainThread]: 
[0m19:35:18.932339 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m19:35:18.933328 [info ] [MainThread]: 
[0m19:35:18.934631 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m19:35:18.936896 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812'
[0m19:35:18.937615 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:35:20.328373 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812, now list_data-pipeline-project-474812_analytics_staging_staging)
[0m19:35:20.329940 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:35:21.604262 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '20f5033c-239b-4d94-b9b4-7c5d83ad5ff1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d9aa3863c80>]}
[0m19:35:21.606377 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:35:21.623062 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.stg_marketing_spend
[0m19:35:21.624361 [info ] [Thread-1 (]: 1 of 1 START sql view model analytics_staging_staging.stg_marketing_spend ...... [RUN]
[0m19:35:21.625504 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now model.data_pipeline_project.stg_marketing_spend)
[0m19:35:21.626260 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.stg_marketing_spend
[0m19:35:21.637757 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.stg_marketing_spend"
[0m19:35:21.638853 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.stg_marketing_spend
[0m19:35:21.682002 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.stg_marketing_spend"
[0m19:35:21.683278 [debug] [Thread-1 (]: On model.data_pipeline_project.stg_marketing_spend: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.stg_marketing_spend"} */


  create or replace view `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_marketing_spend`
  OPTIONS()
  as with source as (

    select * from `data-pipeline-project-474812`.`raw_data`.`marketing_spend`

),

-- This step unpivots the data from a wide format (date as column) 
-- into a long format (date as row).
unpivot_spend as (

    select
        -- The channel column is already clean
        Channel as marketing_channel,
        
        -- 'spend_date_raw' captures the name of the column (the date string)
        spend_date_raw, 
        
        -- 'spend' captures the value in that column
        cast(spend as numeric) as spend_amount_try

    from source

    -- BigQuery's UNPIVOT clause explicitly lists all date columns to melt
    unpivot(spend for spend_date_raw in (
        -- Listing the date columns EXACTLY as they appear in your raw table:
        "2025_09_2", 
        "2025_09_22", 
        "2025_09_27"
        -- NOTE: If you have more columns, add them here!
    ))

),

-- Final selection and date formatting
final_conversion as (

    select
        -- Convert the string date into a proper date object
        parse_date('%Y_%m_%d', spend_date_raw) as spend_date,
        
        marketing_channel,
        spend_amount_try
    
    from unpivot_spend
    where spend_amount_try is not null and spend_amount_try > 0

)

select * from final_conversion;


[0m19:35:21.683890 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m19:35:23.221071 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:e67ce1ec-89b0-4aa0-8323-8521399aa242&page=queryresults
[0m19:35:23.222412 [error] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:e67ce1ec-89b0-4aa0-8323-8521399aa242&page=queryresults
[0m19:35:23.232763 [debug] [Thread-1 (]: Database Error in model stg_marketing_spend (models/staging/stg_marketing_spend.sql)
  Syntax error: Unexpected string literal "2025_09_2" at [31:9]
  compiled code at target/run/data_pipeline_project/models/staging/stg_marketing_spend.sql
[0m19:35:23.234951 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '20f5033c-239b-4d94-b9b4-7c5d83ad5ff1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d9aa487fa70>]}
[0m19:35:23.236449 [error] [Thread-1 (]: 1 of 1 ERROR creating sql view model analytics_staging_staging.stg_marketing_spend  [[31mERROR[0m in 1.61s]
[0m19:35:23.237573 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.stg_marketing_spend
[0m19:35:23.238522 [debug] [Thread-7 (]: Marking all children of 'model.data_pipeline_project.stg_marketing_spend' to be skipped because of status 'error'.  Reason: Database Error in model stg_marketing_spend (models/staging/stg_marketing_spend.sql)
  Syntax error: Unexpected string literal "2025_09_2" at [31:9]
  compiled code at target/run/data_pipeline_project/models/staging/stg_marketing_spend.sql.
[0m19:35:23.243478 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m19:35:23.245712 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:35:23.246427 [debug] [MainThread]: Connection 'model.data_pipeline_project.stg_marketing_spend' was properly closed.
[0m19:35:23.246828 [info ] [MainThread]: 
[0m19:35:23.247524 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 4.31 seconds (4.31s).
[0m19:35:23.248292 [debug] [MainThread]: Command end result
[0m19:35:23.281219 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m19:35:23.286242 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m19:35:23.292692 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m19:35:23.293351 [info ] [MainThread]: 
[0m19:35:23.293981 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m19:35:23.294523 [info ] [MainThread]: 
[0m19:35:23.295123 [error] [MainThread]: [31mFailure in model stg_marketing_spend (models/staging/stg_marketing_spend.sql)[0m
[0m19:35:23.295745 [error] [MainThread]:   Database Error in model stg_marketing_spend (models/staging/stg_marketing_spend.sql)
  Syntax error: Unexpected string literal "2025_09_2" at [31:9]
  compiled code at target/run/data_pipeline_project/models/staging/stg_marketing_spend.sql
[0m19:35:23.297049 [info ] [MainThread]: 
[0m19:35:23.298233 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_project/models/staging/stg_marketing_spend.sql
[0m19:35:23.298875 [info ] [MainThread]: 
[0m19:35:23.299552 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m19:35:23.301798 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 7.975719, "process_in_blocks": "0", "process_kernel_time": 1.161439, "process_mem_max_rss": "378404", "process_out_blocks": "3312", "process_user_time": 4.756191}
[0m19:35:23.305508 [debug] [MainThread]: Command `dbt run` failed at 19:35:23.305331 after 7.98 seconds
[0m19:35:23.306311 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d9aa3eb75f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d9aa48a1b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d9a840689b0>]}
[0m19:35:23.307112 [debug] [MainThread]: Flushing usage events
[0m19:35:24.678676 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m19:38:56.710911 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x789493bb7b30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x789494122030>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x789493e535f0>]}


============================== 19:38:56.715120 | 21913249-08a2-4608-b529-7f298bb70fb7 ==============================
[0m19:38:56.715120 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m19:38:56.716649 [debug] [MainThread]: running dbt with arguments {'invocation_command': 'dbt run --select stg_marketing_spend', 'fail_fast': 'False', 'static_parser': 'True', 'no_print': 'None', 'introspect': 'True', 'empty': 'False', 'profiles_dir': '/home/ecem/.dbt', 'partial_parse': 'True', 'printer_width': '80', 'quiet': 'False', 'log_format': 'default', 'log_cache_events': 'False', 'cache_selected_only': 'False', 'write_json': 'True', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'use_experimental_parser': 'False', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'send_anonymous_usage_stats': 'True', 'warn_error': 'None', 'debug': 'False', 'target_path': 'None', 'indirect_selection': 'eager', 'use_colors': 'True'}
[0m19:38:59.082752 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '21913249-08a2-4608-b529-7f298bb70fb7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x789473b406b0>]}
[0m19:38:59.151492 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '21913249-08a2-4608-b529-7f298bb70fb7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x789473e032c0>]}
[0m19:38:59.152433 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m19:38:59.430686 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m19:38:59.669922 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m19:38:59.672093 [debug] [MainThread]: Partial parsing: updated file: data_pipeline_project://models/staging/stg_marketing_spend.sql
[0m19:38:59.946340 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.data_pipeline_project.marts
[0m19:38:59.964163 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '21913249-08a2-4608-b529-7f298bb70fb7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x789473654920>]}
[0m19:39:00.080343 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m19:39:00.083000 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m19:39:00.103275 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '21913249-08a2-4608-b529-7f298bb70fb7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7894736fb6b0>]}
[0m19:39:00.104335 [info ] [MainThread]: Found 5 models, 10 sources, 508 macros
[0m19:39:00.105097 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '21913249-08a2-4608-b529-7f298bb70fb7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78947691b7d0>]}
[0m19:39:00.106835 [info ] [MainThread]: 
[0m19:39:00.107838 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m19:39:00.108418 [info ] [MainThread]: 
[0m19:39:00.109305 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m19:39:00.111083 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812'
[0m19:39:00.111732 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:39:01.256735 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812, now list_data-pipeline-project-474812_analytics_staging_staging)
[0m19:39:01.257674 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:39:02.439828 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '21913249-08a2-4608-b529-7f298bb70fb7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x789472d59af0>]}
[0m19:39:02.441458 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:39:02.452963 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.stg_marketing_spend
[0m19:39:02.454107 [info ] [Thread-1 (]: 1 of 1 START sql view model analytics_staging_staging.stg_marketing_spend ...... [RUN]
[0m19:39:02.455174 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now model.data_pipeline_project.stg_marketing_spend)
[0m19:39:02.455891 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.stg_marketing_spend
[0m19:39:02.465328 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.stg_marketing_spend"
[0m19:39:02.467430 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.stg_marketing_spend
[0m19:39:02.514636 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.stg_marketing_spend"
[0m19:39:02.516669 [debug] [Thread-1 (]: On model.data_pipeline_project.stg_marketing_spend: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.stg_marketing_spend"} */


  create or replace view `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_marketing_spend`
  OPTIONS()
  as with source as (

    select * from `data-pipeline-project-474812`.`raw_data`.`marketing_spend`

),

unpivot_spend as (

    select
        Channel as marketing_channel,
        spend_date_raw, 
        cast(spend as numeric) as spend_amount_try

    from source

    -- ✅ CORRECT: All columns with proper commas and backticks
    unpivot(spend for spend_date_raw in (
        `2025_09_20`, 
        `2025_09_21`, 
        `2025_09_22`,
        `2025_09_23`,
        `2025_09_24`,
        `2025_09_25`,
        `2025_09_26`,
        `2025_09_27`
    ))

),

final_conversion as (

    select
        parse_date('%Y_%m_%d', spend_date_raw) as spend_date,
        marketing_channel,
        spend_amount_try
    
    from unpivot_spend
    where spend_amount_try is not null and spend_amount_try > 0

)

select * from final_conversion;


[0m19:39:02.517860 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m19:39:03.847371 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:84a005fe-25b4-4a2f-9d8f-014332ce208c&page=queryresults
[0m19:39:04.179628 [error] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:84a005fe-25b4-4a2f-9d8f-014332ce208c&page=queryresults
[0m19:39:04.192437 [debug] [Thread-1 (]: Database Error in model stg_marketing_spend (models/staging/stg_marketing_spend.sql)
  Unrecognized name: `2025_09_20`; Did you mean 2025-09-20? at [23:9]
  compiled code at target/run/data_pipeline_project/models/staging/stg_marketing_spend.sql
[0m19:39:04.194947 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '21913249-08a2-4608-b529-7f298bb70fb7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x789472de44a0>]}
[0m19:39:04.196081 [error] [Thread-1 (]: 1 of 1 ERROR creating sql view model analytics_staging_staging.stg_marketing_spend  [[31mERROR[0m in 1.74s]
[0m19:39:04.197047 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.stg_marketing_spend
[0m19:39:04.198022 [debug] [Thread-7 (]: Marking all children of 'model.data_pipeline_project.stg_marketing_spend' to be skipped because of status 'error'.  Reason: Database Error in model stg_marketing_spend (models/staging/stg_marketing_spend.sql)
  Unrecognized name: `2025_09_20`; Did you mean 2025-09-20? at [23:9]
  compiled code at target/run/data_pipeline_project/models/staging/stg_marketing_spend.sql.
[0m19:39:04.206609 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m19:39:04.209358 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:39:04.211904 [debug] [MainThread]: Connection 'model.data_pipeline_project.stg_marketing_spend' was properly closed.
[0m19:39:04.213038 [info ] [MainThread]: 
[0m19:39:04.213997 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 4.10 seconds (4.10s).
[0m19:39:04.215347 [debug] [MainThread]: Command end result
[0m19:39:04.250955 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m19:39:04.257854 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m19:39:04.270002 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m19:39:04.270668 [info ] [MainThread]: 
[0m19:39:04.271435 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m19:39:04.272180 [info ] [MainThread]: 
[0m19:39:04.273790 [error] [MainThread]: [31mFailure in model stg_marketing_spend (models/staging/stg_marketing_spend.sql)[0m
[0m19:39:04.274820 [error] [MainThread]:   Database Error in model stg_marketing_spend (models/staging/stg_marketing_spend.sql)
  Unrecognized name: `2025_09_20`; Did you mean 2025-09-20? at [23:9]
  compiled code at target/run/data_pipeline_project/models/staging/stg_marketing_spend.sql
[0m19:39:04.275773 [info ] [MainThread]: 
[0m19:39:04.277187 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_project/models/staging/stg_marketing_spend.sql
[0m19:39:04.278446 [info ] [MainThread]: 
[0m19:39:04.279168 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m19:39:04.281581 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 7.6311903, "process_in_blocks": "0", "process_kernel_time": 0.893152, "process_mem_max_rss": "378368", "process_out_blocks": "3296", "process_user_time": 4.984365}
[0m19:39:04.283241 [debug] [MainThread]: Command `dbt run` failed at 19:39:04.282471 after 7.63 seconds
[0m19:39:04.284343 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78949360fa40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x789473870470>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x789473b8d9d0>]}
[0m19:39:04.285083 [debug] [MainThread]: Flushing usage events
[0m19:39:05.772570 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m19:40:14.421272 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ca9d0f779e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ca9d0c7fef0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ca9d0938290>]}


============================== 19:40:14.424867 | f5ca1399-ddf3-480b-a266-31af008b200b ==============================
[0m19:40:14.424867 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m19:40:14.425984 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'send_anonymous_usage_stats': 'True', 'use_colors': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'quiet': 'False', 'write_json': 'True', 'log_format': 'default', 'empty': 'False', 'invocation_command': 'dbt run --select stg_marketing_spend', 'cache_selected_only': 'False', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'static_parser': 'True', 'printer_width': '80', 'warn_error': 'None', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'profiles_dir': '/home/ecem/.dbt', 'no_print': 'None', 'target_path': 'None', 'debug': 'False', 'use_experimental_parser': 'False', 'partial_parse': 'True'}
[0m19:40:16.818557 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f5ca1399-ddf3-480b-a266-31af008b200b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ca9d14c3ad0>]}
[0m19:40:16.889877 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f5ca1399-ddf3-480b-a266-31af008b200b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ca9b15a7170>]}
[0m19:40:16.892451 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m19:40:17.142482 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m19:40:17.394030 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m19:40:17.395393 [debug] [MainThread]: Partial parsing: updated file: data_pipeline_project://models/staging/stg_marketing_spend.sql
[0m19:40:17.666676 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.data_pipeline_project.marts
[0m19:40:17.682555 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f5ca1399-ddf3-480b-a266-31af008b200b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ca9b0acdfa0>]}
[0m19:40:17.790383 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m19:40:17.792998 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m19:40:17.807513 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f5ca1399-ddf3-480b-a266-31af008b200b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ca9b0d47500>]}
[0m19:40:17.808446 [info ] [MainThread]: Found 5 models, 10 sources, 508 macros
[0m19:40:17.809520 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f5ca1399-ddf3-480b-a266-31af008b200b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ca9b0d28650>]}
[0m19:40:17.811395 [info ] [MainThread]: 
[0m19:40:17.812114 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m19:40:17.812640 [info ] [MainThread]: 
[0m19:40:17.813562 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m19:40:17.815193 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812'
[0m19:40:17.816212 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:40:19.363877 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812, now list_data-pipeline-project-474812_analytics_staging_staging)
[0m19:40:19.365281 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:40:20.947325 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f5ca1399-ddf3-480b-a266-31af008b200b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ca9b01f9670>]}
[0m19:40:20.951623 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:40:20.963862 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.stg_marketing_spend
[0m19:40:20.965149 [info ] [Thread-1 (]: 1 of 1 START sql view model analytics_staging_staging.stg_marketing_spend ...... [RUN]
[0m19:40:20.965968 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now model.data_pipeline_project.stg_marketing_spend)
[0m19:40:20.966507 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.stg_marketing_spend
[0m19:40:20.973759 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.stg_marketing_spend"
[0m19:40:20.976062 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.stg_marketing_spend
[0m19:40:21.021011 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.stg_marketing_spend"
[0m19:40:21.022416 [debug] [Thread-1 (]: On model.data_pipeline_project.stg_marketing_spend: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.stg_marketing_spend"} */


  create or replace view `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_marketing_spend`
  OPTIONS()
  as with source as (

    select * from `data-pipeline-project-474812`.`raw_data`.`marketing_spend`

),

unpivot_spend as (

    select
        Channel as marketing_channel,
        spend_date_raw, 
        cast(spend as numeric) as spend_amount_try

    from source

    -- ✅ CORRECT: All columns with proper commas and backticks
    unpivot(spend for spend_date_raw in (
        2025_09_20, 
        2025_09_21, 
        2025_09_22,
        2025_09_23,
        2025_09_24,
        2025_09_25,
        2025_09_26,
        2025_09_27
    ))

),

final_conversion as (

    select
        parse_date('%Y_%m_%d', spend_date_raw) as spend_date,
        marketing_channel,
        spend_amount_try
    
    from unpivot_spend
    where spend_amount_try is not null and spend_amount_try > 0

)

select * from final_conversion;


[0m19:40:21.023966 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m19:40:23.331288 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:03ac3970-bdc5-4029-aa2c-695f2b26b12b&page=queryresults
[0m19:40:23.334974 [error] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:03ac3970-bdc5-4029-aa2c-695f2b26b12b&page=queryresults
[0m19:40:23.342509 [debug] [Thread-1 (]: Database Error in model stg_marketing_spend (models/staging/stg_marketing_spend.sql)
  Syntax error: Unexpected integer literal "2025" at [23:9]
  compiled code at target/run/data_pipeline_project/models/staging/stg_marketing_spend.sql
[0m19:40:23.345643 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f5ca1399-ddf3-480b-a266-31af008b200b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ca9d149f740>]}
[0m19:40:23.346689 [error] [Thread-1 (]: 1 of 1 ERROR creating sql view model analytics_staging_staging.stg_marketing_spend  [[31mERROR[0m in 2.38s]
[0m19:40:23.347983 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.stg_marketing_spend
[0m19:40:23.349277 [debug] [Thread-7 (]: Marking all children of 'model.data_pipeline_project.stg_marketing_spend' to be skipped because of status 'error'.  Reason: Database Error in model stg_marketing_spend (models/staging/stg_marketing_spend.sql)
  Syntax error: Unexpected integer literal "2025" at [23:9]
  compiled code at target/run/data_pipeline_project/models/staging/stg_marketing_spend.sql.
[0m19:40:23.353592 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m19:40:23.355351 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:40:23.356962 [debug] [MainThread]: Connection 'model.data_pipeline_project.stg_marketing_spend' was properly closed.
[0m19:40:23.357622 [info ] [MainThread]: 
[0m19:40:23.358100 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 5.54 seconds (5.54s).
[0m19:40:23.358827 [debug] [MainThread]: Command end result
[0m19:40:23.396509 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m19:40:23.399118 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m19:40:23.418844 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m19:40:23.421261 [info ] [MainThread]: 
[0m19:40:23.423109 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m19:40:23.423913 [info ] [MainThread]: 
[0m19:40:23.424571 [error] [MainThread]: [31mFailure in model stg_marketing_spend (models/staging/stg_marketing_spend.sql)[0m
[0m19:40:23.425060 [error] [MainThread]:   Database Error in model stg_marketing_spend (models/staging/stg_marketing_spend.sql)
  Syntax error: Unexpected integer literal "2025" at [23:9]
  compiled code at target/run/data_pipeline_project/models/staging/stg_marketing_spend.sql
[0m19:40:23.425454 [info ] [MainThread]: 
[0m19:40:23.426760 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_project/models/staging/stg_marketing_spend.sql
[0m19:40:23.427749 [info ] [MainThread]: 
[0m19:40:23.428467 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m19:40:23.429882 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 9.068407, "process_in_blocks": "0", "process_kernel_time": 0.967503, "process_mem_max_rss": "378364", "process_out_blocks": "3304", "process_user_time": 4.828981}
[0m19:40:23.430502 [debug] [MainThread]: Command `dbt run` failed at 19:40:23.430387 after 9.07 seconds
[0m19:40:23.431114 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ca9d0e841a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ca9b0d45b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ca9d22f0cb0>]}
[0m19:40:23.431705 [debug] [MainThread]: Flushing usage events
[0m19:40:24.540557 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m19:42:08.275269 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76ccf9dc9490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76ccfbed6f30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76ccf9530b90>]}


============================== 19:42:08.281155 | 756837ec-fede-48f2-abd2-506977e46aeb ==============================
[0m19:42:08.281155 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m19:42:08.282114 [debug] [MainThread]: running dbt with arguments {'warn_error': 'None', 'introspect': 'True', 'debug': 'False', 'target_path': 'None', 'write_json': 'True', 'no_print': 'None', 'use_experimental_parser': 'False', 'cache_selected_only': 'False', 'quiet': 'False', 'log_format': 'default', 'fail_fast': 'False', 'use_colors': 'True', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'printer_width': '80', 'log_cache_events': 'False', 'profiles_dir': '/home/ecem/.dbt', 'static_parser': 'True', 'invocation_command': 'dbt run --select stg_marketing_spend', 'send_anonymous_usage_stats': 'True', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'empty': 'False', 'indirect_selection': 'eager', 'partial_parse': 'True'}
[0m19:42:10.590777 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '756837ec-fede-48f2-abd2-506977e46aeb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76ccda1c3f20>]}
[0m19:42:10.655507 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '756837ec-fede-48f2-abd2-506977e46aeb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76ccfbba2240>]}
[0m19:42:10.656615 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m19:42:10.920466 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m19:42:11.171407 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m19:42:11.172458 [debug] [MainThread]: Partial parsing: updated file: data_pipeline_project://models/staging/stg_marketing_spend.sql
[0m19:42:11.426055 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.data_pipeline_project.marts
[0m19:42:11.440856 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '756837ec-fede-48f2-abd2-506977e46aeb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76ccd99fdd00>]}
[0m19:42:11.541908 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m19:42:11.545243 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m19:42:11.564380 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '756837ec-fede-48f2-abd2-506977e46aeb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76ccd903c380>]}
[0m19:42:11.565430 [info ] [MainThread]: Found 5 models, 10 sources, 508 macros
[0m19:42:11.566366 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '756837ec-fede-48f2-abd2-506977e46aeb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76ccd9bdb410>]}
[0m19:42:11.568612 [info ] [MainThread]: 
[0m19:42:11.569458 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m19:42:11.570126 [info ] [MainThread]: 
[0m19:42:11.571265 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m19:42:11.573260 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812'
[0m19:42:11.573896 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:42:12.753456 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812, now list_data-pipeline-project-474812_analytics_staging_staging)
[0m19:42:12.754872 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:42:14.651388 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '756837ec-fede-48f2-abd2-506977e46aeb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76ccd99f6a20>]}
[0m19:42:14.652122 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:42:14.662153 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.stg_marketing_spend
[0m19:42:14.663353 [info ] [Thread-1 (]: 1 of 1 START sql view model analytics_staging_staging.stg_marketing_spend ...... [RUN]
[0m19:42:14.664282 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now model.data_pipeline_project.stg_marketing_spend)
[0m19:42:14.665000 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.stg_marketing_spend
[0m19:42:14.675201 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.stg_marketing_spend"
[0m19:42:14.676830 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.stg_marketing_spend
[0m19:42:14.711463 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.stg_marketing_spend"
[0m19:42:14.716530 [debug] [Thread-1 (]: On model.data_pipeline_project.stg_marketing_spend: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.stg_marketing_spend"} */


  create or replace view `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_marketing_spend`
  OPTIONS()
  as with source as (

    select * from `data-pipeline-project-474812`.`raw_data`.`marketing_spend`

),

unpivot_spend as (

    select
        Channel as marketing_channel,
        spend_date_raw, 
        cast(spend as numeric) as spend_amount_try

    from source

    -- ✅ CORRECT BigQuery UNPIVOT syntax: column names as STRINGS
    unpivot(spend for spend_date_raw in (
        '2025_09_20', 
        '2025_09_21', 
        '2025_09_22',
        '2025_09_23',
        '2025_09_24',
        '2025_09_25',
        '2025_09_26',
        '2025_09_27'
    ))

),

final_conversion as (

    select
        parse_date('%Y_%m_%d', spend_date_raw) as spend_date,
        marketing_channel,
        spend_amount_try
    
    from unpivot_spend
    where spend_amount_try is not null and spend_amount_try > 0

)

select * from final_conversion;


[0m19:42:14.718041 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m19:42:17.389433 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:86b96ea9-f894-4cee-91ac-7da341da361e&page=queryresults
[0m19:42:17.390751 [error] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:86b96ea9-f894-4cee-91ac-7da341da361e&page=queryresults
[0m19:42:17.400505 [debug] [Thread-1 (]: Database Error in model stg_marketing_spend (models/staging/stg_marketing_spend.sql)
  Syntax error: Unexpected string literal '2025_09_20' at [23:9]
  compiled code at target/run/data_pipeline_project/models/staging/stg_marketing_spend.sql
[0m19:42:17.402787 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '756837ec-fede-48f2-abd2-506977e46aeb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76ccfa34f740>]}
[0m19:42:17.404637 [error] [Thread-1 (]: 1 of 1 ERROR creating sql view model analytics_staging_staging.stg_marketing_spend  [[31mERROR[0m in 2.74s]
[0m19:42:17.406067 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.stg_marketing_spend
[0m19:42:17.407288 [debug] [Thread-7 (]: Marking all children of 'model.data_pipeline_project.stg_marketing_spend' to be skipped because of status 'error'.  Reason: Database Error in model stg_marketing_spend (models/staging/stg_marketing_spend.sql)
  Syntax error: Unexpected string literal '2025_09_20' at [23:9]
  compiled code at target/run/data_pipeline_project/models/staging/stg_marketing_spend.sql.
[0m19:42:17.412479 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m19:42:17.414798 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:42:17.415350 [debug] [MainThread]: Connection 'model.data_pipeline_project.stg_marketing_spend' was properly closed.
[0m19:42:17.416343 [info ] [MainThread]: 
[0m19:42:17.417358 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 5.85 seconds (5.85s).
[0m19:42:17.418952 [debug] [MainThread]: Command end result
[0m19:42:17.449378 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m19:42:17.453966 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m19:42:17.464996 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m19:42:17.466397 [info ] [MainThread]: 
[0m19:42:17.468298 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m19:42:17.469093 [info ] [MainThread]: 
[0m19:42:17.470088 [error] [MainThread]: [31mFailure in model stg_marketing_spend (models/staging/stg_marketing_spend.sql)[0m
[0m19:42:17.471409 [error] [MainThread]:   Database Error in model stg_marketing_spend (models/staging/stg_marketing_spend.sql)
  Syntax error: Unexpected string literal '2025_09_20' at [23:9]
  compiled code at target/run/data_pipeline_project/models/staging/stg_marketing_spend.sql
[0m19:42:17.472999 [info ] [MainThread]: 
[0m19:42:17.474597 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_project/models/staging/stg_marketing_spend.sql
[0m19:42:17.475461 [info ] [MainThread]: 
[0m19:42:17.476220 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m19:42:17.484769 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 9.264462, "process_in_blocks": "0", "process_kernel_time": 1.190388, "process_mem_max_rss": "378432", "process_out_blocks": "3304", "process_user_time": 4.497815}
[0m19:42:17.488248 [debug] [MainThread]: Command `dbt run` failed at 19:42:17.488047 after 9.27 seconds
[0m19:42:17.488754 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76ccf9469b20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76ccfa370aa0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76ccfa371a00>]}
[0m19:42:17.489431 [debug] [MainThread]: Flushing usage events
[0m19:42:19.502236 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m19:43:55.043298 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c6015d8db50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c60179b70e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c6017a56930>]}


============================== 19:43:55.047232 | cdabde37-e474-4c9f-93c8-909bcd71e0c5 ==============================
[0m19:43:55.047232 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m19:43:55.048456 [debug] [MainThread]: running dbt with arguments {'invocation_command': 'dbt run --select stg_marketing_spend', 'use_experimental_parser': 'False', 'no_print': 'None', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'empty': 'False', 'quiet': 'False', 'cache_selected_only': 'False', 'version_check': 'True', 'write_json': 'True', 'introspect': 'True', 'static_parser': 'True', 'printer_width': '80', 'warn_error': 'None', 'indirect_selection': 'eager', 'send_anonymous_usage_stats': 'True', 'log_format': 'default', 'log_cache_events': 'False', 'partial_parse': 'True', 'use_colors': 'True', 'target_path': 'None', 'debug': 'False', 'fail_fast': 'False', 'profiles_dir': '/home/ecem/.dbt'}
[0m19:43:57.302371 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'cdabde37-e474-4c9f-93c8-909bcd71e0c5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c5ff5834770>]}
[0m19:43:57.378347 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'cdabde37-e474-4c9f-93c8-909bcd71e0c5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c5ff5e5e1e0>]}
[0m19:43:57.383058 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m19:43:57.721995 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m19:43:57.978516 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m19:43:57.979648 [debug] [MainThread]: Partial parsing: updated file: data_pipeline_project://models/staging/stg_marketing_spend.sql
[0m19:43:58.232468 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.data_pipeline_project.marts
[0m19:43:58.244803 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'cdabde37-e474-4c9f-93c8-909bcd71e0c5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c5ff534ab40>]}
[0m19:43:58.352937 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m19:43:58.357184 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m19:43:58.392327 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'cdabde37-e474-4c9f-93c8-909bcd71e0c5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c5ff55880b0>]}
[0m19:43:58.393295 [info ] [MainThread]: Found 5 models, 10 sources, 508 macros
[0m19:43:58.393996 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'cdabde37-e474-4c9f-93c8-909bcd71e0c5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c60156c5550>]}
[0m19:43:58.396273 [info ] [MainThread]: 
[0m19:43:58.397509 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m19:43:58.398867 [info ] [MainThread]: 
[0m19:43:58.400034 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m19:43:58.402636 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812'
[0m19:43:58.404224 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:43:59.400976 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812, now list_data-pipeline-project-474812_analytics_staging_staging)
[0m19:43:59.404164 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:44:00.521616 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'cdabde37-e474-4c9f-93c8-909bcd71e0c5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c6016b8b8c0>]}
[0m19:44:00.522519 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:44:00.533495 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.stg_marketing_spend
[0m19:44:00.534612 [info ] [Thread-1 (]: 1 of 1 START sql view model analytics_staging_staging.stg_marketing_spend ...... [RUN]
[0m19:44:00.535417 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now model.data_pipeline_project.stg_marketing_spend)
[0m19:44:00.536398 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.stg_marketing_spend
[0m19:44:00.545642 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.stg_marketing_spend"
[0m19:44:00.547419 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.stg_marketing_spend
[0m19:44:00.588832 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.stg_marketing_spend"
[0m19:44:00.590945 [debug] [Thread-1 (]: On model.data_pipeline_project.stg_marketing_spend: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.stg_marketing_spend"} */


  create or replace view `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_marketing_spend`
  OPTIONS()
  as with source as (

    select * from `data-pipeline-project-474812`.`raw_data`.`marketing_spend`

),

unpivot_spend as (

    -- Manual UNPIVOT for each date column
    select
        Channel as marketing_channel,
        '2025_09_20' as spend_date_raw,
        cast(`2025_09_20` as numeric) as spend_amount_try
    from source
    where `2025_09_20` is not null and cast(`2025_09_20` as numeric) > 0

    union all

    select
        Channel as marketing_channel,
        '2025_09_21' as spend_date_raw,
        cast(`2025_09_21` as numeric) as spend_amount_try
    from source
    where `2025_09_21` is not null and cast(`2025_09_21` as numeric) > 0

    union all

    select
        Channel as marketing_channel,
        '2025_09_22' as spend_date_raw,
        cast(`2025_09_22` as numeric) as spend_amount_try
    from source
    where `2025_09_22` is not null and cast(`2025_09_22` as numeric) > 0

    union all

    select
        Channel as marketing_channel,
        '2025_09_23' as spend_date_raw,
        cast(`2025_09_23` as numeric) as spend_amount_try
    from source
    where `2025_09_23` is not null and cast(`2025_09_23` as numeric) > 0

    union all

    select
        Channel as marketing_channel,
        '2025_09_24' as spend_date_raw,
        cast(`2025_09_24` as numeric) as spend_amount_try
    from source
    where `2025_09_24` is not null and cast(`2025_09_24` as numeric) > 0

    union all

    select
        Channel as marketing_channel,
        '2025_09_25' as spend_date_raw,
        cast(`2025_09_25` as numeric) as spend_amount_try
    from source
    where `2025_09_25` is not null and cast(`2025_09_25` as numeric) > 0

    union all

    select
        Channel as marketing_channel,
        '2025_09_26' as spend_date_raw,
        cast(`2025_09_26` as numeric) as spend_amount_try
    from source
    where `2025_09_26` is not null and cast(`2025_09_26` as numeric) > 0

    union all

    select
        Channel as marketing_channel,
        '2025_09_27' as spend_date_raw,
        cast(`2025_09_27` as numeric) as spend_amount_try
    from source
    where `2025_09_27` is not null and cast(`2025_09_27` as numeric) > 0

),

final_conversion as (

    select
        parse_date('%Y_%m_%d', spend_date_raw) as spend_date,
        marketing_channel,
        spend_amount_try
    
    from unpivot_spend

)

select * from final_conversion;


[0m19:44:00.591923 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m19:44:01.991454 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:0902a9df-c79b-4d95-80eb-bc5b0d6748b6&page=queryresults
[0m19:44:02.339374 [error] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:0902a9df-c79b-4d95-80eb-bc5b0d6748b6&page=queryresults
[0m19:44:02.349038 [debug] [Thread-1 (]: Database Error in model stg_marketing_spend (models/staging/stg_marketing_spend.sql)
  Unrecognized name: `2025_09_20`; Did you mean 2025-09-20? at [20:11]
  compiled code at target/run/data_pipeline_project/models/staging/stg_marketing_spend.sql
[0m19:44:02.352143 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cdabde37-e474-4c9f-93c8-909bcd71e0c5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c6015d8f5c0>]}
[0m19:44:02.353386 [error] [Thread-1 (]: 1 of 1 ERROR creating sql view model analytics_staging_staging.stg_marketing_spend  [[31mERROR[0m in 1.81s]
[0m19:44:02.355195 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.stg_marketing_spend
[0m19:44:02.356515 [debug] [Thread-7 (]: Marking all children of 'model.data_pipeline_project.stg_marketing_spend' to be skipped because of status 'error'.  Reason: Database Error in model stg_marketing_spend (models/staging/stg_marketing_spend.sql)
  Unrecognized name: `2025_09_20`; Did you mean 2025-09-20? at [20:11]
  compiled code at target/run/data_pipeline_project/models/staging/stg_marketing_spend.sql.
[0m19:44:02.360755 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m19:44:02.363145 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:44:02.363927 [debug] [MainThread]: Connection 'model.data_pipeline_project.stg_marketing_spend' was properly closed.
[0m19:44:02.364422 [info ] [MainThread]: 
[0m19:44:02.364914 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 3.96 seconds (3.96s).
[0m19:44:02.365657 [debug] [MainThread]: Command end result
[0m19:44:02.397380 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m19:44:02.404170 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m19:44:02.417940 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m19:44:02.418684 [info ] [MainThread]: 
[0m19:44:02.420405 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m19:44:02.421123 [info ] [MainThread]: 
[0m19:44:02.422123 [error] [MainThread]: [31mFailure in model stg_marketing_spend (models/staging/stg_marketing_spend.sql)[0m
[0m19:44:02.423225 [error] [MainThread]:   Database Error in model stg_marketing_spend (models/staging/stg_marketing_spend.sql)
  Unrecognized name: `2025_09_20`; Did you mean 2025-09-20? at [20:11]
  compiled code at target/run/data_pipeline_project/models/staging/stg_marketing_spend.sql
[0m19:44:02.424262 [info ] [MainThread]: 
[0m19:44:02.425183 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_project/models/staging/stg_marketing_spend.sql
[0m19:44:02.426343 [info ] [MainThread]: 
[0m19:44:02.427530 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m19:44:02.430250 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 7.4520783, "process_in_blocks": "0", "process_kernel_time": 1.224121, "process_mem_max_rss": "378476", "process_out_blocks": "3320", "process_user_time": 4.487086}
[0m19:44:02.431432 [debug] [MainThread]: Command `dbt run` failed at 19:44:02.431261 after 7.45 seconds
[0m19:44:02.432095 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c6016154d70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c6015821a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c6015da84d0>]}
[0m19:44:02.432711 [debug] [MainThread]: Flushing usage events
[0m19:44:03.625773 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m19:49:56.714598 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b7f340698b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b7f35ad6ff0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b7f338b7cb0>]}


============================== 19:49:56.718589 | 0c6c2ccf-db71-4a64-9f71-3344454de64a ==============================
[0m19:49:56.718589 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m19:49:56.721879 [debug] [MainThread]: running dbt with arguments {'warn_error': 'None', 'profiles_dir': '/home/ecem/.dbt', 'invocation_command': 'dbt run --select stg_marketing_spend', 'debug': 'False', 'version_check': 'True', 'no_print': 'None', 'use_colors': 'True', 'indirect_selection': 'eager', 'send_anonymous_usage_stats': 'True', 'target_path': 'None', 'log_cache_events': 'False', 'use_experimental_parser': 'False', 'fail_fast': 'False', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'static_parser': 'True', 'empty': 'False', 'quiet': 'False', 'partial_parse': 'True', 'printer_width': '80', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'cache_selected_only': 'False', 'introspect': 'True', 'log_format': 'default', 'write_json': 'True'}
[0m19:49:59.651519 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0c6c2ccf-db71-4a64-9f71-3344454de64a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b7f13a09e20>]}
[0m19:49:59.720061 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '0c6c2ccf-db71-4a64-9f71-3344454de64a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b7f13ff4530>]}
[0m19:49:59.721589 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m19:49:59.975868 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m19:50:00.274693 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m19:50:00.276015 [debug] [MainThread]: Partial parsing: updated file: data_pipeline_project://models/staging/stg_marketing_spend.sql
[0m19:50:00.557498 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.data_pipeline_project.marts
[0m19:50:00.571453 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0c6c2ccf-db71-4a64-9f71-3344454de64a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b7f13431280>]}
[0m19:50:00.695635 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m19:50:00.698622 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m19:50:00.717523 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0c6c2ccf-db71-4a64-9f71-3344454de64a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b7f139aaf60>]}
[0m19:50:00.718340 [info ] [MainThread]: Found 5 models, 10 sources, 508 macros
[0m19:50:00.719285 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0c6c2ccf-db71-4a64-9f71-3344454de64a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b7f134eaa20>]}
[0m19:50:00.721046 [info ] [MainThread]: 
[0m19:50:00.721932 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m19:50:00.723089 [info ] [MainThread]: 
[0m19:50:00.725442 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m19:50:00.727670 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812'
[0m19:50:00.728341 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:50:01.824945 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812, now list_data-pipeline-project-474812_analytics_staging_staging)
[0m19:50:01.826273 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:50:02.731442 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0c6c2ccf-db71-4a64-9f71-3344454de64a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b7f1349aed0>]}
[0m19:50:02.732412 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:50:02.742112 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.stg_marketing_spend
[0m19:50:02.744010 [info ] [Thread-1 (]: 1 of 1 START sql view model analytics_staging_staging.stg_marketing_spend ...... [RUN]
[0m19:50:02.745465 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now model.data_pipeline_project.stg_marketing_spend)
[0m19:50:02.746280 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.stg_marketing_spend
[0m19:50:02.755788 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.stg_marketing_spend"
[0m19:50:02.757388 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.stg_marketing_spend
[0m19:50:02.829422 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.stg_marketing_spend"
[0m19:50:02.833139 [debug] [Thread-1 (]: On model.data_pipeline_project.stg_marketing_spend: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.stg_marketing_spend"} */


  create or replace view `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_marketing_spend`
  OPTIONS()
  as with source as (

    select * from `data-pipeline-project-474812`.`raw_data`.`marketing_spend`

),

unpivot_spend as (

    -- Manual UNPIVOT for each date column
    select
        channel as marketing_channel,
        '2025-09-20' as spend_date_raw,
        cast(`2025-09-20` as numeric) as spend_amount_try
    from source
    where `2025-09-20` is not null and cast(`2025-09-20` as numeric) > 0

    union all

    select
        channel as marketing_channel,
        '2025-09-21' as spend_date_raw,
        cast(`2025-09-21` as numeric) as spend_amount_try
    from source
    where `2025-09-21` is not null and cast(`2025-09-21` as numeric) > 0

    union all

    select
        channel as marketing_channel,
        '2025-09-22' as spend_date_raw,
        cast(`2025-09-22` as numeric) as spend_amount_try
    from source
    where `2025-09-22` is not null and cast(`2025-09-22` as numeric) > 0

    union all

    select
        channel as marketing_channel,
        '2025-09-23' as spend_date_raw,
        cast(`2025-09-23` as numeric) as spend_amount_try
    from source
    where `2025-09-23` is not null and cast(`2025-09-23` as numeric) > 0

    union all

    select
        channel as marketing_channel,
        '2025-09-24' as spend_date_raw,
        cast(`2025-09-24` as numeric) as spend_amount_try
    from source
    where `2025-09-24` is not null and cast(`2025-09-24` as numeric) > 0

    union all

    select
        channel as marketing_channel,
        '2025-09-25' as spend_date_raw,
        cast(`2025-09-25` as numeric) as spend_amount_try
    from source
    where `2025-09-25` is not null and cast(`2025-09-25` as numeric) > 0

    union all

    select
        channel as marketing_channel,
        '2025-09-26' as spend_date_raw,
        cast(`2025-09-26` as numeric) as spend_amount_try
    from source
    where `2025-09-26` is not null and cast(`2025-09-26` as numeric) > 0

    union all

    select
        channel as marketing_channel,
        '2025-09-27' as spend_date_raw,
        cast(`2025-09-27` as numeric) as spend_amount_try
    from source
    where `2025-09-27` is not null and cast(`2025-09-27` as numeric) > 0

),

final_conversion as (

    select
        parse_date('%Y-%m-%d', spend_date_raw) as spend_date,
        marketing_channel,
        spend_amount_try
    
    from unpivot_spend

)

select * from final_conversion;


[0m19:50:02.834932 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m19:50:04.099910 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:7ed2d50b-14e9-4302-a3da-0f7615dcb714&page=queryresults
[0m19:50:04.796644 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0c6c2ccf-db71-4a64-9f71-3344454de64a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b7f1674c5c0>]}
[0m19:50:04.799505 [info ] [Thread-1 (]: 1 of 1 OK created sql view model analytics_staging_staging.stg_marketing_spend . [[32mCREATE VIEW (0 processed)[0m in 2.05s]
[0m19:50:04.800754 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.stg_marketing_spend
[0m19:50:04.804563 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m19:50:04.806982 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:50:04.807577 [debug] [MainThread]: Connection 'model.data_pipeline_project.stg_marketing_spend' was properly closed.
[0m19:50:04.808174 [info ] [MainThread]: 
[0m19:50:04.808690 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 4.08 seconds (4.08s).
[0m19:50:04.809456 [debug] [MainThread]: Command end result
[0m19:50:04.842265 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m19:50:04.844889 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m19:50:04.853248 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m19:50:04.854128 [info ] [MainThread]: 
[0m19:50:04.854770 [info ] [MainThread]: [32mCompleted successfully[0m
[0m19:50:04.855375 [info ] [MainThread]: 
[0m19:50:04.855935 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m19:50:04.857667 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 8.205073, "process_in_blocks": "0", "process_kernel_time": 1.445467, "process_mem_max_rss": "381908", "process_out_blocks": "3320", "process_user_time": 5.416185}
[0m19:50:04.858398 [debug] [MainThread]: Command `dbt run` succeeded at 19:50:04.858275 after 8.21 seconds
[0m19:50:04.858898 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b7f337b8da0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b7f137e2450>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b7f13be15b0>]}
[0m19:50:04.859438 [debug] [MainThread]: Flushing usage events
[0m19:50:10.555526 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m19:57:05.363716 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x747dad5076b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x747daf2af110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x747dad0bff80>]}


============================== 19:57:05.368222 | 6d89c471-dd31-4991-b5b4-32570434cafb ==============================
[0m19:57:05.368222 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m19:57:05.369263 [debug] [MainThread]: running dbt with arguments {'log_format': 'default', 'indirect_selection': 'eager', 'target_path': 'None', 'cache_selected_only': 'False', 'warn_error': 'None', 'send_anonymous_usage_stats': 'True', 'use_colors': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'debug': 'False', 'fail_fast': 'False', 'use_experimental_parser': 'False', 'partial_parse': 'True', 'profiles_dir': '/home/ecem/.dbt', 'no_print': 'None', 'quiet': 'False', 'write_json': 'True', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'empty': 'False', 'invocation_command': 'dbt run --select stg_marketing_spend', 'introspect': 'True', 'log_cache_events': 'False', 'version_check': 'True', 'printer_width': '80'}
[0m19:57:08.356751 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6d89c471-dd31-4991-b5b4-32570434cafb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x747daf06be90>]}
[0m19:57:08.518963 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6d89c471-dd31-4991-b5b4-32570434cafb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x747d8d47f200>]}
[0m19:57:08.522959 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m19:57:08.855133 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m19:57:09.119411 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m19:57:09.120382 [debug] [MainThread]: Partial parsing: updated file: data_pipeline_project://models/staging/stg_marketing_spend.sql
[0m19:57:09.366028 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.data_pipeline_project.marts
[0m19:57:09.396729 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6d89c471-dd31-4991-b5b4-32570434cafb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x747d8d0c12e0>]}
[0m19:57:09.503139 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m19:57:09.505504 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m19:57:09.520811 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6d89c471-dd31-4991-b5b4-32570434cafb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x747d8c3f4590>]}
[0m19:57:09.521762 [info ] [MainThread]: Found 5 models, 10 sources, 508 macros
[0m19:57:09.522611 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6d89c471-dd31-4991-b5b4-32570434cafb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x747d8cdaca40>]}
[0m19:57:09.526437 [info ] [MainThread]: 
[0m19:57:09.528624 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m19:57:09.529608 [info ] [MainThread]: 
[0m19:57:09.531336 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m19:57:09.533307 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812'
[0m19:57:09.533992 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:57:10.663160 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812, now list_data-pipeline-project-474812_analytics_staging_staging)
[0m19:57:10.668920 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:57:11.637697 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6d89c471-dd31-4991-b5b4-32570434cafb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x747dada11280>]}
[0m19:57:11.642938 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:57:11.658937 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.stg_marketing_spend
[0m19:57:11.661080 [info ] [Thread-1 (]: 1 of 1 START sql view model analytics_staging_staging.stg_marketing_spend ...... [RUN]
[0m19:57:11.662821 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now model.data_pipeline_project.stg_marketing_spend)
[0m19:57:11.663853 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.stg_marketing_spend
[0m19:57:11.673682 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.stg_marketing_spend"
[0m19:57:11.675049 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.stg_marketing_spend
[0m19:57:11.717960 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.stg_marketing_spend"
[0m19:57:11.719336 [debug] [Thread-1 (]: On model.data_pipeline_project.stg_marketing_spend: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.stg_marketing_spend"} */


  create or replace view `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_marketing_spend`
  OPTIONS()
  as with source as (

    select * from `data-pipeline-project-474812`.`raw_data`.`marketing_spend`

),

unpivot_spend as (

    -- Manual UNPIVOT for each date column
    select
        channel as marketing_channel,
        '2025-09-20' as spend_date_raw,
        cast(replace(`2025-09-20`, ',', '') as numeric) as spend_amount_try
    from source
    where `2025-09-20` is not null and cast(replace(`2025-09-20`, ',', '') as numeric) > 0

    union all

    select
        channel as marketing_channel,
        '2025-09-21' as spend_date_raw,
        cast(replace(`2025-09-21`, ',', '') as numeric) as spend_amount_try
    from source
    where `2025-09-21` is not null and cast(replace(`2025-09-21`, ',', '') as numeric) > 0

    union all

    select
        channel as marketing_channel,
        '2025-09-22' as spend_date_raw,
        cast(replace(`2025-09-22`, ',', '') as numeric) as spend_amount_try
    from source
    where `2025-09-22` is not null and cast(replace(`2025-09-22`, ',', '') as numeric) > 0

    union all

    select
        channel as marketing_channel,
        '2025-09-23' as spend_date_raw,
        cast(replace(`2025-09-23`, ',', '') as numeric) as spend_amount_try
    from source
    where `2025-09-23` is not null and cast(replace(`2025-09-23`, ',', '') as numeric) > 0

    union all

    select
        channel as marketing_channel,
        '2025-09-24' as spend_date_raw,
        cast(replace(`2025-09-24`, ',', '') as numeric) as spend_amount_try
    from source
    where `2025-09-24` is not null and cast(replace(`2025-09-24`, ',', '') as numeric) > 0

    union all

    select
        channel as marketing_channel,
        '2025-09-25' as spend_date_raw,
        cast(replace(`2025-09-25`, ',', '') as numeric) as spend_amount_try
    from source
    where `2025-09-25` is not null and cast(replace(`2025-09-25`, ',', '') as numeric) > 0

    union all

    select
        channel as marketing_channel,
        '2025-09-26' as spend_date_raw,
        cast(replace(`2025-09-26`, ',', '') as numeric) as spend_amount_try
    from source
    where `2025-09-26` is not null and cast(replace(`2025-09-26`, ',', '') as numeric) > 0

    union all

    select
        channel as marketing_channel,
        '2025-09-27' as spend_date_raw,
        cast(replace(`2025-09-27`, ',', '') as numeric) as spend_amount_try
    from source
    where `2025-09-27` is not null and cast(replace(`2025-09-27`, ',', '') as numeric) > 0

),

final_conversion as (

    select
        parse_date('%Y-%m-%d', spend_date_raw) as spend_date,
        marketing_channel,
        spend_amount_try
    
    from unpivot_spend

)

select * from final_conversion;


[0m19:57:11.720683 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m19:57:13.055561 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:99cc7d54-5b2b-45fb-b7cc-388045e83186&page=queryresults
[0m19:57:13.659120 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6d89c471-dd31-4991-b5b4-32570434cafb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x747d8c480d40>]}
[0m19:57:13.660230 [info ] [Thread-1 (]: 1 of 1 OK created sql view model analytics_staging_staging.stg_marketing_spend . [[32mCREATE VIEW (0 processed)[0m in 1.99s]
[0m19:57:13.662089 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.stg_marketing_spend
[0m19:57:13.665436 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m19:57:13.667148 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:57:13.668131 [debug] [MainThread]: Connection 'model.data_pipeline_project.stg_marketing_spend' was properly closed.
[0m19:57:13.669261 [info ] [MainThread]: 
[0m19:57:13.670230 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 4.14 seconds (4.14s).
[0m19:57:13.671519 [debug] [MainThread]: Command end result
[0m19:57:13.715060 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m19:57:13.718662 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m19:57:13.733549 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m19:57:13.735350 [info ] [MainThread]: 
[0m19:57:13.739823 [info ] [MainThread]: [32mCompleted successfully[0m
[0m19:57:13.742741 [info ] [MainThread]: 
[0m19:57:13.744347 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m19:57:13.746344 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 8.448916, "process_in_blocks": "0", "process_kernel_time": 1.278559, "process_mem_max_rss": "381772", "process_out_blocks": "3328", "process_user_time": 5.428227}
[0m19:57:13.746875 [debug] [MainThread]: Command `dbt run` succeeded at 19:57:13.746759 after 8.45 seconds
[0m19:57:13.747318 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x747daebd4a70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x747d8c3f4320>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x747dac7f3950>]}
[0m19:57:13.747807 [debug] [MainThread]: Flushing usage events
[0m19:57:15.758438 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:04:20.286826 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x714217d308f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x714217930200>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x714217bd93d0>]}


============================== 20:04:20.291348 | bd3a278c-84d4-4342-9c4f-84e68a85cf4a ==============================
[0m20:04:20.291348 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m20:04:20.293015 [debug] [MainThread]: running dbt with arguments {'use_colors': 'True', 'printer_width': '80', 'profiles_dir': '/home/ecem/.dbt', 'warn_error': 'None', 'send_anonymous_usage_stats': 'True', 'cache_selected_only': 'False', 'use_experimental_parser': 'False', 'invocation_command': 'dbt run --select stg_countries', 'introspect': 'True', 'quiet': 'False', 'no_print': 'None', 'fail_fast': 'False', 'log_cache_events': 'False', 'write_json': 'True', 'empty': 'False', 'version_check': 'True', 'target_path': 'None', 'debug': 'False', 'indirect_selection': 'eager', 'partial_parse': 'True', 'log_format': 'default', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'static_parser': 'True'}
[0m20:04:22.684958 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'bd3a278c-84d4-4342-9c4f-84e68a85cf4a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7142181254c0>]}
[0m20:04:22.757550 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'bd3a278c-84d4-4342-9c4f-84e68a85cf4a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x714219b1a1b0>]}
[0m20:04:22.762224 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m20:04:23.018411 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m20:04:23.274793 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
[0m20:04:23.275972 [debug] [MainThread]: Partial parsing: added file: data_pipeline_project://models/staging/stg_countries.sql
[0m20:04:23.548416 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.data_pipeline_project.marts
[0m20:04:23.564869 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'bd3a278c-84d4-4342-9c4f-84e68a85cf4a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7141f6f7dd90>]}
[0m20:04:23.699634 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m20:04:23.704981 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m20:04:23.721366 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'bd3a278c-84d4-4342-9c4f-84e68a85cf4a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7141f6f50380>]}
[0m20:04:23.722180 [info ] [MainThread]: Found 6 models, 10 sources, 508 macros
[0m20:04:23.722917 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bd3a278c-84d4-4342-9c4f-84e68a85cf4a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7141f78537d0>]}
[0m20:04:23.724719 [info ] [MainThread]: 
[0m20:04:23.725409 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m20:04:23.725992 [info ] [MainThread]: 
[0m20:04:23.726715 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m20:04:23.728300 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812'
[0m20:04:23.729053 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:04:24.840447 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812, now list_data-pipeline-project-474812_analytics_staging_staging)
[0m20:04:24.842228 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m20:04:25.748521 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bd3a278c-84d4-4342-9c4f-84e68a85cf4a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x714219074830>]}
[0m20:04:25.750252 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:04:25.762523 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.stg_countries
[0m20:04:25.764068 [info ] [Thread-1 (]: 1 of 1 START sql view model analytics_staging_staging.stg_countries ............ [RUN]
[0m20:04:25.765136 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now model.data_pipeline_project.stg_countries)
[0m20:04:25.765943 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.stg_countries
[0m20:04:25.773889 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.stg_countries"
[0m20:04:25.775269 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.stg_countries
[0m20:04:25.818349 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.stg_countries"
[0m20:04:25.819815 [debug] [Thread-1 (]: On model.data_pipeline_project.stg_countries: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.stg_countries"} */


  create or replace view `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_countries`
  OPTIONS()
  as with source as (

    select * from `data-pipeline-project-474812`.`raw_data`.`countries`

),

renamed_and_cleaned as (

    select
        -- Deduplication logic: Select the latest record if duplicates exist
        row_number() over (partition by _id order by createdAt desc) as rn,

        -- Primary Key: Standardize to country_id
        _id as country_id,
        
        -- Attributes
        name as country_name

    from source
    where _id is not null
)

select
    country_id,
    country_name

from renamed_and_cleaned
-- Final selection: only include the latest, unique instance
where rn = 1;


[0m20:04:25.820915 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m20:04:27.320933 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:4e383252-79b5-485b-a5da-15113df768ea&page=queryresults
[0m20:04:27.615221 [error] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:4e383252-79b5-485b-a5da-15113df768ea&page=queryresults
[0m20:04:27.633787 [debug] [Thread-1 (]: Database Error in model stg_countries (models/staging/stg_countries.sql)
  Unrecognized name: createdAt at [16:54]
  compiled code at target/run/data_pipeline_project/models/staging/stg_countries.sql
[0m20:04:27.636120 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bd3a278c-84d4-4342-9c4f-84e68a85cf4a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71421829f560>]}
[0m20:04:27.637286 [error] [Thread-1 (]: 1 of 1 ERROR creating sql view model analytics_staging_staging.stg_countries ... [[31mERROR[0m in 1.87s]
[0m20:04:27.638389 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.stg_countries
[0m20:04:27.642123 [debug] [Thread-7 (]: Marking all children of 'model.data_pipeline_project.stg_countries' to be skipped because of status 'error'.  Reason: Database Error in model stg_countries (models/staging/stg_countries.sql)
  Unrecognized name: createdAt at [16:54]
  compiled code at target/run/data_pipeline_project/models/staging/stg_countries.sql.
[0m20:04:27.647633 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m20:04:27.649642 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:04:27.650104 [debug] [MainThread]: Connection 'model.data_pipeline_project.stg_countries' was properly closed.
[0m20:04:27.650572 [info ] [MainThread]: 
[0m20:04:27.651112 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 3.92 seconds (3.92s).
[0m20:04:27.651898 [debug] [MainThread]: Command end result
[0m20:04:27.685739 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m20:04:27.693426 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m20:04:27.708285 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m20:04:27.709892 [info ] [MainThread]: 
[0m20:04:27.711373 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m20:04:27.712522 [info ] [MainThread]: 
[0m20:04:27.715594 [error] [MainThread]: [31mFailure in model stg_countries (models/staging/stg_countries.sql)[0m
[0m20:04:27.716563 [error] [MainThread]:   Database Error in model stg_countries (models/staging/stg_countries.sql)
  Unrecognized name: createdAt at [16:54]
  compiled code at target/run/data_pipeline_project/models/staging/stg_countries.sql
[0m20:04:27.717347 [info ] [MainThread]: 
[0m20:04:27.718416 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_project/models/staging/stg_countries.sql
[0m20:04:27.719346 [info ] [MainThread]: 
[0m20:04:27.720062 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m20:04:27.722131 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 7.506306, "process_in_blocks": "0", "process_kernel_time": 1.080586, "process_mem_max_rss": "378432", "process_out_blocks": "3336", "process_user_time": 5.109893}
[0m20:04:27.722984 [debug] [MainThread]: Command `dbt run` failed at 20:04:27.722848 after 7.51 seconds
[0m20:04:27.723515 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x714219894c20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7142182c19d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7141f794bcb0>]}
[0m20:04:27.724336 [debug] [MainThread]: Flushing usage events
[0m20:04:28.890545 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:06:01.993275 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7429d4f9b5c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7429d524a1e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7429d7115190>]}


============================== 20:06:02.006358 | fe5e99c8-6ce1-4e4f-924e-8e6156c3f158 ==============================
[0m20:06:02.006358 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m20:06:02.009168 [debug] [MainThread]: running dbt with arguments {'fail_fast': 'False', 'log_format': 'default', 'quiet': 'False', 'no_print': 'None', 'target_path': 'None', 'empty': 'False', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'warn_error': 'None', 'use_colors': 'True', 'static_parser': 'True', 'log_cache_events': 'False', 'debug': 'False', 'send_anonymous_usage_stats': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'indirect_selection': 'eager', 'version_check': 'True', 'partial_parse': 'True', 'profiles_dir': '/home/ecem/.dbt', 'cache_selected_only': 'False', 'write_json': 'True', 'invocation_command': 'dbt run --select stg_countries', 'printer_width': '80', 'use_experimental_parser': 'False'}
[0m20:06:04.720214 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'fe5e99c8-6ce1-4e4f-924e-8e6156c3f158', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7429b4ecbbc0>]}
[0m20:06:04.803426 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'fe5e99c8-6ce1-4e4f-924e-8e6156c3f158', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7429b54eb260>]}
[0m20:06:04.806111 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m20:06:05.081068 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m20:06:05.342143 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m20:06:05.343311 [debug] [MainThread]: Partial parsing: updated file: data_pipeline_project://models/staging/stg_countries.sql
[0m20:06:05.638199 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.data_pipeline_project.marts
[0m20:06:05.664549 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fe5e99c8-6ce1-4e4f-924e-8e6156c3f158', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7429aff7e120>]}
[0m20:06:05.796927 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m20:06:05.800532 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m20:06:05.817207 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fe5e99c8-6ce1-4e4f-924e-8e6156c3f158', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7429b4b7c260>]}
[0m20:06:05.818332 [info ] [MainThread]: Found 6 models, 10 sources, 508 macros
[0m20:06:05.819177 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fe5e99c8-6ce1-4e4f-924e-8e6156c3f158', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7429b49337a0>]}
[0m20:06:05.821283 [info ] [MainThread]: 
[0m20:06:05.822186 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m20:06:05.823179 [info ] [MainThread]: 
[0m20:06:05.824441 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m20:06:05.826413 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812'
[0m20:06:05.826992 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:06:07.452101 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812, now list_data-pipeline-project-474812_analytics_staging_staging)
[0m20:06:07.454119 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m20:06:08.652320 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fe5e99c8-6ce1-4e4f-924e-8e6156c3f158', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7429b4bc8530>]}
[0m20:06:08.653351 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:06:08.661298 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.stg_countries
[0m20:06:08.662800 [info ] [Thread-1 (]: 1 of 1 START sql view model analytics_staging_staging.stg_countries ............ [RUN]
[0m20:06:08.663956 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now model.data_pipeline_project.stg_countries)
[0m20:06:08.664632 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.stg_countries
[0m20:06:08.674569 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.stg_countries"
[0m20:06:08.676231 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.stg_countries
[0m20:06:08.744172 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.stg_countries"
[0m20:06:08.747208 [debug] [Thread-1 (]: On model.data_pipeline_project.stg_countries: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.stg_countries"} */


  create or replace view `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_countries`
  OPTIONS()
  as with source as (

    select * from `data-pipeline-project-474812`.`raw_data`.`countries`

)

select distinct
    -- Primary Key: Standardize to country_id
    _id as country_id,
    
    -- Attributes
    name as country_name

from source
where _id is not null;


[0m20:06:08.748495 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m20:06:10.185707 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:cbff25d4-6ac7-4d01-96ee-cea7bdf25f1b&page=queryresults
[0m20:06:10.795850 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fe5e99c8-6ce1-4e4f-924e-8e6156c3f158', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7429b7c68b00>]}
[0m20:06:10.799236 [info ] [Thread-1 (]: 1 of 1 OK created sql view model analytics_staging_staging.stg_countries ....... [[32mCREATE VIEW (0 processed)[0m in 2.13s]
[0m20:06:10.801647 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.stg_countries
[0m20:06:10.807948 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m20:06:10.810782 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:06:10.812078 [debug] [MainThread]: Connection 'model.data_pipeline_project.stg_countries' was properly closed.
[0m20:06:10.813550 [info ] [MainThread]: 
[0m20:06:10.814574 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 4.99 seconds (4.99s).
[0m20:06:10.816891 [debug] [MainThread]: Command end result
[0m20:06:10.866139 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m20:06:10.878416 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m20:06:10.890360 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m20:06:10.891339 [info ] [MainThread]: 
[0m20:06:10.892284 [info ] [MainThread]: [32mCompleted successfully[0m
[0m20:06:10.893059 [info ] [MainThread]: 
[0m20:06:10.893901 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m20:06:10.896149 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 8.990561, "process_in_blocks": "0", "process_kernel_time": 1.279334, "process_mem_max_rss": "382180", "process_out_blocks": "3336", "process_user_time": 4.999993}
[0m20:06:10.901403 [debug] [MainThread]: Command `dbt run` succeeded at 20:06:10.897948 after 8.99 seconds
[0m20:06:10.905796 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7429d4d29370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7429b4b7c4a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7429afe2c680>]}
[0m20:06:10.907185 [debug] [MainThread]: Flushing usage events
[0m20:06:11.836389 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:09:32.626972 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7422d1ff21e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7422d1c3e570>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7422d1f20f50>]}


============================== 20:09:32.630514 | adff17fd-1e2c-4028-989f-ddeef9096b94 ==============================
[0m20:09:32.630514 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m20:09:32.631821 [debug] [MainThread]: running dbt with arguments {'send_anonymous_usage_stats': 'True', 'fail_fast': 'False', 'write_json': 'True', 'partial_parse': 'True', 'target_path': 'None', 'version_check': 'True', 'cache_selected_only': 'False', 'invocation_command': 'dbt run --select stg_states', 'log_format': 'default', 'quiet': 'False', 'static_parser': 'True', 'introspect': 'True', 'no_print': 'None', 'use_experimental_parser': 'False', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'indirect_selection': 'eager', 'empty': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'warn_error': 'None', 'log_cache_events': 'False', 'debug': 'False', 'use_colors': 'True', 'printer_width': '80', 'profiles_dir': '/home/ecem/.dbt'}
[0m20:09:35.057651 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'adff17fd-1e2c-4028-989f-ddeef9096b94', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7422d38cbd10>]}
[0m20:09:35.142509 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'adff17fd-1e2c-4028-989f-ddeef9096b94', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7422b1d1ab40>]}
[0m20:09:35.143633 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m20:09:35.376941 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m20:09:35.635531 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
[0m20:09:35.637009 [debug] [MainThread]: Partial parsing: added file: data_pipeline_project://models/staging/stg_states.sql
[0m20:09:35.891908 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.data_pipeline_project.marts
[0m20:09:35.905465 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'adff17fd-1e2c-4028-989f-ddeef9096b94', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7422b18c9f70>]}
[0m20:09:36.012170 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m20:09:36.022048 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m20:09:36.039854 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'adff17fd-1e2c-4028-989f-ddeef9096b94', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7422b17a5040>]}
[0m20:09:36.040749 [info ] [MainThread]: Found 7 models, 10 sources, 508 macros
[0m20:09:36.041649 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'adff17fd-1e2c-4028-989f-ddeef9096b94', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7422d19c9a00>]}
[0m20:09:36.044196 [info ] [MainThread]: 
[0m20:09:36.045394 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m20:09:36.046348 [info ] [MainThread]: 
[0m20:09:36.047402 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m20:09:36.048781 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812'
[0m20:09:36.049334 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:09:37.098696 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812, now list_data-pipeline-project-474812_analytics_staging_staging)
[0m20:09:37.102081 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m20:09:38.094668 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'adff17fd-1e2c-4028-989f-ddeef9096b94', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7422b1933bf0>]}
[0m20:09:38.096389 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:09:38.109036 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.stg_states
[0m20:09:38.110364 [info ] [Thread-1 (]: 1 of 1 START sql view model analytics_staging_staging.stg_states ............... [RUN]
[0m20:09:38.111581 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now model.data_pipeline_project.stg_states)
[0m20:09:38.112233 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.stg_states
[0m20:09:38.120667 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.stg_states"
[0m20:09:38.122195 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.stg_states
[0m20:09:38.160719 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.stg_states"
[0m20:09:38.163508 [debug] [Thread-1 (]: On model.data_pipeline_project.stg_states: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.stg_states"} */


  create or replace view `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_states`
  OPTIONS()
  as with source as (

    select * from `data-pipeline-project-474812`.`raw_data`.`states`

)

select distinct
    -- Primary Key
    _id as state_id,
    
    -- Foreign Key (to countries)
    _country as country_id,
    
    -- Attributes
    name as state_name

from source
where _id is not null;


[0m20:09:38.164531 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m20:09:39.253479 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:0b205e01-8dfe-4d69-a363-182622db1db8&page=queryresults
[0m20:09:39.860218 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'adff17fd-1e2c-4028-989f-ddeef9096b94', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7422d1f47b90>]}
[0m20:09:39.861830 [info ] [Thread-1 (]: 1 of 1 OK created sql view model analytics_staging_staging.stg_states .......... [[32mCREATE VIEW (0 processed)[0m in 1.75s]
[0m20:09:39.863108 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.stg_states
[0m20:09:39.867777 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m20:09:39.871835 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:09:39.872853 [debug] [MainThread]: Connection 'model.data_pipeline_project.stg_states' was properly closed.
[0m20:09:39.873360 [info ] [MainThread]: 
[0m20:09:39.874102 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 3.83 seconds (3.83s).
[0m20:09:39.875172 [debug] [MainThread]: Command end result
[0m20:09:39.905453 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m20:09:39.907991 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m20:09:39.915239 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m20:09:39.916377 [info ] [MainThread]: 
[0m20:09:39.917384 [info ] [MainThread]: [32mCompleted successfully[0m
[0m20:09:39.918413 [info ] [MainThread]: 
[0m20:09:39.920080 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m20:09:39.922153 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 7.3618884, "process_in_blocks": "0", "process_kernel_time": 1.079296, "process_mem_max_rss": "382540", "process_out_blocks": "3336", "process_user_time": 4.851692}
[0m20:09:39.923160 [debug] [MainThread]: Command `dbt run` succeeded at 20:09:39.922982 after 7.36 seconds
[0m20:09:39.924004 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7422d19796a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7422b17e8740>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7422b0cf08c0>]}
[0m20:09:39.924657 [debug] [MainThread]: Flushing usage events
[0m20:09:40.934028 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:11:10.988084 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x775778a25430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7757784559d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77577862f320>]}


============================== 20:11:10.991492 | 88e428bc-05f4-42ff-be33-e9b0c8933f87 ==============================
[0m20:11:10.991492 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m20:11:10.992431 [debug] [MainThread]: running dbt with arguments {'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'fail_fast': 'False', 'printer_width': '80', 'empty': 'False', 'indirect_selection': 'eager', 'partial_parse': 'True', 'log_format': 'default', 'invocation_command': 'dbt run --select stg_cities', 'write_json': 'True', 'debug': 'False', 'target_path': 'None', 'use_experimental_parser': 'False', 'version_check': 'True', 'warn_error': 'None', 'send_anonymous_usage_stats': 'True', 'profiles_dir': '/home/ecem/.dbt', 'quiet': 'False', 'log_cache_events': 'False', 'use_colors': 'True', 'static_parser': 'True', 'cache_selected_only': 'False', 'no_print': 'None', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])'}
[0m20:11:13.396633 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '88e428bc-05f4-42ff-be33-e9b0c8933f87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77575ccb5340>]}
[0m20:11:13.461518 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '88e428bc-05f4-42ff-be33-e9b0c8933f87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x775778967110>]}
[0m20:11:13.464383 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m20:11:13.720013 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m20:11:13.966476 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
[0m20:11:13.968313 [debug] [MainThread]: Partial parsing: added file: data_pipeline_project://models/staging/stg_cities.sql
[0m20:11:14.239793 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.data_pipeline_project.marts
[0m20:11:14.253310 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '88e428bc-05f4-42ff-be33-e9b0c8933f87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77577a58ede0>]}
[0m20:11:14.356417 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m20:11:14.360372 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m20:11:14.381576 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '88e428bc-05f4-42ff-be33-e9b0c8933f87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77575889cd10>]}
[0m20:11:14.384018 [info ] [MainThread]: Found 8 models, 10 sources, 508 macros
[0m20:11:14.385516 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '88e428bc-05f4-42ff-be33-e9b0c8933f87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77575889a090>]}
[0m20:11:14.387435 [info ] [MainThread]: 
[0m20:11:14.388197 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m20:11:14.388860 [info ] [MainThread]: 
[0m20:11:14.390650 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m20:11:14.392447 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812'
[0m20:11:14.393095 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:11:15.420684 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812, now list_data-pipeline-project-474812_analytics_staging_staging)
[0m20:11:15.421974 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m20:11:16.300669 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '88e428bc-05f4-42ff-be33-e9b0c8933f87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x775758898320>]}
[0m20:11:16.303556 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:11:16.316230 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.stg_cities
[0m20:11:16.317382 [info ] [Thread-1 (]: 1 of 1 START sql view model analytics_staging_staging.stg_cities ............... [RUN]
[0m20:11:16.318426 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now model.data_pipeline_project.stg_cities)
[0m20:11:16.319305 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.stg_cities
[0m20:11:16.330574 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.stg_cities"
[0m20:11:16.331975 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.stg_cities
[0m20:11:16.383292 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.stg_cities"
[0m20:11:16.387140 [debug] [Thread-1 (]: On model.data_pipeline_project.stg_cities: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.stg_cities"} */


  create or replace view `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_cities`
  OPTIONS()
  as with source as (

    select * from `data-pipeline-project-474812`.`raw_data`.`cities`

)

select distinct
    -- Primary Key
    _id as city_id,
    
    -- Foreign Keys
    _state as state_id,
    _country as country_id,
    
    -- Attributes
    name as city_name

from source
where _id is not null;


[0m20:11:16.388199 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m20:11:17.468538 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:1195f65d-e921-4533-9b7f-70e19a217a6c&page=queryresults
[0m20:11:18.156114 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '88e428bc-05f4-42ff-be33-e9b0c8933f87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77577909f6b0>]}
[0m20:11:18.157549 [info ] [Thread-1 (]: 1 of 1 OK created sql view model analytics_staging_staging.stg_cities .......... [[32mCREATE VIEW (0 processed)[0m in 1.84s]
[0m20:11:18.159044 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.stg_cities
[0m20:11:18.162020 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m20:11:18.164244 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:11:18.164835 [debug] [MainThread]: Connection 'model.data_pipeline_project.stg_cities' was properly closed.
[0m20:11:18.165508 [info ] [MainThread]: 
[0m20:11:18.166438 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 3.78 seconds (3.78s).
[0m20:11:18.167738 [debug] [MainThread]: Command end result
[0m20:11:18.210607 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m20:11:18.213904 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m20:11:18.228277 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m20:11:18.229318 [info ] [MainThread]: 
[0m20:11:18.231008 [info ] [MainThread]: [32mCompleted successfully[0m
[0m20:11:18.232576 [info ] [MainThread]: 
[0m20:11:18.233558 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m20:11:18.235930 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 7.3076425, "process_in_blocks": "0", "process_kernel_time": 1.154501, "process_mem_max_rss": "382116", "process_out_blocks": "3360", "process_user_time": 4.721435}
[0m20:11:18.237396 [debug] [MainThread]: Command `dbt run` succeeded at 20:11:18.237259 after 7.31 seconds
[0m20:11:18.240726 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x775778505af0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7757790bca40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77575889cf50>]}
[0m20:11:18.242439 [debug] [MainThread]: Flushing usage events
[0m20:11:19.207190 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:15:02.370321 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e7fa64d8980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e7fa8a33aa0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e7fa633d3a0>]}


============================== 20:15:02.375231 | 67fd8b32-abd1-4fe7-9a78-ccd3790e4b22 ==============================
[0m20:15:02.375231 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m20:15:02.376420 [debug] [MainThread]: running dbt with arguments {'indirect_selection': 'eager', 'fail_fast': 'False', 'profiles_dir': '/home/ecem/.dbt', 'use_colors': 'True', 'cache_selected_only': 'False', 'introspect': 'True', 'quiet': 'False', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'target_path': 'None', 'invocation_command': 'dbt run --select stg_neighborhoods', 'warn_error': 'None', 'write_json': 'True', 'log_format': 'default', 'debug': 'False', 'log_cache_events': 'False', 'static_parser': 'True', 'use_experimental_parser': 'False', 'version_check': 'True', 'empty': 'False', 'send_anonymous_usage_stats': 'True', 'printer_width': '80', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'partial_parse': 'True'}
[0m20:15:04.783445 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '67fd8b32-abd1-4fe7-9a78-ccd3790e4b22', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e7f867ddd30>]}
[0m20:15:04.850002 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '67fd8b32-abd1-4fe7-9a78-ccd3790e4b22', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e7fa82ae000>]}
[0m20:15:04.850963 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m20:15:05.145434 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m20:15:05.466989 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
[0m20:15:05.471303 [debug] [MainThread]: Partial parsing: added file: data_pipeline_project://models/staging/stg_neighborhoods.sql
[0m20:15:05.826379 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.data_pipeline_project.marts
[0m20:15:05.858692 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '67fd8b32-abd1-4fe7-9a78-ccd3790e4b22', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e7f8576f2f0>]}
[0m20:15:06.029060 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m20:15:06.032241 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m20:15:06.053707 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '67fd8b32-abd1-4fe7-9a78-ccd3790e4b22', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e7f861b7650>]}
[0m20:15:06.054737 [info ] [MainThread]: Found 9 models, 10 sources, 508 macros
[0m20:15:06.055547 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '67fd8b32-abd1-4fe7-9a78-ccd3790e4b22', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e7f8602dbb0>]}
[0m20:15:06.057530 [info ] [MainThread]: 
[0m20:15:06.058755 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m20:15:06.059680 [info ] [MainThread]: 
[0m20:15:06.060896 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m20:15:06.063146 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812'
[0m20:15:06.065946 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:15:07.108409 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812, now list_data-pipeline-project-474812_analytics_staging_staging)
[0m20:15:07.109368 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m20:15:08.109920 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '67fd8b32-abd1-4fe7-9a78-ccd3790e4b22', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e7fa5c1bfb0>]}
[0m20:15:08.110630 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:15:08.121390 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.stg_neighborhoods
[0m20:15:08.122954 [info ] [Thread-1 (]: 1 of 1 START sql view model analytics_staging_staging.stg_neighborhoods ........ [RUN]
[0m20:15:08.123949 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now model.data_pipeline_project.stg_neighborhoods)
[0m20:15:08.124431 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.stg_neighborhoods
[0m20:15:08.133004 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.stg_neighborhoods"
[0m20:15:08.134198 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.stg_neighborhoods
[0m20:15:08.189526 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.stg_neighborhoods"
[0m20:15:08.190592 [debug] [Thread-1 (]: On model.data_pipeline_project.stg_neighborhoods: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.stg_neighborhoods"} */


  create or replace view `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_neighborhoods`
  OPTIONS()
  as with source as (

    select * from `data-pipeline-project-474812`.`raw_data`.`neighborhoods`

)

select distinct
    -- Primary Key
    _id as neighborhood_id,
    
    -- Foreign Keys
    _city as city_id,
    _country as country_id,
    
    -- Attributes
    name as neighborhood_name,
    postalCode as postal_code

from source
where _id is not null;


[0m20:15:08.191484 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m20:15:09.223996 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:759d352a-fdbf-4d85-b57d-ed3cb98eaef3&page=queryresults
[0m20:15:09.847243 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '67fd8b32-abd1-4fe7-9a78-ccd3790e4b22', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e7f89348aa0>]}
[0m20:15:09.849335 [info ] [Thread-1 (]: 1 of 1 OK created sql view model analytics_staging_staging.stg_neighborhoods ... [[32mCREATE VIEW (0 processed)[0m in 1.72s]
[0m20:15:09.851109 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.stg_neighborhoods
[0m20:15:09.854751 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m20:15:09.857324 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:15:09.858035 [debug] [MainThread]: Connection 'model.data_pipeline_project.stg_neighborhoods' was properly closed.
[0m20:15:09.858482 [info ] [MainThread]: 
[0m20:15:09.858878 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 3.80 seconds (3.80s).
[0m20:15:09.859604 [debug] [MainThread]: Command end result
[0m20:15:09.894171 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m20:15:09.896693 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m20:15:09.905329 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m20:15:09.906023 [info ] [MainThread]: 
[0m20:15:09.906736 [info ] [MainThread]: [32mCompleted successfully[0m
[0m20:15:09.907476 [info ] [MainThread]: 
[0m20:15:09.908159 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m20:15:09.909585 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 7.597565, "process_in_blocks": "0", "process_kernel_time": 1.258911, "process_mem_max_rss": "381996", "process_out_blocks": "3376", "process_user_time": 4.87517}
[0m20:15:09.910514 [debug] [MainThread]: Command `dbt run` succeeded at 20:15:09.910390 after 7.60 seconds
[0m20:15:09.911322 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e7fa623c440>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e7fa6a7dc10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e7f86120620>]}
[0m20:15:09.912344 [debug] [MainThread]: Flushing usage events
[0m20:15:10.850701 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:20:16.438075 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x710b2c7e7140>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x710b2e500110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x710b2bc7deb0>]}


============================== 20:20:16.441598 | de72953a-3751-43cb-91df-198f9b811f0f ==============================
[0m20:20:16.441598 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m20:20:16.442269 [debug] [MainThread]: running dbt with arguments {'write_json': 'True', 'log_cache_events': 'False', 'warn_error': 'None', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'static_parser': 'True', 'profiles_dir': '/home/ecem/.dbt', 'quiet': 'False', 'introspect': 'True', 'send_anonymous_usage_stats': 'True', 'printer_width': '80', 'use_colors': 'True', 'log_format': 'default', 'empty': 'False', 'invocation_command': 'dbt run --select stg_addresses', 'version_check': 'True', 'indirect_selection': 'eager', 'debug': 'False', 'no_print': 'None', 'cache_selected_only': 'False', 'target_path': 'None', 'fail_fast': 'False', 'partial_parse': 'True', 'use_experimental_parser': 'False'}
[0m20:20:18.845392 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'de72953a-3751-43cb-91df-198f9b811f0f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x710b0c0aec30>]}
[0m20:20:18.928729 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'de72953a-3751-43cb-91df-198f9b811f0f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x710b0c2eb8c0>]}
[0m20:20:18.930246 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m20:20:19.165954 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m20:20:19.420673 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
[0m20:20:19.424531 [debug] [MainThread]: Partial parsing: added file: data_pipeline_project://models/staging/stg_addresses.sql
[0m20:20:19.690533 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.data_pipeline_project.marts
[0m20:20:19.706675 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'de72953a-3751-43cb-91df-198f9b811f0f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x710b0b16f0e0>]}
[0m20:20:19.825472 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m20:20:19.828669 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m20:20:19.846237 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'de72953a-3751-43cb-91df-198f9b811f0f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x710b0bcae0c0>]}
[0m20:20:19.847078 [info ] [MainThread]: Found 10 models, 10 sources, 508 macros
[0m20:20:19.847670 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'de72953a-3751-43cb-91df-198f9b811f0f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x710b0ba777d0>]}
[0m20:20:19.849769 [info ] [MainThread]: 
[0m20:20:19.850592 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m20:20:19.851309 [info ] [MainThread]: 
[0m20:20:19.852511 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m20:20:19.854238 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812'
[0m20:20:19.855260 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:20:21.049215 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812, now list_data-pipeline-project-474812_analytics_staging_staging)
[0m20:20:21.051193 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m20:20:22.084460 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'de72953a-3751-43cb-91df-198f9b811f0f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x710b0ba4f770>]}
[0m20:20:22.086520 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:20:22.097588 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.stg_addresses
[0m20:20:22.098605 [info ] [Thread-1 (]: 1 of 1 START sql view model analytics_staging_staging.stg_addresses ............ [RUN]
[0m20:20:22.099386 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now model.data_pipeline_project.stg_addresses)
[0m20:20:22.100193 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.stg_addresses
[0m20:20:22.111031 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.stg_addresses"
[0m20:20:22.112009 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.stg_addresses
[0m20:20:22.155401 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.stg_addresses"
[0m20:20:22.156619 [debug] [Thread-1 (]: On model.data_pipeline_project.stg_addresses: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.stg_addresses"} */


  create or replace view `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_addresses`
  OPTIONS()
  as with source as (

    select * from `data-pipeline-project-474812`.`raw_data`.`addresses`

)

select distinct
    -- Primary Key
    _id as address_id,
    
    -- Foreign Keys (Geographical and User Links)
    _city as city_id,
    _state as state_id,
    _country as country_id,
    _user as user_id, 
    neighborhood as neighborhood_id, -- Maps to the neighborhood table ID
    
    -- Attributes
    invoiceType as invoice_type

from source
where _id is not null;


[0m20:20:22.158321 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m20:20:23.258390 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:3574f464-636d-40af-ae9e-ce89e45d2279&page=queryresults
[0m20:20:23.523626 [error] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:3574f464-636d-40af-ae9e-ce89e45d2279&page=queryresults
[0m20:20:23.540919 [debug] [Thread-1 (]: Database Error in model stg_addresses (models/staging/stg_addresses.sql)
  Unrecognized name: neighborhood; Did you mean _neighborhood? at [21:5]
  compiled code at target/run/data_pipeline_project/models/staging/stg_addresses.sql
[0m20:20:23.543930 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'de72953a-3751-43cb-91df-198f9b811f0f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x710b2c383aa0>]}
[0m20:20:23.544767 [error] [Thread-1 (]: 1 of 1 ERROR creating sql view model analytics_staging_staging.stg_addresses ... [[31mERROR[0m in 1.44s]
[0m20:20:23.545526 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.stg_addresses
[0m20:20:23.546585 [debug] [Thread-7 (]: Marking all children of 'model.data_pipeline_project.stg_addresses' to be skipped because of status 'error'.  Reason: Database Error in model stg_addresses (models/staging/stg_addresses.sql)
  Unrecognized name: neighborhood; Did you mean _neighborhood? at [21:5]
  compiled code at target/run/data_pipeline_project/models/staging/stg_addresses.sql.
[0m20:20:23.552444 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m20:20:23.554530 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:20:23.555060 [debug] [MainThread]: Connection 'model.data_pipeline_project.stg_addresses' was properly closed.
[0m20:20:23.555433 [info ] [MainThread]: 
[0m20:20:23.556212 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 3.70 seconds (3.70s).
[0m20:20:23.557191 [debug] [MainThread]: Command end result
[0m20:20:23.597865 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m20:20:23.602323 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m20:20:23.617564 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m20:20:23.618338 [info ] [MainThread]: 
[0m20:20:23.619096 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m20:20:23.620191 [info ] [MainThread]: 
[0m20:20:23.621189 [error] [MainThread]: [31mFailure in model stg_addresses (models/staging/stg_addresses.sql)[0m
[0m20:20:23.622187 [error] [MainThread]:   Database Error in model stg_addresses (models/staging/stg_addresses.sql)
  Unrecognized name: neighborhood; Did you mean _neighborhood? at [21:5]
  compiled code at target/run/data_pipeline_project/models/staging/stg_addresses.sql
[0m20:20:23.623386 [info ] [MainThread]: 
[0m20:20:23.624301 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_project/models/staging/stg_addresses.sql
[0m20:20:23.625281 [info ] [MainThread]: 
[0m20:20:23.626473 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m20:20:23.627981 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 7.2502627, "process_in_blocks": "0", "process_kernel_time": 1.197322, "process_mem_max_rss": "378836", "process_out_blocks": "3392", "process_user_time": 4.597718}
[0m20:20:23.629347 [debug] [MainThread]: Command `dbt run` failed at 20:20:23.629200 after 7.25 seconds
[0m20:20:23.630171 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x710b2b953440>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x710b0ba152e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x710b0c076300>]}
[0m20:20:23.631302 [debug] [MainThread]: Flushing usage events
[0m20:20:24.589757 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:21:08.756009 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a0505212e70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a050331e090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a0503fb81a0>]}


============================== 20:21:08.759629 | c1bb21b1-5c40-4308-9f47-a135a7b51835 ==============================
[0m20:21:08.759629 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m20:21:08.760628 [debug] [MainThread]: running dbt with arguments {'log_cache_events': 'False', 'debug': 'False', 'profiles_dir': '/home/ecem/.dbt', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'send_anonymous_usage_stats': 'True', 'use_colors': 'True', 'fail_fast': 'False', 'empty': 'False', 'use_experimental_parser': 'False', 'write_json': 'True', 'indirect_selection': 'eager', 'target_path': 'None', 'warn_error': 'None', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'partial_parse': 'True', 'static_parser': 'True', 'printer_width': '80', 'quiet': 'False', 'introspect': 'True', 'no_print': 'None', 'invocation_command': 'dbt run --select stg_addresses', 'cache_selected_only': 'False', 'log_format': 'default'}
[0m20:21:12.122629 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c1bb21b1-5c40-4308-9f47-a135a7b51835', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a0504ca3b90>]}
[0m20:21:12.207291 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c1bb21b1-5c40-4308-9f47-a135a7b51835', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a04e30be2a0>]}
[0m20:21:12.209179 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m20:21:12.424963 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m20:21:12.729702 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m20:21:12.731232 [debug] [MainThread]: Partial parsing: updated file: data_pipeline_project://models/staging/stg_addresses.sql
[0m20:21:13.071825 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.data_pipeline_project.marts
[0m20:21:13.092233 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c1bb21b1-5c40-4308-9f47-a135a7b51835', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a04e206f800>]}
[0m20:21:13.222618 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m20:21:13.227412 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m20:21:13.247273 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c1bb21b1-5c40-4308-9f47-a135a7b51835', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a04e2baef00>]}
[0m20:21:13.248111 [info ] [MainThread]: Found 10 models, 10 sources, 508 macros
[0m20:21:13.248705 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c1bb21b1-5c40-4308-9f47-a135a7b51835', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a04e292af90>]}
[0m20:21:13.250937 [info ] [MainThread]: 
[0m20:21:13.252323 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m20:21:13.253384 [info ] [MainThread]: 
[0m20:21:13.256174 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m20:21:13.257806 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812'
[0m20:21:13.258616 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:21:14.276935 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812, now list_data-pipeline-project-474812_analytics_staging_staging)
[0m20:21:14.278029 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m20:21:15.256226 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c1bb21b1-5c40-4308-9f47-a135a7b51835', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a04e2a58950>]}
[0m20:21:15.257350 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:21:15.270385 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.stg_addresses
[0m20:21:15.272399 [info ] [Thread-1 (]: 1 of 1 START sql view model analytics_staging_staging.stg_addresses ............ [RUN]
[0m20:21:15.273989 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now model.data_pipeline_project.stg_addresses)
[0m20:21:15.275021 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.stg_addresses
[0m20:21:15.282603 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.stg_addresses"
[0m20:21:15.285084 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.stg_addresses
[0m20:21:15.337842 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.stg_addresses"
[0m20:21:15.339800 [debug] [Thread-1 (]: On model.data_pipeline_project.stg_addresses: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.stg_addresses"} */


  create or replace view `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_addresses`
  OPTIONS()
  as with source as (

    select * from `data-pipeline-project-474812`.`raw_data`.`addresses`

)

select distinct
    -- Primary Key
    _id as address_id,
    
    -- Foreign Keys (Geographical and User Links)
    _city as city_id,
    _state as state_id,
    _country as country_id,
    _user as user_id, 
    _neighborhood as neighborhood_id, -- Maps to the neighborhood table ID
    
    -- Attributes
    invoiceType as invoice_type

from source
where _id is not null;


[0m20:21:15.341075 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m20:21:16.685506 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:9ea9010d-95dd-490b-8244-24140f96355d&page=queryresults
[0m20:21:17.336020 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c1bb21b1-5c40-4308-9f47-a135a7b51835', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a04e5c48650>]}
[0m20:21:17.339143 [info ] [Thread-1 (]: 1 of 1 OK created sql view model analytics_staging_staging.stg_addresses ....... [[32mCREATE VIEW (0 processed)[0m in 2.06s]
[0m20:21:17.341032 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.stg_addresses
[0m20:21:17.344274 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m20:21:17.346724 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:21:17.347546 [debug] [MainThread]: Connection 'model.data_pipeline_project.stg_addresses' was properly closed.
[0m20:21:17.348444 [info ] [MainThread]: 
[0m20:21:17.349341 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 4.09 seconds (4.09s).
[0m20:21:17.351063 [debug] [MainThread]: Command end result
[0m20:21:17.397493 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m20:21:17.401563 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m20:21:17.411083 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m20:21:17.411965 [info ] [MainThread]: 
[0m20:21:17.413545 [info ] [MainThread]: [32mCompleted successfully[0m
[0m20:21:17.414459 [info ] [MainThread]: 
[0m20:21:17.415467 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m20:21:17.416948 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 8.722721, "process_in_blocks": "0", "process_kernel_time": 1.145729, "process_mem_max_rss": "382200", "process_out_blocks": "3392", "process_user_time": 5.81678}
[0m20:21:17.418311 [debug] [MainThread]: Command `dbt run` succeeded at 20:21:17.418160 after 8.72 seconds
[0m20:21:17.419483 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a05029eddc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a04e2b610d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a04e2b610a0>]}
[0m20:21:17.423025 [debug] [MainThread]: Flushing usage events
[0m20:21:18.114711 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:07:25.302125 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7327945e08f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73279442b6b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73279442a0c0>]}


============================== 13:07:25.308087 | 850d0232-bd19-4d79-afec-280c8645771a ==============================
[0m13:07:25.308087 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m13:07:25.308965 [debug] [MainThread]: running dbt with arguments {'empty': 'None', 'debug': 'False', 'cache_selected_only': 'False', 'invocation_command': 'dbt test --select source:raw_data', 'log_format': 'default', 'introspect': 'True', 'quiet': 'False', 'printer_width': '80', 'no_print': 'None', 'target_path': 'None', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'send_anonymous_usage_stats': 'True', 'static_parser': 'True', 'fail_fast': 'False', 'profiles_dir': '/home/ecem/.dbt', 'partial_parse': 'True', 'warn_error': 'None', 'write_json': 'True', 'log_cache_events': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_experimental_parser': 'False', 'version_check': 'True', 'use_colors': 'True', 'indirect_selection': 'eager'}
[0m13:07:29.812043 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '850d0232-bd19-4d79-afec-280c8645771a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x732773e3b4d0>]}
[0m13:07:29.880989 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '850d0232-bd19-4d79-afec-280c8645771a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x732795c46330>]}
[0m13:07:29.883101 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m13:07:30.160902 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m13:07:30.487005 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
[0m13:07:30.490169 [debug] [MainThread]: Partial parsing: added file: data_pipeline_project://models/staging/staging_schema.yml
[0m13:07:30.981007 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `relationships` defined on 'stg_orders' in
package 'data_pipeline_project' (models/staging/staging_schema.yml). Arguments
to generic tests should be nested under the `arguments` property.
[0m13:07:30.984929 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '850d0232-bd19-4d79-afec-280c8645771a', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x732775b497f0>]}
[0m13:07:31.351481 [error] [MainThread]: Encountered an error:
Compilation Error
  dbt found two sources with the name "raw_data_users".
  
  Since these resources have the same name, dbt will be unable to find the correct resource
  when looking for source("raw_data", "users").
  
  To fix this, change the name of one of these resources:
  - source.data_pipeline_project.raw_data.users (models/staging/staging_schema.yml)
  - source.data_pipeline_project.raw_data.users (models/staging/raw_sources.yml)
[0m13:07:31.355163 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 5 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m13:07:31.358588 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 6.1299825, "process_in_blocks": "4504", "process_kernel_time": 2.350081, "process_mem_max_rss": "372116", "process_out_blocks": "16", "process_user_time": 6.46219}
[0m13:07:31.359424 [debug] [MainThread]: Command `dbt test` failed at 13:07:31.359317 after 6.13 seconds
[0m13:07:31.360159 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x732793b848f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x732773b8b800>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x732773d3fa10>]}
[0m13:07:31.360983 [debug] [MainThread]: Flushing usage events
[0m13:07:32.566936 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:18:17.010832 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7926e64f4b60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7926e5d2d1c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7926e5b9a2a0>]}


============================== 13:18:17.016461 | ca44ab4b-5d8c-42af-8a74-7526cd3b40f1 ==============================
[0m13:18:17.016461 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m13:18:17.017433 [debug] [MainThread]: running dbt with arguments {'cache_selected_only': 'False', 'introspect': 'True', 'no_print': 'None', 'log_format': 'default', 'quiet': 'False', 'static_parser': 'True', 'empty': 'None', 'invocation_command': 'dbt source freshness', 'partial_parse': 'True', 'printer_width': '80', 'version_check': 'True', 'target_path': 'None', 'use_experimental_parser': 'False', 'send_anonymous_usage_stats': 'True', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'log_cache_events': 'False', 'write_json': 'True', 'debug': 'False', 'profiles_dir': '/home/ecem/.dbt', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'indirect_selection': 'eager'}
[0m13:18:19.673104 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ca44ab4b-5d8c-42af-8a74-7526cd3b40f1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7926c6e3fef0>]}
[0m13:18:19.751655 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ca44ab4b-5d8c-42af-8a74-7526cd3b40f1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7926c610b9e0>]}
[0m13:18:19.753647 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m13:18:20.008206 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m13:18:20.267082 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 1 files changed.
[0m13:18:20.269092 [debug] [MainThread]: Partial parsing: added file: data_pipeline_project://models/staging/staging_schema.yml
[0m13:18:20.270553 [debug] [MainThread]: Partial parsing: updated file: data_pipeline_project://models/staging/raw_sources.yml
[0m13:18:20.696786 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `relationships` defined on 'stg_orders' in
package 'data_pipeline_project' (models/staging/staging_schema.yml). Arguments
to generic tests should be nested under the `arguments` property.
[0m13:18:20.698668 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': 'ca44ab4b-5d8c-42af-8a74-7526cd3b40f1', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7926c58297c0>]}
[0m13:18:21.095130 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.data_pipeline_project.marts
[0m13:18:21.114063 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ca44ab4b-5d8c-42af-8a74-7526cd3b40f1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7926c4d92300>]}
[0m13:18:21.254097 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m13:18:21.259059 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m13:18:21.283884 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ca44ab4b-5d8c-42af-8a74-7526cd3b40f1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7926c4df7770>]}
[0m13:18:21.284541 [info ] [MainThread]: Found 10 models, 37 data tests, 10 sources, 508 macros
[0m13:18:21.285232 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ca44ab4b-5d8c-42af-8a74-7526cd3b40f1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7926c4c3ddf0>]}
[0m13:18:21.287861 [info ] [MainThread]: 
[0m13:18:21.288718 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m13:18:21.289237 [info ] [MainThread]: 
[0m13:18:21.290223 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m13:18:21.296685 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_staging'
[0m13:18:21.298029 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:18:22.387913 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ca44ab4b-5d8c-42af-8a74-7526cd3b40f1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7926e5d2f500>]}
[0m13:18:22.389185 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:18:22.428055 [debug] [Thread-1 (]: Began running node source.data_pipeline_project.raw_data.orders
[0m13:18:22.428921 [info ] [Thread-1 (]: 1 of 4 START freshness of raw_data.orders ...................................... [RUN]
[0m13:18:22.429593 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now source.data_pipeline_project.raw_data.orders)
[0m13:18:22.430445 [debug] [Thread-1 (]: Began compiling node source.data_pipeline_project.raw_data.orders
[0m13:18:22.431232 [debug] [Thread-2 (]: Began running node source.data_pipeline_project.raw_data.shipments
[0m13:18:22.432370 [debug] [Thread-1 (]: Began executing node source.data_pipeline_project.raw_data.orders
[0m13:18:22.433613 [debug] [Thread-3 (]: Began running node source.data_pipeline_project.raw_data.subscriptions
[0m13:18:22.434988 [debug] [Thread-4 (]: Began running node source.data_pipeline_project.raw_data.users
[0m13:18:22.434456 [info ] [Thread-2 (]: 2 of 4 START freshness of raw_data.shipments ................................... [RUN]
[0m13:18:22.447828 [info ] [Thread-3 (]: 3 of 4 START freshness of raw_data.subscriptions ............................... [RUN]
[0m13:18:22.452910 [debug] [Thread-1 (]: On source.data_pipeline_project.raw_data.orders: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "source.data_pipeline_project.raw_data.orders"} */
select
      max(createdAt) as max_loaded_at,
      current_timestamp() as snapshotted_at
    from `data-pipeline-project-474812`.`raw_data`.`orders`
    
  
[0m13:18:22.453942 [info ] [Thread-4 (]: 4 of 4 START freshness of raw_data.users ....................................... [RUN]
[0m13:18:22.459944 [debug] [Thread-4 (]: Acquiring new bigquery connection 'source.data_pipeline_project.raw_data.users'
[0m13:18:22.455771 [debug] [Thread-2 (]: Acquiring new bigquery connection 'source.data_pipeline_project.raw_data.shipments'
[0m13:18:22.458383 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:18:22.460432 [debug] [Thread-4 (]: Began compiling node source.data_pipeline_project.raw_data.users
[0m13:18:22.461007 [debug] [Thread-2 (]: Began compiling node source.data_pipeline_project.raw_data.shipments
[0m13:18:22.457538 [debug] [Thread-3 (]: Acquiring new bigquery connection 'source.data_pipeline_project.raw_data.subscriptions'
[0m13:18:22.496881 [debug] [Thread-4 (]: Began executing node source.data_pipeline_project.raw_data.users
[0m13:18:22.499067 [debug] [Thread-2 (]: Began executing node source.data_pipeline_project.raw_data.shipments
[0m13:18:22.500039 [debug] [Thread-3 (]: Began compiling node source.data_pipeline_project.raw_data.subscriptions
[0m13:18:22.505411 [debug] [Thread-4 (]: On source.data_pipeline_project.raw_data.users: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "source.data_pipeline_project.raw_data.users"} */
select
      max(createdAt) as max_loaded_at,
      current_timestamp() as snapshotted_at
    from `data-pipeline-project-474812`.`raw_data`.`users`
    
  
[0m13:18:22.514978 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m13:18:22.513962 [debug] [Thread-3 (]: Began executing node source.data_pipeline_project.raw_data.subscriptions
[0m13:18:22.512963 [debug] [Thread-2 (]: On source.data_pipeline_project.raw_data.shipments: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "source.data_pipeline_project.raw_data.shipments"} */
select
      max(createdAt) as max_loaded_at,
      current_timestamp() as snapshotted_at
    from `data-pipeline-project-474812`.`raw_data`.`shipments`
    
  
[0m13:18:22.518098 [debug] [Thread-3 (]: On source.data_pipeline_project.raw_data.subscriptions: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "source.data_pipeline_project.raw_data.subscriptions"} */
select
      max(createdAt) as max_loaded_at,
      current_timestamp() as snapshotted_at
    from `data-pipeline-project-474812`.`raw_data`.`subscriptions`
    
  
[0m13:18:22.519001 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m13:18:22.566825 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m13:18:23.820812 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:ca01f4ea-b906-45c2-9c0b-091ee3f213bc&page=queryresults
[0m13:18:23.822007 [error] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:ca01f4ea-b906-45c2-9c0b-091ee3f213bc&page=queryresults
[0m13:18:23.822920 [debug] [Thread-2 (]: BigQuery adapter: Unhandled error while running:
macro collect_freshness
[0m13:18:23.823509 [debug] [Thread-2 (]: BigQuery adapter: Database Error
  Unrecognized name: createdAt at [3:11]
[0m13:18:23.836449 [debug] [Thread-2 (]: Database Error in source shipments (models/staging/raw_sources.yml)
  Unrecognized name: createdAt at [3:11]
[0m13:18:23.837611 [error] [Thread-2 (]: 2 of 4 ERROR freshness of raw_data.shipments ................................... [[31mERROR[0m in 1.38s]
[0m13:18:23.838822 [debug] [Thread-2 (]: Finished running node source.data_pipeline_project.raw_data.shipments
[0m13:18:23.921418 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:933fced4-2863-4881-b62a-e32afe396a35&page=queryresults
[0m13:18:24.212415 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:41e8fb7a-440f-47cc-b46a-c1e29da10b7c&page=queryresults
[0m13:18:24.253207 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:f603e8ca-a8ba-42f7-be7e-cd97c3ffda8a&page=queryresults
[0m13:18:24.965415 [error] [Thread-1 (]: 1 of 4 ERROR STALE freshness of raw_data.orders ................................ [[31mERROR STALE[0m in 2.54s]
[0m13:18:24.969717 [debug] [Thread-1 (]: Finished running node source.data_pipeline_project.raw_data.orders
[0m13:18:24.977433 [debug] [Thread-7 (]: Marking all children of 'source.data_pipeline_project.raw_data.orders' to be skipped because of status 'error'. 
[0m13:18:25.195011 [error] [Thread-4 (]: 4 of 4 ERROR STALE freshness of raw_data.users ................................. [[31mERROR STALE[0m in 2.73s]
[0m13:18:25.198575 [debug] [Thread-4 (]: Finished running node source.data_pipeline_project.raw_data.users
[0m13:18:25.203842 [debug] [Thread-7 (]: Marking all children of 'source.data_pipeline_project.raw_data.users' to be skipped because of status 'error'. 
[0m13:18:25.340606 [error] [Thread-3 (]: 3 of 4 ERROR STALE freshness of raw_data.subscriptions ......................... [[31mERROR STALE[0m in 2.88s]
[0m13:18:25.344126 [debug] [Thread-3 (]: Finished running node source.data_pipeline_project.raw_data.subscriptions
[0m13:18:25.345368 [debug] [Thread-7 (]: Marking all children of 'source.data_pipeline_project.raw_data.subscriptions' to be skipped because of status 'error'. 
[0m13:18:25.348695 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m13:18:25.351399 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:18:25.352108 [debug] [MainThread]: Connection 'source.data_pipeline_project.raw_data.orders' was properly closed.
[0m13:18:25.352463 [debug] [MainThread]: Connection 'source.data_pipeline_project.raw_data.shipments' was properly closed.
[0m13:18:25.352800 [debug] [MainThread]: Connection 'source.data_pipeline_project.raw_data.subscriptions' was properly closed.
[0m13:18:25.353136 [debug] [MainThread]: Connection 'source.data_pipeline_project.raw_data.users' was properly closed.
[0m13:18:25.354068 [info ] [MainThread]: 
[0m13:18:25.355053 [info ] [MainThread]: Finished running 4 sources in 0 hours 0 minutes and 4.06 seconds (4.06s).
[0m13:18:25.387333 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m13:18:25.391219 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m13:18:25.396887 [debug] [MainThread]: Wrote artifact FreshnessResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/sources.json
[0m13:18:25.397396 [info ] [MainThread]: 
[0m13:18:25.398051 [error] [MainThread]:   Database Error in source shipments (models/staging/raw_sources.yml)
  Unrecognized name: createdAt at [3:11]
[0m13:18:25.398625 [info ] [MainThread]: 
[0m13:18:25.399202 [error] [MainThread]: [31mFailure in source orders (models/staging/raw_sources.yml)[0m
[0m13:18:25.399760 [error] [MainThread]:   Status: error
[0m13:18:25.400298 [info ] [MainThread]: 
[0m13:18:25.400906 [error] [MainThread]: [31mFailure in source users (models/staging/raw_sources.yml)[0m
[0m13:18:25.401494 [error] [MainThread]:   Status: error
[0m13:18:25.401957 [info ] [MainThread]: 
[0m13:18:25.403638 [error] [MainThread]: [31mFailure in source subscriptions (models/staging/raw_sources.yml)[0m
[0m13:18:25.404444 [error] [MainThread]:   Status: error
[0m13:18:25.406237 [info ] [MainThread]: Done.
[0m13:18:25.407513 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 18 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m13:18:25.409245 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_success": false, "command_wall_clock_time": 8.45696, "process_in_blocks": "2264", "process_kernel_time": 1.424775, "process_mem_max_rss": "393908", "process_out_blocks": "3944", "process_user_time": 5.838365}
[0m13:18:25.410069 [debug] [MainThread]: Command `dbt source freshness` failed at 13:18:25.409937 after 8.46 seconds
[0m13:18:25.410639 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7926e543bf20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7926c4c264b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7926c4c24e00>]}
[0m13:18:25.411309 [debug] [MainThread]: Flushing usage events
[0m13:18:26.594394 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:19:10.851741 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7055637d76b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7055634a4b60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7055634d7080>]}


============================== 13:19:10.872283 | 94f48447-0010-4938-a680-3c9827cfb6e9 ==============================
[0m13:19:10.872283 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m13:19:10.877902 [debug] [MainThread]: running dbt with arguments {'empty': 'None', 'invocation_command': 'dbt test --select source:raw_data', 'fail_fast': 'False', 'use_colors': 'True', 'cache_selected_only': 'False', 'log_cache_events': 'False', 'no_print': 'None', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'log_format': 'default', 'target_path': 'None', 'version_check': 'True', 'debug': 'False', 'profiles_dir': '/home/ecem/.dbt', 'warn_error': 'None', 'write_json': 'True', 'partial_parse': 'True', 'static_parser': 'True', 'introspect': 'True', 'indirect_selection': 'eager', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'quiet': 'False', 'printer_width': '80', 'send_anonymous_usage_stats': 'True', 'use_experimental_parser': 'False'}
[0m13:19:13.216411 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '94f48447-0010-4938-a680-3c9827cfb6e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x705543dbbbf0>]}
[0m13:19:13.300482 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '94f48447-0010-4938-a680-3c9827cfb6e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x705544d52cc0>]}
[0m13:19:13.303802 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m13:19:13.607988 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m13:19:13.905705 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m13:19:13.906931 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m13:19:13.913283 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.data_pipeline_project.marts
[0m13:19:13.963187 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '94f48447-0010-4938-a680-3c9827cfb6e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7055437039e0>]}
[0m13:19:14.082215 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m13:19:14.084909 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m13:19:14.117480 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '94f48447-0010-4938-a680-3c9827cfb6e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7055432598e0>]}
[0m13:19:14.118419 [info ] [MainThread]: Found 10 models, 37 data tests, 10 sources, 508 macros
[0m13:19:14.119158 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '94f48447-0010-4938-a680-3c9827cfb6e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x705543ad9d60>]}
[0m13:19:14.121744 [info ] [MainThread]: 
[0m13:19:14.122917 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m13:19:14.123721 [info ] [MainThread]: 
[0m13:19:14.124808 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m13:19:14.130739 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_staging'
[0m13:19:14.131612 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:19:15.133275 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '94f48447-0010-4938-a680-3c9827cfb6e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7055436df6e0>]}
[0m13:19:15.134934 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:19:15.163134 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc
[0m13:19:15.165997 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd
[0m13:19:15.168311 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6
[0m13:19:15.173343 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be
[0m13:19:15.171208 [info ] [Thread-2 (]: 2 of 13 START test source_not_null_raw_data_cities__id ......................... [RUN]
[0m13:19:15.176307 [info ] [Thread-3 (]: 3 of 13 START test source_not_null_raw_data_countries__id ...................... [RUN]
[0m13:19:15.179277 [info ] [Thread-1 (]: 1 of 13 START test source_not_null_raw_data_addresses__id ...................... [RUN]
[0m13:19:15.186991 [info ] [Thread-4 (]: 4 of 13 START test source_not_null_raw_data_neighborhoods__id .................. [RUN]
[0m13:19:15.194701 [debug] [Thread-2 (]: Acquiring new bigquery connection 'test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc'
[0m13:19:15.200121 [debug] [Thread-3 (]: Acquiring new bigquery connection 'test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd'
[0m13:19:15.204216 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6)
[0m13:19:15.212953 [debug] [Thread-4 (]: Acquiring new bigquery connection 'test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be'
[0m13:19:15.217191 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc
[0m13:19:15.219075 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd
[0m13:19:15.225512 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6
[0m13:19:15.228983 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be
[0m13:19:15.458223 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6"
[0m13:19:15.465198 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc"
[0m13:19:15.473879 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be"
[0m13:19:15.477902 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd"
[0m13:19:15.480455 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6
[0m13:19:15.483524 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc
[0m13:19:15.491418 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be
[0m13:19:15.562416 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd
[0m13:19:15.713337 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6"
[0m13:19:15.715971 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc"
[0m13:19:15.720321 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd"
[0m13:19:15.721835 [debug] [Thread-1 (]: On test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`addresses`
where _id is null



  
  
      
    ) dbt_internal_test
[0m13:19:15.723296 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be"
[0m13:19:15.724754 [debug] [Thread-2 (]: On test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`cities`
where _id is null



  
  
      
    ) dbt_internal_test
[0m13:19:15.725230 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:19:15.726198 [debug] [Thread-4 (]: On test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`neighborhoods`
where _id is null



  
  
      
    ) dbt_internal_test
[0m13:19:15.727081 [debug] [Thread-3 (]: On test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`countries`
where _id is null



  
  
      
    ) dbt_internal_test
[0m13:19:15.727984 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m13:19:15.770998 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m13:19:15.771877 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m13:19:17.266219 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:2b4dfd64-fc18-4f97-b557-12907ab96fcf&page=queryresults
[0m13:19:17.283476 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:26d9d883-5cb0-4d11-b6b2-4a7a8bc4e910&page=queryresults
[0m13:19:17.327017 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:e86f686b-61d7-4987-8850-e97b977d0ac4&page=queryresults
[0m13:19:17.373113 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:54479b2b-edb1-47de-86f8-3f40af300b17&page=queryresults
[0m13:19:18.269898 [info ] [Thread-1 (]: 1 of 13 PASS source_not_null_raw_data_addresses__id ............................ [[32mPASS[0m in 3.07s]
[0m13:19:18.273913 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6
[0m13:19:18.275197 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878
[0m13:19:18.277384 [info ] [Thread-1 (]: 5 of 13 START test source_not_null_raw_data_orders__id ......................... [RUN]
[0m13:19:18.279137 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6, now test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878)
[0m13:19:18.280926 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878
[0m13:19:18.288279 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878"
[0m13:19:18.289705 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878
[0m13:19:18.295274 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878"
[0m13:19:18.298855 [debug] [Thread-1 (]: On test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`orders`
where _id is null



  
  
      
    ) dbt_internal_test
[0m13:19:18.301196 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:19:18.346864 [info ] [Thread-4 (]: 4 of 13 PASS source_not_null_raw_data_neighborhoods__id ........................ [[32mPASS[0m in 3.13s]
[0m13:19:18.355994 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be
[0m13:19:18.353001 [info ] [Thread-3 (]: 3 of 13 PASS source_not_null_raw_data_countries__id ............................ [[32mPASS[0m in 3.15s]
[0m13:19:18.362394 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd
[0m13:19:18.361345 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f
[0m13:19:18.363647 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2
[0m13:19:18.366228 [info ] [Thread-4 (]: 6 of 13 START test source_not_null_raw_data_orders__user ....................... [RUN]
[0m13:19:18.368852 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be, now test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f)
[0m13:19:18.369474 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f
[0m13:19:18.367018 [info ] [Thread-3 (]: 7 of 13 START test source_not_null_raw_data_shipments__id ...................... [RUN]
[0m13:19:18.380878 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f"
[0m13:19:18.381946 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd, now test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2)
[0m13:19:18.383425 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f
[0m13:19:18.384613 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2
[0m13:19:18.392364 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f"
[0m13:19:18.407001 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2"
[0m13:19:18.417482 [info ] [Thread-2 (]: 2 of 13 PASS source_not_null_raw_data_cities__id ............................... [[32mPASS[0m in 3.23s]
[0m13:19:18.420109 [debug] [Thread-4 (]: On test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _user
from `data-pipeline-project-474812`.`raw_data`.`orders`
where _user is null



  
  
      
    ) dbt_internal_test
[0m13:19:18.423234 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2
[0m13:19:18.425655 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc
[0m13:19:18.436504 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1
[0m13:19:18.434412 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2"
[0m13:19:18.430067 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m13:19:18.443943 [info ] [Thread-2 (]: 8 of 13 START test source_not_null_raw_data_shipments__order ................... [RUN]
[0m13:19:18.450442 [debug] [Thread-3 (]: On test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`shipments`
where _id is null



  
  
      
    ) dbt_internal_test
[0m13:19:18.522760 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc, now test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1)
[0m13:19:18.523857 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m13:19:18.524842 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1
[0m13:19:18.608616 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1"
[0m13:19:18.609593 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1
[0m13:19:18.614164 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1"
[0m13:19:18.615309 [debug] [Thread-2 (]: On test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _order
from `data-pipeline-project-474812`.`raw_data`.`shipments`
where _order is null



  
  
      
    ) dbt_internal_test
[0m13:19:18.615939 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m13:19:19.868874 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:a87eb459-436a-4251-91f1-7193938d71af&page=queryresults
[0m13:19:19.879214 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:4f3b83c8-d6c8-466e-9801-3dd86b64b84a&page=queryresults
[0m13:19:20.065895 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:ad40085d-3ddf-4c8a-b5af-0eb48d9c57ae&page=queryresults
[0m13:19:20.245070 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:585909b0-90df-45d6-8338-621b36046516&page=queryresults
[0m13:19:20.921623 [info ] [Thread-1 (]: 5 of 13 PASS source_not_null_raw_data_orders__id ............................... [[32mPASS[0m in 2.64s]
[0m13:19:20.926217 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878
[0m13:19:20.927681 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98
[0m13:19:20.929971 [info ] [Thread-1 (]: 9 of 13 START test source_not_null_raw_data_shipments__user .................... [RUN]
[0m13:19:20.934607 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878, now test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98)
[0m13:19:20.945788 [info ] [Thread-2 (]: 8 of 13 PASS source_not_null_raw_data_shipments__order ......................... [[32mPASS[0m in 2.42s]
[0m13:19:20.947880 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98
[0m13:19:20.951821 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1
[0m13:19:20.958252 [info ] [Thread-4 (]: 6 of 13 PASS source_not_null_raw_data_orders__user ............................. [[32mPASS[0m in 2.59s]
[0m13:19:20.970003 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98"
[0m13:19:20.971727 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86
[0m13:19:20.973564 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f
[0m13:19:20.978096 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98
[0m13:19:20.976046 [info ] [Thread-2 (]: 10 of 13 START test source_not_null_raw_data_states__id ........................ [RUN]
[0m13:19:20.980997 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a
[0m13:19:20.995347 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98"
[0m13:19:20.997263 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1, now test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86)
[0m13:19:21.001624 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86
[0m13:19:20.999659 [info ] [Thread-4 (]: 11 of 13 START test source_not_null_raw_data_subscriptions__id ................. [RUN]
[0m13:19:21.028777 [debug] [Thread-1 (]: On test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _user
from `data-pipeline-project-474812`.`raw_data`.`shipments`
where _user is null



  
  
      
    ) dbt_internal_test
[0m13:19:21.030426 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86"
[0m13:19:21.032204 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f, now test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a)
[0m13:19:21.037645 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:19:21.040393 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86
[0m13:19:21.042039 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a
[0m13:19:21.050551 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86"
[0m13:19:21.120447 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a"
[0m13:19:21.125791 [debug] [Thread-2 (]: On test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`states`
where _id is null



  
  
      
    ) dbt_internal_test
[0m13:19:21.126962 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m13:19:21.128309 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a
[0m13:19:21.209892 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a"
[0m13:19:21.211052 [debug] [Thread-4 (]: On test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`subscriptions`
where _id is null



  
  
      
    ) dbt_internal_test
[0m13:19:21.212349 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m13:19:21.277604 [info ] [Thread-3 (]: 7 of 13 PASS source_not_null_raw_data_shipments__id ............................ [[32mPASS[0m in 2.89s]
[0m13:19:21.280113 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2
[0m13:19:21.291102 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6
[0m13:19:21.292374 [info ] [Thread-3 (]: 12 of 13 START test source_not_null_raw_data_subscriptions__user ............... [RUN]
[0m13:19:21.295930 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2, now test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6)
[0m13:19:21.301057 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6
[0m13:19:21.318828 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6"
[0m13:19:21.322216 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6
[0m13:19:21.339288 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6"
[0m13:19:21.346904 [debug] [Thread-3 (]: On test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _user
from `data-pipeline-project-474812`.`raw_data`.`subscriptions`
where _user is null



  
  
      
    ) dbt_internal_test
[0m13:19:21.352114 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m13:19:22.595634 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:7d394e39-328d-4c7d-b106-db32c23bdceb&page=queryresults
[0m13:19:22.697248 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:b98cbf87-6553-4042-8052-ab504bff2c21&page=queryresults
[0m13:19:22.727378 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:4d34c927-0832-40c5-b9dd-eb921d346793&page=queryresults
[0m13:19:22.782163 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:76d4d055-755a-4de7-ab6d-e5b0f3b4e736&page=queryresults
[0m13:19:23.534122 [info ] [Thread-2 (]: 10 of 13 PASS source_not_null_raw_data_states__id .............................. [[32mPASS[0m in 2.54s]
[0m13:19:23.537855 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86
[0m13:19:23.538844 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b
[0m13:19:23.539417 [info ] [Thread-2 (]: 13 of 13 START test source_not_null_raw_data_users__id ......................... [RUN]
[0m13:19:23.540876 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86, now test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b)
[0m13:19:23.541824 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b
[0m13:19:23.547071 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b"
[0m13:19:23.549215 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b
[0m13:19:23.553468 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b"
[0m13:19:23.555124 [debug] [Thread-2 (]: On test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`users`
where _id is null



  
  
      
    ) dbt_internal_test
[0m13:19:23.556417 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m13:19:23.624806 [info ] [Thread-1 (]: 9 of 13 PASS source_not_null_raw_data_shipments__user .......................... [[32mPASS[0m in 2.69s]
[0m13:19:23.626802 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98
[0m13:19:23.716275 [info ] [Thread-4 (]: 11 of 13 PASS source_not_null_raw_data_subscriptions__id ....................... [[32mPASS[0m in 2.68s]
[0m13:19:23.718310 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a
[0m13:19:23.956123 [info ] [Thread-3 (]: 12 of 13 PASS source_not_null_raw_data_subscriptions__user ..................... [[32mPASS[0m in 2.66s]
[0m13:19:23.958880 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6
[0m13:19:25.106608 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:876f782a-3bdd-4b06-864f-0f5795d82ade&page=queryresults
[0m13:19:26.030531 [info ] [Thread-2 (]: 13 of 13 PASS source_not_null_raw_data_users__id ............................... [[32mPASS[0m in 2.49s]
[0m13:19:26.032992 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b
[0m13:19:26.038415 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m13:19:26.040873 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:19:26.041772 [debug] [MainThread]: Connection 'test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98' was properly closed.
[0m13:19:26.042290 [debug] [MainThread]: Connection 'test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b' was properly closed.
[0m13:19:26.043037 [debug] [MainThread]: Connection 'test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6' was properly closed.
[0m13:19:26.043745 [debug] [MainThread]: Connection 'test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a' was properly closed.
[0m13:19:26.045022 [info ] [MainThread]: 
[0m13:19:26.046097 [info ] [MainThread]: Finished running 13 data tests in 0 hours 0 minutes and 11.92 seconds (11.92s).
[0m13:19:26.057214 [debug] [MainThread]: Command end result
[0m13:19:26.118650 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m13:19:26.123212 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m13:19:26.136420 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m13:19:26.137478 [info ] [MainThread]: 
[0m13:19:26.138825 [info ] [MainThread]: [32mCompleted successfully[0m
[0m13:19:26.139892 [info ] [MainThread]: 
[0m13:19:26.140958 [info ] [MainThread]: Done. PASS=13 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=13
[0m13:19:26.142706 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": true, "command_wall_clock_time": 15.355473, "process_in_blocks": "0", "process_kernel_time": 2.047159, "process_mem_max_rss": "388716", "process_out_blocks": "3024", "process_user_time": 6.821597}
[0m13:19:26.144145 [debug] [MainThread]: Command `dbt test` succeeded at 13:19:26.144008 after 15.36 seconds
[0m13:19:26.145020 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x705562df36e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x705543720680>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x705564b73f50>]}
[0m13:19:26.147362 [debug] [MainThread]: Flushing usage events
[0m13:19:27.119409 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:26:06.010853 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5bf0d7da60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5bf0691ee0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5bf0643950>]}


============================== 13:26:06.014946 | 8b4689d2-ee86-4eee-9bce-e685c203bc04 ==============================
[0m13:26:06.014946 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m13:26:06.022093 [debug] [MainThread]: running dbt with arguments {'empty': 'None', 'use_experimental_parser': 'False', 'printer_width': '80', 'log_cache_events': 'False', 'partial_parse': 'True', 'log_format': 'default', 'quiet': 'False', 'no_print': 'None', 'static_parser': 'True', 'fail_fast': 'False', 'cache_selected_only': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'warn_error': 'None', 'write_json': 'True', 'indirect_selection': 'eager', 'send_anonymous_usage_stats': 'True', 'version_check': 'True', 'use_colors': 'True', 'profiles_dir': '/home/ecem/.dbt', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'target_path': 'None', 'debug': 'False', 'introspect': 'True', 'invocation_command': 'dbt test --select staging'}
[0m13:26:08.374101 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8b4689d2-ee86-4eee-9bce-e685c203bc04', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5bd08757f0>]}
[0m13:26:08.472858 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8b4689d2-ee86-4eee-9bce-e685c203bc04', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5bd07c3200>]}
[0m13:26:08.475032 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m13:26:08.807662 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m13:26:09.087927 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m13:26:09.088611 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m13:26:09.095817 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.data_pipeline_project.marts
[0m13:26:09.147220 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8b4689d2-ee86-4eee-9bce-e685c203bc04', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5bd0037f80>]}
[0m13:26:09.275991 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m13:26:09.278345 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m13:26:09.307881 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8b4689d2-ee86-4eee-9bce-e685c203bc04', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5bcff68ad0>]}
[0m13:26:09.308536 [info ] [MainThread]: Found 10 models, 37 data tests, 10 sources, 508 macros
[0m13:26:09.309287 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8b4689d2-ee86-4eee-9bce-e685c203bc04', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5bd07589b0>]}
[0m13:26:09.312541 [info ] [MainThread]: 
[0m13:26:09.313676 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m13:26:09.314219 [info ] [MainThread]: 
[0m13:26:09.315207 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m13:26:09.320986 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_staging'
[0m13:26:09.322941 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:26:10.697725 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8b4689d2-ee86-4eee-9bce-e685c203bc04', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5bd007e960>]}
[0m13:26:10.698761 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:26:10.707652 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501
[0m13:26:10.708945 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3
[0m13:26:10.709796 [info ] [Thread-1 (]: 1 of 37 START test not_null_stg_addresses_address_id ........................... [RUN]
[0m13:26:10.710921 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2
[0m13:26:10.711584 [info ] [Thread-2 (]: 2 of 37 START test not_null_stg_addresses_user_id .............................. [RUN]
[0m13:26:10.712760 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501)
[0m13:26:10.713728 [info ] [Thread-3 (]: 3 of 37 START test not_null_stg_countries_country_id ........................... [RUN]
[0m13:26:10.714911 [debug] [Thread-2 (]: Acquiring new bigquery connection 'test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3'
[0m13:26:10.718178 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3
[0m13:26:10.716271 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501
[0m13:26:10.717387 [debug] [Thread-3 (]: Acquiring new bigquery connection 'test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2'
[0m13:26:10.715514 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64
[0m13:26:10.753382 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501"
[0m13:26:10.754320 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2
[0m13:26:10.755293 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3"
[0m13:26:10.761807 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2"
[0m13:26:10.762918 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501
[0m13:26:10.756437 [info ] [Thread-4 (]: 4 of 37 START test not_null_stg_orders_order_id ................................ [RUN]
[0m13:26:10.765343 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2
[0m13:26:10.766876 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3
[0m13:26:10.781074 [debug] [Thread-4 (]: Acquiring new bigquery connection 'test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64'
[0m13:26:10.836460 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3"
[0m13:26:10.837704 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64
[0m13:26:10.849489 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2"
[0m13:26:10.860945 [debug] [Thread-3 (]: On test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select country_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_countries`
where country_id is null



  
  
      
    ) dbt_internal_test
[0m13:26:10.859137 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64"
[0m13:26:10.859969 [debug] [Thread-2 (]: On test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_addresses`
where user_id is null



  
  
      
    ) dbt_internal_test
[0m13:26:10.861653 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m13:26:10.831449 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501"
[0m13:26:10.862957 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64
[0m13:26:10.869216 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64"
[0m13:26:10.865964 [debug] [Thread-1 (]: On test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select address_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_addresses`
where address_id is null



  
  
      
    ) dbt_internal_test
[0m13:26:10.864346 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m13:26:10.929012 [debug] [Thread-4 (]: On test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select order_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_orders`
where order_id is null



  
  
      
    ) dbt_internal_test
[0m13:26:10.931064 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:26:11.009985 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m13:26:12.514226 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:41197ef8-a53a-4e58-bd51-45aa9c7374f1&page=queryresults
[0m13:26:12.516558 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:0b144028-b491-444a-9c9d-615ce471dedb&page=queryresults
[0m13:26:12.619759 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:d77d5856-6d15-4a06-a4e8-3d28d81c96fe&page=queryresults
[0m13:26:12.629766 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:9536f06e-dc4c-4458-ae25-fd15c73216b2&page=queryresults
[0m13:26:13.455287 [info ] [Thread-1 (]: 1 of 37 PASS not_null_stg_addresses_address_id ................................. [[32mPASS[0m in 2.74s]
[0m13:26:13.460196 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501
[0m13:26:13.461578 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209
[0m13:26:13.462418 [info ] [Thread-1 (]: 5 of 37 START test not_null_stg_orders_user_id ................................. [RUN]
[0m13:26:13.463707 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501, now test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209)
[0m13:26:13.464203 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209
[0m13:26:13.469936 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209"
[0m13:26:13.470858 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209
[0m13:26:13.473735 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209"
[0m13:26:13.474854 [debug] [Thread-1 (]: On test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_orders`
where user_id is null



  
  
      
    ) dbt_internal_test
[0m13:26:13.486586 [info ] [Thread-4 (]: 4 of 37 PASS not_null_stg_orders_order_id ...................................... [[32mPASS[0m in 2.70s]
[0m13:26:13.489389 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:26:13.494387 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64
[0m13:26:13.497225 [info ] [Thread-3 (]: 3 of 37 PASS not_null_stg_countries_country_id ................................. [[32mPASS[0m in 2.78s]
[0m13:26:13.541135 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.not_null_stg_shipments_order_id.2492ebf25a
[0m13:26:13.543889 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2
[0m13:26:13.547892 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380
[0m13:26:13.546920 [info ] [Thread-4 (]: 6 of 37 START test not_null_stg_shipments_order_id ............................. [RUN]
[0m13:26:13.553021 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64, now test.data_pipeline_project.not_null_stg_shipments_order_id.2492ebf25a)
[0m13:26:13.552089 [info ] [Thread-3 (]: 7 of 37 START test not_null_stg_shipments_shipment_id .......................... [RUN]
[0m13:26:13.556087 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.not_null_stg_shipments_order_id.2492ebf25a
[0m13:26:13.558173 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2, now test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380)
[0m13:26:13.574111 [info ] [Thread-2 (]: 2 of 37 PASS not_null_stg_addresses_user_id .................................... [[32mPASS[0m in 2.86s]
[0m13:26:13.578881 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_shipments_order_id.2492ebf25a"
[0m13:26:13.580112 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380
[0m13:26:13.581547 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3
[0m13:26:13.593677 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380"
[0m13:26:13.597624 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852
[0m13:26:13.598528 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.not_null_stg_shipments_order_id.2492ebf25a
[0m13:26:13.600921 [info ] [Thread-2 (]: 8 of 37 START test not_null_stg_shipments_user_id .............................. [RUN]
[0m13:26:13.606889 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380
[0m13:26:13.605842 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_shipments_order_id.2492ebf25a"
[0m13:26:13.611815 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3, now test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852)
[0m13:26:13.617475 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380"
[0m13:26:13.620493 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852
[0m13:26:13.621694 [debug] [Thread-3 (]: On test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select shipment_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_shipments`
where shipment_id is null



  
  
      
    ) dbt_internal_test
[0m13:26:13.622457 [debug] [Thread-4 (]: On test.data_pipeline_project.not_null_stg_shipments_order_id.2492ebf25a: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_shipments_order_id.2492ebf25a"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select order_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_shipments`
where order_id is null



  
  
      
    ) dbt_internal_test
[0m13:26:13.631964 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m13:26:13.635228 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m13:26:13.638048 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852"
[0m13:26:13.768116 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852
[0m13:26:13.773724 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852"
[0m13:26:13.775011 [debug] [Thread-2 (]: On test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_shipments`
where user_id is null



  
  
      
    ) dbt_internal_test
[0m13:26:13.775855 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m13:26:15.028055 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:98dbe834-c74b-4387-8fc0-26b72fc2d9ee&page=queryresults
[0m13:26:15.271047 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:a544a0a0-1164-40cd-8d99-b38a5192c18c&page=queryresults
[0m13:26:15.315933 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:7cb09793-4571-44c0-9ef0-3284ddde1807&page=queryresults
[0m13:26:15.521116 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:3df2da1e-9676-43c0-ad66-a253c7e4126a&page=queryresults
[0m13:26:15.901463 [info ] [Thread-1 (]: 5 of 37 PASS not_null_stg_orders_user_id ....................................... [[32mPASS[0m in 2.44s]
[0m13:26:15.903731 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209
[0m13:26:15.904930 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8
[0m13:26:15.906058 [info ] [Thread-1 (]: 9 of 37 START test not_null_stg_subscriptions_start_date ....................... [RUN]
[0m13:26:15.907524 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209, now test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8)
[0m13:26:15.908387 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8
[0m13:26:15.915334 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8"
[0m13:26:15.921137 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8
[0m13:26:15.925279 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8"
[0m13:26:15.927173 [debug] [Thread-1 (]: On test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select start_date
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_subscriptions`
where start_date is null



  
  
      
    ) dbt_internal_test
[0m13:26:15.928150 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:26:16.174152 [info ] [Thread-4 (]: 6 of 37 PASS not_null_stg_shipments_order_id ................................... [[32mPASS[0m in 2.62s]
[0m13:26:16.176090 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.not_null_stg_shipments_order_id.2492ebf25a
[0m13:26:16.176999 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m13:26:16.177562 [info ] [Thread-4 (]: 10 of 37 START test not_null_stg_subscriptions_subscription_id ................. [RUN]
[0m13:26:16.179084 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_shipments_order_id.2492ebf25a, now test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91)
[0m13:26:16.179680 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m13:26:16.185635 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91"
[0m13:26:16.187043 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m13:26:16.190549 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91"
[0m13:26:16.191942 [debug] [Thread-4 (]: On test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select subscription_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_subscriptions`
where subscription_id is null



  
  
      
    ) dbt_internal_test
[0m13:26:16.192912 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m13:26:16.241516 [info ] [Thread-2 (]: 8 of 37 PASS not_null_stg_shipments_user_id .................................... [[32mPASS[0m in 2.63s]
[0m13:26:16.261509 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852
[0m13:26:16.262263 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2
[0m13:26:16.263138 [info ] [Thread-2 (]: 11 of 37 START test not_null_stg_subscriptions_user_id ......................... [RUN]
[0m13:26:16.264463 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852, now test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2)
[0m13:26:16.265292 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2
[0m13:26:16.270308 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2"
[0m13:26:16.275724 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2
[0m13:26:16.278868 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2"
[0m13:26:16.281668 [debug] [Thread-2 (]: On test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_subscriptions`
where user_id is null



  
  
      
    ) dbt_internal_test
[0m13:26:16.282826 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m13:26:16.451731 [info ] [Thread-3 (]: 7 of 37 PASS not_null_stg_shipments_shipment_id ................................ [[32mPASS[0m in 2.89s]
[0m13:26:16.454232 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380
[0m13:26:16.455102 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25
[0m13:26:16.457288 [info ] [Thread-3 (]: 12 of 37 START test not_null_stg_users_created_at .............................. [RUN]
[0m13:26:16.464712 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380, now test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25)
[0m13:26:16.466361 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25
[0m13:26:16.473274 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25"
[0m13:26:16.479023 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25
[0m13:26:16.487344 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25"
[0m13:26:16.488484 [debug] [Thread-3 (]: On test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select created_at
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
where created_at is null



  
  
      
    ) dbt_internal_test
[0m13:26:16.489180 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m13:26:17.406778 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:a4d0b365-f0b0-4d3f-ac34-69b9acf7621e&page=queryresults
[0m13:26:17.686524 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:d322914f-81dd-44ba-9c90-481d8dce39c2&page=queryresults
[0m13:26:17.867434 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:0cb5a2f0-dfc9-4413-b8ad-6fed354462f6&page=queryresults
[0m13:26:18.160269 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:8d3d0ed9-e3ff-4d37-bf38-d4deb0776e7e&page=queryresults
[0m13:26:18.378546 [info ] [Thread-1 (]: 9 of 37 PASS not_null_stg_subscriptions_start_date ............................. [[32mPASS[0m in 2.47s]
[0m13:26:18.380849 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8
[0m13:26:18.381525 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77
[0m13:26:18.383485 [info ] [Thread-1 (]: 13 of 37 START test not_null_stg_users_user_id ................................. [RUN]
[0m13:26:18.384780 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8, now test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77)
[0m13:26:18.385358 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77
[0m13:26:18.390814 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77"
[0m13:26:18.392360 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77
[0m13:26:18.395835 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77"
[0m13:26:18.397875 [debug] [Thread-1 (]: On test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
where user_id is null



  
  
      
    ) dbt_internal_test
[0m13:26:18.399031 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:26:18.636664 [info ] [Thread-4 (]: 10 of 37 PASS not_null_stg_subscriptions_subscription_id ....................... [[32mPASS[0m in 2.46s]
[0m13:26:18.652391 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m13:26:18.653857 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781
[0m13:26:18.654706 [info ] [Thread-4 (]: 14 of 37 START test relationships_stg_addresses_user_id__user_id__ref_stg_users_  [RUN]
[0m13:26:18.656437 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91, now test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781)
[0m13:26:18.657997 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781
[0m13:26:18.682658 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781"
[0m13:26:18.684360 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781
[0m13:26:18.687651 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781"
[0m13:26:18.689092 [debug] [Thread-4 (]: On test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select user_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_addresses`
    where user_id is not null
),

parent as (
    select user_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m13:26:18.689984 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m13:26:18.919303 [info ] [Thread-2 (]: 11 of 37 PASS not_null_stg_subscriptions_user_id ............................... [[32mPASS[0m in 2.65s]
[0m13:26:18.921185 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2
[0m13:26:18.922120 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737
[0m13:26:18.922994 [info ] [Thread-2 (]: 15 of 37 START test relationships_stg_orders_user_id__user_id__ref_stg_users_ .. [RUN]
[0m13:26:18.924541 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2, now test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737)
[0m13:26:18.925907 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737
[0m13:26:18.934330 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737"
[0m13:26:18.936807 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737
[0m13:26:18.940335 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737"
[0m13:26:18.942000 [debug] [Thread-2 (]: On test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select user_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_orders`
    where user_id is not null
),

parent as (
    select user_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m13:26:18.943620 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m13:26:19.019647 [error] [Thread-3 (]: 12 of 37 FAIL 2 not_null_stg_users_created_at .................................. [[31mFAIL 2[0m in 2.55s]
[0m13:26:19.023169 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25
[0m13:26:19.025210 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.relationships_stg_shipments_order_id__order_id__ref_stg_orders_.72607843d7
[0m13:26:19.026529 [info ] [Thread-3 (]: 16 of 37 START test relationships_stg_shipments_order_id__order_id__ref_stg_orders_  [RUN]
[0m13:26:19.029661 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25, now test.data_pipeline_project.relationships_stg_shipments_order_id__order_id__ref_stg_orders_.72607843d7)
[0m13:26:19.030242 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.relationships_stg_shipments_order_id__order_id__ref_stg_orders_.72607843d7
[0m13:26:19.047711 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_stg_shipments_order_id__order_id__ref_stg_orders_.72607843d7"
[0m13:26:19.051976 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.relationships_stg_shipments_order_id__order_id__ref_stg_orders_.72607843d7
[0m13:26:19.057546 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_stg_shipments_order_id__order_id__ref_stg_orders_.72607843d7"
[0m13:26:19.060392 [debug] [Thread-3 (]: On test.data_pipeline_project.relationships_stg_shipments_order_id__order_id__ref_stg_orders_.72607843d7: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_stg_shipments_order_id__order_id__ref_stg_orders_.72607843d7"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select order_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_shipments`
    where order_id is not null
),

parent as (
    select order_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_orders`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m13:26:19.061084 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m13:26:19.960155 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:99b24ce7-d100-4116-b729-d0061e64ffc8&page=queryresults
[0m13:26:20.200297 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:fafd2b62-664c-4176-a596-9aa7a14cd1cd&page=queryresults
[0m13:26:20.497622 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:aad3c8e7-f083-4532-8f8f-bf3ea7a97e1f&page=queryresults
[0m13:26:20.560583 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:711c7c53-15e0-4658-8745-30357f2d8835&page=queryresults
[0m13:26:20.882638 [info ] [Thread-1 (]: 13 of 37 PASS not_null_stg_users_user_id ....................................... [[32mPASS[0m in 2.50s]
[0m13:26:20.886677 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77
[0m13:26:20.889661 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8
[0m13:26:20.890378 [info ] [Thread-1 (]: 17 of 37 START test relationships_stg_shipments_user_id__user_id__ref_stg_users_  [RUN]
[0m13:26:20.891804 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77, now test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8)
[0m13:26:20.892366 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8
[0m13:26:20.898025 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8"
[0m13:26:20.898974 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8
[0m13:26:20.902510 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8"
[0m13:26:20.903769 [debug] [Thread-1 (]: On test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select user_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_shipments`
    where user_id is not null
),

parent as (
    select user_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m13:26:20.904731 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:26:21.135334 [info ] [Thread-4 (]: 14 of 37 PASS relationships_stg_addresses_user_id__user_id__ref_stg_users_ ..... [[32mPASS[0m in 2.47s]
[0m13:26:21.137691 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781
[0m13:26:21.138387 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m13:26:21.139435 [info ] [Thread-4 (]: 18 of 37 START test relationships_stg_subscriptions_user_id__user_id__ref_stg_users_  [RUN]
[0m13:26:21.141706 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781, now test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e)
[0m13:26:21.153153 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m13:26:21.172635 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e"
[0m13:26:21.175386 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m13:26:21.180351 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e"
[0m13:26:21.200178 [debug] [Thread-4 (]: On test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select user_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_subscriptions`
    where user_id is not null
),

parent as (
    select user_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m13:26:21.203624 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m13:26:21.349528 [error] [Thread-3 (]: 16 of 37 FAIL 476 relationships_stg_shipments_order_id__order_id__ref_stg_orders_  [[31mFAIL 476[0m in 2.32s]
[0m13:26:21.352527 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.relationships_stg_shipments_order_id__order_id__ref_stg_orders_.72607843d7
[0m13:26:21.355884 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6
[0m13:26:21.361255 [info ] [Thread-3 (]: 19 of 37 START test source_not_null_raw_data_addresses__id ..................... [RUN]
[0m13:26:21.363433 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_stg_shipments_order_id__order_id__ref_stg_orders_.72607843d7, now test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6)
[0m13:26:21.364917 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6
[0m13:26:21.372440 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6"
[0m13:26:21.375802 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6
[0m13:26:21.385335 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6"
[0m13:26:21.393745 [debug] [Thread-3 (]: On test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`addresses`
where _id is null



  
  
      
    ) dbt_internal_test
[0m13:26:21.394345 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m13:26:21.491358 [info ] [Thread-2 (]: 15 of 37 PASS relationships_stg_orders_user_id__user_id__ref_stg_users_ ........ [[32mPASS[0m in 2.57s]
[0m13:26:21.493860 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737
[0m13:26:21.494963 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc
[0m13:26:21.496059 [info ] [Thread-2 (]: 20 of 37 START test source_not_null_raw_data_cities__id ........................ [RUN]
[0m13:26:21.497687 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737, now test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc)
[0m13:26:21.499385 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc
[0m13:26:21.504555 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc"
[0m13:26:21.506775 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc
[0m13:26:21.520314 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc"
[0m13:26:21.526692 [debug] [Thread-2 (]: On test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`cities`
where _id is null



  
  
      
    ) dbt_internal_test
[0m13:26:21.528232 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m13:26:22.579922 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:6dd7f70a-fb5f-4a68-9e7b-dfebdbe09bd5&page=queryresults
[0m13:26:22.728855 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:c8e0382a-2b86-413d-94a2-c13bea1d576c&page=queryresults
[0m13:26:22.801245 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:e1535a5c-00f8-4cec-ad62-cde478e684c9&page=queryresults
[0m13:26:22.825086 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:ebe5c251-46e6-45ca-a172-ed330e412f78&page=queryresults
[0m13:26:23.449067 [info ] [Thread-3 (]: 19 of 37 PASS source_not_null_raw_data_addresses__id ........................... [[32mPASS[0m in 2.09s]
[0m13:26:23.451885 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6
[0m13:26:23.452532 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd
[0m13:26:23.453292 [info ] [Thread-3 (]: 21 of 37 START test source_not_null_raw_data_countries__id ..................... [RUN]
[0m13:26:23.454158 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6, now test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd)
[0m13:26:23.455144 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd
[0m13:26:23.460685 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd"
[0m13:26:23.462410 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd
[0m13:26:23.465376 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd"
[0m13:26:23.467138 [debug] [Thread-3 (]: On test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`countries`
where _id is null



  
  
      
    ) dbt_internal_test
[0m13:26:23.468132 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m13:26:23.613370 [info ] [Thread-2 (]: 20 of 37 PASS source_not_null_raw_data_cities__id .............................. [[32mPASS[0m in 2.11s]
[0m13:26:23.616659 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc
[0m13:26:23.618293 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be
[0m13:26:23.619081 [info ] [Thread-2 (]: 22 of 37 START test source_not_null_raw_data_neighborhoods__id ................. [RUN]
[0m13:26:23.620234 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc, now test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be)
[0m13:26:23.620794 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be
[0m13:26:23.624990 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be"
[0m13:26:23.627824 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be
[0m13:26:23.631880 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be"
[0m13:26:23.634944 [debug] [Thread-2 (]: On test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`neighborhoods`
where _id is null



  
  
      
    ) dbt_internal_test
[0m13:26:23.635802 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m13:26:23.685271 [info ] [Thread-4 (]: 18 of 37 PASS relationships_stg_subscriptions_user_id__user_id__ref_stg_users_ . [[32mPASS[0m in 2.54s]
[0m13:26:23.692075 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m13:26:23.690398 [info ] [Thread-1 (]: 17 of 37 PASS relationships_stg_shipments_user_id__user_id__ref_stg_users_ ..... [[32mPASS[0m in 2.80s]
[0m13:26:23.696124 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878
[0m13:26:23.697258 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8
[0m13:26:23.699156 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f
[0m13:26:23.698029 [info ] [Thread-4 (]: 23 of 37 START test source_not_null_raw_data_orders__id ........................ [RUN]
[0m13:26:23.700218 [info ] [Thread-1 (]: 24 of 37 START test source_not_null_raw_data_orders__user ...................... [RUN]
[0m13:26:23.702631 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8, now test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f)
[0m13:26:23.701980 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e, now test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878)
[0m13:26:23.703338 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f
[0m13:26:23.705412 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878
[0m13:26:23.714506 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f"
[0m13:26:23.725947 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f
[0m13:26:23.729930 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f"
[0m13:26:23.732939 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878"
[0m13:26:23.735154 [debug] [Thread-1 (]: On test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _user
from `data-pipeline-project-474812`.`raw_data`.`orders`
where _user is null



  
  
      
    ) dbt_internal_test
[0m13:26:23.740694 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:26:23.786341 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878
[0m13:26:23.795291 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878"
[0m13:26:23.799072 [debug] [Thread-4 (]: On test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`orders`
where _id is null



  
  
      
    ) dbt_internal_test
[0m13:26:23.800016 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m13:26:24.652189 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:b6455d7f-163b-458a-9862-c85dc9357cd8&page=queryresults
[0m13:26:24.973878 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:ae5e014d-e27b-4b07-b0f1-8e86d484eaff&page=queryresults
[0m13:26:24.999212 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:eb15bf3a-ffcc-4191-bf13-365dfed2382c&page=queryresults
[0m13:26:25.015100 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:2bab8039-d758-4067-8968-e86bd32e8eb4&page=queryresults
[0m13:26:25.500329 [info ] [Thread-3 (]: 21 of 37 PASS source_not_null_raw_data_countries__id ........................... [[32mPASS[0m in 2.04s]
[0m13:26:25.509096 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd
[0m13:26:25.509824 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2
[0m13:26:25.510802 [info ] [Thread-3 (]: 25 of 37 START test source_not_null_raw_data_shipments__id ..................... [RUN]
[0m13:26:25.511459 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd, now test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2)
[0m13:26:25.512111 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2
[0m13:26:25.518053 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2"
[0m13:26:25.520059 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2
[0m13:26:25.529204 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2"
[0m13:26:25.532029 [debug] [Thread-3 (]: On test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`shipments`
where _id is null



  
  
      
    ) dbt_internal_test
[0m13:26:25.533094 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m13:26:25.887721 [info ] [Thread-4 (]: 23 of 37 PASS source_not_null_raw_data_orders__id .............................. [[32mPASS[0m in 2.17s]
[0m13:26:25.907697 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878
[0m13:26:25.911483 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1
[0m13:26:25.924992 [info ] [Thread-2 (]: 22 of 37 PASS source_not_null_raw_data_neighborhoods__id ....................... [[32mPASS[0m in 2.30s]
[0m13:26:25.931786 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be
[0m13:26:25.932744 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98
[0m13:26:25.926043 [info ] [Thread-4 (]: 26 of 37 START test source_not_null_raw_data_shipments__order .................. [RUN]
[0m13:26:25.935279 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878, now test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1)
[0m13:26:25.936164 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1
[0m13:26:25.941200 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1"
[0m13:26:25.921433 [info ] [Thread-1 (]: 24 of 37 PASS source_not_null_raw_data_orders__user ............................ [[32mPASS[0m in 2.22s]
[0m13:26:25.945605 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f
[0m13:26:25.946669 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86
[0m13:26:25.950129 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1
[0m13:26:25.953386 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1"
[0m13:26:25.947411 [info ] [Thread-1 (]: 28 of 37 START test source_not_null_raw_data_states__id ........................ [RUN]
[0m13:26:25.933430 [info ] [Thread-2 (]: 27 of 37 START test source_not_null_raw_data_shipments__user ................... [RUN]
[0m13:26:25.955877 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f, now test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86)
[0m13:26:25.956893 [debug] [Thread-4 (]: On test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _order
from `data-pipeline-project-474812`.`raw_data`.`shipments`
where _order is null



  
  
      
    ) dbt_internal_test
[0m13:26:25.959072 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be, now test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98)
[0m13:26:25.960409 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86
[0m13:26:25.961347 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m13:26:25.962136 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98
[0m13:26:25.967026 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86"
[0m13:26:26.028901 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98"
[0m13:26:26.030766 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86
[0m13:26:26.034058 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86"
[0m13:26:26.035134 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98
[0m13:26:26.038571 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98"
[0m13:26:26.043764 [debug] [Thread-2 (]: On test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _user
from `data-pipeline-project-474812`.`raw_data`.`shipments`
where _user is null



  
  
      
    ) dbt_internal_test
[0m13:26:26.045635 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m13:26:26.050433 [debug] [Thread-1 (]: On test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`states`
where _id is null



  
  
      
    ) dbt_internal_test
[0m13:26:26.111213 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:26:26.902270 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:9960a3cb-00a8-45b5-bbc5-e41f63225cd0&page=queryresults
[0m13:26:27.329694 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:d9bb72eb-ea89-4d9c-a3ff-d245d498d618&page=queryresults
[0m13:26:27.355645 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:3260bf2d-8234-448d-95ad-1e4afbb3e4dd&page=queryresults
[0m13:26:27.399269 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:b48a94dc-11c4-427f-ade7-626852837b07&page=queryresults
[0m13:26:27.774960 [info ] [Thread-3 (]: 25 of 37 PASS source_not_null_raw_data_shipments__id ........................... [[32mPASS[0m in 2.26s]
[0m13:26:27.777735 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2
[0m13:26:27.779936 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a
[0m13:26:27.781762 [info ] [Thread-3 (]: 29 of 37 START test source_not_null_raw_data_subscriptions__id ................. [RUN]
[0m13:26:27.783419 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2, now test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a)
[0m13:26:27.785022 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a
[0m13:26:27.793975 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a"
[0m13:26:27.800486 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a
[0m13:26:27.803454 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a"
[0m13:26:27.804329 [debug] [Thread-3 (]: On test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`subscriptions`
where _id is null



  
  
      
    ) dbt_internal_test
[0m13:26:27.806770 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m13:26:28.240402 [info ] [Thread-2 (]: 27 of 37 PASS source_not_null_raw_data_shipments__user ......................... [[32mPASS[0m in 2.28s]
[0m13:26:28.246115 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98
[0m13:26:28.249301 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6
[0m13:26:28.251035 [info ] [Thread-2 (]: 30 of 37 START test source_not_null_raw_data_subscriptions__user ............... [RUN]
[0m13:26:28.255466 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98, now test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6)
[0m13:26:28.256228 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6
[0m13:26:28.264534 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6"
[0m13:26:28.265957 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6
[0m13:26:28.273918 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6"
[0m13:26:28.275039 [debug] [Thread-2 (]: On test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _user
from `data-pipeline-project-474812`.`raw_data`.`subscriptions`
where _user is null



  
  
      
    ) dbt_internal_test
[0m13:26:28.275691 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m13:26:28.358379 [info ] [Thread-4 (]: 26 of 37 PASS source_not_null_raw_data_shipments__order ........................ [[32mPASS[0m in 2.42s]
[0m13:26:28.360135 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1
[0m13:26:28.361172 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b
[0m13:26:28.361976 [info ] [Thread-4 (]: 31 of 37 START test source_not_null_raw_data_users__id ......................... [RUN]
[0m13:26:28.363007 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1, now test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b)
[0m13:26:28.363586 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b
[0m13:26:28.373564 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b"
[0m13:26:28.376156 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b
[0m13:26:28.383261 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b"
[0m13:26:28.385635 [debug] [Thread-4 (]: On test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`users`
where _id is null



  
  
      
    ) dbt_internal_test
[0m13:26:28.386629 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m13:26:28.549918 [info ] [Thread-1 (]: 28 of 37 PASS source_not_null_raw_data_states__id .............................. [[32mPASS[0m in 2.59s]
[0m13:26:28.551415 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86
[0m13:26:28.552644 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00
[0m13:26:28.553407 [info ] [Thread-1 (]: 32 of 37 START test unique_stg_addresses_address_id ............................ [RUN]
[0m13:26:28.554963 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86, now test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00)
[0m13:26:28.555848 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00
[0m13:26:28.568298 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00"
[0m13:26:28.571200 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00
[0m13:26:28.574203 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00"
[0m13:26:28.576921 [debug] [Thread-1 (]: On test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select address_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_addresses`
  where address_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m13:26:28.577539 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:26:29.021386 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:a680ba58-6e69-4c95-9214-9a42c2cea3fb&page=queryresults
[0m13:26:29.447523 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:5e31d844-dfe5-4028-8657-b599437b74e9&page=queryresults
[0m13:26:29.558220 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:5c22c257-b282-4290-a224-3569c7dd4feb&page=queryresults
[0m13:26:30.030877 [info ] [Thread-3 (]: 29 of 37 PASS source_not_null_raw_data_subscriptions__id ....................... [[32mPASS[0m in 2.25s]
[0m13:26:30.037310 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a
[0m13:26:30.039003 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.unique_stg_countries_country_id.8679936442
[0m13:26:30.040051 [info ] [Thread-3 (]: 33 of 37 START test unique_stg_countries_country_id ............................ [RUN]
[0m13:26:30.041180 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a, now test.data_pipeline_project.unique_stg_countries_country_id.8679936442)
[0m13:26:30.042032 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.unique_stg_countries_country_id.8679936442
[0m13:26:30.048601 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.unique_stg_countries_country_id.8679936442"
[0m13:26:30.050077 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.unique_stg_countries_country_id.8679936442
[0m13:26:30.057892 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.unique_stg_countries_country_id.8679936442"
[0m13:26:30.059723 [debug] [Thread-3 (]: On test.data_pipeline_project.unique_stg_countries_country_id.8679936442: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_stg_countries_country_id.8679936442"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select country_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_countries`
  where country_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m13:26:30.060275 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m13:26:30.181309 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:2ea72287-87f3-4f4d-9523-bafd496ebac8&page=queryresults
[0m13:26:30.346326 [info ] [Thread-2 (]: 30 of 37 PASS source_not_null_raw_data_subscriptions__user ..................... [[32mPASS[0m in 2.09s]
[0m13:26:30.350889 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6
[0m13:26:30.353170 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a
[0m13:26:30.355489 [info ] [Thread-2 (]: 34 of 37 START test unique_stg_orders_order_id ................................. [RUN]
[0m13:26:30.357967 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6, now test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a)
[0m13:26:30.359763 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a
[0m13:26:30.367172 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a"
[0m13:26:30.372404 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a
[0m13:26:30.376409 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a"
[0m13:26:30.380856 [debug] [Thread-2 (]: On test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select order_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_orders`
  where order_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m13:26:30.381798 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m13:26:30.457510 [info ] [Thread-4 (]: 31 of 37 PASS source_not_null_raw_data_users__id ............................... [[32mPASS[0m in 2.09s]
[0m13:26:30.467299 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b
[0m13:26:30.468765 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c
[0m13:26:30.472827 [info ] [Thread-4 (]: 35 of 37 START test unique_stg_shipments_shipment_id ........................... [RUN]
[0m13:26:30.474433 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b, now test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c)
[0m13:26:30.476071 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c
[0m13:26:30.491728 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c"
[0m13:26:30.495356 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c
[0m13:26:30.506468 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c"
[0m13:26:30.509504 [debug] [Thread-4 (]: On test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select shipment_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_shipments`
  where shipment_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m13:26:30.510131 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m13:26:31.169385 [info ] [Thread-1 (]: 32 of 37 PASS unique_stg_addresses_address_id .................................. [[32mPASS[0m in 2.61s]
[0m13:26:31.173361 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00
[0m13:26:31.176160 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m13:26:31.177352 [info ] [Thread-1 (]: 36 of 37 START test unique_stg_subscriptions_subscription_id ................... [RUN]
[0m13:26:31.179762 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00, now test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e)
[0m13:26:31.180826 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m13:26:31.188994 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e"
[0m13:26:31.195004 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m13:26:31.203903 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e"
[0m13:26:31.205016 [debug] [Thread-1 (]: On test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select subscription_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_subscriptions`
  where subscription_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m13:26:31.205818 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:26:31.615368 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:d296ba2b-0b8e-4aa0-9ce5-7e56fc44be9d&page=queryresults
[0m13:26:31.779489 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:053aaedd-b1a8-4214-9a2f-9fe375ba6e90&page=queryresults
[0m13:26:32.071032 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:f5d6aaad-ee29-43e7-94cd-a488d12511e7&page=queryresults
[0m13:26:32.555624 [info ] [Thread-3 (]: 33 of 37 PASS unique_stg_countries_country_id .................................. [[32mPASS[0m in 2.51s]
[0m13:26:32.561304 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.unique_stg_countries_country_id.8679936442
[0m13:26:32.564108 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b
[0m13:26:32.566198 [info ] [Thread-3 (]: 37 of 37 START test unique_stg_users_user_id ................................... [RUN]
[0m13:26:32.568770 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_stg_countries_country_id.8679936442, now test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b)
[0m13:26:32.570026 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b
[0m13:26:32.579525 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b"
[0m13:26:32.581273 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b
[0m13:26:32.584597 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b"
[0m13:26:32.587407 [debug] [Thread-3 (]: On test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select user_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
  where user_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m13:26:32.596125 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m13:26:32.631156 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:cf8385ca-f474-4faa-af60-6f8aba0c4c3f&page=queryresults
[0m13:26:32.683302 [info ] [Thread-2 (]: 34 of 37 PASS unique_stg_orders_order_id ....................................... [[32mPASS[0m in 2.32s]
[0m13:26:32.686368 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a
[0m13:26:32.982876 [info ] [Thread-4 (]: 35 of 37 PASS unique_stg_shipments_shipment_id ................................. [[32mPASS[0m in 2.51s]
[0m13:26:32.986414 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c
[0m13:26:33.646654 [info ] [Thread-1 (]: 36 of 37 PASS unique_stg_subscriptions_subscription_id ......................... [[32mPASS[0m in 2.47s]
[0m13:26:33.648074 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m13:26:34.113364 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:0b4da3ca-b4ea-4f33-b8e6-2abcfafc8512&page=queryresults
[0m13:26:34.990925 [info ] [Thread-3 (]: 37 of 37 PASS unique_stg_users_user_id ......................................... [[32mPASS[0m in 2.42s]
[0m13:26:34.992299 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b
[0m13:26:34.996682 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m13:26:34.998260 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:26:34.998622 [debug] [MainThread]: Connection 'test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e' was properly closed.
[0m13:26:34.998952 [debug] [MainThread]: Connection 'test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a' was properly closed.
[0m13:26:34.999289 [debug] [MainThread]: Connection 'test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b' was properly closed.
[0m13:26:34.999612 [debug] [MainThread]: Connection 'test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c' was properly closed.
[0m13:26:35.000008 [info ] [MainThread]: 
[0m13:26:35.001425 [info ] [MainThread]: Finished running 37 data tests in 0 hours 0 minutes and 25.69 seconds (25.69s).
[0m13:26:35.008933 [debug] [MainThread]: Command end result
[0m13:26:35.061229 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m13:26:35.064374 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m13:26:35.074678 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m13:26:35.075472 [info ] [MainThread]: 
[0m13:26:35.076262 [info ] [MainThread]: [31mCompleted with 2 errors, 0 partial successes, and 0 warnings:[0m
[0m13:26:35.076724 [info ] [MainThread]: 
[0m13:26:35.077523 [error] [MainThread]: [31mFailure in test not_null_stg_users_created_at (models/staging/staging_schema.yml)[0m
[0m13:26:35.080910 [error] [MainThread]:   Got 2 results, configured to fail if != 0
[0m13:26:35.081308 [info ] [MainThread]: 
[0m13:26:35.082010 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_project/models/staging/staging_schema.yml/not_null_stg_users_created_at.sql
[0m13:26:35.082480 [info ] [MainThread]: 
[0m13:26:35.083267 [error] [MainThread]: [31mFailure in test relationships_stg_shipments_order_id__order_id__ref_stg_orders_ (models/staging/staging_schema.yml)[0m
[0m13:26:35.084104 [error] [MainThread]:   Got 476 results, configured to fail if != 0
[0m13:26:35.084511 [info ] [MainThread]: 
[0m13:26:35.085217 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_project/models/staging/staging_schema.yml/relationships_stg_shipments_order_id__order_id__ref_stg_orders_.sql
[0m13:26:35.085962 [info ] [MainThread]: 
[0m13:26:35.086544 [info ] [MainThread]: Done. PASS=35 WARN=0 ERROR=2 SKIP=0 NO-OP=0 TOTAL=37
[0m13:26:35.091062 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 29.133902, "process_in_blocks": "0", "process_kernel_time": 3.20975, "process_mem_max_rss": "391356", "process_out_blocks": "3600", "process_user_time": 10.155675}
[0m13:26:35.091935 [debug] [MainThread]: Command `dbt test` failed at 13:26:35.091821 after 29.13 seconds
[0m13:26:35.092395 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5bf02165a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5befadbec0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5bcff40290>]}
[0m13:26:35.092807 [debug] [MainThread]: Flushing usage events
[0m13:26:36.163733 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:38:34.088424 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75a0a4320a10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75a0a4932120>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75a0a418a540>]}


============================== 13:38:34.125902 | bd782a84-577e-487a-af99-e3c9bb55a375 ==============================
[0m13:38:34.125902 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m13:38:34.147008 [debug] [MainThread]: running dbt with arguments {'indirect_selection': 'eager', 'write_json': 'True', 'static_parser': 'True', 'debug': 'False', 'invocation_command': 'dbt test --select staging', 'fail_fast': 'False', 'profiles_dir': '/home/ecem/.dbt', 'version_check': 'True', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'send_anonymous_usage_stats': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'cache_selected_only': 'False', 'log_format': 'default', 'empty': 'None', 'quiet': 'False', 'target_path': 'None', 'warn_error': 'None', 'printer_width': '80', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_cache_events': 'False', 'partial_parse': 'True', 'introspect': 'True', 'no_print': 'None'}
[0m13:38:36.727310 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'bd782a84-577e-487a-af99-e3c9bb55a375', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75a0843c3e90>]}
[0m13:38:36.796053 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'bd782a84-577e-487a-af99-e3c9bb55a375', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75a0a618e990>]}
[0m13:38:36.796844 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m13:38:37.043816 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m13:38:37.361030 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m13:38:37.362068 [debug] [MainThread]: Partial parsing: updated file: data_pipeline_project://models/staging/staging_schema.yml
[0m13:38:37.698208 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `relationships` defined on 'stg_shipments' in
package 'data_pipeline_project' (models/staging/staging_schema.yml). Arguments
to generic tests should be nested under the `arguments` property.
[0m13:38:37.699011 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': 'bd782a84-577e-487a-af99-e3c9bb55a375', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75a083441580>]}
[0m13:38:37.855497 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.data_pipeline_project.marts
[0m13:38:37.879191 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'bd782a84-577e-487a-af99-e3c9bb55a375', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75a083dbaba0>]}
[0m13:38:38.011734 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m13:38:38.017418 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m13:38:38.048211 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'bd782a84-577e-487a-af99-e3c9bb55a375', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75a0832fccb0>]}
[0m13:38:38.049084 [info ] [MainThread]: Found 10 models, 35 data tests, 10 sources, 508 macros
[0m13:38:38.049766 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bd782a84-577e-487a-af99-e3c9bb55a375', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75a08345f920>]}
[0m13:38:38.052935 [info ] [MainThread]: 
[0m13:38:38.053788 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m13:38:38.054815 [info ] [MainThread]: 
[0m13:38:38.055962 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m13:38:38.063025 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_staging'
[0m13:38:38.064188 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:38:39.147178 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bd782a84-577e-487a-af99-e3c9bb55a375', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75a083d8b6b0>]}
[0m13:38:39.148952 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:38:39.164491 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501
[0m13:38:39.165805 [info ] [Thread-1 (]: 1 of 35 START test not_null_stg_addresses_address_id ........................... [RUN]
[0m13:38:39.166906 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501)
[0m13:38:39.168059 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501
[0m13:38:39.182195 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501"
[0m13:38:39.184217 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3
[0m13:38:39.185847 [info ] [Thread-2 (]: 2 of 35 START test not_null_stg_addresses_user_id .............................. [RUN]
[0m13:38:39.186831 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2
[0m13:38:39.187859 [debug] [Thread-2 (]: Acquiring new bigquery connection 'test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3'
[0m13:38:39.188479 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64
[0m13:38:39.188890 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501
[0m13:38:39.189631 [info ] [Thread-3 (]: 3 of 35 START test not_null_stg_countries_country_id ........................... [RUN]
[0m13:38:39.191192 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3
[0m13:38:39.195899 [info ] [Thread-4 (]: 4 of 35 START test not_null_stg_orders_order_id ................................ [RUN]
[0m13:38:39.218828 [debug] [Thread-3 (]: Acquiring new bigquery connection 'test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2'
[0m13:38:39.243052 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501"
[0m13:38:39.246160 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3"
[0m13:38:39.247461 [debug] [Thread-4 (]: Acquiring new bigquery connection 'test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64'
[0m13:38:39.251260 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2
[0m13:38:39.254841 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3
[0m13:38:39.257331 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64
[0m13:38:39.267770 [debug] [Thread-1 (]: On test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select address_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_addresses`
where address_id is null



  
  
      
    ) dbt_internal_test
[0m13:38:39.266626 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2"
[0m13:38:39.284264 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2
[0m13:38:39.281251 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:38:39.283184 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3"
[0m13:38:39.280318 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64"
[0m13:38:39.287626 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2"
[0m13:38:39.327712 [debug] [Thread-2 (]: On test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_addresses`
where user_id is null



  
  
      
    ) dbt_internal_test
[0m13:38:39.332981 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m13:38:39.390262 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64
[0m13:38:39.394064 [debug] [Thread-3 (]: On test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select country_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_countries`
where country_id is null



  
  
      
    ) dbt_internal_test
[0m13:38:39.398382 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64"
[0m13:38:39.400040 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m13:38:39.401053 [debug] [Thread-4 (]: On test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select order_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_orders`
where order_id is null



  
  
      
    ) dbt_internal_test
[0m13:38:39.430988 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m13:38:40.865258 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:d330ed13-3677-403a-9243-b97fb587c305&page=queryresults
[0m13:38:40.981734 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:90f47488-4713-4e10-9084-32a01efe3b26&page=queryresults
[0m13:38:40.994678 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:0129c161-bc38-4f98-b4f9-e9861fe814b1&page=queryresults
[0m13:38:41.002657 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:22d628c4-5a52-4429-b8aa-e0b65cdc23cc&page=queryresults
[0m13:38:41.843845 [info ] [Thread-3 (]: 3 of 35 PASS not_null_stg_countries_country_id ................................. [[32mPASS[0m in 2.62s]
[0m13:38:41.846599 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2
[0m13:38:41.847517 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209
[0m13:38:41.848480 [info ] [Thread-3 (]: 5 of 35 START test not_null_stg_orders_user_id ................................. [RUN]
[0m13:38:41.849640 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2, now test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209)
[0m13:38:41.850596 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209
[0m13:38:41.856946 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209"
[0m13:38:41.858612 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209
[0m13:38:41.862070 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209"
[0m13:38:41.863357 [debug] [Thread-3 (]: On test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_orders`
where user_id is null



  
  
      
    ) dbt_internal_test
[0m13:38:41.864015 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m13:38:41.929844 [info ] [Thread-2 (]: 2 of 35 PASS not_null_stg_addresses_user_id .................................... [[32mPASS[0m in 2.73s]
[0m13:38:41.934670 [info ] [Thread-1 (]: 1 of 35 PASS not_null_stg_addresses_address_id ................................. [[32mPASS[0m in 2.77s]
[0m13:38:41.935955 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3
[0m13:38:41.937341 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501
[0m13:38:41.938440 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380
[0m13:38:41.939920 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852
[0m13:38:41.941137 [info ] [Thread-2 (]: 6 of 35 START test not_null_stg_shipments_shipment_id .......................... [RUN]
[0m13:38:41.944184 [info ] [Thread-1 (]: 7 of 35 START test not_null_stg_shipments_user_id .............................. [RUN]
[0m13:38:41.945463 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3, now test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380)
[0m13:38:41.946440 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501, now test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852)
[0m13:38:41.947807 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380
[0m13:38:41.948733 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852
[0m13:38:41.961346 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852"
[0m13:38:41.964098 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380"
[0m13:38:41.965307 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852
[0m13:38:41.968720 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852"
[0m13:38:41.970790 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380
[0m13:38:41.973426 [debug] [Thread-1 (]: On test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_shipments`
where user_id is null



  
  
      
    ) dbt_internal_test
[0m13:38:41.978598 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380"
[0m13:38:41.979952 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:38:42.028054 [debug] [Thread-2 (]: On test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select shipment_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_shipments`
where shipment_id is null



  
  
      
    ) dbt_internal_test
[0m13:38:42.029170 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m13:38:42.192392 [info ] [Thread-4 (]: 4 of 35 PASS not_null_stg_orders_order_id ...................................... [[32mPASS[0m in 2.94s]
[0m13:38:42.193607 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64
[0m13:38:42.194371 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8
[0m13:38:42.196751 [info ] [Thread-4 (]: 8 of 35 START test not_null_stg_subscriptions_start_date ....................... [RUN]
[0m13:38:42.197650 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64, now test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8)
[0m13:38:42.198847 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8
[0m13:38:42.206845 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8"
[0m13:38:42.208167 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8
[0m13:38:42.210746 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8"
[0m13:38:42.212040 [debug] [Thread-4 (]: On test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select start_date
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_subscriptions`
where start_date is null



  
  
      
    ) dbt_internal_test
[0m13:38:42.212618 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m13:38:43.202604 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:bfccb1a7-fc1b-4e02-8726-b2722d02d9ab&page=queryresults
[0m13:38:43.474357 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:203e1214-2114-4321-806f-d0f8d08ef418&page=queryresults
[0m13:38:43.509399 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:89a2704d-295b-4ece-be9f-48af24418983&page=queryresults
[0m13:38:43.639852 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:afebcc0f-4bf9-4a19-bfe3-40d5b4970fb1&page=queryresults
[0m13:38:44.097018 [info ] [Thread-3 (]: 5 of 35 PASS not_null_stg_orders_user_id ....................................... [[32mPASS[0m in 2.25s]
[0m13:38:44.098644 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209
[0m13:38:44.099995 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m13:38:44.101322 [info ] [Thread-3 (]: 9 of 35 START test not_null_stg_subscriptions_subscription_id .................. [RUN]
[0m13:38:44.103069 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209, now test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91)
[0m13:38:44.104157 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m13:38:44.116229 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91"
[0m13:38:44.118719 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m13:38:44.124288 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91"
[0m13:38:44.131250 [debug] [Thread-3 (]: On test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select subscription_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_subscriptions`
where subscription_id is null



  
  
      
    ) dbt_internal_test
[0m13:38:44.132497 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m13:38:44.406130 [info ] [Thread-2 (]: 6 of 35 PASS not_null_stg_shipments_shipment_id ................................ [[32mPASS[0m in 2.46s]
[0m13:38:44.408137 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380
[0m13:38:44.409459 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2
[0m13:38:44.410914 [info ] [Thread-2 (]: 10 of 35 START test not_null_stg_subscriptions_user_id ......................... [RUN]
[0m13:38:44.412625 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380, now test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2)
[0m13:38:44.413644 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2
[0m13:38:44.420256 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2"
[0m13:38:44.421378 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2
[0m13:38:44.424571 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2"
[0m13:38:44.425678 [debug] [Thread-2 (]: On test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_subscriptions`
where user_id is null



  
  
      
    ) dbt_internal_test
[0m13:38:44.426442 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m13:38:44.487201 [info ] [Thread-1 (]: 7 of 35 PASS not_null_stg_shipments_user_id .................................... [[32mPASS[0m in 2.54s]
[0m13:38:44.488837 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852
[0m13:38:44.489995 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25
[0m13:38:44.491300 [info ] [Thread-1 (]: 11 of 35 START test not_null_stg_users_created_at .............................. [RUN]
[0m13:38:44.492279 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852, now test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25)
[0m13:38:44.492762 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25
[0m13:38:44.496578 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25"
[0m13:38:44.498039 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25
[0m13:38:44.506026 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25"
[0m13:38:44.509430 [debug] [Thread-1 (]: On test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select created_at
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
where created_at is null



  
  
      
    ) dbt_internal_test
[0m13:38:44.511251 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:38:44.634643 [info ] [Thread-4 (]: 8 of 35 PASS not_null_stg_subscriptions_start_date ............................. [[32mPASS[0m in 2.44s]
[0m13:38:44.636334 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8
[0m13:38:44.637388 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77
[0m13:38:44.638424 [info ] [Thread-4 (]: 12 of 35 START test not_null_stg_users_user_id ................................. [RUN]
[0m13:38:44.639861 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8, now test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77)
[0m13:38:44.640509 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77
[0m13:38:44.648409 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77"
[0m13:38:44.650817 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77
[0m13:38:44.655737 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77"
[0m13:38:44.657326 [debug] [Thread-4 (]: On test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
where user_id is null



  
  
      
    ) dbt_internal_test
[0m13:38:44.658258 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m13:38:45.473639 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:33636448-84fb-4991-a1ab-a41d98ab71ce&page=queryresults
[0m13:38:45.675266 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:d4f4c0ac-e308-490f-882d-41f1c5a319b5&page=queryresults
[0m13:38:45.803251 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:f6c7202e-073e-4848-b780-049a3c21ee06&page=queryresults
[0m13:38:45.870632 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:7445180f-41ee-421c-841b-af8048d83265&page=queryresults
[0m13:38:46.355199 [info ] [Thread-3 (]: 9 of 35 PASS not_null_stg_subscriptions_subscription_id ........................ [[32mPASS[0m in 2.25s]
[0m13:38:46.358305 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m13:38:46.359839 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781
[0m13:38:46.361294 [info ] [Thread-3 (]: 13 of 35 START test relationships_stg_addresses_user_id__user_id__ref_stg_users_  [RUN]
[0m13:38:46.362533 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91, now test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781)
[0m13:38:46.363932 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781
[0m13:38:46.377327 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781"
[0m13:38:46.378433 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781
[0m13:38:46.385330 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781"
[0m13:38:46.389293 [debug] [Thread-3 (]: On test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select user_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_addresses`
    where user_id is not null
),

parent as (
    select user_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m13:38:46.391974 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m13:38:46.832167 [info ] [Thread-4 (]: 12 of 35 PASS not_null_stg_users_user_id ....................................... [[32mPASS[0m in 2.19s]
[0m13:38:46.835202 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77
[0m13:38:46.837084 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737
[0m13:38:46.838448 [info ] [Thread-4 (]: 14 of 35 START test relationships_stg_orders_user_id__user_id__ref_stg_users_ .. [RUN]
[0m13:38:46.839901 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77, now test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737)
[0m13:38:46.840810 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737
[0m13:38:46.849937 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737"
[0m13:38:46.853675 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737
[0m13:38:46.857150 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737"
[0m13:38:46.863299 [debug] [Thread-4 (]: On test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select user_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_orders`
    where user_id is not null
),

parent as (
    select user_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m13:38:46.868777 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m13:38:46.872283 [error] [Thread-1 (]: 11 of 35 FAIL 2 not_null_stg_users_created_at .................................. [[31mFAIL 2[0m in 2.38s]
[0m13:38:46.883254 [info ] [Thread-2 (]: 10 of 35 PASS not_null_stg_subscriptions_user_id ............................... [[32mPASS[0m in 2.47s]
[0m13:38:46.947976 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25
[0m13:38:46.951512 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2
[0m13:38:46.957714 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m13:38:46.955807 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8
[0m13:38:46.959733 [info ] [Thread-2 (]: 16 of 35 START test relationships_stg_subscriptions_user_id__user_id__ref_stg_users_  [RUN]
[0m13:38:46.962373 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2, now test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e)
[0m13:38:46.963231 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m13:38:46.970345 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e"
[0m13:38:46.971852 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m13:38:46.975096 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e"
[0m13:38:46.960611 [info ] [Thread-1 (]: 15 of 35 START test relationships_stg_shipments_user_id__user_id__ref_stg_users_  [RUN]
[0m13:38:46.979278 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25, now test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8)
[0m13:38:46.984104 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8
[0m13:38:46.992763 [debug] [Thread-2 (]: On test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select user_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_subscriptions`
    where user_id is not null
),

parent as (
    select user_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m13:38:46.993371 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m13:38:47.004435 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8"
[0m13:38:47.005373 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8
[0m13:38:47.065100 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8"
[0m13:38:47.069276 [debug] [Thread-1 (]: On test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select user_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_shipments`
    where user_id is not null
),

parent as (
    select user_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m13:38:47.073879 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:38:47.799249 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:368d6c05-c698-4e0f-b092-d5bd1a8860d6&page=queryresults
[0m13:38:48.247474 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:7ddfff9f-bd34-4e89-b0b8-0258879e9b60&page=queryresults
[0m13:38:48.387389 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:4e72463c-583b-4abd-a34c-29b524a6f5fe&page=queryresults
[0m13:38:48.438040 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:d05d1873-2f13-4238-8341-0771520e9c21&page=queryresults
[0m13:38:48.682728 [info ] [Thread-3 (]: 13 of 35 PASS relationships_stg_addresses_user_id__user_id__ref_stg_users_ ..... [[32mPASS[0m in 2.32s]
[0m13:38:48.686988 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781
[0m13:38:48.688954 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6
[0m13:38:48.689862 [info ] [Thread-3 (]: 17 of 35 START test source_not_null_raw_data_addresses__id ..................... [RUN]
[0m13:38:48.692903 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781, now test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6)
[0m13:38:48.693923 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6
[0m13:38:48.699974 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6"
[0m13:38:48.704943 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6
[0m13:38:48.711587 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6"
[0m13:38:48.720513 [debug] [Thread-3 (]: On test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`addresses`
where _id is null



  
  
      
    ) dbt_internal_test
[0m13:38:48.721384 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m13:38:49.225304 [info ] [Thread-4 (]: 14 of 35 PASS relationships_stg_orders_user_id__user_id__ref_stg_users_ ........ [[32mPASS[0m in 2.39s]
[0m13:38:49.226956 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737
[0m13:38:49.227922 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc
[0m13:38:49.228707 [info ] [Thread-4 (]: 18 of 35 START test source_not_null_raw_data_cities__id ........................ [RUN]
[0m13:38:49.230563 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737, now test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc)
[0m13:38:49.231290 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc
[0m13:38:49.236728 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc"
[0m13:38:49.238964 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc
[0m13:38:49.242107 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc"
[0m13:38:49.243767 [debug] [Thread-4 (]: On test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`cities`
where _id is null



  
  
      
    ) dbt_internal_test
[0m13:38:49.244675 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m13:38:49.300876 [info ] [Thread-1 (]: 15 of 35 PASS relationships_stg_shipments_user_id__user_id__ref_stg_users_ ..... [[32mPASS[0m in 2.32s]
[0m13:38:49.306137 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8
[0m13:38:49.307354 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd
[0m13:38:49.308563 [info ] [Thread-1 (]: 19 of 35 START test source_not_null_raw_data_countries__id ..................... [RUN]
[0m13:38:49.309753 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8, now test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd)
[0m13:38:49.310227 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd
[0m13:38:49.315410 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd"
[0m13:38:49.317552 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd
[0m13:38:49.321124 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd"
[0m13:38:49.322826 [debug] [Thread-1 (]: On test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`countries`
where _id is null



  
  
      
    ) dbt_internal_test
[0m13:38:49.323717 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:38:49.388364 [info ] [Thread-2 (]: 16 of 35 PASS relationships_stg_subscriptions_user_id__user_id__ref_stg_users_ . [[32mPASS[0m in 2.42s]
[0m13:38:49.392927 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m13:38:49.393441 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be
[0m13:38:49.393948 [info ] [Thread-2 (]: 20 of 35 START test source_not_null_raw_data_neighborhoods__id ................. [RUN]
[0m13:38:49.396730 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e, now test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be)
[0m13:38:49.401770 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be
[0m13:38:49.411401 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be"
[0m13:38:49.413166 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be
[0m13:38:49.416710 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be"
[0m13:38:49.419021 [debug] [Thread-2 (]: On test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`neighborhoods`
where _id is null



  
  
      
    ) dbt_internal_test
[0m13:38:49.420461 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m13:38:49.879730 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:fa626858-0f60-4aa2-8ed8-241ce73956eb&page=queryresults
[0m13:38:50.470396 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:700efa8e-d08f-490c-94a9-8deca546e8a8&page=queryresults
[0m13:38:50.499011 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:595dd026-3baf-42f2-895f-fbc72bca0c7a&page=queryresults
[0m13:38:50.535150 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:ce02d22c-0c64-47a4-bdd5-b8c3dbb257f4&page=queryresults
[0m13:38:50.791744 [info ] [Thread-3 (]: 17 of 35 PASS source_not_null_raw_data_addresses__id ........................... [[32mPASS[0m in 2.10s]
[0m13:38:50.793898 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6
[0m13:38:50.795322 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878
[0m13:38:50.797786 [info ] [Thread-3 (]: 21 of 35 START test source_not_null_raw_data_orders__id ........................ [RUN]
[0m13:38:50.799027 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6, now test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878)
[0m13:38:50.799604 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878
[0m13:38:50.806038 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878"
[0m13:38:50.807267 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878
[0m13:38:50.812980 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878"
[0m13:38:50.814362 [debug] [Thread-3 (]: On test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`orders`
where _id is null



  
  
      
    ) dbt_internal_test
[0m13:38:50.815324 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m13:38:51.406481 [info ] [Thread-4 (]: 18 of 35 PASS source_not_null_raw_data_cities__id .............................. [[32mPASS[0m in 2.18s]
[0m13:38:51.408315 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc
[0m13:38:51.409299 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f
[0m13:38:51.410315 [info ] [Thread-4 (]: 22 of 35 START test source_not_null_raw_data_orders__user ...................... [RUN]
[0m13:38:51.411247 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc, now test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f)
[0m13:38:51.412179 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f
[0m13:38:51.417058 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f"
[0m13:38:51.418885 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f
[0m13:38:51.422817 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f"
[0m13:38:51.425044 [debug] [Thread-4 (]: On test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _user
from `data-pipeline-project-474812`.`raw_data`.`orders`
where _user is null



  
  
      
    ) dbt_internal_test
[0m13:38:51.425863 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m13:38:51.467442 [info ] [Thread-1 (]: 19 of 35 PASS source_not_null_raw_data_countries__id ........................... [[32mPASS[0m in 2.16s]
[0m13:38:51.474762 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd
[0m13:38:51.479984 [info ] [Thread-2 (]: 20 of 35 PASS source_not_null_raw_data_neighborhoods__id ....................... [[32mPASS[0m in 2.08s]
[0m13:38:51.483566 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2
[0m13:38:51.493099 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be
[0m13:38:51.497052 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1
[0m13:38:51.495917 [info ] [Thread-1 (]: 23 of 35 START test source_not_null_raw_data_shipments__id ..................... [RUN]
[0m13:38:51.503105 [info ] [Thread-2 (]: 24 of 35 START test source_not_null_raw_data_shipments__order .................. [RUN]
[0m13:38:51.506040 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd, now test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2)
[0m13:38:51.510730 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be, now test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1)
[0m13:38:51.515919 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1
[0m13:38:51.513615 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2
[0m13:38:51.533331 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1"
[0m13:38:51.547318 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2"
[0m13:38:51.548867 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1
[0m13:38:51.558518 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2
[0m13:38:51.565915 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2"
[0m13:38:51.555712 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1"
[0m13:38:51.576076 [debug] [Thread-1 (]: On test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`shipments`
where _id is null



  
  
      
    ) dbt_internal_test
[0m13:38:51.578370 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:38:51.655679 [debug] [Thread-2 (]: On test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _order
from `data-pipeline-project-474812`.`raw_data`.`shipments`
where _order is null



  
  
      
    ) dbt_internal_test
[0m13:38:51.657603 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m13:38:51.903112 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:7533f54b-c1b4-4aac-98df-fa05be43903d&page=queryresults
[0m13:38:52.558181 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:3ae63ac9-a0fe-4b1e-a539-ba9853a86e7e&page=queryresults
[0m13:38:52.663965 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:83ae0c5e-197e-4687-b1a2-cc362acc133a&page=queryresults
[0m13:38:52.719107 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:63f9a1a8-519a-4b0e-847a-2aaffe4190fc&page=queryresults
[0m13:38:52.806613 [info ] [Thread-3 (]: 21 of 35 PASS source_not_null_raw_data_orders__id .............................. [[32mPASS[0m in 2.01s]
[0m13:38:52.809394 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878
[0m13:38:52.810928 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98
[0m13:38:52.812140 [info ] [Thread-3 (]: 25 of 35 START test source_not_null_raw_data_shipments__user ................... [RUN]
[0m13:38:52.814148 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878, now test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98)
[0m13:38:52.815660 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98
[0m13:38:52.824280 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98"
[0m13:38:52.826221 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98
[0m13:38:52.829860 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98"
[0m13:38:52.835053 [debug] [Thread-3 (]: On test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _user
from `data-pipeline-project-474812`.`raw_data`.`shipments`
where _user is null



  
  
      
    ) dbt_internal_test
[0m13:38:52.838835 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m13:38:53.457513 [info ] [Thread-4 (]: 22 of 35 PASS source_not_null_raw_data_orders__user ............................ [[32mPASS[0m in 2.05s]
[0m13:38:53.458919 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f
[0m13:38:53.459921 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86
[0m13:38:53.460662 [info ] [Thread-4 (]: 26 of 35 START test source_not_null_raw_data_states__id ........................ [RUN]
[0m13:38:53.461374 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f, now test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86)
[0m13:38:53.462042 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86
[0m13:38:53.468144 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86"
[0m13:38:53.469831 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86
[0m13:38:53.472767 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86"
[0m13:38:53.475453 [debug] [Thread-4 (]: On test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`states`
where _id is null



  
  
      
    ) dbt_internal_test
[0m13:38:53.476276 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m13:38:53.571520 [info ] [Thread-1 (]: 23 of 35 PASS source_not_null_raw_data_shipments__id ........................... [[32mPASS[0m in 2.07s]
[0m13:38:53.573385 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2
[0m13:38:53.574986 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a
[0m13:38:53.576250 [info ] [Thread-1 (]: 27 of 35 START test source_not_null_raw_data_subscriptions__id ................. [RUN]
[0m13:38:53.577620 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2, now test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a)
[0m13:38:53.578730 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a
[0m13:38:53.584020 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a"
[0m13:38:53.585249 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a
[0m13:38:53.587833 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a"
[0m13:38:53.588785 [debug] [Thread-1 (]: On test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`subscriptions`
where _id is null



  
  
      
    ) dbt_internal_test
[0m13:38:53.589859 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:38:53.658918 [info ] [Thread-2 (]: 24 of 35 PASS source_not_null_raw_data_shipments__order ........................ [[32mPASS[0m in 2.14s]
[0m13:38:53.662371 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1
[0m13:38:53.665624 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6
[0m13:38:53.667183 [info ] [Thread-2 (]: 28 of 35 START test source_not_null_raw_data_subscriptions__user ............... [RUN]
[0m13:38:53.668887 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1, now test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6)
[0m13:38:53.669551 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6
[0m13:38:53.681784 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6"
[0m13:38:53.683814 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6
[0m13:38:53.688551 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6"
[0m13:38:53.691864 [debug] [Thread-2 (]: On test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _user
from `data-pipeline-project-474812`.`raw_data`.`subscriptions`
where _user is null



  
  
      
    ) dbt_internal_test
[0m13:38:53.692641 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m13:38:54.020831 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:1b1e52fc-5fbe-41a7-845c-06af0c25346a&page=queryresults
[0m13:38:54.709977 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:ae30f467-b4c2-43a7-8bb2-a7e01ef3e050&page=queryresults
[0m13:38:54.768083 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:30c62f95-d1e1-47bf-9e65-e08323a4222b&page=queryresults
[0m13:38:54.957562 [info ] [Thread-3 (]: 25 of 35 PASS source_not_null_raw_data_shipments__user ......................... [[32mPASS[0m in 2.14s]
[0m13:38:54.959422 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98
[0m13:38:54.960710 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b
[0m13:38:54.961390 [info ] [Thread-3 (]: 29 of 35 START test source_not_null_raw_data_users__id ......................... [RUN]
[0m13:38:54.962242 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98, now test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b)
[0m13:38:54.962895 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b
[0m13:38:54.967654 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b"
[0m13:38:54.969574 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b
[0m13:38:54.973139 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b"
[0m13:38:54.974981 [debug] [Thread-3 (]: On test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`users`
where _id is null



  
  
      
    ) dbt_internal_test
[0m13:38:54.975744 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m13:38:55.060866 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:0d04203b-2f06-44d0-ad7e-66502dd2eefd&page=queryresults
[0m13:38:55.605219 [info ] [Thread-1 (]: 27 of 35 PASS source_not_null_raw_data_subscriptions__id ....................... [[32mPASS[0m in 2.02s]
[0m13:38:55.611805 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a
[0m13:38:55.615026 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00
[0m13:38:55.616997 [info ] [Thread-1 (]: 30 of 35 START test unique_stg_addresses_address_id ............................ [RUN]
[0m13:38:55.618851 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a, now test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00)
[0m13:38:55.620509 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00
[0m13:38:55.634177 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00"
[0m13:38:55.635701 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00
[0m13:38:55.640782 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00"
[0m13:38:55.648927 [debug] [Thread-1 (]: On test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select address_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_addresses`
  where address_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m13:38:55.650962 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:38:55.839818 [info ] [Thread-4 (]: 26 of 35 PASS source_not_null_raw_data_states__id .............................. [[32mPASS[0m in 2.38s]
[0m13:38:55.842991 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86
[0m13:38:55.843981 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.unique_stg_countries_country_id.8679936442
[0m13:38:55.845078 [info ] [Thread-4 (]: 31 of 35 START test unique_stg_countries_country_id ............................ [RUN]
[0m13:38:55.845955 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86, now test.data_pipeline_project.unique_stg_countries_country_id.8679936442)
[0m13:38:55.847247 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.unique_stg_countries_country_id.8679936442
[0m13:38:55.855789 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.unique_stg_countries_country_id.8679936442"
[0m13:38:55.857242 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.unique_stg_countries_country_id.8679936442
[0m13:38:55.860801 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.unique_stg_countries_country_id.8679936442"
[0m13:38:55.863987 [debug] [Thread-4 (]: On test.data_pipeline_project.unique_stg_countries_country_id.8679936442: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_stg_countries_country_id.8679936442"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select country_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_countries`
  where country_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m13:38:55.865038 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m13:38:55.931526 [info ] [Thread-2 (]: 28 of 35 PASS source_not_null_raw_data_subscriptions__user ..................... [[32mPASS[0m in 2.26s]
[0m13:38:55.936395 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6
[0m13:38:55.937304 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a
[0m13:38:55.938101 [info ] [Thread-2 (]: 32 of 35 START test unique_stg_orders_order_id ................................. [RUN]
[0m13:38:55.938887 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6, now test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a)
[0m13:38:55.939886 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a
[0m13:38:55.945188 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a"
[0m13:38:55.948059 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a
[0m13:38:55.952147 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a"
[0m13:38:55.953220 [debug] [Thread-2 (]: On test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select order_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_orders`
  where order_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m13:38:55.954086 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m13:38:56.099061 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:c14cce14-2a21-453d-9d46-8195f69f2657&page=queryresults
[0m13:38:57.077354 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:b2a40e19-c5d5-4f9d-9fb6-f3712c9af81c&page=queryresults
[0m13:38:57.089310 [info ] [Thread-3 (]: 29 of 35 PASS source_not_null_raw_data_users__id ............................... [[32mPASS[0m in 2.13s]
[0m13:38:57.091194 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b
[0m13:38:57.092411 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c
[0m13:38:57.093161 [info ] [Thread-3 (]: 33 of 35 START test unique_stg_shipments_shipment_id ........................... [RUN]
[0m13:38:57.093986 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b, now test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c)
[0m13:38:57.094700 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c
[0m13:38:57.099645 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c"
[0m13:38:57.100951 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c
[0m13:38:57.104421 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c"
[0m13:38:57.105745 [debug] [Thread-3 (]: On test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select shipment_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_shipments`
  where shipment_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m13:38:57.106801 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m13:38:57.139646 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:2051df5a-340f-4969-afec-1b26cb2b4852&page=queryresults
[0m13:38:57.313293 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:a3eab8e6-adda-4094-b8a6-a2aed193c570&page=queryresults
[0m13:38:57.904908 [info ] [Thread-1 (]: 30 of 35 PASS unique_stg_addresses_address_id .................................. [[32mPASS[0m in 2.29s]
[0m13:38:57.906351 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00
[0m13:38:57.907898 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m13:38:57.908828 [info ] [Thread-1 (]: 34 of 35 START test unique_stg_subscriptions_subscription_id ................... [RUN]
[0m13:38:57.909831 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00, now test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e)
[0m13:38:57.910493 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m13:38:57.915802 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e"
[0m13:38:57.917196 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m13:38:57.920204 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e"
[0m13:38:57.921584 [debug] [Thread-1 (]: On test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select subscription_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_subscriptions`
  where subscription_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m13:38:57.922710 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:38:58.016002 [info ] [Thread-4 (]: 31 of 35 PASS unique_stg_countries_country_id .................................. [[32mPASS[0m in 2.17s]
[0m13:38:58.017275 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.unique_stg_countries_country_id.8679936442
[0m13:38:58.018199 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b
[0m13:38:58.019016 [info ] [Thread-4 (]: 35 of 35 START test unique_stg_users_user_id ................................... [RUN]
[0m13:38:58.020036 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_stg_countries_country_id.8679936442, now test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b)
[0m13:38:58.020941 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b
[0m13:38:58.025516 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b"
[0m13:38:58.027116 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b
[0m13:38:58.033114 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b"
[0m13:38:58.034796 [debug] [Thread-4 (]: On test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select user_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
  where user_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m13:38:58.036275 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m13:38:58.276814 [info ] [Thread-2 (]: 32 of 35 PASS unique_stg_orders_order_id ....................................... [[32mPASS[0m in 2.33s]
[0m13:38:58.282138 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a
[0m13:38:58.390834 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:f5b1ae45-5330-41c8-8066-2c2702cfc7cf&page=queryresults
[0m13:38:59.194164 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:7022bc19-5800-4d27-a6bc-81e5f065fecb&page=queryresults
[0m13:38:59.307372 [info ] [Thread-3 (]: 33 of 35 PASS unique_stg_shipments_shipment_id ................................. [[32mPASS[0m in 2.21s]
[0m13:38:59.315409 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c
[0m13:38:59.457996 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:a019b2fe-835f-4884-bde5-923de1cbc569&page=queryresults
[0m13:39:00.066260 [info ] [Thread-1 (]: 34 of 35 PASS unique_stg_subscriptions_subscription_id ......................... [[32mPASS[0m in 2.16s]
[0m13:39:00.068153 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m13:39:00.330767 [info ] [Thread-4 (]: 35 of 35 PASS unique_stg_users_user_id ......................................... [[32mPASS[0m in 2.31s]
[0m13:39:00.336041 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b
[0m13:39:00.346674 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m13:39:00.351264 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:39:00.352263 [debug] [MainThread]: Connection 'test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e' was properly closed.
[0m13:39:00.353313 [debug] [MainThread]: Connection 'test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a' was properly closed.
[0m13:39:00.354104 [debug] [MainThread]: Connection 'test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c' was properly closed.
[0m13:39:00.354686 [debug] [MainThread]: Connection 'test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b' was properly closed.
[0m13:39:00.357108 [info ] [MainThread]: 
[0m13:39:00.359023 [info ] [MainThread]: Finished running 35 data tests in 0 hours 0 minutes and 22.30 seconds (22.30s).
[0m13:39:00.367984 [debug] [MainThread]: Command end result
[0m13:39:00.427594 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m13:39:00.433126 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m13:39:00.448413 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m13:39:00.448896 [info ] [MainThread]: 
[0m13:39:00.451418 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m13:39:00.453106 [info ] [MainThread]: 
[0m13:39:00.453929 [error] [MainThread]: [31mFailure in test not_null_stg_users_created_at (models/staging/staging_schema.yml)[0m
[0m13:39:00.454764 [error] [MainThread]:   Got 2 results, configured to fail if != 0
[0m13:39:00.455741 [info ] [MainThread]: 
[0m13:39:00.457926 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_project/models/staging/staging_schema.yml/not_null_stg_users_created_at.sql
[0m13:39:00.460096 [info ] [MainThread]: 
[0m13:39:00.461140 [info ] [MainThread]: Done. PASS=34 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=35
[0m13:39:00.467960 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m13:39:00.481776 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 26.473944, "process_in_blocks": "640", "process_kernel_time": 3.129862, "process_mem_max_rss": "397076", "process_out_blocks": "4760", "process_user_time": 10.139215}
[0m13:39:00.485165 [debug] [MainThread]: Command `dbt test` failed at 13:39:00.484283 after 26.48 seconds
[0m13:39:00.487851 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75a0a3ee47a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75a083e59eb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75a08332e9c0>]}
[0m13:39:00.489150 [debug] [MainThread]: Flushing usage events
[0m13:39:01.461009 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:00:38.619008 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d2fa42538f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d2fa52c7ef0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d2fa3cc13a0>]}


============================== 14:00:38.642167 | 6e8d5b9c-3834-45f5-bb06-467e95410993 ==============================
[0m14:00:38.642167 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m14:00:38.647228 [debug] [MainThread]: running dbt with arguments {'invocation_command': 'dbt test --select stg_users', 'version_check': 'True', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'write_json': 'True', 'use_experimental_parser': 'False', 'log_cache_events': 'False', 'static_parser': 'True', 'profiles_dir': '/home/ecem/.dbt', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'use_colors': 'True', 'cache_selected_only': 'False', 'no_print': 'None', 'warn_error': 'None', 'fail_fast': 'False', 'quiet': 'False', 'printer_width': '80', 'indirect_selection': 'eager', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'debug': 'False', 'empty': 'None', 'target_path': 'None'}
[0m14:00:41.193646 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6e8d5b9c-3834-45f5-bb06-467e95410993', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d2f871e9d60>]}
[0m14:00:41.260032 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6e8d5b9c-3834-45f5-bb06-467e95410993', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d2f846dd6a0>]}
[0m14:00:41.261075 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m14:00:41.512334 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m14:00:41.778230 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:00:41.779768 [debug] [MainThread]: Partial parsing: updated file: data_pipeline_project://models/staging/stg_addresses.sql
[0m14:00:41.780679 [debug] [MainThread]: Partial parsing: updated file: data_pipeline_project://models/staging/stg_orders.sql
[0m14:00:41.781641 [debug] [MainThread]: Partial parsing: updated file: data_pipeline_project://models/staging/stg_shipments.sql
[0m14:00:41.784077 [debug] [MainThread]: Partial parsing: updated file: data_pipeline_project://models/staging/stg_subscriptions.sql
[0m14:00:41.784527 [debug] [MainThread]: Partial parsing: updated file: data_pipeline_project://models/staging/stg_users.sql
[0m14:00:42.164154 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `relationships` defined on 'stg_addresses' in
package 'data_pipeline_project' (models/staging/staging_schema.yml). Arguments
to generic tests should be nested under the `arguments` property.
[0m14:00:42.165809 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '6e8d5b9c-3834-45f5-bb06-467e95410993', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d2f83147b90>]}
[0m14:00:42.328945 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.data_pipeline_project.marts
[0m14:00:42.348628 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6e8d5b9c-3834-45f5-bb06-467e95410993', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d2f83a8f8f0>]}
[0m14:00:42.478901 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m14:00:42.484911 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m14:00:42.514506 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6e8d5b9c-3834-45f5-bb06-467e95410993', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d2f8317b8f0>]}
[0m14:00:42.515694 [info ] [MainThread]: Found 10 models, 35 data tests, 10 sources, 508 macros
[0m14:00:42.516910 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6e8d5b9c-3834-45f5-bb06-467e95410993', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d2fa35233e0>]}
[0m14:00:42.519586 [info ] [MainThread]: 
[0m14:00:42.520514 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:00:42.521176 [info ] [MainThread]: 
[0m14:00:42.522077 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m14:00:42.527637 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_staging'
[0m14:00:42.529343 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:00:44.238501 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6e8d5b9c-3834-45f5-bb06-467e95410993', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d2f83a3f470>]}
[0m14:00:44.239790 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:00:44.251352 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25
[0m14:00:44.252229 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77
[0m14:00:44.253637 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781
[0m14:00:44.253021 [info ] [Thread-1 (]: 1 of 7 START test not_null_stg_users_created_at ................................ [RUN]
[0m14:00:44.254771 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737
[0m14:00:44.255490 [info ] [Thread-2 (]: 2 of 7 START test not_null_stg_users_user_id ................................... [RUN]
[0m14:00:44.256896 [info ] [Thread-3 (]: 3 of 7 START test relationships_stg_addresses_user_id__user_id__ref_stg_users_ . [RUN]
[0m14:00:44.258085 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25)
[0m14:00:44.264402 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25
[0m14:00:44.263667 [debug] [Thread-3 (]: Acquiring new bigquery connection 'test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781'
[0m14:00:44.261379 [info ] [Thread-4 (]: 4 of 7 START test relationships_stg_orders_user_id__user_id__ref_stg_users_ .... [RUN]
[0m14:00:44.262254 [debug] [Thread-2 (]: Acquiring new bigquery connection 'test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77'
[0m14:00:44.276987 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781
[0m14:00:44.278329 [debug] [Thread-4 (]: Acquiring new bigquery connection 'test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737'
[0m14:00:44.283887 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25"
[0m14:00:44.288239 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77
[0m14:00:44.294729 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781"
[0m14:00:44.296009 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737
[0m14:00:44.298458 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25
[0m14:00:44.316239 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77"
[0m14:00:44.319876 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737"
[0m14:00:44.320907 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781
[0m14:00:44.349481 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25"
[0m14:00:44.351834 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77
[0m14:00:44.355748 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781"
[0m14:00:44.362313 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737
[0m14:00:44.368355 [debug] [Thread-1 (]: On test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select created_at
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
where created_at is null



  
  
      
    ) dbt_internal_test
[0m14:00:44.367245 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77"
[0m14:00:44.373483 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737"
[0m14:00:44.384122 [debug] [Thread-4 (]: On test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select user_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_orders`
    where user_id is not null
),

parent as (
    select user_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m14:00:44.384833 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m14:00:44.383110 [debug] [Thread-2 (]: On test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
where user_id is null



  
  
      
    ) dbt_internal_test
[0m14:00:44.375487 [debug] [Thread-3 (]: On test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select user_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_addresses`
    where user_id is not null
),

parent as (
    select user_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m14:00:44.444089 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m14:00:44.441600 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m14:00:44.378811 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:00:45.840548 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:d4155b53-08f7-4192-86d1-f4de4239d4d3&page=queryresults
[0m14:00:46.213442 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:c2ae1c90-7e3b-4724-a6de-085760403d13&page=queryresults
[0m14:00:46.221962 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:55dfd4cb-ac86-43aa-baaf-392329bd453e&page=queryresults
[0m14:00:46.249971 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:8b996dae-2c32-493f-86e8-6ce25044b200&page=queryresults
[0m14:00:46.869811 [info ] [Thread-4 (]: 4 of 7 PASS relationships_stg_orders_user_id__user_id__ref_stg_users_ .......... [[32mPASS[0m in 2.59s]
[0m14:00:46.871032 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737
[0m14:00:46.871911 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8
[0m14:00:46.873047 [info ] [Thread-4 (]: 5 of 7 START test relationships_stg_shipments_user_id__user_id__ref_stg_users_ . [RUN]
[0m14:00:46.874523 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737, now test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8)
[0m14:00:46.875187 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8
[0m14:00:46.883967 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8"
[0m14:00:46.891366 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8
[0m14:00:46.894824 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8"
[0m14:00:46.896806 [debug] [Thread-4 (]: On test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select user_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_shipments`
    where user_id is not null
),

parent as (
    select user_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m14:00:46.898827 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:00:47.191817 [info ] [Thread-3 (]: 3 of 7 PASS relationships_stg_addresses_user_id__user_id__ref_stg_users_ ....... [[32mPASS[0m in 2.93s]
[0m14:00:47.198041 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781
[0m14:00:47.200085 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m14:00:47.201197 [info ] [Thread-3 (]: 6 of 7 START test relationships_stg_subscriptions_user_id__user_id__ref_stg_users_  [RUN]
[0m14:00:47.202926 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781, now test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e)
[0m14:00:47.204174 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m14:00:47.218326 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e"
[0m14:00:47.216241 [info ] [Thread-2 (]: 2 of 7 PASS not_null_stg_users_user_id ......................................... [[32mPASS[0m in 2.95s]
[0m14:00:47.227114 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77
[0m14:00:47.229098 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b
[0m14:00:47.230371 [info ] [Thread-2 (]: 7 of 7 START test unique_stg_users_user_id ..................................... [RUN]
[0m14:00:47.236039 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77, now test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b)
[0m14:00:47.235234 [error] [Thread-1 (]: 1 of 7 FAIL 2 not_null_stg_users_created_at .................................... [[31mFAIL 2[0m in 2.98s]
[0m14:00:47.237852 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m14:00:47.244510 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e"
[0m14:00:47.240828 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25
[0m14:00:47.239452 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b
[0m14:00:47.252487 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b"
[0m14:00:47.257736 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b
[0m14:00:47.259425 [debug] [Thread-3 (]: On test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select user_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_subscriptions`
    where user_id is not null
),

parent as (
    select user_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m14:00:47.264769 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b"
[0m14:00:47.267272 [debug] [Thread-2 (]: On test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select user_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
  where user_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:00:47.265994 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:00:47.268240 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:00:48.716177 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:12c81868-fea6-4680-a034-839b65142cc2&page=queryresults
[0m14:00:48.724713 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:a0a81236-8566-403b-bc6a-036974ce396d&page=queryresults
[0m14:00:48.888286 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:835ad6aa-a434-4146-9dc6-6259ff9c01d7&page=queryresults
[0m14:00:49.622925 [info ] [Thread-4 (]: 5 of 7 PASS relationships_stg_shipments_user_id__user_id__ref_stg_users_ ....... [[32mPASS[0m in 2.75s]
[0m14:00:49.631473 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8
[0m14:00:49.662003 [info ] [Thread-3 (]: 6 of 7 PASS relationships_stg_subscriptions_user_id__user_id__ref_stg_users_ ... [[32mPASS[0m in 2.46s]
[0m14:00:49.663161 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m14:00:49.799498 [info ] [Thread-2 (]: 7 of 7 PASS unique_stg_users_user_id ........................................... [[32mPASS[0m in 2.56s]
[0m14:00:49.800766 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b
[0m14:00:49.805456 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:00:49.808067 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:00:49.809188 [debug] [MainThread]: Connection 'test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25' was properly closed.
[0m14:00:49.810067 [debug] [MainThread]: Connection 'test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b' was properly closed.
[0m14:00:49.813107 [debug] [MainThread]: Connection 'test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e' was properly closed.
[0m14:00:49.814488 [debug] [MainThread]: Connection 'test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8' was properly closed.
[0m14:00:49.815684 [info ] [MainThread]: 
[0m14:00:49.816802 [info ] [MainThread]: Finished running 7 data tests in 0 hours 0 minutes and 7.29 seconds (7.29s).
[0m14:00:49.819578 [debug] [MainThread]: Command end result
[0m14:00:49.866485 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m14:00:49.868881 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m14:00:49.879303 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m14:00:49.880684 [info ] [MainThread]: 
[0m14:00:49.882382 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m14:00:49.883780 [info ] [MainThread]: 
[0m14:00:49.884364 [error] [MainThread]: [31mFailure in test not_null_stg_users_created_at (models/staging/staging_schema.yml)[0m
[0m14:00:49.885241 [error] [MainThread]:   Got 2 results, configured to fail if != 0
[0m14:00:49.886072 [info ] [MainThread]: 
[0m14:00:49.887072 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_project/models/staging/staging_schema.yml/not_null_stg_users_created_at.sql
[0m14:00:49.887999 [info ] [MainThread]: 
[0m14:00:49.889175 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=7
[0m14:00:49.891281 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 4 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m14:00:49.894098 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 11.343889, "process_in_blocks": "0", "process_kernel_time": 1.943946, "process_mem_max_rss": "392944", "process_out_blocks": "4088", "process_user_time": 5.593549}
[0m14:00:49.895780 [debug] [MainThread]: Command `dbt test` failed at 14:00:49.895340 after 11.35 seconds
[0m14:00:49.896950 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d2fa4385c40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d2f83f62a50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d2f841d97f0>]}
[0m14:00:49.897888 [debug] [MainThread]: Flushing usage events
[0m14:00:55.615988 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:01:36.469913 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x759394797050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x759394795370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x759396ade510>]}


============================== 14:01:36.487167 | 6d28f888-d7e1-4f4e-84a7-f9bb4f77a8a4 ==============================
[0m14:01:36.487167 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m14:01:36.494075 [debug] [MainThread]: running dbt with arguments {'no_print': 'None', 'quiet': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'debug': 'False', 'profiles_dir': '/home/ecem/.dbt', 'partial_parse': 'True', 'version_check': 'True', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'static_parser': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'target_path': 'None', 'indirect_selection': 'eager', 'fail_fast': 'False', 'printer_width': '80', 'write_json': 'True', 'cache_selected_only': 'False', 'log_format': 'default', 'warn_error': 'None', 'use_colors': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'send_anonymous_usage_stats': 'True', 'invocation_command': 'dbt test --select stg_users'}
[0m14:01:38.827082 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6d28f888-d7e1-4f4e-84a7-f9bb4f77a8a4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x759374cc0a70>]}
[0m14:01:38.897721 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6d28f888-d7e1-4f4e-84a7-f9bb4f77a8a4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75937521f2c0>]}
[0m14:01:38.899270 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m14:01:39.220909 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m14:01:39.482669 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:01:39.483226 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:01:39.489199 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.data_pipeline_project.marts
[0m14:01:39.554811 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6d28f888-d7e1-4f4e-84a7-f9bb4f77a8a4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7593749cb320>]}
[0m14:01:39.675604 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m14:01:39.678473 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m14:01:39.703401 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6d28f888-d7e1-4f4e-84a7-f9bb4f77a8a4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75937437c110>]}
[0m14:01:39.704088 [info ] [MainThread]: Found 10 models, 35 data tests, 10 sources, 508 macros
[0m14:01:39.704828 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6d28f888-d7e1-4f4e-84a7-f9bb4f77a8a4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75937443c1d0>]}
[0m14:01:39.711021 [info ] [MainThread]: 
[0m14:01:39.711712 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:01:39.712822 [info ] [MainThread]: 
[0m14:01:39.713479 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m14:01:39.719641 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_staging'
[0m14:01:39.721011 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:01:40.697700 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6d28f888-d7e1-4f4e-84a7-f9bb4f77a8a4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7593744ba3c0>]}
[0m14:01:40.698448 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:01:40.707350 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25
[0m14:01:40.708421 [info ] [Thread-1 (]: 1 of 7 START test not_null_stg_users_created_at ................................ [RUN]
[0m14:01:40.709137 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25)
[0m14:01:40.710161 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25
[0m14:01:40.720134 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77
[0m14:01:40.736680 [info ] [Thread-2 (]: 2 of 7 START test not_null_stg_users_user_id ................................... [RUN]
[0m14:01:40.744216 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25"
[0m14:01:40.745489 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781
[0m14:01:40.746318 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737
[0m14:01:40.748714 [debug] [Thread-2 (]: Acquiring new bigquery connection 'test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77'
[0m14:01:40.750122 [info ] [Thread-3 (]: 3 of 7 START test relationships_stg_addresses_user_id__user_id__ref_stg_users_ . [RUN]
[0m14:01:40.756404 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77
[0m14:01:40.757233 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25
[0m14:01:40.758728 [debug] [Thread-3 (]: Acquiring new bigquery connection 'test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781'
[0m14:01:40.751334 [info ] [Thread-4 (]: 4 of 7 START test relationships_stg_orders_user_id__user_id__ref_stg_users_ .... [RUN]
[0m14:01:40.777678 [debug] [Thread-4 (]: Acquiring new bigquery connection 'test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737'
[0m14:01:40.778728 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737
[0m14:01:40.775181 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781
[0m14:01:40.790525 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77"
[0m14:01:40.834096 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781"
[0m14:01:40.833325 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737"
[0m14:01:40.851994 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737
[0m14:01:40.876877 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77
[0m14:01:40.886989 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25"
[0m14:01:40.889436 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781
[0m14:01:40.895284 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737"
[0m14:01:40.937381 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781"
[0m14:01:40.909986 [debug] [Thread-1 (]: On test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select created_at
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
where created_at is null



  
  
      
    ) dbt_internal_test
[0m14:01:40.942448 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:01:40.920189 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77"
[0m14:01:40.946389 [debug] [Thread-3 (]: On test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select user_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_addresses`
    where user_id is not null
),

parent as (
    select user_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m14:01:40.948170 [debug] [Thread-4 (]: On test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select user_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_orders`
    where user_id is not null
),

parent as (
    select user_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m14:01:41.027504 [debug] [Thread-2 (]: On test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
where user_id is null



  
  
      
    ) dbt_internal_test
[0m14:01:41.029247 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m14:01:41.032079 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m14:01:41.033667 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m14:01:42.490655 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:9f1612c4-5629-4f0d-8f90-eb58367839f3&page=queryresults
[0m14:01:42.594590 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:d701e250-4cf0-4479-96a7-6eabab25e46f&page=queryresults
[0m14:01:42.596046 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:5d51c228-e238-4a80-88e2-ab5079666636&page=queryresults
[0m14:01:42.700990 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:5c0deb12-69db-4d69-aa66-ee2044da8334&page=queryresults
[0m14:01:43.535067 [info ] [Thread-3 (]: 3 of 7 PASS relationships_stg_addresses_user_id__user_id__ref_stg_users_ ....... [[32mPASS[0m in 2.78s]
[0m14:01:43.540196 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781
[0m14:01:43.543537 [info ] [Thread-2 (]: 2 of 7 PASS not_null_stg_users_user_id ......................................... [[32mPASS[0m in 2.80s]
[0m14:01:43.544937 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8
[0m14:01:43.546217 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77
[0m14:01:43.547296 [info ] [Thread-3 (]: 5 of 7 START test relationships_stg_shipments_user_id__user_id__ref_stg_users_ . [RUN]
[0m14:01:43.549181 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m14:01:43.551086 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781, now test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8)
[0m14:01:43.562045 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8
[0m14:01:43.557185 [error] [Thread-1 (]: 1 of 7 FAIL 2 not_null_stg_users_created_at .................................... [[31mFAIL 2[0m in 2.85s]
[0m14:01:43.571512 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8"
[0m14:01:43.558287 [info ] [Thread-2 (]: 6 of 7 START test relationships_stg_subscriptions_user_id__user_id__ref_stg_users_  [RUN]
[0m14:01:43.575841 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77, now test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e)
[0m14:01:43.576679 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m14:01:43.572889 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25
[0m14:01:43.577217 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8
[0m14:01:43.586037 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b
[0m14:01:43.590319 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8"
[0m14:01:43.597219 [info ] [Thread-1 (]: 7 of 7 START test unique_stg_users_user_id ..................................... [RUN]
[0m14:01:43.596270 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e"
[0m14:01:43.599991 [debug] [Thread-3 (]: On test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select user_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_shipments`
    where user_id is not null
),

parent as (
    select user_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m14:01:43.601002 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25, now test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b)
[0m14:01:43.603534 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b
[0m14:01:43.602687 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:01:43.658814 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m14:01:43.668026 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e"
[0m14:01:43.672685 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b"
[0m14:01:43.675658 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b
[0m14:01:43.678664 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b"
[0m14:01:43.674559 [info ] [Thread-4 (]: 4 of 7 PASS relationships_stg_orders_user_id__user_id__ref_stg_users_ .......... [[32mPASS[0m in 2.90s]
[0m14:01:43.680393 [debug] [Thread-1 (]: On test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select user_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
  where user_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:01:43.681992 [debug] [Thread-2 (]: On test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select user_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_subscriptions`
    where user_id is not null
),

parent as (
    select user_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m14:01:43.683360 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737
[0m14:01:43.685135 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:01:43.690223 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:01:45.053770 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:495cab40-64b2-4d38-aa4c-4b0c75efcbee&page=queryresults
[0m14:01:45.088141 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:2157261b-673d-4974-a9d0-75b198a674b8&page=queryresults
[0m14:01:45.236300 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:69f82e87-96ce-42b7-ad9a-9d1a2786ac8a&page=queryresults
[0m14:01:46.045157 [info ] [Thread-3 (]: 5 of 7 PASS relationships_stg_shipments_user_id__user_id__ref_stg_users_ ....... [[32mPASS[0m in 2.49s]
[0m14:01:46.048791 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8
[0m14:01:46.056768 [info ] [Thread-1 (]: 7 of 7 PASS unique_stg_users_user_id ........................................... [[32mPASS[0m in 2.46s]
[0m14:01:46.058682 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b
[0m14:01:46.377404 [info ] [Thread-2 (]: 6 of 7 PASS relationships_stg_subscriptions_user_id__user_id__ref_stg_users_ ... [[32mPASS[0m in 2.79s]
[0m14:01:46.389710 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m14:01:46.443826 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:01:46.449776 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:01:46.451368 [debug] [MainThread]: Connection 'test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b' was properly closed.
[0m14:01:46.452697 [debug] [MainThread]: Connection 'test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e' was properly closed.
[0m14:01:46.453460 [debug] [MainThread]: Connection 'test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8' was properly closed.
[0m14:01:46.455885 [debug] [MainThread]: Connection 'test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737' was properly closed.
[0m14:01:46.456583 [info ] [MainThread]: 
[0m14:01:46.460321 [info ] [MainThread]: Finished running 7 data tests in 0 hours 0 minutes and 6.74 seconds (6.74s).
[0m14:01:46.462532 [debug] [MainThread]: Command end result
[0m14:01:46.509778 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m14:01:46.513970 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m14:01:46.526982 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m14:01:46.527754 [info ] [MainThread]: 
[0m14:01:46.528925 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m14:01:46.530892 [info ] [MainThread]: 
[0m14:01:46.532980 [error] [MainThread]: [31mFailure in test not_null_stg_users_created_at (models/staging/staging_schema.yml)[0m
[0m14:01:46.534140 [error] [MainThread]:   Got 2 results, configured to fail if != 0
[0m14:01:46.535522 [info ] [MainThread]: 
[0m14:01:46.536666 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_project/models/staging/staging_schema.yml/not_null_stg_users_created_at.sql
[0m14:01:46.537224 [info ] [MainThread]: 
[0m14:01:46.537903 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=7
[0m14:01:46.540358 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 10.134309, "process_in_blocks": "0", "process_kernel_time": 1.761438, "process_mem_max_rss": "387576", "process_out_blocks": "2864", "process_user_time": 5.426507}
[0m14:01:46.541375 [debug] [MainThread]: Command `dbt test` failed at 14:01:46.541218 after 10.14 seconds
[0m14:01:46.542822 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x759394389670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x759393fda5a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x759395e608f0>]}
[0m14:01:46.544237 [debug] [MainThread]: Flushing usage events
[0m14:01:47.282431 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:07:22.540604 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f50d67dcb60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f50d5ae79e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f50d62a8e90>]}


============================== 14:07:22.543849 | 86ecd8fa-5f0f-4471-9ae5-ad0b9ecf3707 ==============================
[0m14:07:22.543849 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m14:07:22.545344 [debug] [MainThread]: running dbt with arguments {'quiet': 'False', 'cache_selected_only': 'False', 'send_anonymous_usage_stats': 'True', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'log_format': 'default', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'indirect_selection': 'eager', 'write_json': 'True', 'introspect': 'True', 'no_print': 'None', 'profiles_dir': '/home/ecem/.dbt', 'use_experimental_parser': 'False', 'warn_error': 'None', 'printer_width': '80', 'invocation_command': 'dbt test --select stg_users', 'log_cache_events': 'False', 'debug': 'False', 'partial_parse': 'True', 'static_parser': 'True', 'empty': 'None', 'use_colors': 'True', 'version_check': 'True'}
[0m14:07:24.994423 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '86ecd8fa-5f0f-4471-9ae5-ad0b9ecf3707', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f50d7f9b9e0>]}
[0m14:07:25.063955 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '86ecd8fa-5f0f-4471-9ae5-ad0b9ecf3707', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f50b64ed9a0>]}
[0m14:07:25.066091 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m14:07:25.271375 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m14:07:25.625214 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:07:25.627950 [debug] [MainThread]: Partial parsing: updated file: data_pipeline_project://models/staging/stg_addresses.sql
[0m14:07:25.629024 [debug] [MainThread]: Partial parsing: updated file: data_pipeline_project://models/staging/stg_orders.sql
[0m14:07:25.629828 [debug] [MainThread]: Partial parsing: updated file: data_pipeline_project://models/staging/stg_shipments.sql
[0m14:07:25.630547 [debug] [MainThread]: Partial parsing: updated file: data_pipeline_project://models/staging/stg_subscriptions.sql
[0m14:07:25.631102 [debug] [MainThread]: Partial parsing: updated file: data_pipeline_project://models/staging/stg_users.sql
[0m14:07:25.996223 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `relationships` defined on 'stg_addresses' in
package 'data_pipeline_project' (models/staging/staging_schema.yml). Arguments
to generic tests should be nested under the `arguments` property.
[0m14:07:25.999480 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '86ecd8fa-5f0f-4471-9ae5-ad0b9ecf3707', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f50b529b320>]}
[0m14:07:26.191663 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.data_pipeline_project.marts
[0m14:07:26.218957 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '86ecd8fa-5f0f-4471-9ae5-ad0b9ecf3707', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f50b5be62d0>]}
[0m14:07:26.367888 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m14:07:26.375081 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m14:07:26.417770 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '86ecd8fa-5f0f-4471-9ae5-ad0b9ecf3707', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f50b518b9b0>]}
[0m14:07:26.418651 [info ] [MainThread]: Found 10 models, 35 data tests, 10 sources, 508 macros
[0m14:07:26.419908 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '86ecd8fa-5f0f-4471-9ae5-ad0b9ecf3707', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f50b5b47f80>]}
[0m14:07:26.423008 [info ] [MainThread]: 
[0m14:07:26.424205 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:07:26.425693 [info ] [MainThread]: 
[0m14:07:26.427424 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m14:07:26.436905 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_staging'
[0m14:07:26.437836 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:07:27.526700 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '86ecd8fa-5f0f-4471-9ae5-ad0b9ecf3707', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f50b519c3b0>]}
[0m14:07:27.534380 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:07:27.557509 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25
[0m14:07:27.558942 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77
[0m14:07:27.561512 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781
[0m14:07:27.563080 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737
[0m14:07:27.560368 [info ] [Thread-1 (]: 1 of 7 START test not_null_stg_users_created_at ................................ [RUN]
[0m14:07:27.563985 [info ] [Thread-2 (]: 2 of 7 START test not_null_stg_users_user_id ................................... [RUN]
[0m14:07:27.573019 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25)
[0m14:07:27.565256 [info ] [Thread-3 (]: 3 of 7 START test relationships_stg_addresses_user_id__user_id__ref_stg_users_ . [RUN]
[0m14:07:27.579963 [debug] [Thread-2 (]: Acquiring new bigquery connection 'test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77'
[0m14:07:27.567906 [info ] [Thread-4 (]: 4 of 7 START test relationships_stg_orders_user_id__user_id__ref_stg_users_ .... [RUN]
[0m14:07:27.581831 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25
[0m14:07:27.583240 [debug] [Thread-3 (]: Acquiring new bigquery connection 'test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781'
[0m14:07:27.586019 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77
[0m14:07:27.587911 [debug] [Thread-4 (]: Acquiring new bigquery connection 'test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737'
[0m14:07:27.597769 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781
[0m14:07:27.614564 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25"
[0m14:07:27.619309 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77"
[0m14:07:27.620432 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737
[0m14:07:27.626375 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781"
[0m14:07:27.629478 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25
[0m14:07:27.635970 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737"
[0m14:07:27.636731 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77
[0m14:07:27.638079 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781
[0m14:07:27.648219 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737
[0m14:07:27.724858 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25"
[0m14:07:27.728936 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781"
[0m14:07:27.749749 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77"
[0m14:07:27.750690 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737"
[0m14:07:27.753105 [debug] [Thread-3 (]: On test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select user_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_addresses`
    where user_id is not null
),

parent as (
    select user_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m14:07:27.755156 [debug] [Thread-1 (]: On test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select created_at
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
where created_at is null



  
  
      
    ) dbt_internal_test
[0m14:07:27.758100 [debug] [Thread-2 (]: On test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
where user_id is null



  
  
      
    ) dbt_internal_test
[0m14:07:27.763412 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:07:27.760256 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m14:07:27.761862 [debug] [Thread-4 (]: On test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select user_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_orders`
    where user_id is not null
),

parent as (
    select user_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m14:07:27.770444 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m14:07:27.812528 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m14:07:29.449903 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:eb425503-3cfc-4223-960f-c8a0c09c15c3&page=queryresults
[0m14:07:29.518595 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:8a9e3508-6c22-4f8b-b28c-9e37e7936608&page=queryresults
[0m14:07:29.852585 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:0dba8d5c-89c5-45f3-9f05-3fd2f42fc3fa&page=queryresults
[0m14:07:30.079463 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:e059f7dc-d38d-45bd-a822-1406255938ac&page=queryresults
[0m14:07:30.508083 [info ] [Thread-3 (]: 3 of 7 PASS relationships_stg_addresses_user_id__user_id__ref_stg_users_ ....... [[32mPASS[0m in 2.92s]
[0m14:07:30.510268 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781
[0m14:07:30.511368 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8
[0m14:07:30.512274 [info ] [Thread-3 (]: 5 of 7 START test relationships_stg_shipments_user_id__user_id__ref_stg_users_ . [RUN]
[0m14:07:30.513441 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781, now test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8)
[0m14:07:30.514240 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8
[0m14:07:30.522404 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8"
[0m14:07:30.523933 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8
[0m14:07:30.526662 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8"
[0m14:07:30.528305 [debug] [Thread-3 (]: On test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select user_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_shipments`
    where user_id is not null
),

parent as (
    select user_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m14:07:30.529077 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:07:30.661281 [info ] [Thread-2 (]: 2 of 7 PASS not_null_stg_users_user_id ......................................... [[32mPASS[0m in 3.08s]
[0m14:07:30.667190 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77
[0m14:07:30.670247 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m14:07:30.672001 [info ] [Thread-2 (]: 6 of 7 START test relationships_stg_subscriptions_user_id__user_id__ref_stg_users_  [RUN]
[0m14:07:30.673988 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77, now test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e)
[0m14:07:30.677270 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m14:07:30.684705 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e"
[0m14:07:30.687310 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m14:07:30.694908 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e"
[0m14:07:30.698448 [debug] [Thread-2 (]: On test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select user_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_subscriptions`
    where user_id is not null
),

parent as (
    select user_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m14:07:30.699284 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:07:30.746064 [info ] [Thread-4 (]: 4 of 7 PASS relationships_stg_orders_user_id__user_id__ref_stg_users_ .......... [[32mPASS[0m in 3.16s]
[0m14:07:30.747696 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737
[0m14:07:30.758392 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b
[0m14:07:30.760791 [info ] [Thread-4 (]: 7 of 7 START test unique_stg_users_user_id ..................................... [RUN]
[0m14:07:30.762968 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737, now test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b)
[0m14:07:30.764173 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b
[0m14:07:30.779761 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b"
[0m14:07:30.783126 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b
[0m14:07:30.787941 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b"
[0m14:07:30.789071 [debug] [Thread-4 (]: On test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select user_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
  where user_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:07:30.789830 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:07:31.109920 [error] [Thread-1 (]: 1 of 7 FAIL 2 not_null_stg_users_created_at .................................... [[31mFAIL 2[0m in 3.54s]
[0m14:07:31.113861 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25
[0m14:07:31.822967 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:2f12414a-f6dd-485f-955c-73482500989b&page=queryresults
[0m14:07:31.994275 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:54c47a0a-4178-4118-bc61-b46c7b486ba0&page=queryresults
[0m14:07:32.135016 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:13cafa31-fba6-4059-a7fb-dd9dc0b48114&page=queryresults
[0m14:07:32.756363 [info ] [Thread-3 (]: 5 of 7 PASS relationships_stg_shipments_user_id__user_id__ref_stg_users_ ....... [[32mPASS[0m in 2.24s]
[0m14:07:32.758166 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8
[0m14:07:32.954270 [info ] [Thread-2 (]: 6 of 7 PASS relationships_stg_subscriptions_user_id__user_id__ref_stg_users_ ... [[32mPASS[0m in 2.27s]
[0m14:07:32.962560 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m14:07:33.015580 [info ] [Thread-4 (]: 7 of 7 PASS unique_stg_users_user_id ........................................... [[32mPASS[0m in 2.25s]
[0m14:07:33.017154 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b
[0m14:07:33.021430 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:07:33.028307 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:07:33.029409 [debug] [MainThread]: Connection 'test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25' was properly closed.
[0m14:07:33.029906 [debug] [MainThread]: Connection 'test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e' was properly closed.
[0m14:07:33.030396 [debug] [MainThread]: Connection 'test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8' was properly closed.
[0m14:07:33.030877 [debug] [MainThread]: Connection 'test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b' was properly closed.
[0m14:07:33.031498 [info ] [MainThread]: 
[0m14:07:33.032202 [info ] [MainThread]: Finished running 7 data tests in 0 hours 0 minutes and 6.60 seconds (6.60s).
[0m14:07:33.033941 [debug] [MainThread]: Command end result
[0m14:07:33.102811 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m14:07:33.111652 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m14:07:33.124906 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m14:07:33.125380 [info ] [MainThread]: 
[0m14:07:33.126726 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m14:07:33.128165 [info ] [MainThread]: 
[0m14:07:33.129210 [error] [MainThread]: [31mFailure in test not_null_stg_users_created_at (models/staging/staging_schema.yml)[0m
[0m14:07:33.131721 [error] [MainThread]:   Got 2 results, configured to fail if != 0
[0m14:07:33.132443 [info ] [MainThread]: 
[0m14:07:33.133323 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_project/models/staging/staging_schema.yml/not_null_stg_users_created_at.sql
[0m14:07:33.133902 [info ] [MainThread]: 
[0m14:07:33.134419 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=7
[0m14:07:33.135312 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 4 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m14:07:33.136839 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 10.655757, "process_in_blocks": "0", "process_kernel_time": 1.675411, "process_mem_max_rss": "393256", "process_out_blocks": "4088", "process_user_time": 6.238234}
[0m14:07:33.138973 [debug] [MainThread]: Command `dbt test` failed at 14:07:33.138813 after 10.66 seconds
[0m14:07:33.139982 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f50d571f500>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f50b51b7c80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f50b61657c0>]}
[0m14:07:33.140543 [debug] [MainThread]: Flushing usage events
[0m14:07:34.156947 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:09:31.293821 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73d43c5b5130>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73d43c6d91f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73d43c6da630>]}


============================== 14:09:31.297905 | 50caea59-b45d-4bfd-bd5f-94fdef0fdef8 ==============================
[0m14:09:31.297905 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m14:09:31.298994 [debug] [MainThread]: running dbt with arguments {'introspect': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'send_anonymous_usage_stats': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'quiet': 'False', 'write_json': 'True', 'profiles_dir': '/home/ecem/.dbt', 'partial_parse': 'True', 'printer_width': '80', 'log_cache_events': 'False', 'debug': 'False', 'indirect_selection': 'eager', 'use_colors': 'True', 'no_print': 'None', 'cache_selected_only': 'False', 'log_format': 'default', 'invocation_command': 'dbt clean', 'version_check': 'True', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'fail_fast': 'False', 'warn_error': 'None', 'static_parser': 'True'}
[0m14:09:31.441464 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '50caea59-b45d-4bfd-bd5f-94fdef0fdef8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73d43bce3ec0>]}
[0m14:09:31.469251 [debug] [MainThread]: Resource report: {"command_name": "clean", "command_success": true, "command_wall_clock_time": 0.23442078, "process_in_blocks": "0", "process_kernel_time": 0.275882, "process_mem_max_rss": "101624", "process_out_blocks": "16", "process_user_time": 1.622352}
[0m14:09:31.469956 [debug] [MainThread]: Command `dbt clean` succeeded at 14:09:31.469724 after 0.24 seconds
[0m14:09:31.470358 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73d43c7d0740>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73d43bdbdf40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73d43bcb1880>]}
[0m14:09:31.470757 [debug] [MainThread]: Flushing usage events
[0m14:09:32.446859 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:09:35.130462 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7196375e0a10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7196381ca930>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71963717b380>]}


============================== 14:09:35.133694 | 41a897a7-b615-410e-adbb-f1fa2610d28c ==============================
[0m14:09:35.133694 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m14:09:35.136710 [debug] [MainThread]: running dbt with arguments {'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'write_json': 'True', 'warn_error': 'None', 'quiet': 'False', 'no_print': 'None', 'printer_width': '80', 'partial_parse': 'True', 'version_check': 'True', 'use_colors': 'True', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True', 'empty': 'False', 'cache_selected_only': 'False', 'profiles_dir': '/home/ecem/.dbt', 'invocation_command': 'dbt run --models stg_users --full-refresh', 'use_experimental_parser': 'False', 'indirect_selection': 'eager', 'target_path': 'None', 'log_format': 'default', 'introspect': 'True', 'debug': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_cache_events': 'False', 'fail_fast': 'False'}
[0m14:09:35.137324 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality
Usage of `--models`, `--model`, and `-m` is deprecated in favor of `--select` or
`-s`.
[0m14:09:35.138203 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '41a897a7-b615-410e-adbb-f1fa2610d28c', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71963717b380>]}
[0m14:09:37.763326 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '41a897a7-b615-410e-adbb-f1fa2610d28c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x719616fabfe0>]}
[0m14:09:37.833547 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '41a897a7-b615-410e-adbb-f1fa2610d28c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7196178c0740>]}
[0m14:09:37.834963 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m14:09:38.095522 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m14:09:38.096367 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m14:09:38.097625 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '41a897a7-b615-410e-adbb-f1fa2610d28c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x719616e1c5c0>]}
[0m14:09:39.436465 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `relationships` defined on 'stg_orders' in
package 'data_pipeline_project' (models/staging/staging_schema.yml). Arguments
to generic tests should be nested under the `arguments` property.
[0m14:09:39.438815 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '41a897a7-b615-410e-adbb-f1fa2610d28c', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x719616a10050>]}
[0m14:09:39.830876 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.data_pipeline_project.marts
[0m14:09:39.840849 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '41a897a7-b615-410e-adbb-f1fa2610d28c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7196160cb020>]}
[0m14:09:39.980270 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m14:09:39.982427 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m14:09:39.998059 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '41a897a7-b615-410e-adbb-f1fa2610d28c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7196160ad790>]}
[0m14:09:39.998927 [info ] [MainThread]: Found 10 models, 35 data tests, 10 sources, 508 macros
[0m14:09:39.999607 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '41a897a7-b615-410e-adbb-f1fa2610d28c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x719616196d50>]}
[0m14:09:40.001812 [info ] [MainThread]: 
[0m14:09:40.002749 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:09:40.004961 [info ] [MainThread]: 
[0m14:09:40.006486 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m14:09:40.008777 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812'
[0m14:09:40.009756 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:09:41.043094 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812, now list_data-pipeline-project-474812_analytics_staging_staging)
[0m14:09:41.044519 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:09:42.052019 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '41a897a7-b615-410e-adbb-f1fa2610d28c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7196161022d0>]}
[0m14:09:42.056157 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:09:42.070482 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.stg_users
[0m14:09:42.071941 [info ] [Thread-1 (]: 1 of 1 START sql view model analytics_staging_staging.stg_users ................ [RUN]
[0m14:09:42.073401 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now model.data_pipeline_project.stg_users)
[0m14:09:42.074029 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.stg_users
[0m14:09:42.082467 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.stg_users"
[0m14:09:42.083472 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.stg_users
[0m14:09:42.123687 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.stg_users"
[0m14:09:42.124939 [debug] [Thread-1 (]: On model.data_pipeline_project.stg_users: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.stg_users"} */


  create or replace view `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
  OPTIONS()
  as with source as (

    select * from `data-pipeline-project-474812`.`raw_data`.`users`

),

-- Get the earliest subscription date for each user
subscription_dates as (
    select
        _user as user_id,
        min(cast(createdAt as timestamp)) as first_subscription_date
    from `data-pipeline-project-474812`.`raw_data`.`subscriptions`
    where _user is not null
    group by 1
),

renamed_and_cleaned as (

    select

        -- Primary Key: Standardized to user_id
        _id as user_id,

        -- SMART TIMESTAMP LOGIC: Use subscription date as fallback for missing created_at
        cast(
            case 
                when u.createdAt is not null then u.createdAt
                else s.first_subscription_date  -- Fallback to first subscription date
            end as timestamp
        ) as created_at,
        
        -- Deduplication logic: Assign a row number based on the creation time
        row_number() over (partition by _id order by 
            case 
                when u.createdAt is not null then u.createdAt
                else s.first_subscription_date 
            end desc
        ) as rn

    from source u
    left join subscription_dates s on u._id = s.user_id
    -- Simple cleaning: ensure that _id column is not null
    where _id is not null
)

select
    user_id,
    created_at

from renamed_and_cleaned
-- Final selection: only include the latest, unique instance of a user_id
where rn = 1;


[0m14:09:42.126334 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:09:43.517420 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:7c51142b-69d7-4065-9429-0117309dfc34&page=queryresults
[0m14:09:44.203831 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '41a897a7-b615-410e-adbb-f1fa2610d28c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71963737fbc0>]}
[0m14:09:44.205033 [info ] [Thread-1 (]: 1 of 1 OK created sql view model analytics_staging_staging.stg_users ........... [[32mCREATE VIEW (0 processed)[0m in 2.13s]
[0m14:09:44.206778 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.stg_users
[0m14:09:44.210745 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:09:44.212397 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:09:44.213687 [debug] [MainThread]: Connection 'model.data_pipeline_project.stg_users' was properly closed.
[0m14:09:44.214847 [info ] [MainThread]: 
[0m14:09:44.215599 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 4.21 seconds (4.21s).
[0m14:09:44.217397 [debug] [MainThread]: Command end result
[0m14:09:44.258271 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m14:09:44.260981 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m14:09:44.266918 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m14:09:44.267585 [info ] [MainThread]: 
[0m14:09:44.268354 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:09:44.270243 [info ] [MainThread]: 
[0m14:09:44.271023 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m14:09:44.271850 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- ModelParamUsageDeprecation: 1 occurrence
- MissingArgumentsPropertyInGenericTestDeprecation: 17 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m14:09:44.273076 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 9.207229, "process_in_blocks": "0", "process_kernel_time": 0.883869, "process_mem_max_rss": "387004", "process_out_blocks": "3936", "process_user_time": 6.77834}
[0m14:09:44.273840 [debug] [MainThread]: Command `dbt run` succeeded at 14:09:44.273548 after 9.21 seconds
[0m14:09:44.274332 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7196364f9c10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x719616dddf70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7196143e6ff0>]}
[0m14:09:44.274831 [debug] [MainThread]: Flushing usage events
[0m14:09:45.044083 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m17:28:08.674581 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x784e473cf5c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x784e46fa6c30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x784e46fa4b30>]}


============================== 17:28:08.683558 | 0ae48787-cc02-47dc-9f20-7e8fef913e8f ==============================
[0m17:28:08.683558 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m17:28:08.684379 [debug] [MainThread]: running dbt with arguments {'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'static_parser': 'True', 'partial_parse': 'True', 'fail_fast': 'False', 'empty': 'False', 'debug': 'False', 'send_anonymous_usage_stats': 'True', 'cache_selected_only': 'False', 'invocation_command': 'dbt run --select dim_customer.sql', 'use_experimental_parser': 'False', 'write_json': 'True', 'indirect_selection': 'eager', 'profiles_dir': '/home/ecem/.dbt', 'use_colors': 'True', 'introspect': 'True', 'quiet': 'False', 'version_check': 'True', 'log_cache_events': 'False', 'target_path': 'None', 'warn_error': 'None', 'printer_width': '80', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'log_format': 'default', 'no_print': 'None'}
[0m17:28:12.977607 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0ae48787-cc02-47dc-9f20-7e8fef913e8f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x784e4977f890>]}
[0m17:28:13.057268 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '0ae48787-cc02-47dc-9f20-7e8fef913e8f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x784e492a0f50>]}
[0m17:28:13.060221 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m17:28:13.280186 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m17:28:13.622179 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 3 files added, 0 files changed.
[0m17:28:13.623730 [debug] [MainThread]: Partial parsing: added file: data_pipeline_project://models/dims/dim_customer.sql
[0m17:28:13.626339 [debug] [MainThread]: Partial parsing: added file: data_pipeline_project://models/intermediate/int_subscriptions_parsed.sql
[0m17:28:13.629271 [debug] [MainThread]: Partial parsing: added file: data_pipeline_project://models/dims/dim_subscription.sql
[0m17:28:14.002637 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.data_pipeline_project.marts
[0m17:28:14.020643 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0ae48787-cc02-47dc-9f20-7e8fef913e8f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x784e27327c80>]}
[0m17:28:14.145061 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m17:28:14.147983 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m17:28:14.172393 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0ae48787-cc02-47dc-9f20-7e8fef913e8f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x784e273f7d70>]}
[0m17:28:14.173232 [info ] [MainThread]: Found 13 models, 35 data tests, 10 sources, 508 macros
[0m17:28:14.174239 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0ae48787-cc02-47dc-9f20-7e8fef913e8f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x784e279dd580>]}
[0m17:28:14.177034 [info ] [MainThread]: 
[0m17:28:14.178027 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m17:28:14.179035 [info ] [MainThread]: 
[0m17:28:14.179748 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m17:28:14.181537 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812'
[0m17:28:14.182278 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:28:15.323298 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812, now list_data-pipeline-project-474812_analytics_staging_staging)
[0m17:28:15.324347 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:28:16.289816 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0ae48787-cc02-47dc-9f20-7e8fef913e8f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x784e26a2aa50>]}
[0m17:28:16.291149 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:28:16.300867 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.dim_customer
[0m17:28:16.302038 [info ] [Thread-1 (]: 1 of 1 START sql view model analytics_staging_staging.dim_customer ............. [RUN]
[0m17:28:16.302793 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now model.data_pipeline_project.dim_customer)
[0m17:28:16.303394 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.dim_customer
[0m17:28:16.315480 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.dim_customer"
[0m17:28:16.316838 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.dim_customer
[0m17:28:16.376591 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.dim_customer"
[0m17:28:16.423731 [debug] [Thread-1 (]: On model.data_pipeline_project.dim_customer: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.dim_customer"} */


  create or replace view `data-pipeline-project-474812`.`analytics_staging_staging`.`dim_customer`
  OPTIONS()
  as with users as (
    -- Start with the clean user base (PK: user_id)
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
),

addresses as (
    -- Link users to their primary address/location (FK: user_id)
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_addresses`
),

neighborhoods as (
    -- Descriptive geography (FK: neighborhood_id, city_id, country_id)
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_neighborhoods`
),

cities as (
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_cities`
),

states as (
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_states`
),

countries as (
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_countries`
),

-- Join everything into a single, comprehensive dimension
final as (
    select
        u.user_id as customer_id, -- Rename PK to customer_id for the dim table
        u.created_at as signup_date,
     
        -- Address and Invoice Details
        a.invoice_type,
        
        -- Neighborhood Details
        n.neighborhood_name,
        n.postal_code,
        
        -- City, State, Country Details (The full hierarchy)
        c.city_name,
        s.state_name,
        co.country_name

    from users u
    -- Join to get the address link (assuming a user has one primary address record for simplicity)
    left join addresses a
        on u.user_id = a.user_id
    -- Join up the geography hierarchy
    left join neighborhoods n
        on a.neighborhood_id = n.neighborhood_id
    left join cities c
        on n.city_id = c.city_id
    left join states s
        on c.state_id = s.state_id
    left join countries co
        on c.country_id = co.country_id
)

select * from final;


[0m17:28:16.425190 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m17:28:18.090385 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:118d9887-6eaa-4a28-9cd1-f2ef66d11f61&page=queryresults
[0m17:28:18.755044 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0ae48787-cc02-47dc-9f20-7e8fef913e8f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x784e2a64c710>]}
[0m17:28:18.760000 [info ] [Thread-1 (]: 1 of 1 OK created sql view model analytics_staging_staging.dim_customer ........ [[32mCREATE VIEW (0 processed)[0m in 2.45s]
[0m17:28:18.762166 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.dim_customer
[0m17:28:18.765430 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m17:28:18.768613 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:28:18.770272 [debug] [MainThread]: Connection 'model.data_pipeline_project.dim_customer' was properly closed.
[0m17:28:18.770805 [info ] [MainThread]: 
[0m17:28:18.771551 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 4.59 seconds (4.59s).
[0m17:28:18.772732 [debug] [MainThread]: Command end result
[0m17:28:18.830628 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m17:28:18.834521 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m17:28:18.844624 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m17:28:18.854719 [info ] [MainThread]: 
[0m17:28:18.855396 [info ] [MainThread]: [32mCompleted successfully[0m
[0m17:28:18.855828 [info ] [MainThread]: 
[0m17:28:18.856386 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m17:28:18.857579 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 10.288025, "process_in_blocks": "0", "process_kernel_time": 2.328231, "process_mem_max_rss": "382416", "process_out_blocks": "4024", "process_user_time": 6.180435}
[0m17:28:18.860233 [debug] [MainThread]: Command `dbt run` succeeded at 17:28:18.860079 after 10.29 seconds
[0m17:28:18.867405 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x784e46f2ffe0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x784e275b8d40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x784e2774df40>]}
[0m17:28:18.868654 [debug] [MainThread]: Flushing usage events
[0m17:28:19.886681 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m17:33:44.764473 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ad898fd91f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ad89b0df1d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ad898977200>]}


============================== 17:33:44.768207 | c62de55c-d572-471c-b16f-29dfffb3433c ==============================
[0m17:33:44.768207 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m17:33:44.768973 [debug] [MainThread]: running dbt with arguments {'write_json': 'True', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'profiles_dir': '/home/ecem/.dbt', 'indirect_selection': 'eager', 'target_path': 'None', 'version_check': 'True', 'cache_selected_only': 'False', 'log_format': 'default', 'use_experimental_parser': 'False', 'partial_parse': 'True', 'warn_error': 'None', 'empty': 'None', 'log_cache_events': 'False', 'use_colors': 'True', 'static_parser': 'True', 'no_print': 'None', 'quiet': 'False', 'send_anonymous_usage_stats': 'True', 'debug': 'False', 'fail_fast': 'False', 'invocation_command': 'dbt clean', 'printer_width': '80', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])'}
[0m17:33:44.917574 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c62de55c-d572-471c-b16f-29dfffb3433c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ad89ae97e90>]}
[0m17:33:44.937181 [debug] [MainThread]: Resource report: {"command_name": "clean", "command_success": true, "command_wall_clock_time": 0.23395672, "process_in_blocks": "0", "process_kernel_time": 0.250347, "process_mem_max_rss": "101944", "process_out_blocks": "8", "process_user_time": 1.562438}
[0m17:33:44.938206 [debug] [MainThread]: Command `dbt clean` succeeded at 17:33:44.938070 after 0.24 seconds
[0m17:33:44.938650 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ad899844980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ad8989e1c10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ad898690ad0>]}
[0m17:33:44.939064 [debug] [MainThread]: Flushing usage events
[0m17:33:45.942637 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m17:34:07.876409 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5b9edf7470>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5b9f568980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5ba155a720>]}


============================== 17:34:07.879457 | cb9373d5-6928-4944-aab8-f5c1699ac553 ==============================
[0m17:34:07.879457 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m17:34:07.880883 [debug] [MainThread]: running dbt with arguments {'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'no_print': 'None', 'invocation_command': 'dbt run --select dim_customer --full-refresh', 'log_cache_events': 'False', 'use_colors': 'True', 'log_format': 'default', 'quiet': 'False', 'debug': 'False', 'empty': 'False', 'warn_error': 'None', 'introspect': 'True', 'partial_parse': 'True', 'printer_width': '80', 'profiles_dir': '/home/ecem/.dbt', 'static_parser': 'True', 'indirect_selection': 'eager', 'send_anonymous_usage_stats': 'True', 'fail_fast': 'False', 'use_experimental_parser': 'False', 'target_path': 'None', 'version_check': 'True', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'write_json': 'True', 'cache_selected_only': 'False'}
[0m17:34:10.451500 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'cb9373d5-6928-4944-aab8-f5c1699ac553', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5ba1273c20>]}
[0m17:34:10.518792 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'cb9373d5-6928-4944-aab8-f5c1699ac553', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5b7f348ce0>]}
[0m17:34:10.519944 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m17:34:10.773319 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m17:34:10.774343 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m17:34:10.775028 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'cb9373d5-6928-4944-aab8-f5c1699ac553', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5b7f643b30>]}
[0m17:34:12.098933 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `relationships` defined on 'stg_orders' in
package 'data_pipeline_project' (models/staging/staging_schema.yml). Arguments
to generic tests should be nested under the `arguments` property.
[0m17:34:12.100722 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': 'cb9373d5-6928-4944-aab8-f5c1699ac553', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5b7eec16a0>]}
[0m17:34:12.426584 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.data_pipeline_project.marts
[0m17:34:12.447078 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'cb9373d5-6928-4944-aab8-f5c1699ac553', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5b7e49daf0>]}
[0m17:34:12.617633 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m17:34:12.621088 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m17:34:12.643206 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'cb9373d5-6928-4944-aab8-f5c1699ac553', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5b7e38fbc0>]}
[0m17:34:12.644097 [info ] [MainThread]: Found 13 models, 35 data tests, 10 sources, 508 macros
[0m17:34:12.644670 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'cb9373d5-6928-4944-aab8-f5c1699ac553', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5b7ee5d7f0>]}
[0m17:34:12.647226 [info ] [MainThread]: 
[0m17:34:12.648601 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m17:34:12.649322 [info ] [MainThread]: 
[0m17:34:12.650615 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m17:34:12.653118 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812'
[0m17:34:12.653717 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:34:13.840944 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812, now list_data-pipeline-project-474812_analytics_staging_staging)
[0m17:34:13.842631 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:34:14.866351 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'cb9373d5-6928-4944-aab8-f5c1699ac553', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5b7f39ac90>]}
[0m17:34:14.867497 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:34:14.875763 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.dim_customer
[0m17:34:14.877010 [info ] [Thread-1 (]: 1 of 1 START sql view model analytics_staging_staging.dim_customer ............. [RUN]
[0m17:34:14.877705 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now model.data_pipeline_project.dim_customer)
[0m17:34:14.878486 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.dim_customer
[0m17:34:14.886176 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.dim_customer"
[0m17:34:14.889364 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.dim_customer
[0m17:34:14.927132 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.dim_customer"
[0m17:34:14.928484 [debug] [Thread-1 (]: On model.data_pipeline_project.dim_customer: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.dim_customer"} */


  create or replace view `data-pipeline-project-474812`.`analytics_staging_staging`.`dim_customer`
  OPTIONS()
  as with users as (
    -- Start with the clean user base (PK: user_id)
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
),

addresses as (
    -- Link users to their primary address/location (FK: user_id)
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_addresses`
),

neighborhoods as (
    -- Descriptive geography (FK: neighborhood_id, city_id, country_id)
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_neighborhoods`
),

cities as (
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_cities`
),

states as (
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_states`
),

countries as (
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_countries`
),

-- Join everything into a single, comprehensive dimension
final as (
    select
        u.user_id as customer_id, -- Rename PK to customer_id for the dim table
        u.created_at as signup_date,
     
        -- Address and Invoice Details
        a.invoice_type,
        
        -- Neighborhood Details
        n.neighborhood,
        
        -- City, State, Country Details (The full hierarchy)
        c.city,
        s.state,
        co.country,

        n.postal_code,

    from users u
    -- Join to get the address link (assuming a user has one primary address record for simplicity)
    left join addresses a
        on u.user_id = a.user_id
    -- Join up the geography hierarchy
    left join neighborhoods n
        on a.neighborhood_id = n.neighborhood_id
    left join cities c
        on n.city_id = c.city_id
    left join states s
        on c.state_id = s.state_id
    left join countries co
        on c.country_id = co.country_id
)

select * from final;


[0m17:34:14.929339 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m17:34:16.451293 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:c9f0c304-138b-4634-81d8-19f7581b9781&page=queryresults
[0m17:34:16.818027 [error] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:c9f0c304-138b-4634-81d8-19f7581b9781&page=queryresults
[0m17:34:16.840170 [debug] [Thread-1 (]: Database Error in model dim_customer (models/dims/dim_customer.sql)
  Name neighborhood not found inside n at [43:11]
  compiled code at target/run/data_pipeline_project/models/dims/dim_customer.sql
[0m17:34:16.844862 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cb9373d5-6928-4944-aab8-f5c1699ac553', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5ba1091910>]}
[0m17:34:16.846149 [error] [Thread-1 (]: 1 of 1 ERROR creating sql view model analytics_staging_staging.dim_customer .... [[31mERROR[0m in 1.97s]
[0m17:34:16.847053 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.dim_customer
[0m17:34:16.848097 [debug] [Thread-7 (]: Marking all children of 'model.data_pipeline_project.dim_customer' to be skipped because of status 'error'.  Reason: Database Error in model dim_customer (models/dims/dim_customer.sql)
  Name neighborhood not found inside n at [43:11]
  compiled code at target/run/data_pipeline_project/models/dims/dim_customer.sql.
[0m17:34:16.853544 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m17:34:16.855339 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:34:16.855743 [debug] [MainThread]: Connection 'model.data_pipeline_project.dim_customer' was properly closed.
[0m17:34:16.856264 [info ] [MainThread]: 
[0m17:34:16.857315 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 4.21 seconds (4.21s).
[0m17:34:16.858215 [debug] [MainThread]: Command end result
[0m17:34:16.905858 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m17:34:16.913676 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m17:34:16.926703 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m17:34:16.927203 [info ] [MainThread]: 
[0m17:34:16.927664 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m17:34:16.928088 [info ] [MainThread]: 
[0m17:34:16.928576 [error] [MainThread]: [31mFailure in model dim_customer (models/dims/dim_customer.sql)[0m
[0m17:34:16.929919 [error] [MainThread]:   Database Error in model dim_customer (models/dims/dim_customer.sql)
  Name neighborhood not found inside n at [43:11]
  compiled code at target/run/data_pipeline_project/models/dims/dim_customer.sql
[0m17:34:16.930546 [info ] [MainThread]: 
[0m17:34:16.931112 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_project/models/dims/dim_customer.sql
[0m17:34:16.931698 [info ] [MainThread]: 
[0m17:34:16.932216 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m17:34:16.935337 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 17 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m17:34:16.939995 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 9.1363735, "process_in_blocks": "0", "process_kernel_time": 1.545999, "process_mem_max_rss": "383300", "process_out_blocks": "3992", "process_user_time": 5.73319}
[0m17:34:16.941333 [debug] [MainThread]: Command `dbt run` failed at 17:34:16.941176 after 9.14 seconds
[0m17:34:16.941861 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5b9f1922d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5b7e321bb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5b7e395a00>]}
[0m17:34:16.942482 [debug] [MainThread]: Flushing usage events
[0m17:34:17.809619 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m17:36:45.252121 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78cdb506d640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78cdb72df050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78cdb543c260>]}


============================== 17:36:45.256926 | 9e02d553-550e-4464-8213-8f461d636764 ==============================
[0m17:36:45.256926 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m17:36:45.257893 [debug] [MainThread]: running dbt with arguments {'indirect_selection': 'eager', 'target_path': 'None', 'partial_parse': 'True', 'profiles_dir': '/home/ecem/.dbt', 'send_anonymous_usage_stats': 'True', 'log_format': 'default', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'log_cache_events': 'False', 'use_colors': 'True', 'cache_selected_only': 'False', 'introspect': 'True', 'fail_fast': 'False', 'empty': 'None', 'version_check': 'True', 'quiet': 'False', 'warn_error': 'None', 'write_json': 'True', 'use_experimental_parser': 'False', 'printer_width': '80', 'static_parser': 'True', 'invocation_command': 'dbt clean', 'no_print': 'None', 'debug': 'False'}
[0m17:36:45.405354 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9e02d553-550e-4464-8213-8f461d636764', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78cdb4753e30>]}
[0m17:36:45.426927 [debug] [MainThread]: Resource report: {"command_name": "clean", "command_success": true, "command_wall_clock_time": 0.23250853, "process_in_blocks": "0", "process_kernel_time": 0.19612, "process_mem_max_rss": "102004", "process_out_blocks": "8", "process_user_time": 1.561701}
[0m17:36:45.427580 [debug] [MainThread]: Command `dbt clean` succeeded at 17:36:45.427471 after 0.23 seconds
[0m17:36:45.427991 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78cdb50b9640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78cdb4793cb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78cdb4ef54c0>]}
[0m17:36:45.428398 [debug] [MainThread]: Flushing usage events
[0m17:36:46.511032 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m17:36:50.853210 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78fc301cd370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78fc30661340>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78fc301cd3d0>]}


============================== 17:36:50.856136 | 8a8cf421-ebbe-4ba8-bafe-752654ba8236 ==============================
[0m17:36:50.856136 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m17:36:50.862880 [debug] [MainThread]: running dbt with arguments {'profiles_dir': '/home/ecem/.dbt', 'partial_parse': 'True', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'use_colors': 'True', 'empty': 'False', 'target_path': 'None', 'invocation_command': 'dbt run --select dim_customer --full-refresh', 'log_cache_events': 'False', 'static_parser': 'True', 'printer_width': '80', 'quiet': 'False', 'log_format': 'default', 'warn_error': 'None', 'send_anonymous_usage_stats': 'True', 'debug': 'False', 'no_print': 'None', 'use_experimental_parser': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'indirect_selection': 'eager', 'write_json': 'True', 'cache_selected_only': 'False', 'version_check': 'True', 'fail_fast': 'False', 'introspect': 'True'}
[0m17:36:53.439945 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8a8cf421-ebbe-4ba8-bafe-752654ba8236', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78fc103505c0>]}
[0m17:36:53.504959 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8a8cf421-ebbe-4ba8-bafe-752654ba8236', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78fc10a8baa0>]}
[0m17:36:53.505684 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m17:36:53.771636 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m17:36:53.772383 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m17:36:53.772943 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '8a8cf421-ebbe-4ba8-bafe-752654ba8236', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78fc103c3fb0>]}
[0m17:36:55.084919 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `relationships` defined on 'stg_orders' in
package 'data_pipeline_project' (models/staging/staging_schema.yml). Arguments
to generic tests should be nested under the `arguments` property.
[0m17:36:55.086537 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '8a8cf421-ebbe-4ba8-bafe-752654ba8236', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78fc0f5516a0>]}
[0m17:36:55.463072 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.data_pipeline_project.marts
[0m17:36:55.488944 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8a8cf421-ebbe-4ba8-bafe-752654ba8236', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78fc0f4e2870>]}
[0m17:36:55.648747 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m17:36:55.652111 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m17:36:55.674305 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8a8cf421-ebbe-4ba8-bafe-752654ba8236', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78fc0f4e01d0>]}
[0m17:36:55.675187 [info ] [MainThread]: Found 13 models, 35 data tests, 10 sources, 508 macros
[0m17:36:55.676064 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8a8cf421-ebbe-4ba8-bafe-752654ba8236', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78fc0f3832c0>]}
[0m17:36:55.678352 [info ] [MainThread]: 
[0m17:36:55.679098 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m17:36:55.679953 [info ] [MainThread]: 
[0m17:36:55.680945 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m17:36:55.682708 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812'
[0m17:36:55.683639 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:36:56.850789 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812, now list_data-pipeline-project-474812_analytics_staging_staging)
[0m17:36:56.852252 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:36:57.833938 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8a8cf421-ebbe-4ba8-bafe-752654ba8236', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78fc0fe33860>]}
[0m17:36:57.835028 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:36:57.845577 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.dim_customer
[0m17:36:57.847125 [info ] [Thread-1 (]: 1 of 1 START sql view model analytics_staging_staging.dim_customer ............. [RUN]
[0m17:36:57.848429 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now model.data_pipeline_project.dim_customer)
[0m17:36:57.849163 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.dim_customer
[0m17:36:57.860177 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.dim_customer"
[0m17:36:57.861171 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.dim_customer
[0m17:36:57.897065 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.dim_customer"
[0m17:36:57.899439 [debug] [Thread-1 (]: On model.data_pipeline_project.dim_customer: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.dim_customer"} */


  create or replace view `data-pipeline-project-474812`.`analytics_staging_staging`.`dim_customer`
  OPTIONS()
  as with users as (
    -- Start with the clean user base (PK: user_id)
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
),

addresses as (
    -- Link users to their primary address/location (FK: user_id)
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_addresses`
),

neighborhoods as (
    -- Descriptive geography (FK: neighborhood_id, city_id, country_id)
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_neighborhoods`
),

cities as (
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_cities`
),

states as (
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_states`
),

countries as (
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_countries`
),

-- Join everything into a single, comprehensive dimension
final as (
    select
        u.user_id as customer_id, -- Rename PK to customer_id for the dim table
        u.created_at as signup_date,
     
        -- Address and Invoice Details
        a.invoice_type,
        
        -- Neighborhood Details
        n.neighborhood_name as neighborhood
        
        -- City, State, Country Details (The full hierarchy)
        c.city_name as city,
        s.state_name as state,
        co.country_name as country,

        n.postal_code,

    from users u
    -- Join to get the address link (assuming a user has one primary address record for simplicity)
    left join addresses a
        on u.user_id = a.user_id
    -- Join up the geography hierarchy
    left join neighborhoods n
        on a.neighborhood_id = n.neighborhood_id
    left join cities c
        on n.city_id = c.city_id
    left join states s
        on c.state_id = s.state_id
    left join countries co
        on c.country_id = co.country_id
)

select * from final;


[0m17:36:57.900056 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m17:36:59.202400 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:ae524385-9729-4749-b5dc-e9bb03c1810a&page=queryresults
[0m17:36:59.204246 [error] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:ae524385-9729-4749-b5dc-e9bb03c1810a&page=queryresults
[0m17:36:59.216450 [debug] [Thread-1 (]: Database Error in model dim_customer (models/dims/dim_customer.sql)
  Syntax error: Expected ")" but got identifier "c" at [46:9]
  compiled code at target/run/data_pipeline_project/models/dims/dim_customer.sql
[0m17:36:59.219579 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8a8cf421-ebbe-4ba8-bafe-752654ba8236', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78fc30893740>]}
[0m17:36:59.220632 [error] [Thread-1 (]: 1 of 1 ERROR creating sql view model analytics_staging_staging.dim_customer .... [[31mERROR[0m in 1.37s]
[0m17:36:59.221743 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.dim_customer
[0m17:36:59.222686 [debug] [Thread-7 (]: Marking all children of 'model.data_pipeline_project.dim_customer' to be skipped because of status 'error'.  Reason: Database Error in model dim_customer (models/dims/dim_customer.sql)
  Syntax error: Expected ")" but got identifier "c" at [46:9]
  compiled code at target/run/data_pipeline_project/models/dims/dim_customer.sql.
[0m17:36:59.229306 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m17:36:59.231131 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:36:59.231628 [debug] [MainThread]: Connection 'model.data_pipeline_project.dim_customer' was properly closed.
[0m17:36:59.232092 [info ] [MainThread]: 
[0m17:36:59.232637 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 3.55 seconds (3.55s).
[0m17:36:59.233432 [debug] [MainThread]: Command end result
[0m17:36:59.278589 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m17:36:59.282733 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m17:36:59.292362 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m17:36:59.293549 [info ] [MainThread]: 
[0m17:36:59.294727 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m17:36:59.295537 [info ] [MainThread]: 
[0m17:36:59.296522 [error] [MainThread]: [31mFailure in model dim_customer (models/dims/dim_customer.sql)[0m
[0m17:36:59.297312 [error] [MainThread]:   Database Error in model dim_customer (models/dims/dim_customer.sql)
  Syntax error: Expected ")" but got identifier "c" at [46:9]
  compiled code at target/run/data_pipeline_project/models/dims/dim_customer.sql
[0m17:36:59.298593 [info ] [MainThread]: 
[0m17:36:59.300145 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_project/models/dims/dim_customer.sql
[0m17:36:59.301093 [info ] [MainThread]: 
[0m17:36:59.301985 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m17:36:59.304703 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 17 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m17:36:59.313586 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 8.515713, "process_in_blocks": "0", "process_kernel_time": 1.127187, "process_mem_max_rss": "383724", "process_out_blocks": "3992", "process_user_time": 6.185345}
[0m17:36:59.318348 [debug] [MainThread]: Command `dbt run` failed at 17:36:59.317952 after 8.52 seconds
[0m17:36:59.319512 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78fc30447920>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78fc308ad9a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78fc0f5efcb0>]}
[0m17:36:59.320702 [debug] [MainThread]: Flushing usage events
[0m17:37:00.226868 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m17:37:23.703871 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74507ff45400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x745081c86ba0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74507ff45fd0>]}


============================== 17:37:23.707360 | 3be3fb05-4188-4606-9a5b-527f38d96afb ==============================
[0m17:37:23.707360 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m17:37:23.708294 [debug] [MainThread]: running dbt with arguments {'fail_fast': 'False', 'cache_selected_only': 'False', 'use_colors': 'True', 'introspect': 'True', 'debug': 'False', 'send_anonymous_usage_stats': 'True', 'empty': 'None', 'profiles_dir': '/home/ecem/.dbt', 'use_experimental_parser': 'False', 'log_cache_events': 'False', 'version_check': 'True', 'partial_parse': 'True', 'write_json': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'quiet': 'False', 'printer_width': '80', 'warn_error': 'None', 'no_print': 'None', 'log_format': 'default', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'indirect_selection': 'eager', 'invocation_command': 'dbt clean', 'target_path': 'None'}
[0m17:37:23.849730 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3be3fb05-4188-4606-9a5b-527f38d96afb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x745081a438f0>]}
[0m17:37:23.868622 [debug] [MainThread]: Resource report: {"command_name": "clean", "command_success": true, "command_wall_clock_time": 0.23120736, "process_in_blocks": "0", "process_kernel_time": 0.194877, "process_mem_max_rss": "101824", "process_out_blocks": "8", "process_user_time": 1.521453}
[0m17:37:23.870087 [debug] [MainThread]: Command `dbt clean` succeeded at 17:37:23.869538 after 0.23 seconds
[0m17:37:23.873083 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74507f23bf50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74507f6039e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74507f10b740>]}
[0m17:37:23.873699 [debug] [MainThread]: Flushing usage events
[0m17:37:24.553848 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m17:37:28.333670 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e983b1692e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e983b733530>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e983a925f40>]}


============================== 17:37:28.337261 | a5c36af8-a9cd-4ef2-a3d7-cacbdc316e82 ==============================
[0m17:37:28.337261 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m17:37:28.338153 [debug] [MainThread]: running dbt with arguments {'cache_selected_only': 'False', 'no_print': 'None', 'introspect': 'True', 'profiles_dir': '/home/ecem/.dbt', 'log_format': 'default', 'static_parser': 'True', 'quiet': 'False', 'version_check': 'True', 'indirect_selection': 'eager', 'warn_error': 'None', 'log_cache_events': 'False', 'partial_parse': 'True', 'use_experimental_parser': 'False', 'invocation_command': 'dbt run --select dim_customer --full-refresh', 'empty': 'False', 'write_json': 'True', 'send_anonymous_usage_stats': 'True', 'use_colors': 'True', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'printer_width': '80', 'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'debug': 'False'}
[0m17:37:30.812832 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a5c36af8-a9cd-4ef2-a3d7-cacbdc316e82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e981b1276b0>]}
[0m17:37:30.882449 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a5c36af8-a9cd-4ef2-a3d7-cacbdc316e82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e981b27ab40>]}
[0m17:37:30.883822 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m17:37:31.136292 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m17:37:31.142643 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m17:37:31.149393 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'a5c36af8-a9cd-4ef2-a3d7-cacbdc316e82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e981c566720>]}
[0m17:37:32.430224 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `relationships` defined on 'stg_orders' in
package 'data_pipeline_project' (models/staging/staging_schema.yml). Arguments
to generic tests should be nested under the `arguments` property.
[0m17:37:32.431536 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': 'a5c36af8-a9cd-4ef2-a3d7-cacbdc316e82', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e981a21ab10>]}
[0m17:37:32.809858 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.data_pipeline_project.marts
[0m17:37:32.829863 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a5c36af8-a9cd-4ef2-a3d7-cacbdc316e82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e981a1f45f0>]}
[0m17:37:32.946906 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m17:37:32.949637 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m17:37:32.974800 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a5c36af8-a9cd-4ef2-a3d7-cacbdc316e82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e981a0af980>]}
[0m17:37:32.976337 [info ] [MainThread]: Found 13 models, 35 data tests, 10 sources, 508 macros
[0m17:37:32.978032 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a5c36af8-a9cd-4ef2-a3d7-cacbdc316e82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e981a084ec0>]}
[0m17:37:32.980952 [info ] [MainThread]: 
[0m17:37:32.982533 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m17:37:32.983572 [info ] [MainThread]: 
[0m17:37:32.984919 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m17:37:32.989447 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812'
[0m17:37:32.994015 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:37:34.304111 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812, now list_data-pipeline-project-474812_analytics_staging_staging)
[0m17:37:34.304998 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:37:35.228194 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a5c36af8-a9cd-4ef2-a3d7-cacbdc316e82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e981aaefbf0>]}
[0m17:37:35.229996 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:37:35.294278 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.dim_customer
[0m17:37:35.295218 [info ] [Thread-1 (]: 1 of 1 START sql view model analytics_staging_staging.dim_customer ............. [RUN]
[0m17:37:35.296014 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now model.data_pipeline_project.dim_customer)
[0m17:37:35.296538 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.dim_customer
[0m17:37:35.304741 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.dim_customer"
[0m17:37:35.306026 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.dim_customer
[0m17:37:35.342604 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.dim_customer"
[0m17:37:35.344118 [debug] [Thread-1 (]: On model.data_pipeline_project.dim_customer: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.dim_customer"} */


  create or replace view `data-pipeline-project-474812`.`analytics_staging_staging`.`dim_customer`
  OPTIONS()
  as with users as (
    -- Start with the clean user base (PK: user_id)
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
),

addresses as (
    -- Link users to their primary address/location (FK: user_id)
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_addresses`
),

neighborhoods as (
    -- Descriptive geography (FK: neighborhood_id, city_id, country_id)
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_neighborhoods`
),

cities as (
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_cities`
),

states as (
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_states`
),

countries as (
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_countries`
),

-- Join everything into a single, comprehensive dimension
final as (
    select
        u.user_id as customer_id, -- Rename PK to customer_id for the dim table
        u.created_at as signup_date,
     
        -- Address and Invoice Details
        a.invoice_type,
        
        -- Neighborhood Details
        n.neighborhood_name as neighborhood
        
        -- City, State, Country Details (The full hierarchy)
        c.city_name as city,
        s.state_name as state,
        co.country_name as country,

        n.postal_code

    from users u
    -- Join to get the address link (assuming a user has one primary address record for simplicity)
    left join addresses a
        on u.user_id = a.user_id
    -- Join up the geography hierarchy
    left join neighborhoods n
        on a.neighborhood_id = n.neighborhood_id
    left join cities c
        on n.city_id = c.city_id
    left join states s
        on c.state_id = s.state_id
    left join countries co
        on c.country_id = co.country_id
)

select * from final;


[0m17:37:35.345161 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m17:37:36.466726 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:00c2b8b2-7ebc-41d7-8a89-d8392739ce0a&page=queryresults
[0m17:37:36.471214 [error] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:00c2b8b2-7ebc-41d7-8a89-d8392739ce0a&page=queryresults
[0m17:37:36.483000 [debug] [Thread-1 (]: Database Error in model dim_customer (models/dims/dim_customer.sql)
  Syntax error: Expected ")" but got identifier "c" at [46:9]
  compiled code at target/run/data_pipeline_project/models/dims/dim_customer.sql
[0m17:37:36.485524 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a5c36af8-a9cd-4ef2-a3d7-cacbdc316e82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e981a97cfe0>]}
[0m17:37:36.486898 [error] [Thread-1 (]: 1 of 1 ERROR creating sql view model analytics_staging_staging.dim_customer .... [[31mERROR[0m in 1.19s]
[0m17:37:36.488149 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.dim_customer
[0m17:37:36.489480 [debug] [Thread-7 (]: Marking all children of 'model.data_pipeline_project.dim_customer' to be skipped because of status 'error'.  Reason: Database Error in model dim_customer (models/dims/dim_customer.sql)
  Syntax error: Expected ")" but got identifier "c" at [46:9]
  compiled code at target/run/data_pipeline_project/models/dims/dim_customer.sql.
[0m17:37:36.493638 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m17:37:36.495727 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:37:36.496222 [debug] [MainThread]: Connection 'model.data_pipeline_project.dim_customer' was properly closed.
[0m17:37:36.497297 [info ] [MainThread]: 
[0m17:37:36.497924 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 3.51 seconds (3.51s).
[0m17:37:36.499357 [debug] [MainThread]: Command end result
[0m17:37:36.545545 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m17:37:36.549850 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m17:37:36.566700 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m17:37:36.568147 [info ] [MainThread]: 
[0m17:37:36.569222 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m17:37:36.571899 [info ] [MainThread]: 
[0m17:37:36.572547 [error] [MainThread]: [31mFailure in model dim_customer (models/dims/dim_customer.sql)[0m
[0m17:37:36.573322 [error] [MainThread]:   Database Error in model dim_customer (models/dims/dim_customer.sql)
  Syntax error: Expected ")" but got identifier "c" at [46:9]
  compiled code at target/run/data_pipeline_project/models/dims/dim_customer.sql
[0m17:37:36.573775 [info ] [MainThread]: 
[0m17:37:36.574589 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_project/models/dims/dim_customer.sql
[0m17:37:36.575028 [info ] [MainThread]: 
[0m17:37:36.575416 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m17:37:36.576150 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 17 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m17:37:36.577267 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 8.303487, "process_in_blocks": "0", "process_kernel_time": 1.335978, "process_mem_max_rss": "383392", "process_out_blocks": "3992", "process_user_time": 5.798214}
[0m17:37:36.577758 [debug] [MainThread]: Command `dbt run` failed at 17:37:36.577661 after 8.30 seconds
[0m17:37:36.578197 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e983ab70380>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e983b395a60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e981a2c6c90>]}
[0m17:37:36.578959 [debug] [MainThread]: Flushing usage events
[0m17:37:37.378140 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m17:38:08.758865 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b4508092300>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b4508831f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b45088320f0>]}


============================== 17:38:08.762452 | 1fd3067e-a864-4541-9586-44276200f869 ==============================
[0m17:38:08.762452 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m17:38:08.763155 [debug] [MainThread]: running dbt with arguments {'profiles_dir': '/home/ecem/.dbt', 'write_json': 'True', 'warn_error': 'None', 'printer_width': '80', 'target_path': 'None', 'log_format': 'default', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'indirect_selection': 'eager', 'partial_parse': 'True', 'invocation_command': 'dbt clean', 'use_experimental_parser': 'False', 'use_colors': 'True', 'cache_selected_only': 'False', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'fail_fast': 'False', 'version_check': 'True', 'introspect': 'True', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'send_anonymous_usage_stats': 'True', 'debug': 'False', 'log_cache_events': 'False', 'static_parser': 'True'}
[0m17:38:08.916377 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1fd3067e-a864-4541-9586-44276200f869', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b4507904a70>]}
[0m17:38:08.933911 [debug] [MainThread]: Resource report: {"command_name": "clean", "command_success": true, "command_wall_clock_time": 0.2343412, "process_in_blocks": "0", "process_kernel_time": 0.251187, "process_mem_max_rss": "101720", "process_out_blocks": "8", "process_user_time": 1.534036}
[0m17:38:08.934435 [debug] [MainThread]: Command `dbt clean` succeeded at 17:38:08.934337 after 0.24 seconds
[0m17:38:08.935162 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b4507cf78f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b450a3b2d50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b450a3b00b0>]}
[0m17:38:08.935586 [debug] [MainThread]: Flushing usage events
[0m17:38:10.015653 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m17:38:46.562864 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ccf099b0920>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ccf09be81d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ccf09b1da60>]}


============================== 17:38:46.566213 | df189d75-f55b-4216-9fe8-a98a3413a134 ==============================
[0m17:38:46.566213 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m17:38:46.567124 [debug] [MainThread]: running dbt with arguments {'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'version_check': 'True', 'empty': 'False', 'target_path': 'None', 'printer_width': '80', 'log_cache_events': 'False', 'partial_parse': 'True', 'use_colors': 'True', 'introspect': 'True', 'static_parser': 'True', 'debug': 'False', 'quiet': 'False', 'write_json': 'True', 'use_experimental_parser': 'False', 'send_anonymous_usage_stats': 'True', 'cache_selected_only': 'False', 'invocation_command': 'dbt run --select dim_customer --full-refresh', 'warn_error': 'None', 'profiles_dir': '/home/ecem/.dbt', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_format': 'default', 'fail_fast': 'False', 'no_print': 'None', 'indirect_selection': 'eager'}
[0m17:38:48.933388 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'df189d75-f55b-4216-9fe8-a98a3413a134', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ccf0b9abbf0>]}
[0m17:38:48.998305 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'df189d75-f55b-4216-9fe8-a98a3413a134', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ccee9c05d60>]}
[0m17:38:48.999592 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m17:38:49.233914 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m17:38:49.234986 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m17:38:49.235717 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'df189d75-f55b-4216-9fe8-a98a3413a134', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cceeb1b1e20>]}
[0m17:38:50.566799 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `relationships` defined on 'stg_orders' in
package 'data_pipeline_project' (models/staging/staging_schema.yml). Arguments
to generic tests should be nested under the `arguments` property.
[0m17:38:50.567932 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': 'df189d75-f55b-4216-9fe8-a98a3413a134', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ccee9809d00>]}
[0m17:38:50.948175 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.data_pipeline_project.marts
[0m17:38:50.971104 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'df189d75-f55b-4216-9fe8-a98a3413a134', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ccee8b2b680>]}
[0m17:38:51.114658 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m17:38:51.117363 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m17:38:51.137692 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'df189d75-f55b-4216-9fe8-a98a3413a134', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ccee8cf3710>]}
[0m17:38:51.138481 [info ] [MainThread]: Found 13 models, 35 data tests, 10 sources, 508 macros
[0m17:38:51.139341 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'df189d75-f55b-4216-9fe8-a98a3413a134', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ccee8c1c170>]}
[0m17:38:51.141609 [info ] [MainThread]: 
[0m17:38:51.142265 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m17:38:51.143118 [info ] [MainThread]: 
[0m17:38:51.143768 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m17:38:51.145069 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812'
[0m17:38:51.146274 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:38:52.206888 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812, now list_data-pipeline-project-474812_analytics_staging_staging)
[0m17:38:52.208594 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:38:53.253426 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'df189d75-f55b-4216-9fe8-a98a3413a134', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ccee8bafb00>]}
[0m17:38:53.255128 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:38:53.273821 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.dim_customer
[0m17:38:53.275686 [info ] [Thread-1 (]: 1 of 1 START sql view model analytics_staging_staging.dim_customer ............. [RUN]
[0m17:38:53.277304 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now model.data_pipeline_project.dim_customer)
[0m17:38:53.277927 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.dim_customer
[0m17:38:53.288696 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.dim_customer"
[0m17:38:53.290710 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.dim_customer
[0m17:38:53.325222 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.dim_customer"
[0m17:38:53.327889 [debug] [Thread-1 (]: On model.data_pipeline_project.dim_customer: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.dim_customer"} */


  create or replace view `data-pipeline-project-474812`.`analytics_staging_staging`.`dim_customer`
  OPTIONS()
  as with users as (
    -- Start with the clean user base (PK: user_id)
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
),

addresses as (
    -- Link users to their primary address/location (FK: user_id)
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_addresses`
),

neighborhoods as (
    -- Descriptive geography (FK: neighborhood_id, city_id, country_id)
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_neighborhoods`
),

cities as (
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_cities`
),

states as (
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_states`
),

countries as (
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_countries`
),

-- Join everything into a single, comprehensive dimension
final as (
    select
        u.user_id as customer_id, -- Rename PK to customer_id for the dim table
        u.created_at as signup_date,
     
        -- Address and Invoice Details
        a.invoice_type,
        
        -- Neighborhood Details
        n.neighborhood_name as neighborhood
        
        -- City, State, Country Details (The full hierarchy)
        c.city_name as city,
        s.state_name as state,
        co.country_name as country,

        n.postal_code
)
    from users u
    -- Join to get the address link (assuming a user has one primary address record for simplicity)
    left join addresses a
        on u.user_id = a.user_id
    -- Join up the geography hierarchy
    left join neighborhoods n
        on a.neighborhood_id = n.neighborhood_id
    left join cities c
        on n.city_id = c.city_id
    left join states s
        on c.state_id = s.state_id
    left join countries co
        on c.country_id = co.country_id
)

select * from final;


[0m17:38:53.330248 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m17:38:54.357932 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:819a9b81-3339-4a73-9f97-23c09230c63d&page=queryresults
[0m17:38:54.360062 [error] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:819a9b81-3339-4a73-9f97-23c09230c63d&page=queryresults
[0m17:38:54.371292 [debug] [Thread-1 (]: Database Error in model dim_customer (models/dims/dim_customer.sql)
  Syntax error: Expected ")" but got identifier "c" at [46:9]
  compiled code at target/run/data_pipeline_project/models/dims/dim_customer.sql
[0m17:38:54.376380 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'df189d75-f55b-4216-9fe8-a98a3413a134', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ccee8cdb2f0>]}
[0m17:38:54.379202 [error] [Thread-1 (]: 1 of 1 ERROR creating sql view model analytics_staging_staging.dim_customer .... [[31mERROR[0m in 1.10s]
[0m17:38:54.381715 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.dim_customer
[0m17:38:54.383257 [debug] [Thread-7 (]: Marking all children of 'model.data_pipeline_project.dim_customer' to be skipped because of status 'error'.  Reason: Database Error in model dim_customer (models/dims/dim_customer.sql)
  Syntax error: Expected ")" but got identifier "c" at [46:9]
  compiled code at target/run/data_pipeline_project/models/dims/dim_customer.sql.
[0m17:38:54.388833 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m17:38:54.394557 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:38:54.395432 [debug] [MainThread]: Connection 'model.data_pipeline_project.dim_customer' was properly closed.
[0m17:38:54.395877 [info ] [MainThread]: 
[0m17:38:54.396532 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 3.25 seconds (3.25s).
[0m17:38:54.397293 [debug] [MainThread]: Command end result
[0m17:38:54.447068 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m17:38:54.452271 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m17:38:54.465090 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m17:38:54.466301 [info ] [MainThread]: 
[0m17:38:54.467002 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m17:38:54.468691 [info ] [MainThread]: 
[0m17:38:54.475055 [error] [MainThread]: [31mFailure in model dim_customer (models/dims/dim_customer.sql)[0m
[0m17:38:54.477863 [error] [MainThread]:   Database Error in model dim_customer (models/dims/dim_customer.sql)
  Syntax error: Expected ")" but got identifier "c" at [46:9]
  compiled code at target/run/data_pipeline_project/models/dims/dim_customer.sql
[0m17:38:54.481143 [info ] [MainThread]: 
[0m17:38:54.483291 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_project/models/dims/dim_customer.sql
[0m17:38:54.484467 [info ] [MainThread]: 
[0m17:38:54.486549 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m17:38:54.490515 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 17 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m17:38:54.495598 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 7.990509, "process_in_blocks": "0", "process_kernel_time": 1.218858, "process_mem_max_rss": "383504", "process_out_blocks": "4000", "process_user_time": 5.904193}
[0m17:38:54.497432 [debug] [MainThread]: Command `dbt run` failed at 17:38:54.497017 after 7.99 seconds
[0m17:38:54.498495 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ccf09fbe5a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ccf09fbda30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ccee97d0290>]}
[0m17:38:54.500909 [debug] [MainThread]: Flushing usage events
[0m17:38:55.287557 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m17:40:17.819557 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x790c535843e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x790c54023680>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x790c54021d30>]}


============================== 17:40:17.822496 | 4ae0e31a-a6cf-4596-a889-2a5a0f1b3c8c ==============================
[0m17:40:17.822496 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m17:40:17.825479 [debug] [MainThread]: running dbt with arguments {'fail_fast': 'False', 'warn_error': 'None', 'indirect_selection': 'eager', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'use_colors': 'True', 'partial_parse': 'True', 'profiles_dir': '/home/ecem/.dbt', 'introspect': 'True', 'printer_width': '80', 'debug': 'False', 'send_anonymous_usage_stats': 'True', 'invocation_command': 'dbt clean', 'write_json': 'True', 'log_cache_events': 'False', 'use_experimental_parser': 'False', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'quiet': 'False', 'target_path': 'None', 'log_format': 'default', 'empty': 'None', 'cache_selected_only': 'False', 'version_check': 'True', 'no_print': 'None'}
[0m17:40:17.974015 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4ae0e31a-a6cf-4596-a889-2a5a0f1b3c8c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x790c53fa7800>]}
[0m17:40:17.990557 [debug] [MainThread]: Resource report: {"command_name": "clean", "command_success": true, "command_wall_clock_time": 0.23759092, "process_in_blocks": "0", "process_kernel_time": 0.175946, "process_mem_max_rss": "101732", "process_out_blocks": "16", "process_user_time": 1.576755}
[0m17:40:17.991074 [debug] [MainThread]: Command `dbt clean` succeeded at 17:40:17.990975 after 0.24 seconds
[0m17:40:17.992436 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x790c5314c6e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x790c53a15400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x790c53a16330>]}
[0m17:40:17.992909 [debug] [MainThread]: Flushing usage events
[0m17:40:19.000283 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m17:40:25.493803 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75fe30f1d370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75fe309edd30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75fe3132da00>]}


============================== 17:40:25.497846 | 527db680-4b84-4ada-9304-6fc78465acd2 ==============================
[0m17:40:25.497846 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m17:40:25.500454 [debug] [MainThread]: running dbt with arguments {'indirect_selection': 'eager', 'invocation_command': 'dbt run --select dim_customer --full-refresh', 'debug': 'False', 'log_format': 'default', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'empty': 'False', 'write_json': 'True', 'warn_error': 'None', 'printer_width': '80', 'partial_parse': 'True', 'introspect': 'True', 'send_anonymous_usage_stats': 'True', 'profiles_dir': '/home/ecem/.dbt', 'quiet': 'False', 'target_path': 'None', 'use_experimental_parser': 'False', 'static_parser': 'True', 'use_colors': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'version_check': 'True', 'log_cache_events': 'False', 'cache_selected_only': 'False', 'no_print': 'None'}
[0m17:40:27.853029 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '527db680-4b84-4ada-9304-6fc78465acd2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75fe32e0faa0>]}
[0m17:40:27.922310 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '527db680-4b84-4ada-9304-6fc78465acd2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75fe32ce6270>]}
[0m17:40:27.928920 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m17:40:28.187982 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m17:40:28.189964 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m17:40:28.190527 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '527db680-4b84-4ada-9304-6fc78465acd2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75fe10e01430>]}
[0m17:40:29.548232 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `relationships` defined on 'stg_orders' in
package 'data_pipeline_project' (models/staging/staging_schema.yml). Arguments
to generic tests should be nested under the `arguments` property.
[0m17:40:29.549379 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '527db680-4b84-4ada-9304-6fc78465acd2', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75fe10b7d0a0>]}
[0m17:40:29.932803 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.data_pipeline_project.marts
[0m17:40:29.982923 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '527db680-4b84-4ada-9304-6fc78465acd2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75fe101d2de0>]}
[0m17:40:30.153673 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m17:40:30.160358 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m17:40:30.241483 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '527db680-4b84-4ada-9304-6fc78465acd2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75fe03fca360>]}
[0m17:40:30.242786 [info ] [MainThread]: Found 13 models, 35 data tests, 10 sources, 508 macros
[0m17:40:30.243957 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '527db680-4b84-4ada-9304-6fc78465acd2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75fe03e70ce0>]}
[0m17:40:30.249852 [info ] [MainThread]: 
[0m17:40:30.251583 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m17:40:30.252983 [info ] [MainThread]: 
[0m17:40:30.255523 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m17:40:30.260095 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812'
[0m17:40:30.261074 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:40:31.423973 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812, now list_data-pipeline-project-474812_analytics_staging_staging)
[0m17:40:31.425662 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:40:32.471792 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '527db680-4b84-4ada-9304-6fc78465acd2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75fe03fda060>]}
[0m17:40:32.473329 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:40:32.486255 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.dim_customer
[0m17:40:32.487761 [info ] [Thread-1 (]: 1 of 1 START sql view model analytics_staging_staging.dim_customer ............. [RUN]
[0m17:40:32.489012 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now model.data_pipeline_project.dim_customer)
[0m17:40:32.489553 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.dim_customer
[0m17:40:32.500605 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.dim_customer"
[0m17:40:32.501805 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.dim_customer
[0m17:40:32.542064 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.dim_customer"
[0m17:40:32.543355 [debug] [Thread-1 (]: On model.data_pipeline_project.dim_customer: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.dim_customer"} */


  create or replace view `data-pipeline-project-474812`.`analytics_staging_staging`.`dim_customer`
  OPTIONS()
  as with users as (
    -- Start with the clean user base (PK: user_id)
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
),

addresses as (
    -- Link users to their primary address/location (FK: user_id)
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_addresses`
),

neighborhoods as (
    -- Descriptive geography (FK: neighborhood_id, city_id, country_id)
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_neighborhoods`
),

cities as (
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_cities`
),

states as (
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_states`
),

countries as (
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_countries`
),

-- Join everything into a single, comprehensive dimension
final as (
    select
        u.user_id as customer_id, -- Rename PK to customer_id for the dim table
        u.created_at as signup_date,
     
        -- Address and Invoice Details
        a.invoice_type,
        
        -- Neighborhood Details
        n.neighborhood_name as neighborhood, -- ✅ ADDED MISSING COMMA HERE
        
        -- City, State, Country Details (The full hierarchy)
        c.city_name as city,
        s.state_name as state,
        co.country_name as country,

        n.postal_code
        
    from users u
    -- Join to get the address link (assuming a user has one primary address record for simplicity)
    left join addresses a
        on u.user_id = a.user_id
    -- Join up the geography hierarchy
    left join neighborhoods n
        on a.neighborhood_id = n.neighborhood_id
    left join cities c
        on n.city_id = c.city_id
    left join states s
        on c.state_id = s.state_id
    left join countries co
        on c.country_id = co.country_id
)

select * from final;


[0m17:40:32.546486 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m17:40:34.028925 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:544bbbcd-d11f-416b-af45-e778ca79c254&page=queryresults
[0m17:40:34.746972 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '527db680-4b84-4ada-9304-6fc78465acd2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75fe13d44b60>]}
[0m17:40:34.748906 [info ] [Thread-1 (]: 1 of 1 OK created sql view model analytics_staging_staging.dim_customer ........ [[32mCREATE VIEW (0 processed)[0m in 2.26s]
[0m17:40:34.750262 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.dim_customer
[0m17:40:34.753735 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m17:40:34.758544 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:40:34.759211 [debug] [MainThread]: Connection 'model.data_pipeline_project.dim_customer' was properly closed.
[0m17:40:34.759810 [info ] [MainThread]: 
[0m17:40:34.760579 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 4.51 seconds (4.51s).
[0m17:40:34.762230 [debug] [MainThread]: Command end result
[0m17:40:34.806311 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m17:40:34.810374 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m17:40:34.817127 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m17:40:34.818453 [info ] [MainThread]: 
[0m17:40:34.819360 [info ] [MainThread]: [32mCompleted successfully[0m
[0m17:40:34.820134 [info ] [MainThread]: 
[0m17:40:34.820953 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m17:40:34.821840 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 17 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m17:40:34.824063 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 9.396317, "process_in_blocks": "0", "process_kernel_time": 1.114445, "process_mem_max_rss": "387280", "process_out_blocks": "3992", "process_user_time": 6.191956}
[0m17:40:34.825361 [debug] [MainThread]: Command `dbt run` succeeded at 17:40:34.825239 after 9.40 seconds
[0m17:40:34.825856 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75fe30a18740>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75fe03ff54c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75fe03fb26c0>]}
[0m17:40:34.827089 [debug] [MainThread]: Flushing usage events
[0m17:40:35.526472 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m19:04:44.068470 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x742bfb73eba0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x742bfb5b15b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x742bfb5b14c0>]}


============================== 19:04:44.075479 | 3bb0741b-0489-4848-a5d7-bc0030b8f89c ==============================
[0m19:04:44.075479 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m19:04:44.077675 [debug] [MainThread]: running dbt with arguments {'cache_selected_only': 'False', 'profiles_dir': '/home/ecem/.dbt', 'warn_error': 'None', 'partial_parse': 'True', 'write_json': 'True', 'debug': 'False', 'use_experimental_parser': 'False', 'log_cache_events': 'False', 'fail_fast': 'False', 'quiet': 'False', 'printer_width': '80', 'invocation_command': 'dbt run --select dim_subscription', 'use_colors': 'True', 'static_parser': 'True', 'version_check': 'True', 'no_print': 'None', 'log_format': 'default', 'indirect_selection': 'eager', 'send_anonymous_usage_stats': 'True', 'empty': 'False', 'introspect': 'True', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])'}
[0m19:04:46.484852 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3bb0741b-0489-4848-a5d7-bc0030b8f89c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x742bdbb630e0>]}
[0m19:04:46.550694 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '3bb0741b-0489-4848-a5d7-bc0030b8f89c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x742be4d0f2c0>]}
[0m19:04:46.551848 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m19:04:46.844916 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m19:04:47.319680 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m19:04:47.321771 [debug] [MainThread]: Partial parsing: updated file: data_pipeline_project://models/dims/dim_customer.sql
[0m19:04:47.322323 [debug] [MainThread]: Partial parsing: updated file: data_pipeline_project://models/dims/dim_subscription.sql
[0m19:04:47.603800 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.data_pipeline_project.marts
[0m19:04:47.620482 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3bb0741b-0489-4848-a5d7-bc0030b8f89c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x742bdb32aa80>]}
[0m19:04:47.756108 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m19:04:47.759489 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m19:04:47.786868 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3bb0741b-0489-4848-a5d7-bc0030b8f89c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x742bdb5465a0>]}
[0m19:04:47.787471 [info ] [MainThread]: Found 13 models, 35 data tests, 10 sources, 508 macros
[0m19:04:47.788166 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3bb0741b-0489-4848-a5d7-bc0030b8f89c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x742bfd575c10>]}
[0m19:04:47.790491 [info ] [MainThread]: 
[0m19:04:47.791269 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m19:04:47.791887 [info ] [MainThread]: 
[0m19:04:47.792704 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m19:04:47.794143 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812'
[0m19:04:47.794864 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:04:48.736174 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812, now list_data-pipeline-project-474812_analytics_staging_staging)
[0m19:04:48.737175 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:04:49.827890 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3bb0741b-0489-4848-a5d7-bc0030b8f89c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x742bdb32aed0>]}
[0m19:04:49.828733 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:04:49.862460 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.dim_subscription
[0m19:04:49.863874 [info ] [Thread-1 (]: 1 of 1 START sql view model analytics_staging_staging.dim_subscription ......... [RUN]
[0m19:04:49.864910 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now model.data_pipeline_project.dim_subscription)
[0m19:04:49.865426 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.dim_subscription
[0m19:04:49.873896 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.dim_subscription"
[0m19:04:49.875925 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.dim_subscription
[0m19:04:49.919361 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.dim_subscription"
[0m19:04:49.922566 [debug] [Thread-1 (]: On model.data_pipeline_project.dim_subscription: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.dim_subscription"} */


  create or replace view `data-pipeline-project-474812`.`analytics_staging_staging`.`dim_subscription`
  OPTIONS()
  as with subscriptions as (
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_subscriptions`
),

final_parsing as (
    select
        subscription_id,
        user_id as customer_id, 
        start_date,  
        case
            when is_active = FALSE then 'Cancelled'
            else 'Active'
        end as status,
        -- Business logic: end_date only exists upon explicit cancellation
        case
            when is_active = FALSE then NULL  -- Would be cancellation date
            else NULL                         -- Active = no end date
        end as end_date
)

select * from final_parsing;


[0m19:04:49.923458 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m19:04:51.147011 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:570c5278-a23c-4b4e-bdca-11d2936bb3a4&page=queryresults
[0m19:04:51.585793 [error] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:570c5278-a23c-4b4e-bdca-11d2936bb3a4&page=queryresults
[0m19:04:51.676818 [debug] [Thread-1 (]: Database Error in model dim_subscription (models/dims/dim_subscription.sql)
  Unrecognized name: subscription_id at [12:9]
  compiled code at target/run/data_pipeline_project/models/dims/dim_subscription.sql
[0m19:04:51.682217 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3bb0741b-0489-4848-a5d7-bc0030b8f89c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x742bdb3df9e0>]}
[0m19:04:51.683590 [error] [Thread-1 (]: 1 of 1 ERROR creating sql view model analytics_staging_staging.dim_subscription  [[31mERROR[0m in 1.81s]
[0m19:04:51.685172 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.dim_subscription
[0m19:04:51.686290 [debug] [Thread-7 (]: Marking all children of 'model.data_pipeline_project.dim_subscription' to be skipped because of status 'error'.  Reason: Database Error in model dim_subscription (models/dims/dim_subscription.sql)
  Unrecognized name: subscription_id at [12:9]
  compiled code at target/run/data_pipeline_project/models/dims/dim_subscription.sql.
[0m19:04:51.691280 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m19:04:51.695473 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:04:51.696883 [debug] [MainThread]: Connection 'model.data_pipeline_project.dim_subscription' was properly closed.
[0m19:04:51.697679 [info ] [MainThread]: 
[0m19:04:51.698168 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 3.91 seconds (3.91s).
[0m19:04:51.699293 [debug] [MainThread]: Command end result
[0m19:04:51.751848 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m19:04:51.754041 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m19:04:51.763409 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m19:04:51.764103 [info ] [MainThread]: 
[0m19:04:51.765272 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m19:04:51.766667 [info ] [MainThread]: 
[0m19:04:51.767948 [error] [MainThread]: [31mFailure in model dim_subscription (models/dims/dim_subscription.sql)[0m
[0m19:04:51.772682 [error] [MainThread]:   Database Error in model dim_subscription (models/dims/dim_subscription.sql)
  Unrecognized name: subscription_id at [12:9]
  compiled code at target/run/data_pipeline_project/models/dims/dim_subscription.sql
[0m19:04:51.775099 [info ] [MainThread]: 
[0m19:04:51.776968 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_project/models/dims/dim_subscription.sql
[0m19:04:51.777576 [info ] [MainThread]: 
[0m19:04:51.778161 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m19:04:51.780677 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 7.7656374, "process_in_blocks": "4552", "process_kernel_time": 1.165663, "process_mem_max_rss": "377348", "process_out_blocks": "4032", "process_user_time": 4.774077}
[0m19:04:51.781881 [debug] [MainThread]: Command `dbt run` failed at 19:04:51.781757 after 7.77 seconds
[0m19:04:51.782848 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x742bfb578fe0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x742bdb356b70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x742bdb3dfcb0>]}
[0m19:04:51.783331 [debug] [MainThread]: Flushing usage events
[0m19:04:52.948634 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m19:08:07.030779 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74a4e4e715b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74a4e68af080>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74a4e46cb080>]}


============================== 19:08:07.035522 | 35e9e0ce-6150-4b4d-9794-eb907037176d ==============================
[0m19:08:07.035522 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m19:08:07.036500 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'version_check': 'True', 'profiles_dir': '/home/ecem/.dbt', 'indirect_selection': 'eager', 'log_format': 'default', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'introspect': 'True', 'write_json': 'True', 'fail_fast': 'False', 'cache_selected_only': 'False', 'invocation_command': 'dbt clean', 'printer_width': '80', 'partial_parse': 'True', 'empty': 'None', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'use_colors': 'True', 'log_cache_events': 'False', 'send_anonymous_usage_stats': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'warn_error': 'None', 'target_path': 'None', 'debug': 'False'}
[0m19:08:07.192917 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '35e9e0ce-6150-4b4d-9794-eb907037176d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74a4e3e9cbc0>]}
[0m19:08:07.244130 [debug] [MainThread]: Resource report: {"command_name": "clean", "command_success": true, "command_wall_clock_time": 0.2755155, "process_in_blocks": "16", "process_kernel_time": 0.201007, "process_mem_max_rss": "101760", "process_out_blocks": "8", "process_user_time": 1.570839}
[0m19:08:07.245377 [debug] [MainThread]: Command `dbt clean` succeeded at 19:08:07.245228 after 0.28 seconds
[0m19:08:07.246306 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74a4e4985dc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74a4e46cb080>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74a4e4587200>]}
[0m19:08:07.247189 [debug] [MainThread]: Flushing usage events
[0m19:08:08.402698 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m19:08:28.697290 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b644cb74e60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b644a948920>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b644ac42240>]}


============================== 19:08:28.703007 | 572ca429-3c12-4b99-8ac6-9e2da5a18619 ==============================
[0m19:08:28.703007 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m19:08:28.704349 [debug] [MainThread]: running dbt with arguments {'partial_parse': 'True', 'log_cache_events': 'False', 'indirect_selection': 'eager', 'no_print': 'None', 'write_json': 'True', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'cache_selected_only': 'False', 'empty': 'False', 'send_anonymous_usage_stats': 'True', 'debug': 'False', 'printer_width': '80', 'quiet': 'False', 'use_experimental_parser': 'False', 'static_parser': 'True', 'invocation_command': 'dbt run --select dim_subscription --full-refresh', 'introspect': 'True', 'profiles_dir': '/home/ecem/.dbt', 'target_path': 'None', 'use_colors': 'True', 'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'version_check': 'True', 'warn_error': 'None', 'log_format': 'default'}
[0m19:08:30.991394 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '572ca429-3c12-4b99-8ac6-9e2da5a18619', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b642ae5c920>]}
[0m19:08:31.065555 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '572ca429-3c12-4b99-8ac6-9e2da5a18619', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b642aad4050>]}
[0m19:08:31.068797 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m19:08:31.292379 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m19:08:31.297643 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m19:08:31.300119 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '572ca429-3c12-4b99-8ac6-9e2da5a18619', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b642a80b620>]}
[0m19:08:32.634680 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `relationships` defined on 'stg_orders' in
package 'data_pipeline_project' (models/staging/staging_schema.yml). Arguments
to generic tests should be nested under the `arguments` property.
[0m19:08:32.639974 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '572ca429-3c12-4b99-8ac6-9e2da5a18619', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b644a73af30>]}
[0m19:08:32.955034 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.data_pipeline_project.marts
[0m19:08:32.973440 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '572ca429-3c12-4b99-8ac6-9e2da5a18619', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b6429ab7350>]}
[0m19:08:33.120018 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m19:08:33.122279 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m19:08:33.139885 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '572ca429-3c12-4b99-8ac6-9e2da5a18619', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b642a4a67b0>]}
[0m19:08:33.140797 [info ] [MainThread]: Found 13 models, 35 data tests, 10 sources, 508 macros
[0m19:08:33.141598 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '572ca429-3c12-4b99-8ac6-9e2da5a18619', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b6429976e70>]}
[0m19:08:33.143731 [info ] [MainThread]: 
[0m19:08:33.145034 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m19:08:33.146351 [info ] [MainThread]: 
[0m19:08:33.147283 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m19:08:33.149936 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812'
[0m19:08:33.150459 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:08:34.225849 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812, now list_data-pipeline-project-474812_analytics_staging_staging)
[0m19:08:34.227709 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:08:35.227446 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '572ca429-3c12-4b99-8ac6-9e2da5a18619', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b6429ab4620>]}
[0m19:08:35.229846 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:08:35.243694 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.dim_subscription
[0m19:08:35.245380 [info ] [Thread-1 (]: 1 of 1 START sql view model analytics_staging_staging.dim_subscription ......... [RUN]
[0m19:08:35.246922 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now model.data_pipeline_project.dim_subscription)
[0m19:08:35.247786 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.dim_subscription
[0m19:08:35.255833 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.dim_subscription"
[0m19:08:35.258244 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.dim_subscription
[0m19:08:35.300720 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.dim_subscription"
[0m19:08:35.301905 [debug] [Thread-1 (]: On model.data_pipeline_project.dim_subscription: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.dim_subscription"} */


  create or replace view `data-pipeline-project-474812`.`analytics_staging_staging`.`dim_subscription`
  OPTIONS()
  as with subscriptions as (
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_subscriptions`
),

final_parsing as (
    select
        subscription_id,
        user_id as customer_id, 
        start_date,  
        case
            when is_active = FALSE then 'Cancelled'
            else 'Active'
        end as status,
        -- Business logic: end_date only exists upon explicit cancellation
        case
            when is_active = FALSE then NULL  -- Would be cancellation date
            else NULL                         -- Active = no end date
        end as end_date
)

select * from final_parsing;


[0m19:08:35.302541 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m19:08:36.832804 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:deb0fb08-6702-4965-822f-a0a7358deb69&page=queryresults
[0m19:08:37.140738 [error] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:deb0fb08-6702-4965-822f-a0a7358deb69&page=queryresults
[0m19:08:37.152853 [debug] [Thread-1 (]: Database Error in model dim_subscription (models/dims/dim_subscription.sql)
  Unrecognized name: subscription_id at [12:9]
  compiled code at target/run/data_pipeline_project/models/dims/dim_subscription.sql
[0m19:08:37.156538 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '572ca429-3c12-4b99-8ac6-9e2da5a18619', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b6429a99af0>]}
[0m19:08:37.158649 [error] [Thread-1 (]: 1 of 1 ERROR creating sql view model analytics_staging_staging.dim_subscription  [[31mERROR[0m in 1.91s]
[0m19:08:37.159613 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.dim_subscription
[0m19:08:37.166910 [debug] [Thread-7 (]: Marking all children of 'model.data_pipeline_project.dim_subscription' to be skipped because of status 'error'.  Reason: Database Error in model dim_subscription (models/dims/dim_subscription.sql)
  Unrecognized name: subscription_id at [12:9]
  compiled code at target/run/data_pipeline_project/models/dims/dim_subscription.sql.
[0m19:08:37.171342 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m19:08:37.172881 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:08:37.173244 [debug] [MainThread]: Connection 'model.data_pipeline_project.dim_subscription' was properly closed.
[0m19:08:37.173617 [info ] [MainThread]: 
[0m19:08:37.174237 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 4.03 seconds (4.03s).
[0m19:08:37.174976 [debug] [MainThread]: Command end result
[0m19:08:37.223849 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m19:08:37.226452 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m19:08:37.233325 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m19:08:37.234125 [info ] [MainThread]: 
[0m19:08:37.235416 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m19:08:37.243366 [info ] [MainThread]: 
[0m19:08:37.244488 [error] [MainThread]: [31mFailure in model dim_subscription (models/dims/dim_subscription.sql)[0m
[0m19:08:37.248557 [error] [MainThread]:   Database Error in model dim_subscription (models/dims/dim_subscription.sql)
  Unrecognized name: subscription_id at [12:9]
  compiled code at target/run/data_pipeline_project/models/dims/dim_subscription.sql
[0m19:08:37.251194 [info ] [MainThread]: 
[0m19:08:37.252030 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_project/models/dims/dim_subscription.sql
[0m19:08:37.252740 [info ] [MainThread]: 
[0m19:08:37.253212 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m19:08:37.254026 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 17 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m19:08:37.257214 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 8.621705, "process_in_blocks": "288", "process_kernel_time": 1.133192, "process_mem_max_rss": "381772", "process_out_blocks": "3992", "process_user_time": 5.816413}
[0m19:08:37.261774 [debug] [MainThread]: Command `dbt run` failed at 19:08:37.261402 after 8.63 seconds
[0m19:08:37.263804 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b644ae945f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b644ae95b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b64298b3080>]}
[0m19:08:37.265198 [debug] [MainThread]: Flushing usage events
[0m19:08:37.976636 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m19:10:12.052251 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c00bbdeb740>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c00bb30d760>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c00bd5af260>]}


============================== 19:10:12.057642 | 66c72e3f-62b8-4a6d-b9ec-1cf54f38f589 ==============================
[0m19:10:12.057642 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m19:10:12.058739 [debug] [MainThread]: running dbt with arguments {'fail_fast': 'False', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'quiet': 'False', 'target_path': 'None', 'indirect_selection': 'eager', 'profiles_dir': '/home/ecem/.dbt', 'printer_width': '80', 'invocation_command': 'dbt clean', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'no_print': 'None', 'use_colors': 'True', 'warn_error': 'None', 'static_parser': 'True', 'log_format': 'default', 'cache_selected_only': 'False', 'write_json': 'True', 'log_cache_events': 'False', 'version_check': 'True', 'empty': 'None', 'use_experimental_parser': 'False', 'debug': 'False', 'introspect': 'True'}
[0m19:10:12.208495 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '66c72e3f-62b8-4a6d-b9ec-1cf54f38f589', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c00bafbf800>]}
[0m19:10:12.229252 [debug] [MainThread]: Resource report: {"command_name": "clean", "command_success": true, "command_wall_clock_time": 0.23500308, "process_in_blocks": "0", "process_kernel_time": 0.201456, "process_mem_max_rss": "101664", "process_out_blocks": "8", "process_user_time": 1.534171}
[0m19:10:12.230251 [debug] [MainThread]: Command `dbt clean` succeeded at 19:10:12.230087 after 0.24 seconds
[0m19:10:12.230894 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c00ba9bff50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c00bd004dd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c00bd2df860>]}
[0m19:10:12.231570 [debug] [MainThread]: Flushing usage events
[0m19:10:13.209813 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m19:10:24.592337 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7312c4e3bd40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7312c2995dc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7312c29963c0>]}


============================== 19:10:24.596112 | 8aacc293-134d-422a-b57f-b19d9479e754 ==============================
[0m19:10:24.596112 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m19:10:24.597033 [debug] [MainThread]: running dbt with arguments {'invocation_command': 'dbt run --select dim_subscription --full-refresh', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'warn_error': 'None', 'send_anonymous_usage_stats': 'True', 'profiles_dir': '/home/ecem/.dbt', 'fail_fast': 'False', 'cache_selected_only': 'False', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'use_experimental_parser': 'False', 'partial_parse': 'True', 'debug': 'False', 'printer_width': '80', 'use_colors': 'True', 'version_check': 'True', 'empty': 'False', 'quiet': 'False', 'log_format': 'default', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_cache_events': 'False', 'no_print': 'None', 'indirect_selection': 'eager', 'write_json': 'True'}
[0m19:10:26.939450 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8aacc293-134d-422a-b57f-b19d9479e754', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7312a2ee5af0>]}
[0m19:10:27.030919 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8aacc293-134d-422a-b57f-b19d9479e754', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7312a30cc500>]}
[0m19:10:27.033278 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m19:10:27.279629 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m19:10:27.281067 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m19:10:27.281987 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '8aacc293-134d-422a-b57f-b19d9479e754', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7312a29f5790>]}
[0m19:10:28.617370 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `relationships` defined on 'stg_orders' in
package 'data_pipeline_project' (models/staging/staging_schema.yml). Arguments
to generic tests should be nested under the `arguments` property.
[0m19:10:28.620711 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '8aacc293-134d-422a-b57f-b19d9479e754', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7312a26c50a0>]}
[0m19:10:28.951771 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.data_pipeline_project.marts
[0m19:10:28.964195 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8aacc293-134d-422a-b57f-b19d9479e754', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7312a1ceaa80>]}
[0m19:10:29.106886 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m19:10:29.109282 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m19:10:29.127694 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8aacc293-134d-422a-b57f-b19d9479e754', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7312a1b9b9b0>]}
[0m19:10:29.128290 [info ] [MainThread]: Found 13 models, 35 data tests, 10 sources, 508 macros
[0m19:10:29.129144 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8aacc293-134d-422a-b57f-b19d9479e754', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7312a2cb3080>]}
[0m19:10:29.131247 [info ] [MainThread]: 
[0m19:10:29.131981 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m19:10:29.132464 [info ] [MainThread]: 
[0m19:10:29.133278 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m19:10:29.135096 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812'
[0m19:10:29.136814 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:10:30.160857 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812, now list_data-pipeline-project-474812_analytics_staging_staging)
[0m19:10:30.164786 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:10:31.092116 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8aacc293-134d-422a-b57f-b19d9479e754', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7312a1dada90>]}
[0m19:10:31.094874 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:10:31.102966 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.dim_subscription
[0m19:10:31.104119 [info ] [Thread-1 (]: 1 of 1 START sql view model analytics_staging_staging.dim_subscription ......... [RUN]
[0m19:10:31.105460 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now model.data_pipeline_project.dim_subscription)
[0m19:10:31.106247 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.dim_subscription
[0m19:10:31.112895 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.dim_subscription"
[0m19:10:31.114945 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.dim_subscription
[0m19:10:31.182879 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.dim_subscription"
[0m19:10:31.187276 [debug] [Thread-1 (]: On model.data_pipeline_project.dim_subscription: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.dim_subscription"} */


  create or replace view `data-pipeline-project-474812`.`analytics_staging_staging`.`dim_subscription`
  OPTIONS()
  as with subscriptions as (
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_subscriptions`
),

final_parsing as (
    select
        subscription_id,
        user_id as customer_id, 
        start_date,  
        case
            when is_active = FALSE then 'Cancelled'
            else 'Active'
        end as status,
        -- Business logic: end_date only exists upon explicit cancellation
        case
            when is_active = FALSE then NULL  -- Would be cancellation date
            else NULL                         -- Active = no end date
        end as end_date

        from subscriptions
)

select * from final_parsing;


[0m19:10:31.189390 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m19:10:32.422351 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:c91c4865-ea95-43db-9065-17409581062b&page=queryresults
[0m19:10:33.105474 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8aacc293-134d-422a-b57f-b19d9479e754', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7312a595c8c0>]}
[0m19:10:33.107077 [info ] [Thread-1 (]: 1 of 1 OK created sql view model analytics_staging_staging.dim_subscription .... [[32mCREATE VIEW (0 processed)[0m in 2.00s]
[0m19:10:33.108550 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.dim_subscription
[0m19:10:33.113387 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m19:10:33.116300 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:10:33.116859 [debug] [MainThread]: Connection 'model.data_pipeline_project.dim_subscription' was properly closed.
[0m19:10:33.117493 [info ] [MainThread]: 
[0m19:10:33.118400 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 3.98 seconds (3.98s).
[0m19:10:33.119465 [debug] [MainThread]: Command end result
[0m19:10:33.171672 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m19:10:33.174823 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m19:10:33.181802 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m19:10:33.182420 [info ] [MainThread]: 
[0m19:10:33.183434 [info ] [MainThread]: [32mCompleted successfully[0m
[0m19:10:33.185361 [info ] [MainThread]: 
[0m19:10:33.186443 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m19:10:33.187459 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 17 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m19:10:33.189103 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 8.657027, "process_in_blocks": "0", "process_kernel_time": 1.236395, "process_mem_max_rss": "385336", "process_out_blocks": "3992", "process_user_time": 5.960727}
[0m19:10:33.189734 [debug] [MainThread]: Command `dbt run` succeeded at 19:10:33.189628 after 8.66 seconds
[0m19:10:33.190378 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7312c65b3bc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7312a1d976b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7312a1deb170>]}
[0m19:10:33.190805 [debug] [MainThread]: Flushing usage events
[0m19:10:33.905231 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:21:52.147147 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d2f21913c50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d2f236f8d70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d2f215c79e0>]}


============================== 20:21:52.165049 | fb25adb9-9992-46c1-af03-780f672f423c ==============================
[0m20:21:52.165049 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m20:21:52.169576 [debug] [MainThread]: running dbt with arguments {'warn_error': 'None', 'printer_width': '80', 'no_print': 'None', 'use_experimental_parser': 'False', 'quiet': 'False', 'send_anonymous_usage_stats': 'True', 'empty': 'False', 'cache_selected_only': 'False', 'debug': 'False', 'target_path': 'None', 'indirect_selection': 'eager', 'use_colors': 'True', 'profiles_dir': '/home/ecem/.dbt', 'log_format': 'default', 'log_cache_events': 'False', 'introspect': 'True', 'partial_parse': 'True', 'fail_fast': 'False', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'invocation_command': 'dbt run --select int_orders_decomposed', 'write_json': 'True', 'static_parser': 'True', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])'}
[0m20:21:55.502453 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'fb25adb9-9992-46c1-af03-780f672f423c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d2f2117eab0>]}
[0m20:21:55.575991 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'fb25adb9-9992-46c1-af03-780f672f423c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d2f0293ed20>]}
[0m20:21:55.577396 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m20:21:55.932597 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m20:21:56.216428 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 1 files added, 0 files changed.
[0m20:21:56.218222 [debug] [MainThread]: Partial parsing: added file: data_pipeline_project://models/intermediate/int_orders_decomposed.sql
[0m20:21:56.218801 [debug] [MainThread]: Partial parsing: deleted file: data_pipeline_project://models/intermediate/int_subscriptions_parsed.sql
[0m20:21:56.994521 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.data_pipeline_project.marts
[0m20:21:57.063973 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fb25adb9-9992-46c1-af03-780f672f423c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d2f017a1310>]}
[0m20:21:57.339157 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m20:21:57.342532 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m20:21:57.368206 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fb25adb9-9992-46c1-af03-780f672f423c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d2f00e27cb0>]}
[0m20:21:57.369222 [info ] [MainThread]: Found 13 models, 35 data tests, 10 sources, 508 macros
[0m20:21:57.370154 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fb25adb9-9992-46c1-af03-780f672f423c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d2f01acbb00>]}
[0m20:21:57.372495 [info ] [MainThread]: 
[0m20:21:57.373966 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m20:21:57.375413 [info ] [MainThread]: 
[0m20:21:57.377120 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m20:21:57.379326 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812'
[0m20:21:57.381283 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:21:58.556159 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812, now list_data-pipeline-project-474812_analytics_staging_staging)
[0m20:21:58.561575 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m20:21:59.540344 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fb25adb9-9992-46c1-af03-780f672f423c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d2f0199af00>]}
[0m20:21:59.542940 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:21:59.552750 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.int_orders_decomposed
[0m20:21:59.553887 [info ] [Thread-1 (]: 1 of 1 START sql view model analytics_staging_staging.int_orders_decomposed .... [RUN]
[0m20:21:59.555074 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now model.data_pipeline_project.int_orders_decomposed)
[0m20:21:59.555821 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.int_orders_decomposed
[0m20:21:59.569356 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.int_orders_decomposed"
[0m20:21:59.573575 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.int_orders_decomposed
[0m20:21:59.675484 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.int_orders_decomposed"
[0m20:21:59.678780 [debug] [Thread-1 (]: On model.data_pipeline_project.int_orders_decomposed: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.int_orders_decomposed"} */


  create or replace view `data-pipeline-project-474812`.`analytics_staging_staging`.`int_orders_decomposed`
  OPTIONS()
  as with orders as (
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_orders`
),

pricing_extracted as (
    select
        order_id,
        user_id,
        created_at,
        order_status,
        
        -- EXTRACT REQUIRED FIELDS FROM PRICE JSON
        cast(json_extract_scalar(price_data, '$.grossOriginalAmount') as numeric) as items_total,
        cast(json_extract_scalar(price_data, '$.grossPromoDiscountAmount') as numeric) as discount_total,
        cast(json_extract_scalar(price_data, '$.shipmentFeeAmount') as numeric) as shipping_fee,
        cast(json_extract_scalar(price_data, '$.chargedAmount') as numeric) as charged_amount

    from orders
    where order_id is not null
)

select 
    order_id,
    user_id as customer_id,
    created_at as order_date,
    order_status as payment_status,
    items_total,
    discount_total,
    shipping_fee,
    charged_amount
from pricing_extracted;


[0m20:21:59.682210 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m20:22:00.950474 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:ce273136-c4ed-4cf9-8903-ec562ef03857&page=queryresults
[0m20:22:01.655681 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fb25adb9-9992-46c1-af03-780f672f423c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d2f049f08f0>]}
[0m20:22:01.656803 [info ] [Thread-1 (]: 1 of 1 OK created sql view model analytics_staging_staging.int_orders_decomposed  [[32mCREATE VIEW (0 processed)[0m in 2.10s]
[0m20:22:01.657490 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.int_orders_decomposed
[0m20:22:01.663005 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m20:22:01.666438 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:22:01.667673 [debug] [MainThread]: Connection 'model.data_pipeline_project.int_orders_decomposed' was properly closed.
[0m20:22:01.668755 [info ] [MainThread]: 
[0m20:22:01.670295 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 4.29 seconds (4.29s).
[0m20:22:01.673156 [debug] [MainThread]: Command end result
[0m20:22:01.790812 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m20:22:01.800970 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m20:22:01.820291 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m20:22:01.821730 [info ] [MainThread]: 
[0m20:22:01.822644 [info ] [MainThread]: [32mCompleted successfully[0m
[0m20:22:01.823369 [info ] [MainThread]: 
[0m20:22:01.824711 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m20:22:01.827435 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 9.734231, "process_in_blocks": "0", "process_kernel_time": 1.460701, "process_mem_max_rss": "381148", "process_out_blocks": "4024", "process_user_time": 6.300895}
[0m20:22:01.828490 [debug] [MainThread]: Command `dbt run` succeeded at 20:22:01.828343 after 9.74 seconds
[0m20:22:01.829472 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d2f238bb980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d2f00dc3fb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d2f00dc3230>]}
[0m20:22:01.830415 [debug] [MainThread]: Flushing usage events
[0m20:22:02.764418 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:25:45.008189 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e5002173a70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e5001e1b920>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e500211d700>]}


============================== 20:25:45.011845 | 0d3e9ee2-a3da-40b0-ba29-115cc03e3621 ==============================
[0m20:25:45.011845 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m20:25:45.015092 [debug] [MainThread]: running dbt with arguments {'write_json': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'warn_error': 'None', 'printer_width': '80', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'empty': 'False', 'target_path': 'None', 'indirect_selection': 'eager', 'partial_parse': 'True', 'no_print': 'None', 'debug': 'False', 'static_parser': 'True', 'use_experimental_parser': 'False', 'profiles_dir': '/home/ecem/.dbt', 'quiet': 'False', 'send_anonymous_usage_stats': 'True', 'cache_selected_only': 'False', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt run --select fct_order', 'version_check': 'True', 'log_cache_events': 'False', 'fail_fast': 'False', 'use_colors': 'True'}
[0m20:25:47.931208 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0d3e9ee2-a3da-40b0-ba29-115cc03e3621', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e50021ee690>]}
[0m20:25:48.007183 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '0d3e9ee2-a3da-40b0-ba29-115cc03e3621', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e4fe2dbed80>]}
[0m20:25:48.008157 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m20:25:48.269248 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m20:25:48.570871 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
[0m20:25:48.572284 [debug] [MainThread]: Partial parsing: added file: data_pipeline_project://models/facts/fct_order.sql
[0m20:25:49.145521 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.data_pipeline_project.marts
[0m20:25:49.182196 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0d3e9ee2-a3da-40b0-ba29-115cc03e3621', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e4fe17f5c10>]}
[0m20:25:49.353730 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m20:25:49.357267 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m20:25:49.374916 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0d3e9ee2-a3da-40b0-ba29-115cc03e3621', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e4fe198f830>]}
[0m20:25:49.376034 [info ] [MainThread]: Found 14 models, 35 data tests, 10 sources, 508 macros
[0m20:25:49.376715 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0d3e9ee2-a3da-40b0-ba29-115cc03e3621', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e4fe17f76e0>]}
[0m20:25:49.378670 [info ] [MainThread]: 
[0m20:25:49.379790 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m20:25:49.380686 [info ] [MainThread]: 
[0m20:25:49.381842 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m20:25:49.383291 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812'
[0m20:25:49.384541 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:25:50.397388 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812, now list_data-pipeline-project-474812_analytics_staging_staging)
[0m20:25:50.398986 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m20:25:51.462800 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0d3e9ee2-a3da-40b0-ba29-115cc03e3621', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e4fe172f1a0>]}
[0m20:25:51.463746 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:25:51.472327 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.fct_order
[0m20:25:51.473342 [info ] [Thread-1 (]: 1 of 1 START sql view model analytics_staging_staging.fct_order ................ [RUN]
[0m20:25:51.474213 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now model.data_pipeline_project.fct_order)
[0m20:25:51.475001 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.fct_order
[0m20:25:51.484883 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.fct_order"
[0m20:25:51.486061 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.fct_order
[0m20:25:51.531716 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.fct_order"
[0m20:25:51.534394 [debug] [Thread-1 (]: On model.data_pipeline_project.fct_order: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.fct_order"} */


  create or replace view `data-pipeline-project-474812`.`analytics_staging_staging`.`fct_order`
  OPTIONS()
  as with base_orders as (
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`int_orders_decomposed`
),

subscriptions as (
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_subscriptions`
),

final as (
    select
        bo.order_id,
        bo.customer_id,
        s.subscription_id,  -- ← THIS COMES FROM THE JOIN!
        bo.order_date,
        bo.items_total,
        bo.discount_total,
        bo.shipping_fee,
        case
            when bo.payment_status = 'paid' then
                bo.items_total - bo.discount_total + bo.shipping_fee
            else 0
        end as net_revenue,
        bo.payment_status
    from base_orders bo
    left join subscriptions s 
        on bo.customer_id = s.user_id
        and bo.order_date between s.start_date and coalesce(s.end_date, current_timestamp())
)

select * from final;


[0m20:25:51.537025 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m20:25:52.982322 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:e8255774-0c2a-4cd3-9903-8094a721f996&page=queryresults
[0m20:25:53.279970 [error] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:e8255774-0c2a-4cd3-9903-8094a721f996&page=queryresults
[0m20:25:53.288306 [debug] [Thread-1 (]: Database Error in model fct_order (models/facts/fct_order.sql)
  Name end_date not found inside s at [32:63]
  compiled code at target/run/data_pipeline_project/models/facts/fct_order.sql
[0m20:25:53.290681 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0d3e9ee2-a3da-40b0-ba29-115cc03e3621', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e50038edb50>]}
[0m20:25:53.292287 [error] [Thread-1 (]: 1 of 1 ERROR creating sql view model analytics_staging_staging.fct_order ....... [[31mERROR[0m in 1.81s]
[0m20:25:53.293877 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.fct_order
[0m20:25:53.294972 [debug] [Thread-7 (]: Marking all children of 'model.data_pipeline_project.fct_order' to be skipped because of status 'error'.  Reason: Database Error in model fct_order (models/facts/fct_order.sql)
  Name end_date not found inside s at [32:63]
  compiled code at target/run/data_pipeline_project/models/facts/fct_order.sql.
[0m20:25:53.300351 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m20:25:53.303320 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:25:53.303993 [debug] [MainThread]: Connection 'model.data_pipeline_project.fct_order' was properly closed.
[0m20:25:53.305256 [info ] [MainThread]: 
[0m20:25:53.306393 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 3.92 seconds (3.92s).
[0m20:25:53.309016 [debug] [MainThread]: Command end result
[0m20:25:53.369297 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m20:25:53.377284 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m20:25:53.394519 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m20:25:53.395931 [info ] [MainThread]: 
[0m20:25:53.406592 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m20:25:53.410133 [info ] [MainThread]: 
[0m20:25:53.411348 [error] [MainThread]: [31mFailure in model fct_order (models/facts/fct_order.sql)[0m
[0m20:25:53.412353 [error] [MainThread]:   Database Error in model fct_order (models/facts/fct_order.sql)
  Name end_date not found inside s at [32:63]
  compiled code at target/run/data_pipeline_project/models/facts/fct_order.sql
[0m20:25:53.413941 [info ] [MainThread]: 
[0m20:25:53.415394 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_project/models/facts/fct_order.sql
[0m20:25:53.416793 [info ] [MainThread]: 
[0m20:25:53.418059 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m20:25:53.425829 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 8.469294, "process_in_blocks": "0", "process_kernel_time": 1.130032, "process_mem_max_rss": "377652", "process_out_blocks": "4056", "process_user_time": 5.621657}
[0m20:25:53.428965 [debug] [MainThread]: Command `dbt run` failed at 20:25:53.428194 after 8.47 seconds
[0m20:25:53.430184 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e5001d82e40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e5002170b30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e5002171ac0>]}
[0m20:25:53.433368 [debug] [MainThread]: Flushing usage events
[0m20:25:54.448112 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:26:33.248073 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7452e5f31a30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7452e78e3020>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7452e55d7bc0>]}


============================== 20:26:33.251237 | 29d4c5da-a19a-41f9-82cc-cb815a4af4fe ==============================
[0m20:26:33.251237 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m20:26:33.252370 [debug] [MainThread]: running dbt with arguments {'quiet': 'False', 'write_json': 'True', 'profiles_dir': '/home/ecem/.dbt', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_colors': 'True', 'static_parser': 'True', 'cache_selected_only': 'False', 'log_format': 'default', 'printer_width': '80', 'invocation_command': 'dbt clean', 'fail_fast': 'False', 'log_cache_events': 'False', 'partial_parse': 'True', 'introspect': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'send_anonymous_usage_stats': 'True', 'empty': 'None', 'debug': 'False', 'target_path': 'None', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'version_check': 'True', 'warn_error': 'None', 'no_print': 'None'}
[0m20:26:33.394868 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '29d4c5da-a19a-41f9-82cc-cb815a4af4fe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7452e565d640>]}
[0m20:26:33.411868 [debug] [MainThread]: Resource report: {"command_name": "clean", "command_success": true, "command_wall_clock_time": 0.22364931, "process_in_blocks": "0", "process_kernel_time": 0.241647, "process_mem_max_rss": "101784", "process_out_blocks": "8", "process_user_time": 1.47766}
[0m20:26:33.412930 [debug] [MainThread]: Command `dbt clean` succeeded at 20:26:33.412816 after 0.23 seconds
[0m20:26:33.413759 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7452e4ebbe30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7452e4fa4230>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7452e5acb080>]}
[0m20:26:33.414254 [debug] [MainThread]: Flushing usage events
[0m20:26:34.248597 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:29:49.694651 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7553f0374740>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7553f0b85010>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7553ef439460>]}


============================== 20:29:49.698288 | fac168ab-864c-4a5b-81dd-1a8cb526fdf0 ==============================
[0m20:29:49.698288 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m20:29:49.699616 [debug] [MainThread]: running dbt with arguments {'debug': 'False', 'no_print': 'None', 'partial_parse': 'True', 'log_cache_events': 'False', 'quiet': 'False', 'empty': 'False', 'fail_fast': 'False', 'profiles_dir': '/home/ecem/.dbt', 'invocation_command': 'dbt run --select fct_order', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'use_experimental_parser': 'False', 'printer_width': '80', 'introspect': 'True', 'indirect_selection': 'eager', 'use_colors': 'True', 'cache_selected_only': 'False', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True', 'version_check': 'True', 'warn_error': 'None', 'log_format': 'default', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'write_json': 'True'}
[0m20:29:51.906438 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'fac168ab-864c-4a5b-81dd-1a8cb526fdf0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7553ced6bf50>]}
[0m20:29:51.970125 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'fac168ab-864c-4a5b-81dd-1a8cb526fdf0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7553cfdd0bc0>]}
[0m20:29:51.970917 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m20:29:52.304812 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m20:29:52.306100 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m20:29:52.307134 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'fac168ab-864c-4a5b-81dd-1a8cb526fdf0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7553cec06db0>]}
[0m20:29:53.659085 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `relationships` defined on 'stg_orders' in
package 'data_pipeline_project' (models/staging/staging_schema.yml). Arguments
to generic tests should be nested under the `arguments` property.
[0m20:29:53.660113 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': 'fac168ab-864c-4a5b-81dd-1a8cb526fdf0', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7553ce881250>]}
[0m20:29:53.988722 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.data_pipeline_project.marts
[0m20:29:54.000329 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fac168ab-864c-4a5b-81dd-1a8cb526fdf0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7553cdef5f70>]}
[0m20:29:54.137446 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m20:29:54.139751 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m20:29:54.157675 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fac168ab-864c-4a5b-81dd-1a8cb526fdf0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7553cdeccb30>]}
[0m20:29:54.158317 [info ] [MainThread]: Found 14 models, 35 data tests, 10 sources, 508 macros
[0m20:29:54.159031 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fac168ab-864c-4a5b-81dd-1a8cb526fdf0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7553cddfcce0>]}
[0m20:29:54.161053 [info ] [MainThread]: 
[0m20:29:54.161778 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m20:29:54.162603 [info ] [MainThread]: 
[0m20:29:54.163558 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m20:29:54.165405 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812'
[0m20:29:54.169303 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:29:55.384384 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812, now list_data-pipeline-project-474812_analytics_staging_staging)
[0m20:29:55.387895 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m20:29:56.510930 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fac168ab-864c-4a5b-81dd-1a8cb526fdf0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7553cdef41a0>]}
[0m20:29:56.511586 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:29:56.520746 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.fct_order
[0m20:29:56.521737 [info ] [Thread-1 (]: 1 of 1 START sql view model analytics_staging_staging.fct_order ................ [RUN]
[0m20:29:56.522796 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now model.data_pipeline_project.fct_order)
[0m20:29:56.523400 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.fct_order
[0m20:29:56.533271 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.fct_order"
[0m20:29:56.537266 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.fct_order
[0m20:29:56.597793 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.fct_order"
[0m20:29:56.601151 [debug] [Thread-1 (]: On model.data_pipeline_project.fct_order: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.fct_order"} */


  create or replace view `data-pipeline-project-474812`.`analytics_staging_staging`.`fct_order`
  OPTIONS()
  as with base_orders as (
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`int_orders_decomposed`
),

subscriptions as (
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_subscriptions`
),

final as (
    select
        bo.order_id,
        bo.customer_id,
        s.subscription_id,  -- ← THIS COMES FROM THE JOIN!
        bo.order_date,
        bo.items_total,
        bo.discount_total,
        bo.shipping_fee,
        case
            when bo.payment_status = 'paid' then
                bo.items_total - bo.discount_total + bo.shipping_fee
            else 0
        end as net_revenue,
        bo.payment_status
    from base_orders bo
    left join subscriptions s 
        on bo.customer_id = s.user_id
)

select * from final;


[0m20:29:56.602602 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m20:29:58.069968 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:955f6466-59e4-4f6f-9624-4d391c19471b&page=queryresults
[0m20:29:58.727132 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fac168ab-864c-4a5b-81dd-1a8cb526fdf0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7553d1b4c830>]}
[0m20:29:58.728155 [info ] [Thread-1 (]: 1 of 1 OK created sql view model analytics_staging_staging.fct_order ........... [[32mCREATE VIEW (0 processed)[0m in 2.20s]
[0m20:29:58.729979 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.fct_order
[0m20:29:58.733204 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m20:29:58.735044 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:29:58.735614 [debug] [MainThread]: Connection 'model.data_pipeline_project.fct_order' was properly closed.
[0m20:29:58.736693 [info ] [MainThread]: 
[0m20:29:58.737983 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 4.57 seconds (4.57s).
[0m20:29:58.739321 [debug] [MainThread]: Command end result
[0m20:29:58.815267 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m20:29:58.819176 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m20:29:58.836205 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m20:29:58.836961 [info ] [MainThread]: 
[0m20:29:58.843121 [info ] [MainThread]: [32mCompleted successfully[0m
[0m20:29:58.847391 [info ] [MainThread]: 
[0m20:29:58.849767 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m20:29:58.851106 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 17 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m20:29:58.853428 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 9.228918, "process_in_blocks": "0", "process_kernel_time": 1.157292, "process_mem_max_rss": "385548", "process_out_blocks": "4016", "process_user_time": 5.823794}
[0m20:29:58.854308 [debug] [MainThread]: Command `dbt run` succeeded at 20:29:58.854184 after 9.23 seconds
[0m20:29:58.855316 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7553eeb82510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7553ce87e1e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7553cdeb6420>]}
[0m20:29:58.859633 [debug] [MainThread]: Flushing usage events
[0m20:30:00.098425 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:32:25.525826 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x772a5158f5c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x772a50fcf170>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x772a5142b4d0>]}


============================== 20:32:25.529556 | 667c0605-9612-455f-ab32-20f76516cad0 ==============================
[0m20:32:25.529556 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m20:32:25.530756 [debug] [MainThread]: running dbt with arguments {'partial_parse': 'True', 'static_parser': 'True', 'log_format': 'default', 'use_experimental_parser': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'indirect_selection': 'eager', 'debug': 'False', 'version_check': 'True', 'no_print': 'None', 'introspect': 'True', 'use_colors': 'True', 'target_path': 'None', 'cache_selected_only': 'False', 'invocation_command': 'dbt clean', 'send_anonymous_usage_stats': 'True', 'profiles_dir': '/home/ecem/.dbt', 'quiet': 'False', 'printer_width': '80', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'fail_fast': 'False', 'empty': 'None', 'warn_error': 'None', 'write_json': 'True', 'log_cache_events': 'False'}
[0m20:32:25.680823 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '667c0605-9612-455f-ab32-20f76516cad0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x772a509fbdd0>]}
[0m20:32:25.697951 [debug] [MainThread]: Resource report: {"command_name": "clean", "command_success": true, "command_wall_clock_time": 0.23732564, "process_in_blocks": "0", "process_kernel_time": 0.256029, "process_mem_max_rss": "101832", "process_out_blocks": "16", "process_user_time": 1.545745}
[0m20:32:25.698468 [debug] [MainThread]: Command `dbt clean` succeeded at 20:32:25.698369 after 0.24 seconds
[0m20:32:25.698870 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x772a50b37cb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x772a50a4ff50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x772a514cf920>]}
[0m20:32:25.699275 [debug] [MainThread]: Flushing usage events
[0m20:32:31.518265 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:32:35.770953 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7410a3d59640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7410a41c76e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7410a641a5a0>]}


============================== 20:32:35.775679 | 4b4eb0a5-d7b9-4f3d-926f-285c18b47889 ==============================
[0m20:32:35.775679 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m20:32:35.779177 [debug] [MainThread]: running dbt with arguments {'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'introspect': 'True', 'warn_error': 'None', 'use_experimental_parser': 'False', 'log_cache_events': 'False', 'indirect_selection': 'eager', 'log_format': 'default', 'write_json': 'True', 'printer_width': '80', 'invocation_command': 'dbt run --select fct_order', 'no_print': 'None', 'target_path': 'None', 'partial_parse': 'True', 'profiles_dir': '/home/ecem/.dbt', 'quiet': 'False', 'send_anonymous_usage_stats': 'True', 'fail_fast': 'False', 'debug': 'False', 'cache_selected_only': 'False', 'empty': 'False', 'version_check': 'True', 'static_parser': 'True', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'use_colors': 'True'}
[0m20:32:38.128399 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4b4eb0a5-d7b9-4f3d-926f-285c18b47889', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7410850a1520>]}
[0m20:32:38.197966 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4b4eb0a5-d7b9-4f3d-926f-285c18b47889', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7410847eacc0>]}
[0m20:32:38.199052 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m20:32:38.445433 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m20:32:38.446588 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m20:32:38.447327 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '4b4eb0a5-d7b9-4f3d-926f-285c18b47889', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x741084143560>]}
[0m20:32:39.815572 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `relationships` defined on 'stg_orders' in
package 'data_pipeline_project' (models/staging/staging_schema.yml). Arguments
to generic tests should be nested under the `arguments` property.
[0m20:32:39.816949 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '4b4eb0a5-d7b9-4f3d-926f-285c18b47889', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x741083ea8980>]}
[0m20:32:40.165264 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.data_pipeline_project.marts
[0m20:32:40.177658 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4b4eb0a5-d7b9-4f3d-926f-285c18b47889', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7410841219d0>]}
[0m20:32:40.306293 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m20:32:40.308879 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m20:32:40.326706 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4b4eb0a5-d7b9-4f3d-926f-285c18b47889', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74108328f320>]}
[0m20:32:40.329083 [info ] [MainThread]: Found 14 models, 35 data tests, 10 sources, 508 macros
[0m20:32:40.329822 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4b4eb0a5-d7b9-4f3d-926f-285c18b47889', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x741083e5c0b0>]}
[0m20:32:40.331813 [info ] [MainThread]: 
[0m20:32:40.332726 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m20:32:40.333478 [info ] [MainThread]: 
[0m20:32:40.334330 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m20:32:40.336037 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812'
[0m20:32:40.336809 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:32:41.450849 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812, now list_data-pipeline-project-474812_analytics_staging_staging)
[0m20:32:41.452391 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m20:32:42.519559 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4b4eb0a5-d7b9-4f3d-926f-285c18b47889', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x741083ea9820>]}
[0m20:32:42.520200 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:32:42.528945 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.fct_order
[0m20:32:42.529953 [info ] [Thread-1 (]: 1 of 1 START sql view model analytics_staging_staging.fct_order ................ [RUN]
[0m20:32:42.530769 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now model.data_pipeline_project.fct_order)
[0m20:32:42.531378 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.fct_order
[0m20:32:42.540963 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.fct_order"
[0m20:32:42.542063 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.fct_order
[0m20:32:42.583988 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.fct_order"
[0m20:32:42.589348 [debug] [Thread-1 (]: On model.data_pipeline_project.fct_order: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.fct_order"} */


  create or replace view `data-pipeline-project-474812`.`analytics_staging_staging`.`fct_order`
  OPTIONS()
  as with base_orders as (
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`int_orders_decomposed`
),

subscriptions as (
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_subscriptions`
),

final as (
    select
        bo.order_id,
        bo.customer_id,
        s.subscription_id,  -- ← THIS COMES FROM THE JOIN!
        bo.order_date,
        bo.items_total,
        bo.discount_total,
        bo.shipping_fee,
        case
            when bo.payment_status = 'PAID' then
                bo.items_total - bo.discount_total + bo.shipping_fee
            else 0
        end as net_revenue,
        bo.payment_status
    from base_orders bo
    left join subscriptions s 
        on bo.customer_id = s.user_id
)

select * from final;


[0m20:32:42.590964 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m20:32:43.963551 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:ed4e9696-112a-4513-a929-cdd103d4c2fc&page=queryresults
[0m20:32:44.655818 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4b4eb0a5-d7b9-4f3d-926f-285c18b47889', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x741087050170>]}
[0m20:32:44.657125 [info ] [Thread-1 (]: 1 of 1 OK created sql view model analytics_staging_staging.fct_order ........... [[32mCREATE VIEW (0 processed)[0m in 2.12s]
[0m20:32:44.658434 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.fct_order
[0m20:32:44.661613 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m20:32:44.663689 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:32:44.664258 [debug] [MainThread]: Connection 'model.data_pipeline_project.fct_order' was properly closed.
[0m20:32:44.664937 [info ] [MainThread]: 
[0m20:32:44.665920 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 4.33 seconds (4.33s).
[0m20:32:44.667084 [debug] [MainThread]: Command end result
[0m20:32:44.708945 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m20:32:44.713957 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m20:32:44.724096 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m20:32:44.725973 [info ] [MainThread]: 
[0m20:32:44.728238 [info ] [MainThread]: [32mCompleted successfully[0m
[0m20:32:44.730436 [info ] [MainThread]: 
[0m20:32:44.731442 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m20:32:44.737184 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 17 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m20:32:44.740472 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 9.024175, "process_in_blocks": "0", "process_kernel_time": 1.025716, "process_mem_max_rss": "385452", "process_out_blocks": "4016", "process_user_time": 6.137036}
[0m20:32:44.741229 [debug] [MainThread]: Command `dbt run` succeeded at 20:32:44.741112 after 9.03 seconds
[0m20:32:44.742767 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7410a4383a40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7410833a2390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74108328ffb0>]}
[0m20:32:44.743617 [debug] [MainThread]: Flushing usage events
[0m20:32:45.440777 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:47:40.253417 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7084866df740>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x708488882f30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7084866df500>]}


============================== 20:47:40.262095 | bfc8dd11-4c3c-41da-9879-e7835188f19d ==============================
[0m20:47:40.262095 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m20:47:40.263939 [debug] [MainThread]: running dbt with arguments {'warn_error': 'None', 'log_cache_events': 'False', 'printer_width': '80', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'debug': 'False', 'quiet': 'False', 'partial_parse': 'True', 'version_check': 'True', 'invocation_command': 'dbt run --select dims', 'profiles_dir': '/home/ecem/.dbt', 'use_colors': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'send_anonymous_usage_stats': 'True', 'empty': 'False', 'introspect': 'True', 'no_print': 'None', 'write_json': 'True', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'static_parser': 'True', 'fail_fast': 'False', 'log_format': 'default', 'target_path': 'None', 'cache_selected_only': 'False'}
[0m20:47:45.284325 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'bfc8dd11-4c3c-41da-9879-e7835188f19d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7084667736e0>]}
[0m20:47:45.363857 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'bfc8dd11-4c3c-41da-9879-e7835188f19d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x708466e23770>]}
[0m20:47:45.364714 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m20:47:45.674070 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m20:47:45.985577 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 3 files changed.
[0m20:47:45.986626 [debug] [MainThread]: Partial parsing: updated file: data_pipeline_project://models/dims/dim_subscription.sql
[0m20:47:45.987150 [debug] [MainThread]: Partial parsing: updated file: data_pipeline_project://models/dims/dim_customer.sql
[0m20:47:45.987643 [debug] [MainThread]: Partial parsing: updated file: data_pipeline_project://models/facts/fct_order.sql
[0m20:47:46.397845 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.data_pipeline_project.marts
[0m20:47:46.421365 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'bfc8dd11-4c3c-41da-9879-e7835188f19d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70846623a6c0>]}
[0m20:47:46.632087 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m20:47:46.643334 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m20:47:46.700490 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'bfc8dd11-4c3c-41da-9879-e7835188f19d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7084664b6ed0>]}
[0m20:47:46.701304 [info ] [MainThread]: Found 14 models, 35 data tests, 10 sources, 508 macros
[0m20:47:46.702220 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bfc8dd11-4c3c-41da-9879-e7835188f19d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70846633a150>]}
[0m20:47:46.704435 [info ] [MainThread]: 
[0m20:47:46.705212 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m20:47:46.706003 [info ] [MainThread]: 
[0m20:47:46.707485 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m20:47:46.718111 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812'
[0m20:47:46.719368 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:47:47.905920 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812, now create_data-pipeline-project-474812_analytics_staging_marts)
[0m20:47:47.907029 [debug] [ThreadPool]: Creating schema "database: "data-pipeline-project-474812"
schema: "analytics_staging_marts"
"
[0m20:47:47.920860 [debug] [ThreadPool]: On create_data-pipeline-project-474812_analytics_staging_marts: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "connection_name": "create_data-pipeline-project-474812_analytics_staging_marts"} */
create schema if not exists `data-pipeline-project-474812`.`analytics_staging_marts`
  
[0m20:47:47.921925 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m20:47:49.253734 [debug] [ThreadPool]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:fb31d87a-b084-4c76-aadb-4febd6ee9ca4&page=queryresults
[0m20:47:50.110513 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_data-pipeline-project-474812_analytics_staging_marts, now list_data-pipeline-project-474812_analytics_staging_marts)
[0m20:47:50.112170 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_staging'
[0m20:47:50.112812 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m20:47:50.113668 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:47:51.249608 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bfc8dd11-4c3c-41da-9879-e7835188f19d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x708488515f70>]}
[0m20:47:51.254708 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:47:51.267178 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.dim_customer
[0m20:47:51.268016 [info ] [Thread-1 (]: 1 of 2 START sql table model analytics_staging_marts.dim_customer .............. [RUN]
[0m20:47:51.268723 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now model.data_pipeline_project.dim_customer)
[0m20:47:51.269251 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.dim_customer
[0m20:47:51.279523 [debug] [Thread-2 (]: Began running node model.data_pipeline_project.dim_subscription
[0m20:47:51.280824 [info ] [Thread-2 (]: 2 of 2 START sql table model analytics_staging_marts.dim_subscription .......... [RUN]
[0m20:47:51.287961 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.dim_customer"
[0m20:47:51.289165 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_marts, now model.data_pipeline_project.dim_subscription)
[0m20:47:51.292204 [debug] [Thread-2 (]: Began compiling node model.data_pipeline_project.dim_subscription
[0m20:47:51.293045 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.dim_customer
[0m20:47:51.298079 [debug] [Thread-2 (]: Writing injected SQL for node "model.data_pipeline_project.dim_subscription"
[0m20:47:51.322148 [debug] [Thread-2 (]: Began executing node model.data_pipeline_project.dim_subscription
[0m20:47:51.387149 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.dim_customer"
[0m20:47:51.406716 [debug] [Thread-2 (]: Writing runtime sql for node "model.data_pipeline_project.dim_subscription"
[0m20:47:51.419669 [debug] [Thread-2 (]: On model.data_pipeline_project.dim_subscription: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.dim_subscription"} */

  
    

    create or replace table `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_subscription`
      
    
    

    
    OPTIONS()
    as (
      
with subscriptions as (
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_subscriptions`
),

final_parsing as (
    select
        subscription_id,
        user_id as customer_id, 
        start_date,  
        case
            when is_active = FALSE then 'Cancelled'
            else 'Active'
        end as status,
        -- Business logic: end_date only exists upon explicit cancellation
        case
            when is_active = FALSE then NULL  -- Would be cancellation date
            else NULL                         -- Active = no end date
        end as end_date

        from subscriptions
)

select * from final_parsing
    );
  
[0m20:47:51.417874 [debug] [Thread-1 (]: On model.data_pipeline_project.dim_customer: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.dim_customer"} */

  
    

    create or replace table `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_customer`
      
    
    

    
    OPTIONS()
    as (
      


with users as (
    -- Start with the clean user base (PK: user_id)
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
),

addresses as (
    -- Link users to their primary address/location (FK: user_id)
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_addresses`
),

neighborhoods as (
    -- Descriptive geography (FK: neighborhood_id, city_id, country_id)
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_neighborhoods`
),

cities as (
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_cities`
),

states as (
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_states`
),

countries as (
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_countries`
),

-- Join everything into a single, comprehensive dimension
final as (
    select
        u.user_id as customer_id, -- Rename PK to customer_id for the dim table
        u.created_at as signup_date,
     
        -- Address and Invoice Details
        a.invoice_type,
        
        -- Neighborhood Details
        n.neighborhood_name as neighborhood, 
        
        -- City, State, Country Details (The full hierarchy)
        c.city_name as city,
        s.state_name as state,
        co.country_name as country,

        n.postal_code
        
    from users u
    -- Join to get the address link (assuming a user has one primary address record for simplicity)
    left join addresses a
        on u.user_id = a.user_id
    -- Join up the geography hierarchy
    left join neighborhoods n
        on a.neighborhood_id = n.neighborhood_id
    left join cities c
        on n.city_id = c.city_id
    left join states s
        on c.state_id = s.state_id
    left join countries co
        on c.country_id = co.country_id
)

select * from final
    );
  
[0m20:47:51.421230 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m20:47:51.424903 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m20:47:52.984607 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:cbb9db80-afc5-4100-b7c2-e2528999308b&page=queryresults
[0m20:47:53.152967 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:8ee3fd04-a96c-431a-bfdf-24bf2fb962f7&page=queryresults
[0m20:47:55.955372 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bfc8dd11-4c3c-41da-9879-e7835188f19d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x708486c63920>]}
[0m20:47:55.958037 [info ] [Thread-2 (]: 2 of 2 OK created sql table model analytics_staging_marts.dim_subscription ..... [[32mCREATE TABLE (1.5k rows, 112.0 KiB processed)[0m in 4.66s]
[0m20:47:55.959097 [debug] [Thread-2 (]: Finished running node model.data_pipeline_project.dim_subscription
[0m20:47:56.364355 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bfc8dd11-4c3c-41da-9879-e7835188f19d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7084667bb4d0>]}
[0m20:47:56.366766 [info ] [Thread-1 (]: 1 of 2 OK created sql table model analytics_staging_marts.dim_customer ......... [[32mCREATE TABLE (4.5k rows, 8.3 MiB processed)[0m in 5.09s]
[0m20:47:56.369792 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.dim_customer
[0m20:47:56.382177 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m20:47:56.388149 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:47:56.388945 [debug] [MainThread]: Connection 'model.data_pipeline_project.dim_subscription' was properly closed.
[0m20:47:56.390012 [debug] [MainThread]: Connection 'model.data_pipeline_project.dim_customer' was properly closed.
[0m20:47:56.392310 [info ] [MainThread]: 
[0m20:47:56.394351 [info ] [MainThread]: Finished running 2 table models in 0 hours 0 minutes and 9.69 seconds (9.69s).
[0m20:47:56.398827 [debug] [MainThread]: Command end result
[0m20:47:56.460087 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m20:47:56.473588 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m20:47:56.484322 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m20:47:56.485929 [info ] [MainThread]: 
[0m20:47:56.487161 [info ] [MainThread]: [32mCompleted successfully[0m
[0m20:47:56.488281 [info ] [MainThread]: 
[0m20:47:56.489357 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=2
[0m20:47:56.491694 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 16.315346, "process_in_blocks": "0", "process_kernel_time": 1.533225, "process_mem_max_rss": "383428", "process_out_blocks": "4080", "process_user_time": 8.239167}
[0m20:47:56.495343 [debug] [MainThread]: Command `dbt run` succeeded at 20:47:56.495126 after 16.32 seconds
[0m20:47:56.497541 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x708486f6b020>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x708465b51700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x708466771ca0>]}
[0m20:47:56.498240 [debug] [MainThread]: Flushing usage events
[0m20:47:57.567577 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:48:09.707207 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e3e0c69dca0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e3e0caa6f00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e3e0cb8c290>]}


============================== 20:48:09.710895 | 4fd1b8fb-71a7-410d-9d61-b18e8824513c ==============================
[0m20:48:09.710895 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m20:48:09.712029 [debug] [MainThread]: running dbt with arguments {'indirect_selection': 'eager', 'cache_selected_only': 'False', 'log_format': 'default', 'printer_width': '80', 'target_path': 'None', 'profiles_dir': '/home/ecem/.dbt', 'log_cache_events': 'False', 'introspect': 'True', 'no_print': 'None', 'use_colors': 'True', 'empty': 'False', 'debug': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'quiet': 'False', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'write_json': 'True', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'static_parser': 'True', 'use_experimental_parser': 'False', 'invocation_command': 'dbt run --select facts'}
[0m20:48:13.984279 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4fd1b8fb-71a7-410d-9d61-b18e8824513c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e3e0d19fcb0>]}
[0m20:48:14.094487 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4fd1b8fb-71a7-410d-9d61-b18e8824513c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e3decf3f0b0>]}
[0m20:48:14.095649 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m20:48:14.433995 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m20:48:14.747477 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:48:14.748154 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:48:14.756254 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.data_pipeline_project.marts
[0m20:48:14.807980 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4fd1b8fb-71a7-410d-9d61-b18e8824513c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e3decc4d100>]}
[0m20:48:14.928637 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m20:48:14.932995 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m20:48:14.955555 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4fd1b8fb-71a7-410d-9d61-b18e8824513c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e3dec861190>]}
[0m20:48:14.956604 [info ] [MainThread]: Found 14 models, 35 data tests, 10 sources, 508 macros
[0m20:48:14.957199 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4fd1b8fb-71a7-410d-9d61-b18e8824513c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e3dec92e630>]}
[0m20:48:14.959621 [info ] [MainThread]: 
[0m20:48:14.960626 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m20:48:14.961138 [info ] [MainThread]: 
[0m20:48:14.962066 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m20:48:14.965001 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812'
[0m20:48:14.966087 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:48:19.820569 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812, now list_data-pipeline-project-474812_analytics_staging_staging)
[0m20:48:19.822062 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m20:48:19.880374 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_marts'
[0m20:48:19.881402 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:48:21.036083 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4fd1b8fb-71a7-410d-9d61-b18e8824513c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e3dec92e690>]}
[0m20:48:21.037903 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:48:21.059729 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.fct_order
[0m20:48:21.063262 [info ] [Thread-1 (]: 1 of 1 START sql table model analytics_staging_marts.fct_order ................. [RUN]
[0m20:48:21.065490 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now model.data_pipeline_project.fct_order)
[0m20:48:21.066986 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.fct_order
[0m20:48:21.085435 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.fct_order"
[0m20:48:21.086444 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.fct_order
[0m20:48:21.220839 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.fct_order"
[0m20:48:21.228499 [debug] [Thread-1 (]: On model.data_pipeline_project.fct_order: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.fct_order"} */

  
    

    create or replace table `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_order`
      
    
    

    
    OPTIONS()
    as (
      
with base_orders as (
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`int_orders_decomposed`
),

subscriptions as (
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_subscriptions`
),

final as (
    select
        bo.order_id,
        bo.customer_id,
        s.subscription_id,  -- ← THIS COMES FROM THE JOIN!
        bo.order_date,
        bo.items_total,
        bo.discount_total,
        bo.shipping_fee,
        case
            when bo.payment_status = 'PAID' then
                bo.items_total - bo.discount_total + bo.shipping_fee
            else 0
        end as net_revenue,
        bo.payment_status
    from base_orders bo
    left join subscriptions s 
        on bo.customer_id = s.user_id
)

select * from final
    );
  
[0m20:48:21.234852 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m20:48:22.931144 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:7a88e6b9-b419-47f9-91d6-e2b0c12b60a7&page=queryresults
[0m20:48:26.009663 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4fd1b8fb-71a7-410d-9d61-b18e8824513c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e3dec92e7b0>]}
[0m20:48:26.013850 [info ] [Thread-1 (]: 1 of 1 OK created sql table model analytics_staging_marts.fct_order ............ [[32mCREATE TABLE (1.8k rows, 1.1 MiB processed)[0m in 4.94s]
[0m20:48:26.016967 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.fct_order
[0m20:48:26.030785 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m20:48:26.039285 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:48:26.041334 [debug] [MainThread]: Connection 'model.data_pipeline_project.fct_order' was properly closed.
[0m20:48:26.043315 [debug] [MainThread]: Connection 'list_data-pipeline-project-474812_analytics_staging_marts' was properly closed.
[0m20:48:26.050303 [info ] [MainThread]: 
[0m20:48:26.051993 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 11.09 seconds (11.09s).
[0m20:48:26.056583 [debug] [MainThread]: Command end result
[0m20:48:26.145235 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m20:48:26.151925 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m20:48:26.176523 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m20:48:26.177904 [info ] [MainThread]: 
[0m20:48:26.178945 [info ] [MainThread]: [32mCompleted successfully[0m
[0m20:48:26.180190 [info ] [MainThread]: 
[0m20:48:26.181295 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m20:48:26.183966 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 16.537716, "process_in_blocks": "0", "process_kernel_time": 1.507581, "process_mem_max_rss": "380356", "process_out_blocks": "2792", "process_user_time": 6.589395}
[0m20:48:26.185172 [debug] [MainThread]: Command `dbt run` succeeded at 20:48:26.184768 after 16.54 seconds
[0m20:48:26.185869 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e3e0c7dd7f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e3dec862840>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e3e0c6d3620>]}
[0m20:48:26.186548 [debug] [MainThread]: Flushing usage events
[0m20:48:26.979457 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m21:16:34.584125 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x798e201e4fb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x798e1f65b2c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x798e21659c40>]}


============================== 21:16:34.588191 | 8554f5fe-c5c1-44f7-93c2-79fed2b0094e ==============================
[0m21:16:34.588191 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m21:16:34.590235 [debug] [MainThread]: running dbt with arguments {'use_colors': 'True', 'printer_width': '80', 'fail_fast': 'False', 'write_json': 'True', 'debug': 'False', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'invocation_command': 'dbt run --select int_shipments_decomposed', 'warn_error': 'None', 'use_experimental_parser': 'False', 'partial_parse': 'True', 'indirect_selection': 'eager', 'profiles_dir': '/home/ecem/.dbt', 'cache_selected_only': 'False', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'no_print': 'None', 'log_cache_events': 'False', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'version_check': 'True', 'empty': 'False'}
[0m21:16:36.975296 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8554f5fe-c5c1-44f7-93c2-79fed2b0094e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x798e001661e0>]}
[0m21:16:37.044105 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8554f5fe-c5c1-44f7-93c2-79fed2b0094e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x798e2187a300>]}
[0m21:16:37.045179 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m21:16:37.290802 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m21:16:37.561082 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 2 files added, 0 files changed.
[0m21:16:37.562155 [debug] [MainThread]: Partial parsing: added file: data_pipeline_project://models/facts/fct_shipment.sql
[0m21:16:37.562613 [debug] [MainThread]: Partial parsing: added file: data_pipeline_project://models/intermediate/int_shipments.decomposed.sql
[0m21:16:37.773133 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.data_pipeline_project.fct_shipment' (models/facts/fct_shipment.sql) depends on a node named 'int_shipments' which was not found
[0m21:16:37.774766 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 3.250864, "process_in_blocks": "0", "process_kernel_time": 0.907717, "process_mem_max_rss": "369836", "process_out_blocks": "16", "process_user_time": 4.045738}
[0m21:16:37.775743 [debug] [MainThread]: Command `dbt run` failed at 21:16:37.775634 after 3.25 seconds
[0m21:16:37.776365 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x798e21be7200>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x798dff61ce60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x798dffbb1730>]}
[0m21:16:37.776902 [debug] [MainThread]: Flushing usage events
[0m21:16:39.812271 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m21:16:58.777664 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c5806df2f60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c580707b8c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c580707bb60>]}


============================== 21:16:58.806882 | f7decde8-24c4-4215-9aea-7d8ace3d4ecb ==============================
[0m21:16:58.806882 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m21:16:58.812165 [debug] [MainThread]: running dbt with arguments {'use_colors': 'True', 'target_path': 'None', 'fail_fast': 'False', 'write_json': 'True', 'debug': 'False', 'introspect': 'True', 'invocation_command': 'dbt run --select fct_shipment', 'log_cache_events': 'False', 'no_print': 'None', 'empty': 'False', 'warn_error': 'None', 'log_format': 'default', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'cache_selected_only': 'False', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'indirect_selection': 'eager', 'version_check': 'True', 'static_parser': 'True', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'profiles_dir': '/home/ecem/.dbt', 'use_experimental_parser': 'False', 'printer_width': '80', 'quiet': 'False'}
[0m21:17:01.210131 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f7decde8-24c4-4215-9aea-7d8ace3d4ecb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c57e8017740>]}
[0m21:17:01.282793 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f7decde8-24c4-4215-9aea-7d8ace3d4ecb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c57e6f50b60>]}
[0m21:17:01.283779 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m21:17:01.532275 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m21:17:01.814394 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 2 files added, 0 files changed.
[0m21:17:01.815385 [debug] [MainThread]: Partial parsing: added file: data_pipeline_project://models/intermediate/int_shipments.decomposed.sql
[0m21:17:01.815867 [debug] [MainThread]: Partial parsing: added file: data_pipeline_project://models/facts/fct_shipment.sql
[0m21:17:02.031469 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.data_pipeline_project.fct_shipment' (models/facts/fct_shipment.sql) depends on a node named 'int_shipments' which was not found
[0m21:17:02.033233 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 3.3083773, "process_in_blocks": "0", "process_kernel_time": 1.002705, "process_mem_max_rss": "369616", "process_out_blocks": "8", "process_user_time": 3.987427}
[0m21:17:02.034221 [debug] [MainThread]: Command `dbt run` failed at 21:17:02.034112 after 3.31 seconds
[0m21:17:02.034974 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c5806c8f3b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c57e69c5370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c57e683fbc0>]}
[0m21:17:02.035577 [debug] [MainThread]: Flushing usage events
[0m21:17:02.710102 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m21:20:28.800839 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x702113de35f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7021137acaa0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7021137ad610>]}


============================== 21:20:28.806151 | 8156ed89-32a1-4b20-bf10-1ca637d6edb7 ==============================
[0m21:20:28.806151 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m21:20:28.807200 [debug] [MainThread]: running dbt with arguments {'log_format': 'default', 'fail_fast': 'False', 'debug': 'False', 'quiet': 'False', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'printer_width': '80', 'use_colors': 'True', 'cache_selected_only': 'False', 'write_json': 'True', 'partial_parse': 'True', 'no_print': 'None', 'static_parser': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'target_path': 'None', 'send_anonymous_usage_stats': 'True', 'warn_error': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'version_check': 'True', 'introspect': 'True', 'profiles_dir': '/home/ecem/.dbt', 'invocation_command': 'dbt clean', 'indirect_selection': 'eager', 'log_cache_events': 'False'}
[0m21:20:28.970239 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8156ed89-32a1-4b20-bf10-1ca637d6edb7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x702115823c80>]}
[0m21:20:28.986966 [debug] [MainThread]: Resource report: {"command_name": "clean", "command_success": true, "command_wall_clock_time": 0.24453314, "process_in_blocks": "0", "process_kernel_time": 0.224661, "process_mem_max_rss": "101624", "process_out_blocks": "8", "process_user_time": 1.630958}
[0m21:20:28.987690 [debug] [MainThread]: Command `dbt clean` succeeded at 21:20:28.987566 after 0.25 seconds
[0m21:20:28.988138 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x702113a97920>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x702112d9fad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x702115392d20>]}
[0m21:20:28.988586 [debug] [MainThread]: Flushing usage events
[0m21:20:29.948014 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m21:20:41.696502 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f48ca71b890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f48cac1d0d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f48cbe4a1e0>]}


============================== 21:20:41.700231 | 29779282-15a0-40c1-974d-baccaa6afa16 ==============================
[0m21:20:41.700231 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m21:20:41.701218 [debug] [MainThread]: running dbt with arguments {'log_format': 'default', 'profiles_dir': '/home/ecem/.dbt', 'no_print': 'None', 'partial_parse': 'True', 'debug': 'False', 'send_anonymous_usage_stats': 'True', 'static_parser': 'True', 'version_check': 'True', 'use_experimental_parser': 'False', 'invocation_command': 'dbt run --select int_shipments_decomposed', 'indirect_selection': 'eager', 'cache_selected_only': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'quiet': 'False', 'printer_width': '80', 'empty': 'False', 'target_path': 'None', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'write_json': 'True', 'introspect': 'True', 'warn_error': 'None', 'log_cache_events': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])'}
[0m21:20:43.997014 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '29779282-15a0-40c1-974d-baccaa6afa16', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f48ca3263c0>]}
[0m21:20:44.076644 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '29779282-15a0-40c1-974d-baccaa6afa16', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f48ca3005c0>]}
[0m21:20:44.077834 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m21:20:44.316113 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m21:20:44.317285 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m21:20:44.319334 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '29779282-15a0-40c1-974d-baccaa6afa16', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f48aaca9e50>]}
[0m21:20:45.682734 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `relationships` defined on 'stg_orders' in
package 'data_pipeline_project' (models/staging/staging_schema.yml). Arguments
to generic tests should be nested under the `arguments` property.
[0m21:20:45.684362 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '29779282-15a0-40c1-974d-baccaa6afa16', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f48aacaadb0>]}
[0m21:20:45.926236 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.data_pipeline_project.fct_shipment' (models/facts/fct_shipment.sql) depends on a node named 'int_shipment_parsed' which was not found
[0m21:20:45.928016 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 17 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m21:20:45.929500 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 4.293303, "process_in_blocks": "0", "process_kernel_time": 0.858643, "process_mem_max_rss": "373668", "process_out_blocks": "8", "process_user_time": 5.246625}
[0m21:20:45.930428 [debug] [MainThread]: Command `dbt run` failed at 21:20:45.930325 after 4.29 seconds
[0m21:20:45.931150 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f48cb15a1b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f48aa849820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f48a9d5c770>]}
[0m21:20:45.931787 [debug] [MainThread]: Flushing usage events
[0m21:20:46.676550 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m21:22:05.326097 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7dc50dcc84d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7dc50fdd9e20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7dc510292570>]}


============================== 21:22:05.335145 | a1a04cd5-7df5-491e-9219-99feae775009 ==============================
[0m21:22:05.335145 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m21:22:05.338869 [debug] [MainThread]: running dbt with arguments {'warn_error': 'None', 'partial_parse': 'True', 'no_print': 'None', 'empty': 'None', 'log_format': 'default', 'printer_width': '80', 'debug': 'False', 'target_path': 'None', 'write_json': 'True', 'send_anonymous_usage_stats': 'True', 'introspect': 'True', 'version_check': 'True', 'static_parser': 'True', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'quiet': 'False', 'log_cache_events': 'False', 'indirect_selection': 'eager', 'invocation_command': 'dbt clean', 'fail_fast': 'False', 'cache_selected_only': 'False', 'profiles_dir': '/home/ecem/.dbt', 'use_colors': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_experimental_parser': 'False'}
[0m21:22:05.457188 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a1a04cd5-7df5-491e-9219-99feae775009', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7dc50d675d00>]}
[0m21:22:05.474905 [debug] [MainThread]: Resource report: {"command_name": "clean", "command_success": true, "command_wall_clock_time": 0.21669073, "process_in_blocks": "0", "process_kernel_time": 0.493509, "process_mem_max_rss": "101652", "process_out_blocks": "8", "process_user_time": 1.153658}
[0m21:22:05.475792 [debug] [MainThread]: Command `dbt clean` succeeded at 21:22:05.475683 after 0.22 seconds
[0m21:22:05.476406 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7dc50dbe9a30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7dc50e506060>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7dc50de98260>]}
[0m21:22:05.477126 [debug] [MainThread]: Flushing usage events
[0m21:22:06.459317 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m21:22:12.003859 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7110ac246780>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7110ad5daff0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7110ac0b3860>]}


============================== 21:22:12.010224 | 976398f0-5c6f-4939-bb90-e2117f542e76 ==============================
[0m21:22:12.010224 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m21:22:12.011310 [debug] [MainThread]: running dbt with arguments {'log_format': 'default', 'use_colors': 'True', 'empty': 'False', 'version_check': 'True', 'partial_parse': 'True', 'no_print': 'None', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'cache_selected_only': 'False', 'send_anonymous_usage_stats': 'True', 'fail_fast': 'False', 'warn_error': 'None', 'printer_width': '80', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'profiles_dir': '/home/ecem/.dbt', 'introspect': 'True', 'target_path': 'None', 'write_json': 'True', 'invocation_command': 'dbt run --select int_shipments_decomposed', 'quiet': 'False', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_cache_events': 'False', 'static_parser': 'True', 'debug': 'False'}
[0m21:22:14.265341 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '976398f0-5c6f-4939-bb90-e2117f542e76', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71108bc9dee0>]}
[0m21:22:14.337643 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '976398f0-5c6f-4939-bb90-e2117f542e76', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7110ad96a3f0>]}
[0m21:22:14.341211 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m21:22:14.607031 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m21:22:14.609835 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m21:22:14.610676 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '976398f0-5c6f-4939-bb90-e2117f542e76', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7110ab2336e0>]}
[0m21:22:15.918952 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `relationships` defined on 'stg_orders' in
package 'data_pipeline_project' (models/staging/staging_schema.yml). Arguments
to generic tests should be nested under the `arguments` property.
[0m21:22:15.921689 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '976398f0-5c6f-4939-bb90-e2117f542e76', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71108b775760>]}
[0m21:22:16.163796 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.data_pipeline_project.fct_shipment' (models/facts/fct_shipment.sql) depends on a node named 'int_shipments_decomposed' which was not found
[0m21:22:16.166339 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 17 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m21:22:16.167824 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 4.2319264, "process_in_blocks": "0", "process_kernel_time": 1.282683, "process_mem_max_rss": "373316", "process_out_blocks": "8", "process_user_time": 4.611024}
[0m21:22:16.168782 [debug] [MainThread]: Command `dbt run` failed at 21:22:16.168491 after 4.23 seconds
[0m21:22:16.169287 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7110ad1c8380>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71108adbea20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71108adbf9b0>]}
[0m21:22:16.169962 [debug] [MainThread]: Flushing usage events
[0m21:22:16.889034 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m21:27:59.357840 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x718b0d73cfe0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x718b0d20f920>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x718b0d68f860>]}


============================== 21:27:59.362749 | 070c39fb-8b3e-4abf-b380-c20e4b5ab5db ==============================
[0m21:27:59.362749 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m21:27:59.364012 [debug] [MainThread]: running dbt with arguments {'log_cache_events': 'False', 'invocation_command': 'dbt clean', 'debug': 'False', 'static_parser': 'True', 'no_print': 'None', 'empty': 'None', 'cache_selected_only': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_format': 'default', 'partial_parse': 'True', 'indirect_selection': 'eager', 'fail_fast': 'False', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'use_colors': 'True', 'profiles_dir': '/home/ecem/.dbt', 'quiet': 'False', 'write_json': 'True', 'target_path': 'None', 'printer_width': '80', 'use_experimental_parser': 'False', 'version_check': 'True', 'warn_error': 'None', 'send_anonymous_usage_stats': 'True', 'introspect': 'True'}
[0m21:27:59.508707 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '070c39fb-8b3e-4abf-b380-c20e4b5ab5db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x718b0cf04350>]}
[0m21:27:59.523754 [debug] [MainThread]: Resource report: {"command_name": "clean", "command_success": true, "command_wall_clock_time": 0.22914414, "process_in_blocks": "0", "process_kernel_time": 0.190797, "process_mem_max_rss": "101628", "process_out_blocks": "16", "process_user_time": 1.540408}
[0m21:27:59.524623 [debug] [MainThread]: Command `dbt clean` succeeded at 21:27:59.524229 after 0.23 seconds
[0m21:27:59.525217 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x718b0ce16b70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x718b0cd0bcb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x718b0dc7c050>]}
[0m21:27:59.525811 [debug] [MainThread]: Flushing usage events
[0m21:28:00.456477 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m21:28:43.052072 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74534a577e60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74534b624f80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74534b1b3fe0>]}


============================== 21:28:43.056368 | 5a14d569-d14c-486e-90a6-0901ee383432 ==============================
[0m21:28:43.056368 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m21:28:43.058507 [debug] [MainThread]: running dbt with arguments {'log_format': 'default', 'static_parser': 'True', 'use_colors': 'True', 'version_check': 'True', 'indirect_selection': 'eager', 'empty': 'False', 'log_cache_events': 'False', 'write_json': 'True', 'send_anonymous_usage_stats': 'True', 'invocation_command': 'dbt run --select int_shipments_decomposed', 'no_print': 'None', 'debug': 'False', 'warn_error': 'None', 'introspect': 'True', 'target_path': 'None', 'printer_width': '80', 'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_experimental_parser': 'False', 'partial_parse': 'True', 'quiet': 'False', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'profiles_dir': '/home/ecem/.dbt', 'cache_selected_only': 'False'}
[0m21:28:45.554887 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5a14d569-d14c-486e-90a6-0901ee383432', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74532a0244d0>]}
[0m21:28:45.620795 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5a14d569-d14c-486e-90a6-0901ee383432', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74532a3178f0>]}
[0m21:28:45.622159 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m21:28:45.968367 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m21:28:45.976114 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m21:28:45.978674 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '5a14d569-d14c-486e-90a6-0901ee383432', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74532a38cbf0>]}
[0m21:28:47.397012 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `relationships` defined on 'stg_orders' in
package 'data_pipeline_project' (models/staging/staging_schema.yml). Arguments
to generic tests should be nested under the `arguments` property.
[0m21:28:47.397946 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '5a14d569-d14c-486e-90a6-0901ee383432', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x745329de75c0>]}
[0m21:28:47.644137 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.data_pipeline_project.fct_shipment' (models/facts/fct_shipment.sql) depends on a node named 'int_shipments_decomposed' which was not found
[0m21:28:47.645659 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 17 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m21:28:47.648451 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 4.6503444, "process_in_blocks": "0", "process_kernel_time": 1.090472, "process_mem_max_rss": "373680", "process_out_blocks": "16", "process_user_time": 5.466208}
[0m21:28:47.649334 [debug] [MainThread]: Command `dbt run` failed at 21:28:47.649227 after 4.65 seconds
[0m21:28:47.650045 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7453497b4d40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x745349a89b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74532918f980>]}
[0m21:28:47.650657 [debug] [MainThread]: Flushing usage events
[0m21:28:48.324841 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m21:29:55.725291 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b0edfbd9430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b0ee1f26660>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b0edfda70b0>]}


============================== 21:29:55.729996 | bb1b911a-baad-4b4e-9c0d-0b4eb609679c ==============================
[0m21:29:55.729996 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m21:29:55.730928 [debug] [MainThread]: running dbt with arguments {'warn_error': 'None', 'invocation_command': 'dbt list --resource-type model', 'empty': 'None', 'no_print': 'None', 'log_format': 'default', 'debug': 'False', 'write_json': 'True', 'profiles_dir': '/home/ecem/.dbt', 'fail_fast': 'False', 'introspect': 'True', 'static_parser': 'True', 'version_check': 'True', 'indirect_selection': 'eager', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_experimental_parser': 'False', 'cache_selected_only': 'False', 'target_path': 'None', 'log_cache_events': 'False', 'use_colors': 'True', 'printer_width': '80', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'partial_parse': 'True'}
[0m21:29:57.956570 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'bb1b911a-baad-4b4e-9c0d-0b4eb609679c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b0ee1c43b60>]}
[0m21:29:58.036261 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'bb1b911a-baad-4b4e-9c0d-0b4eb609679c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b0ee1b0a270>]}
[0m21:29:58.038276 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m21:29:58.339561 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m21:29:58.341717 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m21:29:58.343727 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'bb1b911a-baad-4b4e-9c0d-0b4eb609679c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b0ebfdfc170>]}
[0m21:29:59.668410 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `relationships` defined on 'stg_orders' in
package 'data_pipeline_project' (models/staging/staging_schema.yml). Arguments
to generic tests should be nested under the `arguments` property.
[0m21:29:59.669496 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': 'bb1b911a-baad-4b4e-9c0d-0b4eb609679c', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b0ebf88d0a0>]}
[0m21:29:59.848834 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.data_pipeline_project.fct_shipment' (models/facts/fct_shipment.sql) depends on a node named 'int_shipments_decomposed' which was not found
[0m21:29:59.850057 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 17 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m21:29:59.852865 [debug] [MainThread]: Resource report: {"command_name": "list", "command_success": false, "command_wall_clock_time": 4.1954637, "process_in_blocks": "0", "process_kernel_time": 1.405215, "process_mem_max_rss": "373572", "process_out_blocks": "16", "process_user_time": 4.574219}
[0m21:29:59.853957 [debug] [MainThread]: Command `dbt list` failed at 21:29:59.853796 after 4.20 seconds
[0m21:29:59.854759 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b0ee1e82ff0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b0ebf913b30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b0ebf9762d0>]}
[0m21:29:59.855902 [debug] [MainThread]: Flushing usage events
[0m21:30:00.813191 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m21:30:34.879664 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78c757fb0f80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78c7573b43e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78c7579fa1e0>]}


============================== 21:30:34.885416 | d9cec356-6820-4065-9d85-b802caee7394 ==============================
[0m21:30:34.885416 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m21:30:34.886565 [debug] [MainThread]: running dbt with arguments {'no_print': 'None', 'quiet': 'False', 'write_json': 'True', 'empty': 'False', 'profiles_dir': '/home/ecem/.dbt', 'version_check': 'True', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'invocation_command': 'dbt run --select int_shipments_decomposed', 'static_parser': 'True', 'log_format': 'default', 'use_experimental_parser': 'False', 'use_colors': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'send_anonymous_usage_stats': 'True', 'target_path': 'None', 'indirect_selection': 'eager', 'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'introspect': 'True', 'debug': 'False', 'partial_parse': 'True', 'log_cache_events': 'False', 'printer_width': '80'}
[0m21:30:37.108678 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd9cec356-6820-4065-9d85-b802caee7394', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78c737ce9ca0>]}
[0m21:30:37.239764 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd9cec356-6820-4065-9d85-b802caee7394', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78c757dee480>]}
[0m21:30:37.240732 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m21:30:37.481806 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m21:30:37.482625 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m21:30:37.483182 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'd9cec356-6820-4065-9d85-b802caee7394', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78c737897650>]}
[0m21:30:38.845055 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `relationships` defined on 'stg_orders' in
package 'data_pipeline_project' (models/staging/staging_schema.yml). Arguments
to generic tests should be nested under the `arguments` property.
[0m21:30:38.845778 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': 'd9cec356-6820-4065-9d85-b802caee7394', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78c7373f6510>]}
[0m21:30:39.199310 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.data_pipeline_project.marts
[0m21:30:39.213969 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd9cec356-6820-4065-9d85-b802caee7394', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78c73696d610>]}
[0m21:30:39.368313 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m21:30:39.370813 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m21:30:39.387930 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd9cec356-6820-4065-9d85-b802caee7394', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78c7369c5ac0>]}
[0m21:30:39.388792 [info ] [MainThread]: Found 16 models, 35 data tests, 10 sources, 508 macros
[0m21:30:39.389720 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd9cec356-6820-4065-9d85-b802caee7394', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78c7369f1100>]}
[0m21:30:39.391647 [info ] [MainThread]: 
[0m21:30:39.392851 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m21:30:39.393649 [info ] [MainThread]: 
[0m21:30:39.394505 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m21:30:39.396552 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812'
[0m21:30:39.397326 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:30:40.788854 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812, now list_data-pipeline-project-474812_analytics_staging_staging)
[0m21:30:40.789510 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:30:40.823852 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_marts'
[0m21:30:40.824877 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:30:41.801890 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd9cec356-6820-4065-9d85-b802caee7394', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78c7374c01d0>]}
[0m21:30:41.802744 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:30:41.812159 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.int_shipments_decomposed
[0m21:30:41.813218 [info ] [Thread-1 (]: 1 of 1 START sql table model analytics_staging_staging.int_shipments_decomposed  [RUN]
[0m21:30:41.814146 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_marts, now model.data_pipeline_project.int_shipments_decomposed)
[0m21:30:41.814818 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.int_shipments_decomposed
[0m21:30:41.823543 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.int_shipments_decomposed"
[0m21:30:41.825053 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.int_shipments_decomposed
[0m21:30:41.882425 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.int_shipments_decomposed"
[0m21:30:41.884122 [debug] [Thread-1 (]: On model.data_pipeline_project.int_shipments_decomposed: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.int_shipments_decomposed"} */

  
    

    create or replace table `data-pipeline-project-474812`.`analytics_staging_staging`.`int_shipments_decomposed`
      
    
    

    
    OPTIONS()
    as (
      -- models/intermediate/int_shipments_decomposed.sql


with shipments as (
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_shipments`
),

parsed_shipments as (
    select
        shipment_id,
        order_id,
        user_id,
        created_at,
        
        -- Extract from details_json
        cast(json_extract_scalar(details_json, '$.deliveryDate') as timestamp) as delivered_at,
        cast(json_extract_scalar(details_json, '$.collectDate') as timestamp) as collect_date,
        
        -- Extract from label_json  
        json_extract_scalar(label_json, '$.provider') as carrier,
        
        -- Data quality checks
        case 
            when json_extract_scalar(details_json, '$.deliveryDate') is not null then 'DELIVERED'
            when json_extract_scalar(details_json, '$.collectDate') is not null then 'IN_TRANSIT'
            else 'UNKNOWN'
        end as inferred_status

    from shipments
    where shipment_id is not null
)

select * from parsed_shipments
    );
  
[0m21:30:41.885141 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:30:43.267096 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:19862897-9ff1-4546-a5de-3978752aa5be&page=queryresults
[0m21:30:43.546532 [error] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:19862897-9ff1-4546-a5de-3978752aa5be&page=queryresults
[0m21:30:43.570932 [debug] [Thread-1 (]: Database Error in model int_shipments_decomposed (models/intermediate/int_shipments_decomposed.sql)
  Unrecognized name: created_at; Did you mean collected_at? at [26:9]
  compiled code at target/run/data_pipeline_project/models/intermediate/int_shipments_decomposed.sql
[0m21:30:43.574122 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd9cec356-6820-4065-9d85-b802caee7394', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78c736ac6c90>]}
[0m21:30:43.575014 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model analytics_staging_staging.int_shipments_decomposed  [[31mERROR[0m in 1.76s]
[0m21:30:43.575739 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.int_shipments_decomposed
[0m21:30:43.577225 [debug] [Thread-7 (]: Marking all children of 'model.data_pipeline_project.int_shipments_decomposed' to be skipped because of status 'error'.  Reason: Database Error in model int_shipments_decomposed (models/intermediate/int_shipments_decomposed.sql)
  Unrecognized name: created_at; Did you mean collected_at? at [26:9]
  compiled code at target/run/data_pipeline_project/models/intermediate/int_shipments_decomposed.sql.
[0m21:30:43.583136 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:30:43.584714 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:30:43.585183 [debug] [MainThread]: Connection 'list_data-pipeline-project-474812_analytics_staging_staging' was properly closed.
[0m21:30:43.585571 [debug] [MainThread]: Connection 'model.data_pipeline_project.int_shipments_decomposed' was properly closed.
[0m21:30:43.585988 [info ] [MainThread]: 
[0m21:30:43.586729 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 4.19 seconds (4.19s).
[0m21:30:43.589093 [debug] [MainThread]: Command end result
[0m21:30:43.651246 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m21:30:43.656984 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m21:30:43.665976 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m21:30:43.667192 [info ] [MainThread]: 
[0m21:30:43.668656 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m21:30:43.669287 [info ] [MainThread]: 
[0m21:30:43.670048 [error] [MainThread]: [31mFailure in model int_shipments_decomposed (models/intermediate/int_shipments_decomposed.sql)[0m
[0m21:30:43.670845 [error] [MainThread]:   Database Error in model int_shipments_decomposed (models/intermediate/int_shipments_decomposed.sql)
  Unrecognized name: created_at; Did you mean collected_at? at [26:9]
  compiled code at target/run/data_pipeline_project/models/intermediate/int_shipments_decomposed.sql
[0m21:30:43.672103 [info ] [MainThread]: 
[0m21:30:43.673059 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_project/models/intermediate/int_shipments_decomposed.sql
[0m21:30:43.675342 [info ] [MainThread]: 
[0m21:30:43.682030 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m21:30:43.687197 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 17 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m21:30:43.690190 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 8.871752, "process_in_blocks": "0", "process_kernel_time": 1.145194, "process_mem_max_rss": "383940", "process_out_blocks": "4072", "process_user_time": 5.876655}
[0m21:30:43.691137 [debug] [MainThread]: Command `dbt run` failed at 21:30:43.690915 after 8.87 seconds
[0m21:30:43.692542 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78c7578d99d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78c7374c3e90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78c737998cb0>]}
[0m21:30:43.693370 [debug] [MainThread]: Flushing usage events
[0m21:30:44.398962 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m21:37:17.823611 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75ec653723f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75ec63673740>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75ec63741fa0>]}


============================== 21:37:17.827110 | fbaf03eb-2a61-431f-8e6d-fcb1f0bd83a3 ==============================
[0m21:37:17.827110 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m21:37:17.828344 [debug] [MainThread]: running dbt with arguments {'introspect': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'empty': 'None', 'log_cache_events': 'False', 'debug': 'False', 'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'version_check': 'True', 'profiles_dir': '/home/ecem/.dbt', 'write_json': 'True', 'printer_width': '80', 'use_experimental_parser': 'False', 'static_parser': 'True', 'target_path': 'None', 'indirect_selection': 'eager', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'quiet': 'False', 'send_anonymous_usage_stats': 'True', 'log_format': 'default', 'use_colors': 'True', 'invocation_command': 'dbt clean', 'no_print': 'None', 'partial_parse': 'True'}
[0m21:37:17.969754 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'fbaf03eb-2a61-431f-8e6d-fcb1f0bd83a3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75ec65243680>]}
[0m21:37:17.993397 [debug] [MainThread]: Resource report: {"command_name": "clean", "command_success": true, "command_wall_clock_time": 0.22495718, "process_in_blocks": "0", "process_kernel_time": 0.184457, "process_mem_max_rss": "101820", "process_out_blocks": "8", "process_user_time": 1.570523}
[0m21:37:17.994067 [debug] [MainThread]: Command `dbt clean` succeeded at 21:37:17.993959 after 0.23 seconds
[0m21:37:17.994492 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75ec637c49e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75ec62f53cb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75ec62954050>]}
[0m21:37:17.995183 [debug] [MainThread]: Flushing usage events
[0m21:37:18.952460 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m21:37:27.655983 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f00766c37a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0076dcb080>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f007717b980>]}


============================== 21:37:27.659758 | 151f6d30-7fc9-42ea-8526-b89c826df7f5 ==============================
[0m21:37:27.659758 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m21:37:27.660852 [debug] [MainThread]: running dbt with arguments {'cache_selected_only': 'False', 'profiles_dir': '/home/ecem/.dbt', 'no_print': 'None', 'empty': 'False', 'debug': 'False', 'printer_width': '80', 'quiet': 'False', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'log_format': 'default', 'fail_fast': 'False', 'invocation_command': 'dbt run --select int_shipments_decomposed', 'send_anonymous_usage_stats': 'True', 'static_parser': 'True', 'use_colors': 'True', 'indirect_selection': 'eager', 'partial_parse': 'True', 'log_cache_events': 'False', 'warn_error': 'None', 'use_experimental_parser': 'False', 'version_check': 'True', 'write_json': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None'}
[0m21:37:29.896029 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '151f6d30-7fc9-42ea-8526-b89c826df7f5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0056c14110>]}
[0m21:37:29.967696 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '151f6d30-7fc9-42ea-8526-b89c826df7f5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0078b03d70>]}
[0m21:37:29.970154 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m21:37:30.212021 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m21:37:30.218748 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m21:37:30.222759 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '151f6d30-7fc9-42ea-8526-b89c826df7f5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f005702f5c0>]}
[0m21:37:31.503654 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `relationships` defined on 'stg_orders' in
package 'data_pipeline_project' (models/staging/staging_schema.yml). Arguments
to generic tests should be nested under the `arguments` property.
[0m21:37:31.508306 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '151f6d30-7fc9-42ea-8526-b89c826df7f5', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f00568ed820>]}
[0m21:37:31.852376 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.data_pipeline_project.marts
[0m21:37:31.865161 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '151f6d30-7fc9-42ea-8526-b89c826df7f5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0055eaa4e0>]}
[0m21:37:31.997891 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m21:37:32.001155 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m21:37:32.020541 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '151f6d30-7fc9-42ea-8526-b89c826df7f5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0055c6bf80>]}
[0m21:37:32.021609 [info ] [MainThread]: Found 16 models, 35 data tests, 10 sources, 508 macros
[0m21:37:32.022168 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '151f6d30-7fc9-42ea-8526-b89c826df7f5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0055d8e420>]}
[0m21:37:32.024292 [info ] [MainThread]: 
[0m21:37:32.025091 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m21:37:32.025982 [info ] [MainThread]: 
[0m21:37:32.026936 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m21:37:32.028972 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812'
[0m21:37:32.029783 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:37:32.997597 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812, now list_data-pipeline-project-474812_analytics_staging_marts)
[0m21:37:33.001592 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:37:33.000870 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_staging'
[0m21:37:33.038509 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:37:33.958966 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '151f6d30-7fc9-42ea-8526-b89c826df7f5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0056758290>]}
[0m21:37:33.959848 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:37:33.967267 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.int_shipments_decomposed
[0m21:37:33.968188 [info ] [Thread-1 (]: 1 of 1 START sql view model analytics_staging_staging.int_shipments_decomposed . [RUN]
[0m21:37:33.968978 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now model.data_pipeline_project.int_shipments_decomposed)
[0m21:37:33.969433 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.int_shipments_decomposed
[0m21:37:33.978454 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.int_shipments_decomposed"
[0m21:37:33.979708 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.int_shipments_decomposed
[0m21:37:34.028380 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.int_shipments_decomposed"
[0m21:37:34.034617 [debug] [Thread-1 (]: On model.data_pipeline_project.int_shipments_decomposed: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.int_shipments_decomposed"} */


  create or replace view `data-pipeline-project-474812`.`analytics_staging_staging`.`int_shipments_decomposed`
  OPTIONS()
  as -- models/intermediate/int_shipments_decomposed.sql

with shipments as (
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_shipments`
),

parsed_shipments as (
    select
        shipment_id,
        order_id,
        user_id,
        
        -- Extract from details_json
        cast(json_extract_scalar(details_json, '$.deliveryDate') as timestamp) as delivered_at,
        cast(json_extract_scalar(details_json, '$.collectDate') as timestamp) as collect_date,
        
        -- Extract from label_json  
        json_extract_scalar(label_json, '$.provider') as carrier
        

    from shipments
    where shipment_id is not null
)

select * from parsed_shipments;


[0m21:37:34.035584 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:37:35.387928 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:14d00f9f-bf3d-4fb0-8270-7ed60824d720&page=queryresults
[0m21:37:36.055475 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '151f6d30-7fc9-42ea-8526-b89c826df7f5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0059a24650>]}
[0m21:37:36.056497 [info ] [Thread-1 (]: 1 of 1 OK created sql view model analytics_staging_staging.int_shipments_decomposed  [[32mCREATE VIEW (0 processed)[0m in 2.09s]
[0m21:37:36.058026 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.int_shipments_decomposed
[0m21:37:36.064148 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:37:36.066053 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:37:36.066558 [debug] [MainThread]: Connection 'list_data-pipeline-project-474812_analytics_staging_marts' was properly closed.
[0m21:37:36.067622 [debug] [MainThread]: Connection 'model.data_pipeline_project.int_shipments_decomposed' was properly closed.
[0m21:37:36.068148 [info ] [MainThread]: 
[0m21:37:36.068623 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 4.04 seconds (4.04s).
[0m21:37:36.070000 [debug] [MainThread]: Command end result
[0m21:37:36.112259 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m21:37:36.115082 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m21:37:36.123056 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m21:37:36.123962 [info ] [MainThread]: 
[0m21:37:36.124813 [info ] [MainThread]: [32mCompleted successfully[0m
[0m21:37:36.125442 [info ] [MainThread]: 
[0m21:37:36.126262 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m21:37:36.127345 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 17 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m21:37:36.129338 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 8.526931, "process_in_blocks": "0", "process_kernel_time": 2.124449, "process_mem_max_rss": "387520", "process_out_blocks": "4048", "process_user_time": 4.743022}
[0m21:37:36.130830 [debug] [MainThread]: Command `dbt run` succeeded at 21:37:36.130636 after 8.53 seconds
[0m21:37:36.137168 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0076a99520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0055cb99a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0055cba4e0>]}
[0m21:37:36.138088 [debug] [MainThread]: Flushing usage events
[0m21:37:36.867693 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m21:37:54.574157 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x748d286c1340>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x748d28a3e0c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x748d2a5c6630>]}


============================== 21:37:54.579896 | d78baca8-f9de-4350-ae5a-1203821d46db ==============================
[0m21:37:54.579896 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m21:37:54.580912 [debug] [MainThread]: running dbt with arguments {'use_colors': 'True', 'partial_parse': 'True', 'use_experimental_parser': 'False', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'introspect': 'True', 'invocation_command': 'dbt run --select fct_shipment', 'profiles_dir': '/home/ecem/.dbt', 'static_parser': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'write_json': 'True', 'printer_width': '80', 'log_format': 'default', 'version_check': 'True', 'debug': 'False', 'quiet': 'False', 'fail_fast': 'False', 'target_path': 'None', 'no_print': 'None', 'empty': 'False', 'send_anonymous_usage_stats': 'True'}
[0m21:37:57.077224 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd78baca8-f9de-4350-ae5a-1203821d46db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x748d0a6f4f20>]}
[0m21:37:57.140688 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd78baca8-f9de-4350-ae5a-1203821d46db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x748d2851fd70>]}
[0m21:37:57.142018 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m21:37:57.394022 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m21:37:57.666007 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:37:57.666771 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:37:57.676110 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.data_pipeline_project.marts
[0m21:37:57.723241 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd78baca8-f9de-4350-ae5a-1203821d46db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x748d085336e0>]}
[0m21:37:57.842495 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m21:37:57.845011 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m21:37:57.863108 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd78baca8-f9de-4350-ae5a-1203821d46db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x748d08361a90>]}
[0m21:37:57.864058 [info ] [MainThread]: Found 16 models, 35 data tests, 10 sources, 508 macros
[0m21:37:57.864743 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd78baca8-f9de-4350-ae5a-1203821d46db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x748d299742f0>]}
[0m21:37:57.866793 [info ] [MainThread]: 
[0m21:37:57.867661 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m21:37:57.868174 [info ] [MainThread]: 
[0m21:37:57.869009 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m21:37:57.870456 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812'
[0m21:37:57.872842 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:37:58.786609 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812, now list_data-pipeline-project-474812_analytics_staging_staging)
[0m21:37:58.790028 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:37:58.825389 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_marts'
[0m21:37:58.826760 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:37:59.745461 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd78baca8-f9de-4350-ae5a-1203821d46db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x748d08531640>]}
[0m21:37:59.746851 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:37:59.758000 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.fct_shipment
[0m21:37:59.758999 [info ] [Thread-1 (]: 1 of 1 START sql table model analytics_staging_marts.fct_shipment .............. [RUN]
[0m21:37:59.760922 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_marts, now model.data_pipeline_project.fct_shipment)
[0m21:37:59.761455 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.fct_shipment
[0m21:37:59.771026 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.fct_shipment"
[0m21:37:59.773850 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.fct_shipment
[0m21:37:59.841316 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.fct_shipment"
[0m21:37:59.843275 [debug] [Thread-1 (]: On model.data_pipeline_project.fct_shipment: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.fct_shipment"} */

  
    

    create or replace table `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_shipment`
      
    
    

    
    OPTIONS()
    as (
      -- models/facts/fct_shipment.sql


with parsed_shipments as (
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`int_shipments_decomposed`  
),

orders as (
    select 
        order_id,
        order_status as latest_status
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_orders`
),

final as (
    select
        -- Primary Key
        ps.shipment_id,
        
        -- Foreign Key
        ps.order_id,
        
        -- Status (from orders table)
        o.latest_status,
        
        -- Delivery timestamp
        ps.delivered_at,
        
        -- Carrier information
        ps.carrier

    from parsed_shipments ps
    left join orders o 
        on ps.order_id = o.order_id
)

select * from final
    );
  
[0m21:37:59.844408 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:38:01.181617 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:006b97dc-5f4a-43ea-a09c-9a74a37195e7&page=queryresults
[0m21:38:04.054247 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd78baca8-f9de-4350-ae5a-1203821d46db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x748d0b464920>]}
[0m21:38:04.057270 [info ] [Thread-1 (]: 1 of 1 OK created sql table model analytics_staging_marts.fct_shipment ......... [[32mCREATE TABLE (2.3k rows, 1.4 MiB processed)[0m in 4.29s]
[0m21:38:04.059310 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.fct_shipment
[0m21:38:04.063165 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:38:04.065651 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:38:04.066229 [debug] [MainThread]: Connection 'list_data-pipeline-project-474812_analytics_staging_staging' was properly closed.
[0m21:38:04.066915 [debug] [MainThread]: Connection 'model.data_pipeline_project.fct_shipment' was properly closed.
[0m21:38:04.067514 [info ] [MainThread]: 
[0m21:38:04.068242 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 6.20 seconds (6.20s).
[0m21:38:04.069242 [debug] [MainThread]: Command end result
[0m21:38:04.131832 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m21:38:04.139877 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m21:38:04.155185 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m21:38:04.158308 [info ] [MainThread]: 
[0m21:38:04.159334 [info ] [MainThread]: [32mCompleted successfully[0m
[0m21:38:04.160240 [info ] [MainThread]: 
[0m21:38:04.162796 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m21:38:04.166392 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 9.651992, "process_in_blocks": "0", "process_kernel_time": 1.217417, "process_mem_max_rss": "380540", "process_out_blocks": "2816", "process_user_time": 4.898887}
[0m21:38:04.167606 [debug] [MainThread]: Command `dbt run` succeeded at 21:38:04.167417 after 9.65 seconds
[0m21:38:04.168539 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x748d2a5c6630>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x748d27d31d00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x748d29bb9580>]}
[0m21:38:04.169365 [debug] [MainThread]: Flushing usage events
[0m21:38:04.892201 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:04:50.250057 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72e5d0d3a1e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72e5d0ad4f80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72e5d0ad4530>]}


============================== 22:04:50.254188 | 39ba694d-9a91-428f-bbef-90071d2b752c ==============================
[0m22:04:50.254188 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m22:04:50.255789 [debug] [MainThread]: running dbt with arguments {'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'no_print': 'None', 'introspect': 'True', 'log_cache_events': 'False', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'log_format': 'default', 'cache_selected_only': 'False', 'target_path': 'None', 'debug': 'False', 'indirect_selection': 'eager', 'empty': 'None', 'profiles_dir': '/home/ecem/.dbt', 'printer_width': '80', 'use_experimental_parser': 'False', 'write_json': 'True', 'version_check': 'True', 'static_parser': 'True', 'invocation_command': 'dbt clean', 'send_anonymous_usage_stats': 'True', 'use_colors': 'True'}
[0m22:04:50.410341 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '39ba694d-9a91-428f-bbef-90071d2b752c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72e5d0e64830>]}
[0m22:04:50.427643 [debug] [MainThread]: Resource report: {"command_name": "clean", "command_success": true, "command_wall_clock_time": 0.25184032, "process_in_blocks": "0", "process_kernel_time": 0.207852, "process_mem_max_rss": "101564", "process_out_blocks": "8", "process_user_time": 1.632793}
[0m22:04:50.428437 [debug] [MainThread]: Command `dbt clean` succeeded at 22:04:50.428326 after 0.25 seconds
[0m22:04:50.428847 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72e5d002bd70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72e5cfefbcb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72e5cfefb500>]}
[0m22:04:50.429254 [debug] [MainThread]: Flushing usage events
[0m22:04:51.419041 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:05:03.657956 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ace8cfc8a40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ace8cfca300>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ace8cda9130>]}


============================== 22:05:03.664295 | 0c1f62a5-c4c8-4ba2-88a5-f4e8696102db ==============================
[0m22:05:03.664295 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m22:05:03.673706 [debug] [MainThread]: running dbt with arguments {'invocation_command': 'dbt run --select int_shipments_decomposed --full-refresh', 'cache_selected_only': 'False', 'static_parser': 'True', 'use_colors': 'True', 'log_format': 'default', 'use_experimental_parser': 'False', 'warn_error': 'None', 'send_anonymous_usage_stats': 'True', 'empty': 'False', 'no_print': 'None', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'write_json': 'True', 'version_check': 'True', 'introspect': 'True', 'quiet': 'False', 'debug': 'False', 'partial_parse': 'True', 'indirect_selection': 'eager', 'printer_width': '80', 'target_path': 'None', 'profiles_dir': '/home/ecem/.dbt', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_cache_events': 'False', 'fail_fast': 'False'}
[0m22:05:06.051218 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0c1f62a5-c4c8-4ba2-88a5-f4e8696102db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ace8eecbf50>]}
[0m22:05:06.115713 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '0c1f62a5-c4c8-4ba2-88a5-f4e8696102db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ace6d01f7d0>]}
[0m22:05:06.119534 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m22:05:06.401557 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m22:05:06.402830 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m22:05:06.403394 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '0c1f62a5-c4c8-4ba2-88a5-f4e8696102db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ace6d0f4620>]}
[0m22:05:07.715626 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `relationships` defined on 'stg_orders' in
package 'data_pipeline_project' (models/staging/staging_schema.yml). Arguments
to generic tests should be nested under the `arguments` property.
[0m22:05:07.718666 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '0c1f62a5-c4c8-4ba2-88a5-f4e8696102db', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ace6cccd880>]}
[0m22:05:08.042348 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.data_pipeline_project.marts
[0m22:05:08.059524 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0c1f62a5-c4c8-4ba2-88a5-f4e8696102db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ace6c2c4e00>]}
[0m22:05:08.199203 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m22:05:08.203401 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m22:05:08.221620 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0c1f62a5-c4c8-4ba2-88a5-f4e8696102db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ace6c2c7380>]}
[0m22:05:08.222519 [info ] [MainThread]: Found 16 models, 35 data tests, 10 sources, 508 macros
[0m22:05:08.223273 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0c1f62a5-c4c8-4ba2-88a5-f4e8696102db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ace6ccf1430>]}
[0m22:05:08.225708 [info ] [MainThread]: 
[0m22:05:08.226678 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m22:05:08.227144 [info ] [MainThread]: 
[0m22:05:08.227987 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m22:05:08.230165 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812'
[0m22:05:08.231708 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:05:09.250255 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812, now list_data-pipeline-project-474812_analytics_staging_marts)
[0m22:05:09.251012 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:05:09.281700 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_staging'
[0m22:05:09.285312 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:05:10.217778 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0c1f62a5-c4c8-4ba2-88a5-f4e8696102db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ace6c2d35f0>]}
[0m22:05:10.218390 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:05:10.229237 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.int_shipments_decomposed
[0m22:05:10.230189 [info ] [Thread-1 (]: 1 of 1 START sql view model analytics_staging_staging.int_shipments_decomposed . [RUN]
[0m22:05:10.231044 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_marts, now model.data_pipeline_project.int_shipments_decomposed)
[0m22:05:10.231509 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.int_shipments_decomposed
[0m22:05:10.239575 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.int_shipments_decomposed"
[0m22:05:10.241130 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.int_shipments_decomposed
[0m22:05:10.285930 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.int_shipments_decomposed"
[0m22:05:10.292026 [debug] [Thread-1 (]: On model.data_pipeline_project.int_shipments_decomposed: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.int_shipments_decomposed"} */


  create or replace view `data-pipeline-project-474812`.`analytics_staging_staging`.`int_shipments_decomposed`
  OPTIONS()
  as -- models/intermediate/int_shipments_decomposed.sql
with shipments as (
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_shipments`
),

orders as (
    select 
        order_id,
        order_status as latest_status
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_orders`
),

-- Extract datetime components from the Python datetime strings
datetime_extracted as (
    select
        *,
        -- Extract delivery date components from the Python datetime string
        regexp_extract(details_json, r"deliveryDate': datetime\\.datetime\\((\\d+), (\\d+), (\\d+), (\\d+), (\\d+), (\\d+)\\)") as delivery_components,
        regexp_extract(details_json, r"collectDate': datetime\\.datetime\\((\\d+), (\\d+), (\\d+), (\\d+), (\\d+), (\\d+)\\)") as collect_components
    from shipments
),

parsed_shipments as (
    select
        s.shipment_id,
        s.order_id,
        s.user_id,
        
        -- Get latest_status from orders table
        o.latest_status,
        
        -- Build timestamp from extracted datetime components
        case
            when delivery_components is not null then
                timestamp(
                    cast(regexp_extract(delivery_components, r'^(\\d+)') as int64),  -- year
                    cast(regexp_extract(delivery_components, r'^\\d+, (\\d+)') as int64),  -- month
                    cast(regexp_extract(delivery_components, r'^\\d+, \\d+, (\\d+)') as int64),  -- day
                    cast(regexp_extract(delivery_components, r'^\\d+, \\d+, \\d+, (\\d+)') as int64),  -- hour
                    cast(regexp_extract(delivery_components, r'^\\d+, \\d+, \\d+, \\d+, (\\d+)') as int64),  -- minute
                    cast(regexp_extract(delivery_components, r'^\\d+, \\d+, \\d+, \\d+, \\d+, (\\d+)') as int64)   -- second
                )
            else null
        end as delivered_at,
        
        -- Build collect date from components
        case
            when collect_components is not null then
                timestamp(
                    cast(regexp_extract(collect_components, r'^(\\d+)') as int64),
                    cast(regexp_extract(collect_components, r'^\\d+, (\\d+)') as int64),
                    cast(regexp_extract(collect_components, r'^\\d+, \\d+, (\\d+)') as int64),
                    cast(regexp_extract(collect_components, r'^\\d+, \\d+, \\d+, (\\d+)') as int64),
                    cast(regexp_extract(collect_components, r'^\\d+, \\d+, \\d+, \\d+, (\\d+)') as int64),
                    cast(regexp_extract(collect_components, r'^\\d+, \\d+, \\d+, \\d+, \\d+, (\\d+)') as int64)
                )
            else null
        end as collect_date,
        
        -- Extract carrier information
        json_extract_scalar(s.label_json, '$.provider') as carrier

    from datetime_extracted s
    left join orders o on s.order_id = o.order_id
    where s.shipment_id is not null
)

select * from parsed_shipments;


[0m22:05:10.294899 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:05:11.657604 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:936675dd-831a-40f5-9d7e-9c559d01c54a&page=queryresults
[0m22:05:11.944143 [error] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:936675dd-831a-40f5-9d7e-9c559d01c54a&page=queryresults
[0m22:05:11.983037 [debug] [Thread-1 (]: Database Error in model int_shipments_decomposed (models/intermediate/int_shipments_decomposed.sql)
  No matching signature for function TIMESTAMP
    Argument types: INT64, INT64, INT64, INT64, INT64, INT64
    Signature: TIMESTAMP(STRING, [STRING])
      Signature accepts at most 2 arguments, found 6 arguments
    Signature: TIMESTAMP(DATE, [STRING])
      Signature accepts at most 2 arguments, found 6 arguments
    Signature: TIMESTAMP(DATETIME, [STRING])
      Signature accepts at most 2 arguments, found 6 arguments
    Signature: TIMESTAMP(TIMESTAMP)
      Signature accepts at most 1 argument, found 6 arguments at [40:17]
  compiled code at target/run/data_pipeline_project/models/intermediate/int_shipments_decomposed.sql
[0m22:05:11.986803 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0c1f62a5-c4c8-4ba2-88a5-f4e8696102db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ace5fe34c50>]}
[0m22:05:11.988180 [error] [Thread-1 (]: 1 of 1 ERROR creating sql view model analytics_staging_staging.int_shipments_decomposed  [[31mERROR[0m in 1.75s]
[0m22:05:11.989542 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.int_shipments_decomposed
[0m22:05:11.991118 [debug] [Thread-7 (]: Marking all children of 'model.data_pipeline_project.int_shipments_decomposed' to be skipped because of status 'error'.  Reason: Database Error in model int_shipments_decomposed (models/intermediate/int_shipments_decomposed.sql)
  No matching signature for function TIMESTAMP
    Argument types: INT64, INT64, INT64, INT64, INT64, INT64
    Signature: TIMESTAMP(STRING, [STRING])
      Signature accepts at most 2 arguments, found 6 arguments
    Signature: TIMESTAMP(DATE, [STRING])
      Signature accepts at most 2 arguments, found 6 arguments
    Signature: TIMESTAMP(DATETIME, [STRING])
      Signature accepts at most 2 arguments, found 6 arguments
    Signature: TIMESTAMP(TIMESTAMP)
      Signature accepts at most 1 argument, found 6 arguments at [40:17]
  compiled code at target/run/data_pipeline_project/models/intermediate/int_shipments_decomposed.sql.
[0m22:05:12.000114 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:05:12.002300 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:05:12.002802 [debug] [MainThread]: Connection 'model.data_pipeline_project.int_shipments_decomposed' was properly closed.
[0m22:05:12.003189 [debug] [MainThread]: Connection 'list_data-pipeline-project-474812_analytics_staging_staging' was properly closed.
[0m22:05:12.003609 [info ] [MainThread]: 
[0m22:05:12.004233 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 3.78 seconds (3.78s).
[0m22:05:12.005189 [debug] [MainThread]: Command end result
[0m22:05:12.063720 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m22:05:12.067477 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m22:05:12.076915 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m22:05:12.077723 [info ] [MainThread]: 
[0m22:05:12.079012 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m22:05:12.080102 [info ] [MainThread]: 
[0m22:05:12.082176 [error] [MainThread]: [31mFailure in model int_shipments_decomposed (models/intermediate/int_shipments_decomposed.sql)[0m
[0m22:05:12.083545 [error] [MainThread]:   Database Error in model int_shipments_decomposed (models/intermediate/int_shipments_decomposed.sql)
  No matching signature for function TIMESTAMP
    Argument types: INT64, INT64, INT64, INT64, INT64, INT64
    Signature: TIMESTAMP(STRING, [STRING])
      Signature accepts at most 2 arguments, found 6 arguments
    Signature: TIMESTAMP(DATE, [STRING])
      Signature accepts at most 2 arguments, found 6 arguments
    Signature: TIMESTAMP(DATETIME, [STRING])
      Signature accepts at most 2 arguments, found 6 arguments
    Signature: TIMESTAMP(TIMESTAMP)
      Signature accepts at most 1 argument, found 6 arguments at [40:17]
  compiled code at target/run/data_pipeline_project/models/intermediate/int_shipments_decomposed.sql
[0m22:05:12.085076 [info ] [MainThread]: 
[0m22:05:12.087112 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_project/models/intermediate/int_shipments_decomposed.sql
[0m22:05:12.088986 [info ] [MainThread]: 
[0m22:05:12.090491 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m22:05:12.092635 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 17 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m22:05:12.094340 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 8.491128, "process_in_blocks": "0", "process_kernel_time": 1.687974, "process_mem_max_rss": "384024", "process_out_blocks": "4104", "process_user_time": 5.446104}
[0m22:05:12.095435 [debug] [MainThread]: Command `dbt run` failed at 22:05:12.095230 after 8.49 seconds
[0m22:05:12.096130 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ace8cab0ef0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ace5ff7cef0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ace6c1403e0>]}
[0m22:05:12.097157 [debug] [MainThread]: Flushing usage events
[0m22:05:12.866342 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:06:32.025190 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72b13186bb30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72b13186bb00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72b132876ba0>]}


============================== 22:06:32.037399 | 6c602765-be80-4185-9179-55f5cf6115ce ==============================
[0m22:06:32.037399 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m22:06:32.039810 [debug] [MainThread]: running dbt with arguments {'write_json': 'True', 'partial_parse': 'True', 'warn_error': 'None', 'log_cache_events': 'False', 'target_path': 'None', 'quiet': 'False', 'debug': 'False', 'fail_fast': 'False', 'log_format': 'default', 'profiles_dir': '/home/ecem/.dbt', 'static_parser': 'True', 'no_print': 'None', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'version_check': 'True', 'introspect': 'True', 'empty': 'None', 'invocation_command': 'dbt clean', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'printer_width': '80', 'send_anonymous_usage_stats': 'True', 'use_colors': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'cache_selected_only': 'False'}
[0m22:06:32.263339 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6c602765-be80-4185-9179-55f5cf6115ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72b1309e2a20>]}
[0m22:06:32.280983 [debug] [MainThread]: Resource report: {"command_name": "clean", "command_success": true, "command_wall_clock_time": 0.3110179, "process_in_blocks": "0", "process_kernel_time": 0.405591, "process_mem_max_rss": "101584", "process_out_blocks": "16", "process_user_time": 1.385114}
[0m22:06:32.282017 [debug] [MainThread]: Command `dbt clean` succeeded at 22:06:32.281733 after 0.31 seconds
[0m22:06:32.282502 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72b130e6b9e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72b131197170>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72b1310f0260>]}
[0m22:06:32.283393 [debug] [MainThread]: Flushing usage events
[0m22:06:33.236585 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:06:38.540953 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7822cf16d8b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7822cef85ca0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7822cedc0290>]}


============================== 22:06:38.547727 | 5d866639-ae48-4a82-862f-d281f0544750 ==============================
[0m22:06:38.547727 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m22:06:38.550818 [debug] [MainThread]: running dbt with arguments {'write_json': 'True', 'use_colors': 'True', 'target_path': 'None', 'log_cache_events': 'False', 'log_format': 'default', 'debug': 'False', 'invocation_command': 'dbt run --select int_shipments_decomposed --full-refresh', 'use_experimental_parser': 'False', 'send_anonymous_usage_stats': 'True', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'fail_fast': 'False', 'indirect_selection': 'eager', 'no_print': 'None', 'version_check': 'True', 'quiet': 'False', 'partial_parse': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'profiles_dir': '/home/ecem/.dbt', 'cache_selected_only': 'False', 'warn_error': 'None', 'static_parser': 'True', 'introspect': 'True', 'empty': 'False', 'printer_width': '80'}
[0m22:06:40.844501 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5d866639-ae48-4a82-862f-d281f0544750', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7822ce0f9220>]}
[0m22:06:40.964851 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5d866639-ae48-4a82-862f-d281f0544750', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7822ce0fab40>]}
[0m22:06:40.965652 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m22:06:41.213043 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m22:06:41.214728 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m22:06:41.216105 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '5d866639-ae48-4a82-862f-d281f0544750', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7822ce8bf620>]}
[0m22:06:42.543741 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `relationships` defined on 'stg_orders' in
package 'data_pipeline_project' (models/staging/staging_schema.yml). Arguments
to generic tests should be nested under the `arguments` property.
[0m22:06:42.547072 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '5d866639-ae48-4a82-862f-d281f0544750', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7822ae485220>]}
[0m22:06:42.875195 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.data_pipeline_project.marts
[0m22:06:42.888350 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5d866639-ae48-4a82-862f-d281f0544750', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7822aebe5d60>]}
[0m22:06:43.025532 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m22:06:43.028657 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m22:06:43.048979 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5d866639-ae48-4a82-862f-d281f0544750', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7822adbdc950>]}
[0m22:06:43.049805 [info ] [MainThread]: Found 16 models, 35 data tests, 10 sources, 508 macros
[0m22:06:43.050331 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5d866639-ae48-4a82-862f-d281f0544750', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7822adc6cb90>]}
[0m22:06:43.052809 [info ] [MainThread]: 
[0m22:06:43.053601 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m22:06:43.054156 [info ] [MainThread]: 
[0m22:06:43.055100 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m22:06:43.056875 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812'
[0m22:06:43.057564 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:06:43.992733 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812, now list_data-pipeline-project-474812_analytics_staging_staging)
[0m22:06:43.995034 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_marts'
[0m22:06:43.997013 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:06:43.998492 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:06:44.958154 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5d866639-ae48-4a82-862f-d281f0544750', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7822cf319100>]}
[0m22:06:44.958721 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:06:44.967880 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.int_shipments_decomposed
[0m22:06:44.969145 [info ] [Thread-1 (]: 1 of 1 START sql view model analytics_staging_staging.int_shipments_decomposed . [RUN]
[0m22:06:44.970922 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_marts, now model.data_pipeline_project.int_shipments_decomposed)
[0m22:06:44.971434 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.int_shipments_decomposed
[0m22:06:44.987087 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.int_shipments_decomposed"
[0m22:06:44.988235 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.int_shipments_decomposed
[0m22:06:45.035244 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.int_shipments_decomposed"
[0m22:06:45.038684 [debug] [Thread-1 (]: On model.data_pipeline_project.int_shipments_decomposed: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.int_shipments_decomposed"} */


  create or replace view `data-pipeline-project-474812`.`analytics_staging_staging`.`int_shipments_decomposed`
  OPTIONS()
  as -- models/intermediate/int_shipments_decomposed.sql
with shipments as (
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_shipments`
),

orders as (
    select 
        order_id,
        order_status as latest_status
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_orders`
),

-- Extract datetime components from the Python datetime strings
datetime_extracted as (
    select
        *,
        -- Extract delivery date components
        regexp_extract(details_json, r"deliveryDate': datetime\\.datetime\\((\\d+), (\\d+), (\\d+), (\\d+), (\\d+), (\\d+)\\)") as delivery_components,
        regexp_extract(details_json, r"collectDate': datetime\\.datetime\\((\\d+), (\\d+), (\\d+), (\\d+), (\\d+), (\\d+)\\)") as collect_components
    from shipments
),

parsed_shipments as (
    select
        s.shipment_id,
        s.order_id,
        s.user_id,
        
        -- Get latest_status from orders table
        o.latest_status,
        
        -- Build datetime from extracted components using DATETIME constructor
        case
            when delivery_components is not null then
                datetime(
                    cast(regexp_extract(delivery_components, r'^(\\d+)') as int64),  -- year
                    cast(regexp_extract(delivery_components, r'^\\d+, (\\d+)') as int64),  -- month
                    cast(regexp_extract(delivery_components, r'^\\d+, \\d+, (\\d+)') as int64),  -- day
                    cast(regexp_extract(delivery_components, r'^\\d+, \\d+, \\d+, (\\d+)') as int64),  -- hour
                    cast(regexp_extract(delivery_components, r'^\\d+, \\d+, \\d+, \\d+, (\\d+)') as int64),  -- minute
                    cast(regexp_extract(delivery_components, r'^\\d+, \\d+, \\d+, \\d+, \\d+, (\\d+)') as int64)   -- second
                )
            else null
        end as delivered_at,
        
        -- Build collect date from components
        case
            when collect_components is not null then
                datetime(
                    cast(regexp_extract(collect_components, r'^(\\d+)') as int64),
                    cast(regexp_extract(collect_components, r'^\\d+, (\\d+)') as int64),
                    cast(regexp_extract(collect_components, r'^\\d+, \\d+, (\\d+)') as int64),
                    cast(regexp_extract(collect_components, r'^\\d+, \\d+, \\d+, (\\d+)') as int64),
                    cast(regexp_extract(collect_components, r'^\\d+, \\d+, \\d+, \\d+, (\\d+)') as int64),
                    cast(regexp_extract(collect_components, r'^\\d+, \\d+, \\d+, \\d+, \\d+, (\\d+)') as int64)
                )
            else null
        end as collect_date,
        
        -- Extract carrier information
        json_extract_scalar(s.label_json, '$.provider') as carrier,

        -- Debug fields
        delivery_components,
        collect_components

    from datetime_extracted s
    left join orders o on s.order_id = o.order_id
    where s.shipment_id is not null
)

select * from parsed_shipments;


[0m22:06:45.042279 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:06:46.256517 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:6e566d84-6f69-4fda-94aa-150ecc0038a7&page=queryresults
[0m22:06:46.913180 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5d866639-ae48-4a82-862f-d281f0544750', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7822b1754530>]}
[0m22:06:46.915207 [info ] [Thread-1 (]: 1 of 1 OK created sql view model analytics_staging_staging.int_shipments_decomposed  [[32mCREATE VIEW (0 processed)[0m in 1.94s]
[0m22:06:46.919997 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.int_shipments_decomposed
[0m22:06:46.924036 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:06:46.926301 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:06:46.927106 [debug] [MainThread]: Connection 'list_data-pipeline-project-474812_analytics_staging_staging' was properly closed.
[0m22:06:46.927744 [debug] [MainThread]: Connection 'model.data_pipeline_project.int_shipments_decomposed' was properly closed.
[0m22:06:46.928182 [info ] [MainThread]: 
[0m22:06:46.928902 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 3.87 seconds (3.87s).
[0m22:06:46.930019 [debug] [MainThread]: Command end result
[0m22:06:46.972624 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m22:06:46.978691 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m22:06:47.000643 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m22:06:47.002748 [info ] [MainThread]: 
[0m22:06:47.004054 [info ] [MainThread]: [32mCompleted successfully[0m
[0m22:06:47.004554 [info ] [MainThread]: 
[0m22:06:47.005933 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m22:06:47.007439 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 17 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m22:06:47.009712 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 8.529628, "process_in_blocks": "0", "process_kernel_time": 1.036894, "process_mem_max_rss": "387656", "process_out_blocks": "4096", "process_user_time": 6.147099}
[0m22:06:47.013852 [debug] [MainThread]: Command `dbt run` succeeded at 22:06:47.013668 after 8.53 seconds
[0m22:06:47.014748 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7822ce0fa150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7822adcaa300>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7822aeaae930>]}
[0m22:06:47.015429 [debug] [MainThread]: Flushing usage events
[0m22:06:47.855015 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:07:08.926912 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cd69996d5e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cd69ab54140>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cd69a82fec0>]}


============================== 22:07:08.931427 | 469af7b1-953e-4d71-b88f-aa3e2b9e8d6f ==============================
[0m22:07:08.931427 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m22:07:08.932661 [debug] [MainThread]: running dbt with arguments {'log_format': 'default', 'invocation_command': 'dbt run --select fct_shipment --full-refresh', 'partial_parse': 'True', 'log_cache_events': 'False', 'version_check': 'True', 'fail_fast': 'False', 'no_print': 'None', 'use_experimental_parser': 'False', 'static_parser': 'True', 'use_colors': 'True', 'quiet': 'False', 'introspect': 'True', 'warn_error': 'None', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'empty': 'False', 'indirect_selection': 'eager', 'cache_selected_only': 'False', 'printer_width': '80', 'profiles_dir': '/home/ecem/.dbt', 'target_path': 'None', 'write_json': 'True', 'send_anonymous_usage_stats': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'debug': 'False'}
[0m22:07:11.277895 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '469af7b1-953e-4d71-b88f-aa3e2b9e8d6f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cd6794b3830>]}
[0m22:07:11.340285 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '469af7b1-953e-4d71-b88f-aa3e2b9e8d6f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cd69b042330>]}
[0m22:07:11.341076 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m22:07:11.591789 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m22:07:11.850506 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:07:11.851026 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:07:11.862849 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.data_pipeline_project.marts
[0m22:07:11.908356 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '469af7b1-953e-4d71-b88f-aa3e2b9e8d6f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cd679105310>]}
[0m22:07:12.022621 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m22:07:12.026716 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m22:07:12.046425 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '469af7b1-953e-4d71-b88f-aa3e2b9e8d6f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cd678d3b260>]}
[0m22:07:12.047182 [info ] [MainThread]: Found 16 models, 35 data tests, 10 sources, 508 macros
[0m22:07:12.047908 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '469af7b1-953e-4d71-b88f-aa3e2b9e8d6f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cd6791209e0>]}
[0m22:07:12.050326 [info ] [MainThread]: 
[0m22:07:12.051138 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m22:07:12.051931 [info ] [MainThread]: 
[0m22:07:12.052803 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m22:07:12.054208 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812'
[0m22:07:12.055060 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:07:12.902546 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812, now list_data-pipeline-project-474812_analytics_staging_marts)
[0m22:07:12.904323 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_staging'
[0m22:07:12.905120 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:07:12.906317 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:07:13.864519 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '469af7b1-953e-4d71-b88f-aa3e2b9e8d6f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cd6794d24e0>]}
[0m22:07:13.865326 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:07:13.873475 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.fct_shipment
[0m22:07:13.874719 [info ] [Thread-1 (]: 1 of 1 START sql table model analytics_staging_marts.fct_shipment .............. [RUN]
[0m22:07:13.875821 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_marts, now model.data_pipeline_project.fct_shipment)
[0m22:07:13.876689 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.fct_shipment
[0m22:07:13.887728 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.fct_shipment"
[0m22:07:13.889785 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.fct_shipment
[0m22:07:13.912259 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:07:14.824715 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.fct_shipment"
[0m22:07:14.826830 [debug] [Thread-1 (]: On model.data_pipeline_project.fct_shipment: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.fct_shipment"} */

  
    

    create or replace table `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_shipment`
      
    
    

    
    OPTIONS()
    as (
      -- models/facts/fct_shipment.sql


with parsed_shipments as (
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`int_shipments_decomposed`  
),

orders as (
    select 
        order_id,
        order_status as latest_status
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_orders`
),

final as (
    select
        -- Primary Key
        ps.shipment_id,
        
        -- Foreign Key
        ps.order_id,
        
        -- Status (from orders table)
        o.latest_status,
        
        -- Delivery timestamp
        ps.delivered_at,
        
        -- Carrier information
        ps.carrier

    from parsed_shipments ps
    left join orders o 
        on ps.order_id = o.order_id
)

select * from final
    );
  
[0m22:07:15.549903 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:95c56857-8ae8-4ad3-822e-28ee596a15d8&page=queryresults
[0m22:07:16.153146 [error] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:95c56857-8ae8-4ad3-822e-28ee596a15d8&page=queryresults
[0m22:07:16.208559 [debug] [Thread-1 (]: Database Error in model fct_shipment (models/facts/fct_shipment.sql)
  Regular expressions passed into extraction functions must not have more than 1 capturing group
  compiled code at target/run/data_pipeline_project/models/facts/fct_shipment.sql
[0m22:07:16.213139 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '469af7b1-953e-4d71-b88f-aa3e2b9e8d6f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cd679560f80>]}
[0m22:07:16.215534 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model analytics_staging_marts.fct_shipment ..... [[31mERROR[0m in 2.33s]
[0m22:07:16.217368 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.fct_shipment
[0m22:07:16.219356 [debug] [Thread-7 (]: Marking all children of 'model.data_pipeline_project.fct_shipment' to be skipped because of status 'error'.  Reason: Database Error in model fct_shipment (models/facts/fct_shipment.sql)
  Regular expressions passed into extraction functions must not have more than 1 capturing group
  compiled code at target/run/data_pipeline_project/models/facts/fct_shipment.sql.
[0m22:07:16.227149 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:07:16.230363 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:07:16.230957 [debug] [MainThread]: Connection 'model.data_pipeline_project.fct_shipment' was properly closed.
[0m22:07:16.231398 [debug] [MainThread]: Connection 'list_data-pipeline-project-474812_analytics_staging_staging' was properly closed.
[0m22:07:16.231926 [info ] [MainThread]: 
[0m22:07:16.233614 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 4.18 seconds (4.18s).
[0m22:07:16.234983 [debug] [MainThread]: Command end result
[0m22:07:16.284634 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m22:07:16.287833 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m22:07:16.300140 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m22:07:16.301434 [info ] [MainThread]: 
[0m22:07:16.303174 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m22:07:16.304361 [info ] [MainThread]: 
[0m22:07:16.306219 [error] [MainThread]: [31mFailure in model fct_shipment (models/facts/fct_shipment.sql)[0m
[0m22:07:16.307330 [error] [MainThread]:   Database Error in model fct_shipment (models/facts/fct_shipment.sql)
  Regular expressions passed into extraction functions must not have more than 1 capturing group
  compiled code at target/run/data_pipeline_project/models/facts/fct_shipment.sql
[0m22:07:16.308522 [info ] [MainThread]: 
[0m22:07:16.309837 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_project/models/facts/fct_shipment.sql
[0m22:07:16.310602 [info ] [MainThread]: 
[0m22:07:16.311666 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m22:07:16.313497 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 7.461099, "process_in_blocks": "456", "process_kernel_time": 1.021465, "process_mem_max_rss": "377896", "process_out_blocks": "2848", "process_user_time": 4.857124}
[0m22:07:16.314901 [debug] [MainThread]: Command `dbt run` failed at 22:07:16.314733 after 7.46 seconds
[0m22:07:16.315776 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cd698c7dfd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cd6791073e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cd679107410>]}
[0m22:07:16.316446 [debug] [MainThread]: Flushing usage events
[0m22:07:17.033677 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:14:10.708714 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b715ac2f8f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b715cae2db0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b715cd5f6b0>]}


============================== 22:14:10.713787 | 99e903f3-e726-4fa0-9b79-4b2a193c0cba ==============================
[0m22:14:10.713787 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m22:14:10.714850 [debug] [MainThread]: running dbt with arguments {'use_experimental_parser': 'False', 'introspect': 'True', 'static_parser': 'True', 'cache_selected_only': 'False', 'partial_parse': 'True', 'write_json': 'True', 'indirect_selection': 'eager', 'use_colors': 'True', 'invocation_command': 'dbt clean', 'quiet': 'False', 'profiles_dir': '/home/ecem/.dbt', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_cache_events': 'False', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'target_path': 'None', 'fail_fast': 'False', 'log_format': 'default', 'printer_width': '80', 'debug': 'False', 'version_check': 'True', 'empty': 'None', 'warn_error': 'None', 'send_anonymous_usage_stats': 'True', 'no_print': 'None'}
[0m22:14:10.863182 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '99e903f3-e726-4fa0-9b79-4b2a193c0cba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b7159eb3470>]}
[0m22:14:10.881691 [debug] [MainThread]: Resource report: {"command_name": "clean", "command_success": true, "command_wall_clock_time": 0.23639818, "process_in_blocks": "0", "process_kernel_time": 0.174081, "process_mem_max_rss": "101868", "process_out_blocks": "16", "process_user_time": 1.591212}
[0m22:14:10.882501 [debug] [MainThread]: Command `dbt clean` succeeded at 22:14:10.882374 after 0.24 seconds
[0m22:14:10.882910 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b715a77afc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b7159f06300>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b7159eb1460>]}
[0m22:14:10.883325 [debug] [MainThread]: Flushing usage events
[0m22:14:11.826603 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:14:20.389832 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77c44be6ddc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77c44a426720>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77c44a826900>]}


============================== 22:14:20.395167 | 6d587ba6-8959-4b4e-87e5-7fad8612e10f ==============================
[0m22:14:20.395167 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m22:14:20.401036 [debug] [MainThread]: running dbt with arguments {'use_experimental_parser': 'False', 'static_parser': 'True', 'partial_parse': 'True', 'log_format': 'default', 'printer_width': '80', 'invocation_command': 'dbt run --select fct_shipment --full-refresh', 'empty': 'False', 'no_print': 'None', 'indirect_selection': 'eager', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'debug': 'False', 'use_colors': 'True', 'write_json': 'True', 'cache_selected_only': 'False', 'introspect': 'True', 'log_cache_events': 'False', 'warn_error': 'None', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'quiet': 'False', 'version_check': 'True', 'send_anonymous_usage_stats': 'True', 'fail_fast': 'False', 'profiles_dir': '/home/ecem/.dbt', 'target_path': 'None'}
[0m22:14:22.673509 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6d587ba6-8959-4b4e-87e5-7fad8612e10f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77c44c8c7ce0>]}
[0m22:14:22.745248 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6d587ba6-8959-4b4e-87e5-7fad8612e10f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77c44c79e360>]}
[0m22:14:22.749135 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m22:14:22.961020 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m22:14:22.965826 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m22:14:22.971737 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '6d587ba6-8959-4b4e-87e5-7fad8612e10f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77c42aa4c440>]}
[0m22:14:24.395420 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `relationships` defined on 'stg_orders' in
package 'data_pipeline_project' (models/staging/staging_schema.yml). Arguments
to generic tests should be nested under the `arguments` property.
[0m22:14:24.396766 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '6d587ba6-8959-4b4e-87e5-7fad8612e10f', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77c42a5c50a0>]}
[0m22:14:24.644688 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.data_pipeline_project.fct_shipment' (models/facts/fct_shipment.sql) depends on a node named 'int_shipment_parsed' which was not found
[0m22:14:24.646348 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 17 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m22:14:24.648087 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 4.3295465, "process_in_blocks": "0", "process_kernel_time": 1.165219, "process_mem_max_rss": "373408", "process_out_blocks": "8", "process_user_time": 4.845176}
[0m22:14:24.648875 [debug] [MainThread]: Command `dbt run` failed at 22:14:24.648632 after 4.33 seconds
[0m22:14:24.649377 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77c44a7ebef0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77c42a915430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77c42a4f2c90>]}
[0m22:14:24.649861 [debug] [MainThread]: Flushing usage events
[0m22:14:25.437187 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:14:31.368933 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c75f302fb90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c75f67c71d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c75f3e669f0>]}


============================== 22:14:31.374072 | fbc45ee3-e2f7-4ffd-ac1b-45a5d32ea4a2 ==============================
[0m22:14:31.374072 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m22:14:31.375002 [debug] [MainThread]: running dbt with arguments {'profiles_dir': '/home/ecem/.dbt', 'printer_width': '80', 'log_cache_events': 'False', 'indirect_selection': 'eager', 'write_json': 'True', 'fail_fast': 'False', 'partial_parse': 'True', 'introspect': 'True', 'invocation_command': 'dbt run --select fct_shipment --full-refresh', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'send_anonymous_usage_stats': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'cache_selected_only': 'False', 'log_format': 'default', 'no_print': 'None', 'version_check': 'True', 'warn_error': 'None', 'use_experimental_parser': 'False', 'use_colors': 'True', 'quiet': 'False', 'static_parser': 'True', 'empty': 'False', 'debug': 'False'}
[0m22:14:33.657195 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'fbc45ee3-e2f7-4ffd-ac1b-45a5d32ea4a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c75d6614650>]}
[0m22:14:33.719800 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'fbc45ee3-e2f7-4ffd-ac1b-45a5d32ea4a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c75f3532870>]}
[0m22:14:33.721234 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m22:14:33.983465 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m22:14:33.984916 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m22:14:33.985486 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'fbc45ee3-e2f7-4ffd-ac1b-45a5d32ea4a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c75d35da3c0>]}
[0m22:14:35.321325 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `relationships` defined on 'stg_orders' in
package 'data_pipeline_project' (models/staging/staging_schema.yml). Arguments
to generic tests should be nested under the `arguments` property.
[0m22:14:35.322673 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': 'fbc45ee3-e2f7-4ffd-ac1b-45a5d32ea4a2', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c75d31180b0>]}
[0m22:14:35.558764 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.data_pipeline_project.fct_shipment' (models/facts/fct_shipment.sql) depends on a node named 'int_shipment_parsed' which was not found
[0m22:14:35.559728 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 17 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m22:14:35.560912 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 4.2498484, "process_in_blocks": "0", "process_kernel_time": 0.96465, "process_mem_max_rss": "373596", "process_out_blocks": "8", "process_user_time": 5.093493}
[0m22:14:35.561467 [debug] [MainThread]: Command `dbt run` failed at 22:14:35.561367 after 4.25 seconds
[0m22:14:35.561934 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c75f306b9e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c75d30f44d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c75d31180b0>]}
[0m22:14:35.562397 [debug] [MainThread]: Flushing usage events
[0m22:14:36.246195 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:14:42.958724 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e3c3630a120>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e3c36225550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e3c35d60ad0>]}


============================== 22:14:42.964896 | 6b79d001-44e2-46a4-8001-b7d9388488da ==============================
[0m22:14:42.964896 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m22:14:42.967984 [debug] [MainThread]: running dbt with arguments {'debug': 'False', 'static_parser': 'True', 'target_path': 'None', 'empty': 'False', 'write_json': 'True', 'fail_fast': 'False', 'quiet': 'False', 'use_experimental_parser': 'False', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'profiles_dir': '/home/ecem/.dbt', 'introspect': 'True', 'indirect_selection': 'eager', 'partial_parse': 'True', 'log_cache_events': 'False', 'no_print': 'None', 'warn_error': 'None', 'printer_width': '80', 'invocation_command': 'dbt run --select int_shipments_decomposed --full-refresh', 'version_check': 'True', 'send_anonymous_usage_stats': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_colors': 'True', 'cache_selected_only': 'False', 'log_format': 'default'}
[0m22:14:45.185008 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6b79d001-44e2-46a4-8001-b7d9388488da', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e3c1614c650>]}
[0m22:14:45.265191 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6b79d001-44e2-46a4-8001-b7d9388488da', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e3c16002b40>]}
[0m22:14:45.266695 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m22:14:45.516281 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m22:14:45.518608 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m22:14:45.519950 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '6b79d001-44e2-46a4-8001-b7d9388488da', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e3c16393530>]}
[0m22:14:46.900720 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `relationships` defined on 'stg_orders' in
package 'data_pipeline_project' (models/staging/staging_schema.yml). Arguments
to generic tests should be nested under the `arguments` property.
[0m22:14:46.906947 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '6b79d001-44e2-46a4-8001-b7d9388488da', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e3c15c9d880>]}
[0m22:14:47.126716 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.data_pipeline_project.fct_shipment' (models/facts/fct_shipment.sql) depends on a node named 'int_shipment_parsed' which was not found
[0m22:14:47.127734 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 17 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m22:14:47.129125 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 4.240115, "process_in_blocks": "0", "process_kernel_time": 1.170754, "process_mem_max_rss": "373412", "process_out_blocks": "16", "process_user_time": 4.803466}
[0m22:14:47.129731 [debug] [MainThread]: Command `dbt run` failed at 22:14:47.129629 after 4.24 seconds
[0m22:14:47.130216 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e3c372eb6e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e3c15c26330>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e3c15d71f70>]}
[0m22:14:47.130712 [debug] [MainThread]: Flushing usage events
[0m22:14:47.864997 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:15:46.908883 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b6d81341730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b6d8068a4b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b6d80b07f80>]}


============================== 22:15:46.912307 | 1b56dc9d-7050-409e-8c96-b40b8ef93d37 ==============================
[0m22:15:46.912307 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m22:15:46.913172 [debug] [MainThread]: running dbt with arguments {'fail_fast': 'False', 'partial_parse': 'True', 'warn_error': 'None', 'debug': 'False', 'printer_width': '80', 'target_path': 'None', 'invocation_command': 'dbt clean', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'indirect_selection': 'eager', 'use_colors': 'True', 'version_check': 'True', 'cache_selected_only': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_experimental_parser': 'False', 'introspect': 'True', 'write_json': 'True', 'no_print': 'None', 'send_anonymous_usage_stats': 'True', 'log_cache_events': 'False', 'empty': 'None', 'static_parser': 'True', 'quiet': 'False', 'profiles_dir': '/home/ecem/.dbt', 'log_format': 'default'}
[0m22:15:47.061395 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1b56dc9d-7050-409e-8c96-b40b8ef93d37', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b6d801b8080>]}
[0m22:15:47.072989 [debug] [MainThread]: Resource report: {"command_name": "clean", "command_success": true, "command_wall_clock_time": 0.22234276, "process_in_blocks": "0", "process_kernel_time": 0.231647, "process_mem_max_rss": "101788", "process_out_blocks": "16", "process_user_time": 1.540335}
[0m22:15:47.073677 [debug] [MainThread]: Command `dbt clean` succeeded at 22:15:47.073448 after 0.22 seconds
[0m22:15:47.074264 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b6d80a33320>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b6d80057b30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b6d8004dee0>]}
[0m22:15:47.074992 [debug] [MainThread]: Flushing usage events
[0m22:15:48.032865 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:15:55.708128 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e706b989a00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e706b06eea0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e706b262930>]}


============================== 22:15:55.711925 | 773864f7-d130-4d4f-a440-7f0cb2d6dd5e ==============================
[0m22:15:55.711925 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m22:15:55.712969 [debug] [MainThread]: running dbt with arguments {'quiet': 'False', 'partial_parse': 'True', 'debug': 'False', 'send_anonymous_usage_stats': 'True', 'version_check': 'True', 'target_path': 'None', 'use_experimental_parser': 'False', 'empty': 'False', 'use_colors': 'True', 'warn_error': 'None', 'log_cache_events': 'False', 'printer_width': '80', 'fail_fast': 'False', 'introspect': 'True', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'static_parser': 'True', 'log_format': 'default', 'invocation_command': 'dbt run --select int_shipments_decomposed --full-refresh', 'cache_selected_only': 'False', 'write_json': 'True', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'indirect_selection': 'eager', 'profiles_dir': '/home/ecem/.dbt'}
[0m22:15:58.012296 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '773864f7-d130-4d4f-a440-7f0cb2d6dd5e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e704b4f41d0>]}
[0m22:15:58.076371 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '773864f7-d130-4d4f-a440-7f0cb2d6dd5e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e704e13bb00>]}
[0m22:15:58.077407 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m22:15:58.321647 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m22:15:58.322445 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m22:15:58.323526 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '773864f7-d130-4d4f-a440-7f0cb2d6dd5e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e704bf31820>]}
[0m22:15:59.668222 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `relationships` defined on 'stg_orders' in
package 'data_pipeline_project' (models/staging/staging_schema.yml). Arguments
to generic tests should be nested under the `arguments` property.
[0m22:15:59.668928 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '773864f7-d130-4d4f-a440-7f0cb2d6dd5e', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e704ae9dd00>]}
[0m22:15:59.905019 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.data_pipeline_project.fct_shipment' (models/facts/fct_shipment.sql) depends on a node named 'int_shipment_decomposed' which was not found
[0m22:15:59.906598 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 17 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m22:15:59.912601 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 4.2664366, "process_in_blocks": "0", "process_kernel_time": 0.94898, "process_mem_max_rss": "373616", "process_out_blocks": "8", "process_user_time": 5.155434}
[0m22:15:59.913595 [debug] [MainThread]: Command `dbt run` failed at 22:15:59.913464 after 4.27 seconds
[0m22:15:59.914287 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e706ada4560>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e704afa1d60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e704afa2a50>]}
[0m22:15:59.915037 [debug] [MainThread]: Flushing usage events
[0m22:16:00.655816 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:16:34.578162 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70d3bade95e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70d3bac15ee0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70d3baf0a330>]}


============================== 22:16:34.608181 | 10c2b354-1789-47de-b411-6dbc8022d6cc ==============================
[0m22:16:34.608181 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m22:16:34.611835 [debug] [MainThread]: running dbt with arguments {'profiles_dir': '/home/ecem/.dbt', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'log_format': 'default', 'use_experimental_parser': 'False', 'target_path': 'None', 'write_json': 'True', 'static_parser': 'True', 'log_cache_events': 'False', 'debug': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'warn_error': 'None', 'no_print': 'None', 'fail_fast': 'False', 'empty': 'None', 'quiet': 'False', 'introspect': 'True', 'invocation_command': 'dbt clean', 'cache_selected_only': 'False', 'send_anonymous_usage_stats': 'True', 'use_colors': 'True', 'version_check': 'True', 'indirect_selection': 'eager', 'partial_parse': 'True'}
[0m22:16:34.745784 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '10c2b354-1789-47de-b411-6dbc8022d6cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70d3ba0f89b0>]}
[0m22:16:34.767003 [debug] [MainThread]: Resource report: {"command_name": "clean", "command_success": true, "command_wall_clock_time": 0.24238032, "process_in_blocks": "0", "process_kernel_time": 0.431294, "process_mem_max_rss": "101632", "process_out_blocks": "16", "process_user_time": 1.201967}
[0m22:16:34.769809 [debug] [MainThread]: Command `dbt clean` succeeded at 22:16:34.769158 after 0.25 seconds
[0m22:16:34.771461 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70d3ba1661b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70d3bad9fb00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70d3bb346b70>]}
[0m22:16:34.773852 [debug] [MainThread]: Flushing usage events
[0m22:16:35.480547 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:16:49.666077 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fabecbc9b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fabec05f260>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fabed9480e0>]}


============================== 22:16:49.669222 | f009b8f1-20eb-4050-a3b7-6e9723260256 ==============================
[0m22:16:49.669222 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m22:16:49.675060 [debug] [MainThread]: running dbt with arguments {'profiles_dir': '/home/ecem/.dbt', 'use_colors': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'debug': 'False', 'cache_selected_only': 'False', 'partial_parse': 'True', 'no_print': 'None', 'use_experimental_parser': 'False', 'log_cache_events': 'False', 'fail_fast': 'False', 'write_json': 'True', 'introspect': 'True', 'warn_error': 'None', 'printer_width': '80', 'quiet': 'False', 'version_check': 'True', 'static_parser': 'True', 'empty': 'False', 'indirect_selection': 'eager', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'invocation_command': 'dbt run --select int_shipments_decomposed --full-refresh', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m22:16:52.030715 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f009b8f1-20eb-4050-a3b7-6e9723260256', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fabcbad1220>]}
[0m22:16:52.097356 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f009b8f1-20eb-4050-a3b7-6e9723260256', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fabcbd0aff0>]}
[0m22:16:52.098867 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m22:16:52.346691 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m22:16:52.348436 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m22:16:52.350800 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'f009b8f1-20eb-4050-a3b7-6e9723260256', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fabed5b2cf0>]}
[0m22:16:53.692003 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `relationships` defined on 'stg_orders' in
package 'data_pipeline_project' (models/staging/staging_schema.yml). Arguments
to generic tests should be nested under the `arguments` property.
[0m22:16:53.695024 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': 'f009b8f1-20eb-4050-a3b7-6e9723260256', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fabcb321c40>]}
[0m22:16:54.046809 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.data_pipeline_project.marts
[0m22:16:54.060271 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f009b8f1-20eb-4050-a3b7-6e9723260256', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fabcb3cc140>]}
[0m22:16:54.205002 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m22:16:54.207257 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m22:16:54.223698 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f009b8f1-20eb-4050-a3b7-6e9723260256', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fabca7c7290>]}
[0m22:16:54.224326 [info ] [MainThread]: Found 16 models, 35 data tests, 10 sources, 508 macros
[0m22:16:54.225059 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f009b8f1-20eb-4050-a3b7-6e9723260256', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fabca71cec0>]}
[0m22:16:54.227187 [info ] [MainThread]: 
[0m22:16:54.227888 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m22:16:54.228454 [info ] [MainThread]: 
[0m22:16:54.229277 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m22:16:54.231357 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812'
[0m22:16:54.233172 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:16:55.220177 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812, now list_data-pipeline-project-474812_analytics_staging_staging)
[0m22:16:55.222655 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:16:55.224166 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_marts'
[0m22:16:55.258412 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:16:56.203928 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f009b8f1-20eb-4050-a3b7-6e9723260256', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fabcb3848f0>]}
[0m22:16:56.204881 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:16:56.213219 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.int_shipments_decomposed
[0m22:16:56.214232 [info ] [Thread-1 (]: 1 of 1 START sql view model analytics_staging_staging.int_shipments_decomposed . [RUN]
[0m22:16:56.215226 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_marts, now model.data_pipeline_project.int_shipments_decomposed)
[0m22:16:56.215968 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.int_shipments_decomposed
[0m22:16:56.223147 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.int_shipments_decomposed"
[0m22:16:56.225106 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.int_shipments_decomposed
[0m22:16:56.276184 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.int_shipments_decomposed"
[0m22:16:56.278664 [debug] [Thread-1 (]: On model.data_pipeline_project.int_shipments_decomposed: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.int_shipments_decomposed"} */


  create or replace view `data-pipeline-project-474812`.`analytics_staging_staging`.`int_shipments_decomposed`
  OPTIONS()
  as with shipments as (
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_shipments`
),

-- Step 1: Extract the comma-separated numeric components string (using only ONE capturing group)
numeric_components_extracted as (
    select
        *,
        -- Captures the numbers inside the parentheses (e.g., '2023, 10, 27, 10, 30, 0')
        regexp_extract(details_json, r"deliveryDate': datetime\.datetime\(([^)]+)\)") as delivery_components_str,
        regexp_extract(details_json, r"collectDate': datetime\.datetime\(([^)]+)\)") as collect_components_str
    from shipments
),

-- Step 2: Split the components and construct the DATETIME object
parsed_shipments as (
    select
        s.shipment_id,
        s.order_id,
        s.user_id,
        
        -- DATETIME Construction for delivered_at using SPLIT and SAFE_OFFSET
        case
            when s.delivery_components_str is not null then
                datetime(
                    cast(split(s.delivery_components_str, ', ')[safe_offset(0)] as int64), -- Year
                    cast(split(s.delivery_components_str, ', ')[safe_offset(1)] as int64), -- Month
                    cast(split(s.delivery_components_str, ', ')[safe_offset(2)] as int64), -- Day
                    cast(split(s.delivery_components_str, ', ')[safe_offset(3)] as int64), -- Hour
                    cast(split(s.delivery_components_str, ', ')[safe_offset(4)] as int64), -- Minute
                    cast(split(s.delivery_components_str, ', ')[safe_offset(5)] as int64)  -- Second
                )
            else null
        end as delivered_at,

        -- DATETIME Construction for collect_date
        case
            when s.collect_components_str is not null then
                datetime(
                    cast(split(s.collect_components_str, ', ')[safe_offset(0)] as int64), -- Year
                    cast(split(s.collect_components_str, ', ')[safe_offset(1)] as int64), -- Month
                    cast(split(s.collect_components_str, ', ')[safe_offset(2)] as int64), -- Day
                    cast(split(s.collect_components_str, ', ')[safe_offset(3)] as int64), -- Hour
                    cast(split(s.collect_components_str, ', ')[safe_offset(4)] as int64), -- Minute
                    cast(split(s.collect_components_str, ', ')[safe_offset(5)] as int64)  -- Second
                )
            else null
        end as collect_date,

        -- Extract carrier information
        json_extract_scalar(s.label_json, '$.provider') as carrier

    from numeric_components_extracted s
    where s.shipment_id is not null
)

select 
    shipment_id,
    order_id,
    user_id,
    delivered_at,
    collect_date,
    carrier,
    -- We don't need inferred_status here, we just need the final dates/carrier for the Fact table
    'TEMP' as latest_status -- Placeholder. Status comes from FCT_ORDER join.
from parsed_shipments;


[0m22:16:56.281158 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:16:57.662851 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:60f31339-f576-425d-929d-39641e4cc5fe&page=queryresults
[0m22:16:58.412165 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f009b8f1-20eb-4050-a3b7-6e9723260256', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fabce574410>]}
[0m22:16:58.413198 [info ] [Thread-1 (]: 1 of 1 OK created sql view model analytics_staging_staging.int_shipments_decomposed  [[32mCREATE VIEW (0 processed)[0m in 2.20s]
[0m22:16:58.414618 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.int_shipments_decomposed
[0m22:16:58.418211 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:16:58.420102 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:16:58.420734 [debug] [MainThread]: Connection 'list_data-pipeline-project-474812_analytics_staging_staging' was properly closed.
[0m22:16:58.421222 [debug] [MainThread]: Connection 'model.data_pipeline_project.int_shipments_decomposed' was properly closed.
[0m22:16:58.422056 [info ] [MainThread]: 
[0m22:16:58.422603 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 4.19 seconds (4.19s).
[0m22:16:58.427951 [debug] [MainThread]: Command end result
[0m22:16:58.503393 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m22:16:58.507416 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m22:16:58.524419 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m22:16:58.526358 [info ] [MainThread]: 
[0m22:16:58.533031 [info ] [MainThread]: [32mCompleted successfully[0m
[0m22:16:58.534962 [info ] [MainThread]: 
[0m22:16:58.535811 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m22:16:58.537443 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 17 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m22:16:58.544040 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 8.933929, "process_in_blocks": "0", "process_kernel_time": 1.303042, "process_mem_max_rss": "387516", "process_out_blocks": "4096", "process_user_time": 5.980471}
[0m22:16:58.545641 [debug] [MainThread]: Command `dbt run` succeeded at 22:16:58.545307 after 8.94 seconds
[0m22:16:58.546227 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fabeb084b60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fabcb89a360>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fabca729b20>]}
[0m22:16:58.547034 [debug] [MainThread]: Flushing usage events
[0m22:16:59.475234 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:17:13.716504 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a05579122a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a05560cd760>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a055693d6a0>]}


============================== 22:17:13.720270 | 75bb5b97-1a77-401e-8d4f-f0544e2f6666 ==============================
[0m22:17:13.720270 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m22:17:13.722567 [debug] [MainThread]: running dbt with arguments {'invocation_command': 'dbt run --select fct_shipment --full-refresh', 'profiles_dir': '/home/ecem/.dbt', 'empty': 'False', 'write_json': 'True', 'log_cache_events': 'False', 'debug': 'False', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'version_check': 'True', 'fail_fast': 'False', 'introspect': 'True', 'quiet': 'False', 'send_anonymous_usage_stats': 'True', 'target_path': 'None', 'use_colors': 'True', 'partial_parse': 'True', 'static_parser': 'True', 'printer_width': '80', 'warn_error': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'cache_selected_only': 'False', 'use_experimental_parser': 'False', 'log_format': 'default', 'indirect_selection': 'eager', 'no_print': 'None'}
[0m22:17:16.873288 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '75bb5b97-1a77-401e-8d4f-f0544e2f6666', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a05367bc8f0>]}
[0m22:17:16.936582 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '75bb5b97-1a77-401e-8d4f-f0544e2f6666', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a053b58ab40>]}
[0m22:17:16.937721 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m22:17:17.206829 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m22:17:17.476302 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:17:17.476815 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:17:17.482697 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.data_pipeline_project.marts
[0m22:17:17.531689 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '75bb5b97-1a77-401e-8d4f-f0544e2f6666', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a05362f9460>]}
[0m22:17:17.648419 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m22:17:17.650896 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m22:17:17.666530 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '75bb5b97-1a77-401e-8d4f-f0544e2f6666', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a0535f3d250>]}
[0m22:17:17.667260 [info ] [MainThread]: Found 16 models, 35 data tests, 10 sources, 508 macros
[0m22:17:17.668310 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '75bb5b97-1a77-401e-8d4f-f0544e2f6666', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a05362f82c0>]}
[0m22:17:17.670901 [info ] [MainThread]: 
[0m22:17:17.671711 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m22:17:17.673096 [info ] [MainThread]: 
[0m22:17:17.674317 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m22:17:17.676210 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812'
[0m22:17:17.677230 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:17:18.464492 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812, now list_data-pipeline-project-474812_analytics_staging_marts)
[0m22:17:18.466138 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_staging'
[0m22:17:18.467065 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:17:18.468082 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:17:19.385405 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '75bb5b97-1a77-401e-8d4f-f0544e2f6666', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a0535f52240>]}
[0m22:17:19.386214 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:17:19.394413 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.fct_shipment
[0m22:17:19.395776 [info ] [Thread-1 (]: 1 of 1 START sql table model analytics_staging_marts.fct_shipment .............. [RUN]
[0m22:17:19.396862 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now model.data_pipeline_project.fct_shipment)
[0m22:17:19.397384 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.fct_shipment
[0m22:17:19.410773 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.fct_shipment"
[0m22:17:19.412662 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.fct_shipment
[0m22:17:19.430786 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:17:20.329984 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.fct_shipment"
[0m22:17:20.332303 [debug] [Thread-1 (]: On model.data_pipeline_project.fct_shipment: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.fct_shipment"} */

  
    

    create or replace table `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_shipment`
      
    
    

    
    OPTIONS()
    as (
      

with parsed_shipments as (
    -- CORRECTED: Referencing the new intermediate model
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`int_shipments_decomposed`  
),

orders as (
    select 
        order_id,
        order_status as latest_status
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_orders`
),

final as (
    select
        ps.shipment_id,
        ps.order_id,
        
        -- Use status from orders table (assuming it holds the definitive latest status)
        o.latest_status,
        
        ps.delivered_at,
        ps.carrier

    from parsed_shipments ps
    left join orders o 
        on ps.order_id = o.order_id
)

select * from final
    );
  
[0m22:17:21.431035 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:cfd7df5f-1c45-4f57-9573-fc7bf3fedcf6&page=queryresults
[0m22:17:24.371507 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '75bb5b97-1a77-401e-8d4f-f0544e2f6666', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a0536303170>]}
[0m22:17:24.374007 [info ] [Thread-1 (]: 1 of 1 OK created sql table model analytics_staging_marts.fct_shipment ......... [[32mCREATE TABLE (2.3k rows, 1.4 MiB processed)[0m in 4.97s]
[0m22:17:24.376699 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.fct_shipment
[0m22:17:24.386054 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:17:24.390184 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:17:24.391316 [debug] [MainThread]: Connection 'list_data-pipeline-project-474812_analytics_staging_marts' was properly closed.
[0m22:17:24.392595 [debug] [MainThread]: Connection 'model.data_pipeline_project.fct_shipment' was properly closed.
[0m22:17:24.393541 [info ] [MainThread]: 
[0m22:17:24.394798 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 6.72 seconds (6.72s).
[0m22:17:24.396884 [debug] [MainThread]: Command end result
[0m22:17:24.473644 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m22:17:24.487299 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m22:17:24.536493 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m22:17:24.542863 [info ] [MainThread]: 
[0m22:17:24.554796 [info ] [MainThread]: [32mCompleted successfully[0m
[0m22:17:24.574976 [info ] [MainThread]: 
[0m22:17:24.590190 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m22:17:24.594065 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 10.931629, "process_in_blocks": "0", "process_kernel_time": 1.115889, "process_mem_max_rss": "380820", "process_out_blocks": "2840", "process_user_time": 5.410304}
[0m22:17:24.594993 [debug] [MainThread]: Command `dbt run` succeeded at 22:17:24.594825 after 10.93 seconds
[0m22:17:24.596963 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a05565e4a70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a053600d550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a05367858e0>]}
[0m22:17:24.599974 [debug] [MainThread]: Flushing usage events
[0m22:17:25.344143 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:24:44.848119 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72bc40579c10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72bc3fead850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72bc3ff038c0>]}


============================== 22:24:44.851356 | ae560e22-603c-43dd-9bee-c86c920e03c2 ==============================
[0m22:24:44.851356 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m22:24:44.852726 [debug] [MainThread]: running dbt with arguments {'empty': 'False', 'target_path': 'None', 'version_check': 'True', 'introspect': 'True', 'static_parser': 'True', 'printer_width': '80', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'indirect_selection': 'eager', 'cache_selected_only': 'False', 'send_anonymous_usage_stats': 'True', 'profiles_dir': '/home/ecem/.dbt', 'no_print': 'None', 'write_json': 'True', 'use_colors': 'True', 'log_format': 'default', 'debug': 'False', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'invocation_command': 'dbt run --select fct_marketing_spend', 'warn_error': 'None', 'fail_fast': 'False', 'log_cache_events': 'False', 'use_experimental_parser': 'False', 'partial_parse': 'True'}
[0m22:24:47.283055 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ae560e22-603c-43dd-9bee-c86c920e03c2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72bc2018e7e0>]}
[0m22:24:47.350074 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ae560e22-603c-43dd-9bee-c86c920e03c2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72bc2034e630>]}
[0m22:24:47.351274 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m22:24:47.615526 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m22:24:47.905267 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
[0m22:24:47.906272 [debug] [MainThread]: Partial parsing: added file: data_pipeline_project://models/facts/fct_marketing_spend.sql
[0m22:24:48.123967 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.data_pipeline_project.fct_marketing_spend' (models/facts/fct_marketing_spend.sql) depends on a node named 'stg_raw_marketing_spend' which was not found
[0m22:24:48.125880 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 3.3324776, "process_in_blocks": "0", "process_kernel_time": 0.819185, "process_mem_max_rss": "369976", "process_out_blocks": "8", "process_user_time": 4.37991}
[0m22:24:48.126616 [debug] [MainThread]: Command `dbt run` failed at 22:24:48.126508 after 3.33 seconds
[0m22:24:48.127352 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72bc405975f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72bc1fefc260>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72bc214051f0>]}
[0m22:24:48.127840 [debug] [MainThread]: Flushing usage events
[0m22:24:49.062735 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:26:33.390363 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa2fc66d940>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa2fbadd040>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa2fbadfef0>]}


============================== 22:26:33.398130 | 41bb42aa-069f-4351-948f-96e952d8c890 ==============================
[0m22:26:33.398130 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m22:26:33.399214 [debug] [MainThread]: running dbt with arguments {'invocation_command': 'dbt run --select fct_marketing_spend', 'indirect_selection': 'eager', 'no_print': 'None', 'send_anonymous_usage_stats': 'True', 'empty': 'False', 'partial_parse': 'True', 'fail_fast': 'False', 'debug': 'False', 'profiles_dir': '/home/ecem/.dbt', 'write_json': 'True', 'use_colors': 'True', 'version_check': 'True', 'quiet': 'False', 'use_experimental_parser': 'False', 'target_path': 'None', 'cache_selected_only': 'False', 'log_format': 'default', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'introspect': 'True', 'warn_error': 'None', 'log_cache_events': 'False', 'static_parser': 'True', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs'}
[0m22:26:35.887058 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '41bb42aa-069f-4351-948f-96e952d8c890', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa2dbfafb00>]}
[0m22:26:35.962821 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '41bb42aa-069f-4351-948f-96e952d8c890', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa2fdd3a4e0>]}
[0m22:26:35.963997 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m22:26:36.215589 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m22:26:36.533658 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
[0m22:26:36.535045 [debug] [MainThread]: Partial parsing: added file: data_pipeline_project://models/facts/fct_marketing_spend.sql
[0m22:26:36.906115 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.data_pipeline_project.marts
[0m22:26:36.924832 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '41bb42aa-069f-4351-948f-96e952d8c890', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa2db176ab0>]}
[0m22:26:37.062963 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m22:26:37.066458 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m22:26:37.085161 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '41bb42aa-069f-4351-948f-96e952d8c890', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa2db190ef0>]}
[0m22:26:37.086038 [info ] [MainThread]: Found 17 models, 35 data tests, 10 sources, 508 macros
[0m22:26:37.086547 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '41bb42aa-069f-4351-948f-96e952d8c890', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa2dbb3b770>]}
[0m22:26:37.088456 [info ] [MainThread]: 
[0m22:26:37.089297 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m22:26:37.089978 [info ] [MainThread]: 
[0m22:26:37.091152 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m22:26:37.093006 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812'
[0m22:26:37.093832 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:26:38.123504 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812, now list_data-pipeline-project-474812_analytics_staging_staging)
[0m22:26:38.124865 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_marts'
[0m22:26:38.125984 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:26:38.126893 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:26:39.153205 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '41bb42aa-069f-4351-948f-96e952d8c890', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa2fdd3a750>]}
[0m22:26:39.160427 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:26:39.193236 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.fct_marketing_spend
[0m22:26:39.195361 [info ] [Thread-1 (]: 1 of 1 START sql table model analytics_staging_marts.fct_marketing_spend ....... [RUN]
[0m22:26:39.198629 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now model.data_pipeline_project.fct_marketing_spend)
[0m22:26:39.200964 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.fct_marketing_spend
[0m22:26:39.216806 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.fct_marketing_spend"
[0m22:26:39.217911 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.fct_marketing_spend
[0m22:26:39.301252 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.fct_marketing_spend"
[0m22:26:39.303291 [debug] [Thread-1 (]: On model.data_pipeline_project.fct_marketing_spend: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.fct_marketing_spend"} */

  
    

    create or replace table `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_marketing_spend`
      
    
    

    
    OPTIONS()
    as (
      

with marketing_data as (
    -- Source the raw marketing data 
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_marketing_spend`
),

final as (
    select
        -- Renamed Time Dimension (Grain)
        cast(spend_date as date) as date,
        
        -- Renamed Dimension
        marketing_channel as channel,
        
        -- Renamed Measure
        spend_amount_try as spend_try,
        
        -- Placeholder for missing data, set to NULL to indicate absence
        -- We use NULL instead of 0 as we don't know the true value.
        cast(NULL as numeric) as clicks
        
    from marketing_data
)

select * from final
    );
  
[0m22:26:39.305547 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:26:40.637198 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:182d978a-f152-4098-850e-2edc1545602c&page=queryresults
[0m22:26:43.045587 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '41bb42aa-069f-4351-948f-96e952d8c890', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa2dbc4acc0>]}
[0m22:26:43.047940 [info ] [Thread-1 (]: 1 of 1 OK created sql table model analytics_staging_marts.fct_marketing_spend .. [[32mCREATE TABLE (24.0 rows, 190.0 Bytes processed)[0m in 3.85s]
[0m22:26:43.050017 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.fct_marketing_spend
[0m22:26:43.053568 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:26:43.057284 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:26:43.058218 [debug] [MainThread]: Connection 'model.data_pipeline_project.fct_marketing_spend' was properly closed.
[0m22:26:43.059264 [debug] [MainThread]: Connection 'list_data-pipeline-project-474812_analytics_staging_marts' was properly closed.
[0m22:26:43.060096 [info ] [MainThread]: 
[0m22:26:43.060891 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 5.97 seconds (5.97s).
[0m22:26:43.062143 [debug] [MainThread]: Command end result
[0m22:26:43.132774 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m22:26:43.137771 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m22:26:43.143768 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m22:26:43.144328 [info ] [MainThread]: 
[0m22:26:43.145096 [info ] [MainThread]: [32mCompleted successfully[0m
[0m22:26:43.145729 [info ] [MainThread]: 
[0m22:26:43.152229 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m22:26:43.157888 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 9.819069, "process_in_blocks": "0", "process_kernel_time": 1.136831, "process_mem_max_rss": "382908", "process_out_blocks": "4136", "process_user_time": 5.327375}
[0m22:26:43.160122 [debug] [MainThread]: Command `dbt run` succeeded at 22:26:43.159932 after 9.82 seconds
[0m22:26:43.161884 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa2fc026a80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa2db1920c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa2dba539e0>]}
[0m22:26:43.162926 [debug] [MainThread]: Flushing usage events
[0m22:26:44.140897 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:40:57.928110 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7be911c81160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7be9114a7a10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7be911691310>]}


============================== 22:40:57.933668 | 15d71d51-7dfa-499f-a794-2f84d94298db ==============================
[0m22:40:57.933668 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m22:40:57.934592 [debug] [MainThread]: running dbt with arguments {'use_experimental_parser': 'False', 'cache_selected_only': 'False', 'debug': 'False', 'quiet': 'False', 'target_path': 'None', 'write_json': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'send_anonymous_usage_stats': 'True', 'introspect': 'True', 'invocation_command': 'dbt run --select mart_subscription_daily', 'partial_parse': 'True', 'printer_width': '80', 'fail_fast': 'False', 'log_cache_events': 'False', 'use_colors': 'True', 'version_check': 'True', 'indirect_selection': 'eager', 'no_print': 'None', 'warn_error': 'None', 'static_parser': 'True', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'empty': 'False', 'profiles_dir': '/home/ecem/.dbt', 'log_format': 'default'}
[0m22:41:00.231322 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '15d71d51-7dfa-499f-a794-2f84d94298db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7be8f199fe00>]}
[0m22:41:00.341709 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '15d71d51-7dfa-499f-a794-2f84d94298db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7be8f1c2faa0>]}
[0m22:41:00.344829 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m22:41:00.609094 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m22:41:00.877653 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
[0m22:41:00.881981 [debug] [MainThread]: Partial parsing: added file: data_pipeline_project://models/marts/mart_subscription_daily.sql
[0m22:41:01.188928 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '15d71d51-7dfa-499f-a794-2f84d94298db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7be8f15c2240>]}
[0m22:41:01.303725 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m22:41:01.306630 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m22:41:01.326721 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '15d71d51-7dfa-499f-a794-2f84d94298db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7be8f11104a0>]}
[0m22:41:01.327389 [info ] [MainThread]: Found 18 models, 35 data tests, 10 sources, 508 macros
[0m22:41:01.328163 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '15d71d51-7dfa-499f-a794-2f84d94298db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7be8f1153500>]}
[0m22:41:01.330145 [info ] [MainThread]: 
[0m22:41:01.330887 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m22:41:01.331710 [info ] [MainThread]: 
[0m22:41:01.332455 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m22:41:01.333994 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812'
[0m22:41:01.334665 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:41:02.422773 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812, now list_data-pipeline-project-474812_analytics_staging_marts)
[0m22:41:02.424082 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:41:02.425885 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_staging'
[0m22:41:02.460728 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:41:03.380372 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '15d71d51-7dfa-499f-a794-2f84d94298db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7be8f109be30>]}
[0m22:41:03.381397 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:41:03.391113 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.mart_subscription_daily
[0m22:41:03.392081 [info ] [Thread-1 (]: 1 of 1 START sql table model analytics_staging_marts.mart_subscription_daily ... [RUN]
[0m22:41:03.393115 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_marts, now model.data_pipeline_project.mart_subscription_daily)
[0m22:41:03.393632 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.mart_subscription_daily
[0m22:41:03.403239 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.mart_subscription_daily"
[0m22:41:03.404282 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.mart_subscription_daily
[0m22:41:03.471035 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.mart_subscription_daily"
[0m22:41:03.473428 [debug] [Thread-1 (]: On model.data_pipeline_project.mart_subscription_daily: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.mart_subscription_daily"} */

  
    

    create or replace table `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily`
      
    
    

    
    OPTIONS()
    as (
      -- models/marts/mart_subscription_daily.sql

with subscriptions as (
    select * from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_subscription`
),

-- Generate date series for the period you have data
date_series as (
    select date_day
    from unnest(generate_date_array(
        (select min(start_date) from subscriptions),
        current_date(),
        interval 1 day
    )) as date_day
),

daily_metrics as (
    select
        ds.date_day as date,
        
        -- Active subscribers (on that specific date)
        count(case 
            when s.start_date <= ds.date_day 
            and (s.end_date is null or s.end_date > ds.date_day)
            and s.status = 'Active'
            then 1 
        end) as active_subscribers,
        
        -- New subscribers (started on that date)
        count(case 
            when s.start_date = ds.date_day 
            then 1 
        end) as new_subscribers,
        
        -- Cancellations (ended on that date)
        count(case 
            when s.end_date = ds.date_day 
            then 1 
        end) as cancellations

    from date_series ds
    cross join subscriptions s
    group by ds.date_day
)

select * from daily_metrics
order by date
    );
  
[0m22:41:03.474866 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:41:04.623222 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:5e635eb4-a9fa-4ec0-948a-00cd4b187fe4&page=queryresults
[0m22:41:04.911305 [error] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:5e635eb4-a9fa-4ec0-948a-00cd4b187fe4&page=queryresults
[0m22:41:04.920913 [debug] [Thread-1 (]: Database Error in model mart_subscription_daily (models/marts/mart_subscription_daily.sql)
  No matching signature for function GENERATE_DATE_ARRAY for argument types: TIMESTAMP, DATE, INTERVAL INT64 DATE_TIME_PART
    Signature: GENERATE_DATE_ARRAY(DATE, DATE, [INTERVAL INT64 DATE_TIME_PART])
      Argument 1: Unable to coerce type TIMESTAMP to expected type DATE at [23:17]
  compiled code at target/run/data_pipeline_project/models/marts/mart_subscription_daily.sql
[0m22:41:04.923709 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '15d71d51-7dfa-499f-a794-2f84d94298db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7be8f060ee10>]}
[0m22:41:04.924931 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model analytics_staging_marts.mart_subscription_daily  [[31mERROR[0m in 1.53s]
[0m22:41:04.926236 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.mart_subscription_daily
[0m22:41:04.927436 [debug] [Thread-7 (]: Marking all children of 'model.data_pipeline_project.mart_subscription_daily' to be skipped because of status 'error'.  Reason: Database Error in model mart_subscription_daily (models/marts/mart_subscription_daily.sql)
  No matching signature for function GENERATE_DATE_ARRAY for argument types: TIMESTAMP, DATE, INTERVAL INT64 DATE_TIME_PART
    Signature: GENERATE_DATE_ARRAY(DATE, DATE, [INTERVAL INT64 DATE_TIME_PART])
      Argument 1: Unable to coerce type TIMESTAMP to expected type DATE at [23:17]
  compiled code at target/run/data_pipeline_project/models/marts/mart_subscription_daily.sql.
[0m22:41:04.937066 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:41:04.938704 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:41:04.939068 [debug] [MainThread]: Connection 'model.data_pipeline_project.mart_subscription_daily' was properly closed.
[0m22:41:04.939399 [debug] [MainThread]: Connection 'list_data-pipeline-project-474812_analytics_staging_staging' was properly closed.
[0m22:41:04.939793 [info ] [MainThread]: 
[0m22:41:04.940987 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 3.61 seconds (3.61s).
[0m22:41:04.942371 [debug] [MainThread]: Command end result
[0m22:41:04.985536 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m22:41:04.994799 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m22:41:05.006784 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m22:41:05.007804 [info ] [MainThread]: 
[0m22:41:05.009940 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m22:41:05.011571 [info ] [MainThread]: 
[0m22:41:05.014311 [error] [MainThread]: [31mFailure in model mart_subscription_daily (models/marts/mart_subscription_daily.sql)[0m
[0m22:41:05.015804 [error] [MainThread]:   Database Error in model mart_subscription_daily (models/marts/mart_subscription_daily.sql)
  No matching signature for function GENERATE_DATE_ARRAY for argument types: TIMESTAMP, DATE, INTERVAL INT64 DATE_TIME_PART
    Signature: GENERATE_DATE_ARRAY(DATE, DATE, [INTERVAL INT64 DATE_TIME_PART])
      Argument 1: Unable to coerce type TIMESTAMP to expected type DATE at [23:17]
  compiled code at target/run/data_pipeline_project/models/marts/mart_subscription_daily.sql
[0m22:41:05.017118 [info ] [MainThread]: 
[0m22:41:05.023149 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_project/models/marts/mart_subscription_daily.sql
[0m22:41:05.024079 [info ] [MainThread]: 
[0m22:41:05.026907 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m22:41:05.034267 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 7.162904, "process_in_blocks": "0", "process_kernel_time": 1.450162, "process_mem_max_rss": "379340", "process_out_blocks": "4168", "process_user_time": 4.48009}
[0m22:41:05.035659 [debug] [MainThread]: Command `dbt run` failed at 22:41:05.034832 after 7.17 seconds
[0m22:41:05.038045 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7be912a2f560>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7be8f1403890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7be911ab4740>]}
[0m22:41:05.038659 [debug] [MainThread]: Flushing usage events
[0m22:41:06.028253 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:44:19.734098 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73d2681bb2f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73d266b723c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73d268f16690>]}


============================== 22:44:19.737621 | f68fb3d1-dce5-4de4-b0ee-632ff5f1295b ==============================
[0m22:44:19.737621 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m22:44:19.738666 [debug] [MainThread]: running dbt with arguments {'indirect_selection': 'eager', 'write_json': 'True', 'target_path': 'None', 'profiles_dir': '/home/ecem/.dbt', 'use_colors': 'True', 'send_anonymous_usage_stats': 'True', 'version_check': 'True', 'quiet': 'False', 'log_cache_events': 'False', 'partial_parse': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'empty': 'None', 'warn_error': 'None', 'no_print': 'None', 'log_format': 'default', 'fail_fast': 'False', 'introspect': 'True', 'use_experimental_parser': 'False', 'invocation_command': 'dbt clean', 'debug': 'False', 'cache_selected_only': 'False', 'printer_width': '80', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs'}
[0m22:44:19.883231 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f68fb3d1-dce5-4de4-b0ee-632ff5f1295b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73d2663e5af0>]}
[0m22:44:19.902025 [debug] [MainThread]: Resource report: {"command_name": "clean", "command_success": true, "command_wall_clock_time": 0.22825453, "process_in_blocks": "0", "process_kernel_time": 0.24142, "process_mem_max_rss": "101512", "process_out_blocks": "16", "process_user_time": 1.496302}
[0m22:44:19.903767 [debug] [MainThread]: Command `dbt clean` succeeded at 22:44:19.902575 after 0.23 seconds
[0m22:44:19.904410 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73d2663ba090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73d266409010>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73d2679c3800>]}
[0m22:44:19.904847 [debug] [MainThread]: Flushing usage events
[0m22:44:20.852822 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:44:29.152781 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d3ddc56b1d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d3ddb58ca10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d3ddb58d8b0>]}


============================== 22:44:29.157619 | dcd99c04-b4b7-4c77-8e98-3f74032ceb46 ==============================
[0m22:44:29.157619 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m22:44:29.161834 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'invocation_command': 'dbt run --select mart_subscription_daily', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'write_json': 'True', 'partial_parse': 'True', 'version_check': 'True', 'profiles_dir': '/home/ecem/.dbt', 'log_cache_events': 'False', 'printer_width': '80', 'warn_error': 'None', 'indirect_selection': 'eager', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'send_anonymous_usage_stats': 'True', 'use_experimental_parser': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'debug': 'False', 'fail_fast': 'False', 'target_path': 'None', 'log_format': 'default', 'no_print': 'None', 'empty': 'False', 'use_colors': 'True', 'quiet': 'False'}
[0m22:44:31.486356 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'dcd99c04-b4b7-4c77-8e98-3f74032ceb46', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d3dddb63890>]}
[0m22:44:31.552165 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'dcd99c04-b4b7-4c77-8e98-3f74032ceb46', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d3dbbd59d60>]}
[0m22:44:31.553081 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m22:44:31.813326 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m22:44:31.814691 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m22:44:31.815877 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'dcd99c04-b4b7-4c77-8e98-3f74032ceb46', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d3dbbdbda30>]}
[0m22:44:33.208011 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `relationships` defined on 'stg_orders' in
package 'data_pipeline_project' (models/staging/staging_schema.yml). Arguments
to generic tests should be nested under the `arguments` property.
[0m22:44:33.209161 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': 'dcd99c04-b4b7-4c77-8e98-3f74032ceb46', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d3dbb72f6e0>]}
[0m22:44:33.553492 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'dcd99c04-b4b7-4c77-8e98-3f74032ceb46', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d3dbb77cbc0>]}
[0m22:44:33.677617 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m22:44:33.682146 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m22:44:33.705020 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'dcd99c04-b4b7-4c77-8e98-3f74032ceb46', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d3dbacd2c60>]}
[0m22:44:33.705897 [info ] [MainThread]: Found 18 models, 35 data tests, 10 sources, 508 macros
[0m22:44:33.706626 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'dcd99c04-b4b7-4c77-8e98-3f74032ceb46', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d3dbb794590>]}
[0m22:44:33.709394 [info ] [MainThread]: 
[0m22:44:33.710385 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m22:44:33.711168 [info ] [MainThread]: 
[0m22:44:33.712272 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m22:44:33.714829 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812'
[0m22:44:33.715522 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:44:34.757909 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812, now list_data-pipeline-project-474812_analytics_staging_staging)
[0m22:44:34.761590 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:44:34.768211 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_marts'
[0m22:44:34.771465 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:44:35.731463 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'dcd99c04-b4b7-4c77-8e98-3f74032ceb46', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d3dbbd84170>]}
[0m22:44:35.732683 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:44:35.741989 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.mart_subscription_daily
[0m22:44:35.743423 [info ] [Thread-1 (]: 1 of 1 START sql table model analytics_staging_marts.mart_subscription_daily ... [RUN]
[0m22:44:35.744880 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now model.data_pipeline_project.mart_subscription_daily)
[0m22:44:35.746088 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.mart_subscription_daily
[0m22:44:35.757575 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.mart_subscription_daily"
[0m22:44:35.759028 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.mart_subscription_daily
[0m22:44:35.824665 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.mart_subscription_daily"
[0m22:44:35.826573 [debug] [Thread-1 (]: On model.data_pipeline_project.mart_subscription_daily: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.mart_subscription_daily"} */

  
    

    create or replace table `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily`
      
    
    

    
    OPTIONS()
    as (
      with subscriptions as (
    -- Use the dimension table for clean start/end dates
    select 
        subscription_id,
        user_id,
        -- CRITICAL FIX: Cast to DATE to match GENERATE_DATE_ARRAY requirements
        cast(start_date as date) as start_date, 
        cast(end_date as date) as end_date,
        status
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_subscription`
),

-- Generate date series for the period you have data
date_series as (
    select date_day
    from unnest(generate_date_array(
        -- FIX APPLIED HERE: Ensure MIN and MAX dates are calculated from CAST(DATE)
        (select min(start_date) from subscriptions),
        current_date(),
        interval 1 day
    )) as date_day
),

daily_metrics as (
    select
        ds.date_day as date,
        
        -- Active subscribers (on that specific date)
        count(case 
            -- Note: We use the CAST start/end dates from the CTE above
            when s.start_date <= ds.date_day 
            and (s.end_date is null or s.end_date > ds.date_day)
            and s.status = 'Active' -- If status is already clean in dim_subscription
            then 1 
        end) as active_subscribers,
        
        -- New subscribers (started on that date)
        count(case 
            when s.start_date = ds.date_day 
            then 1 
        end) as new_subscribers,
        
        -- Cancellations (ended on that date)
        count(case 
            when s.end_date = ds.date_day 
            then 1 
        end) as cancellations

    from date_series ds
    cross join subscriptions s
    group by ds.date_day
)

select * from daily_metrics
order by date
    );
  
[0m22:44:35.828075 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:44:37.110474 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:0d6e3a07-3690-4a16-a91d-96c59c109de5&page=queryresults
[0m22:44:37.394424 [error] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:0d6e3a07-3690-4a16-a91d-96c59c109de5&page=queryresults
[0m22:44:37.419040 [debug] [Thread-1 (]: Database Error in model mart_subscription_daily (models/marts/mart_subscription_daily.sql)
  Unrecognized name: user_id at [18:9]
  compiled code at target/run/data_pipeline_project/models/marts/mart_subscription_daily.sql
[0m22:44:37.422042 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'dcd99c04-b4b7-4c77-8e98-3f74032ceb46', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d3dbadcb710>]}
[0m22:44:37.424261 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model analytics_staging_marts.mart_subscription_daily  [[31mERROR[0m in 1.68s]
[0m22:44:37.425483 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.mart_subscription_daily
[0m22:44:37.427087 [debug] [Thread-7 (]: Marking all children of 'model.data_pipeline_project.mart_subscription_daily' to be skipped because of status 'error'.  Reason: Database Error in model mart_subscription_daily (models/marts/mart_subscription_daily.sql)
  Unrecognized name: user_id at [18:9]
  compiled code at target/run/data_pipeline_project/models/marts/mart_subscription_daily.sql.
[0m22:44:37.435726 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:44:37.438002 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:44:37.438447 [debug] [MainThread]: Connection 'model.data_pipeline_project.mart_subscription_daily' was properly closed.
[0m22:44:37.440619 [debug] [MainThread]: Connection 'list_data-pipeline-project-474812_analytics_staging_marts' was properly closed.
[0m22:44:37.441199 [info ] [MainThread]: 
[0m22:44:37.441933 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 3.73 seconds (3.73s).
[0m22:44:37.443886 [debug] [MainThread]: Command end result
[0m22:44:37.505450 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m22:44:37.510383 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m22:44:37.518009 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m22:44:37.518951 [info ] [MainThread]: 
[0m22:44:37.520115 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m22:44:37.523967 [info ] [MainThread]: 
[0m22:44:37.525816 [error] [MainThread]: [31mFailure in model mart_subscription_daily (models/marts/mart_subscription_daily.sql)[0m
[0m22:44:37.527219 [error] [MainThread]:   Database Error in model mart_subscription_daily (models/marts/mart_subscription_daily.sql)
  Unrecognized name: user_id at [18:9]
  compiled code at target/run/data_pipeline_project/models/marts/mart_subscription_daily.sql
[0m22:44:37.530647 [info ] [MainThread]: 
[0m22:44:37.531277 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_project/models/marts/mart_subscription_daily.sql
[0m22:44:37.531870 [info ] [MainThread]: 
[0m22:44:37.532694 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m22:44:37.534193 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 17 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m22:44:37.535965 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 8.44789, "process_in_blocks": "0", "process_kernel_time": 1.14764, "process_mem_max_rss": "384348", "process_out_blocks": "4136", "process_user_time": 6.220214}
[0m22:44:37.537315 [debug] [MainThread]: Command `dbt run` failed at 22:44:37.537175 after 8.45 seconds
[0m22:44:37.538180 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d3ddc1c5dc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d3dbacb19d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d3dbadae570>]}
[0m22:44:37.539002 [debug] [MainThread]: Flushing usage events
[0m22:44:38.289532 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:46:35.722181 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d3b50819280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d3b52586750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d3b50a135f0>]}


============================== 22:46:35.741781 | e128bdef-7126-4673-8136-55c7a64726c8 ==============================
[0m22:46:35.741781 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m22:46:35.744662 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'profiles_dir': '/home/ecem/.dbt', 'target_path': 'None', 'send_anonymous_usage_stats': 'True', 'introspect': 'True', 'empty': 'None', 'log_cache_events': 'False', 'cache_selected_only': 'False', 'write_json': 'True', 'version_check': 'True', 'static_parser': 'True', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'use_colors': 'True', 'use_experimental_parser': 'False', 'debug': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'no_print': 'None', 'quiet': 'False', 'indirect_selection': 'eager', 'warn_error': 'None', 'fail_fast': 'False', 'partial_parse': 'True', 'invocation_command': 'dbt clean', 'log_format': 'default'}
[0m22:46:35.944453 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e128bdef-7126-4673-8136-55c7a64726c8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d3b5081b590>]}
[0m22:46:35.962663 [debug] [MainThread]: Resource report: {"command_name": "clean", "command_success": true, "command_wall_clock_time": 0.29411295, "process_in_blocks": "0", "process_kernel_time": 0.223133, "process_mem_max_rss": "101068", "process_out_blocks": "16", "process_user_time": 1.425091}
[0m22:46:35.963174 [debug] [MainThread]: Command `dbt clean` succeeded at 22:46:35.963076 after 0.29 seconds
[0m22:46:35.963565 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d3b4ff1d1c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d3b50902e70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d3b50901340>]}
[0m22:46:35.964421 [debug] [MainThread]: Flushing usage events
[0m22:46:36.933694 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:46:53.091124 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74e8f377ade0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74e8f3afe3c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74e8f5645430>]}


============================== 22:46:53.097111 | 526aedd7-7e53-4418-881b-3a751f56ac27 ==============================
[0m22:46:53.097111 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m22:46:53.099772 [debug] [MainThread]: running dbt with arguments {'target_path': 'None', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'warn_error': 'None', 'log_cache_events': 'False', 'introspect': 'True', 'write_json': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_experimental_parser': 'False', 'quiet': 'False', 'no_print': 'None', 'partial_parse': 'True', 'empty': 'False', 'printer_width': '80', 'send_anonymous_usage_stats': 'True', 'log_format': 'default', 'indirect_selection': 'eager', 'static_parser': 'True', 'version_check': 'True', 'fail_fast': 'False', 'cache_selected_only': 'False', 'debug': 'False', 'profiles_dir': '/home/ecem/.dbt', 'use_colors': 'True', 'invocation_command': 'dbt run --select mart_subscription_daily --full-refresh'}
[0m22:46:55.439480 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '526aedd7-7e53-4418-881b-3a751f56ac27', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74e8f3a10ad0>]}
[0m22:46:55.502580 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '526aedd7-7e53-4418-881b-3a751f56ac27', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74e8d3606660>]}
[0m22:46:55.503809 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m22:46:55.752029 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m22:46:55.754347 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m22:46:55.755000 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '526aedd7-7e53-4418-881b-3a751f56ac27', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74e8f2cbc2c0>]}
[0m22:46:57.080398 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `relationships` defined on 'stg_orders' in
package 'data_pipeline_project' (models/staging/staging_schema.yml). Arguments
to generic tests should be nested under the `arguments` property.
[0m22:46:57.081641 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '526aedd7-7e53-4418-881b-3a751f56ac27', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74e8d27000b0>]}
[0m22:46:57.426940 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '526aedd7-7e53-4418-881b-3a751f56ac27', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74e8d30b1be0>]}
[0m22:46:57.539570 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m22:46:57.542033 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m22:46:57.560145 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '526aedd7-7e53-4418-881b-3a751f56ac27', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74e8d26ac9e0>]}
[0m22:46:57.560964 [info ] [MainThread]: Found 18 models, 35 data tests, 10 sources, 508 macros
[0m22:46:57.561641 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '526aedd7-7e53-4418-881b-3a751f56ac27', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74e8d25a0710>]}
[0m22:46:57.563925 [info ] [MainThread]: 
[0m22:46:57.564478 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m22:46:57.565239 [info ] [MainThread]: 
[0m22:46:57.566816 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m22:46:57.568853 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812'
[0m22:46:57.569835 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:46:58.503996 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812, now list_data-pipeline-project-474812_analytics_staging_marts)
[0m22:46:58.505899 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:46:58.544181 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_staging'
[0m22:46:58.546534 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:46:59.507236 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '526aedd7-7e53-4418-881b-3a751f56ac27', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74e8d25618e0>]}
[0m22:46:59.508180 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:46:59.516754 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.mart_subscription_daily
[0m22:46:59.517556 [info ] [Thread-1 (]: 1 of 1 START sql table model analytics_staging_marts.mart_subscription_daily ... [RUN]
[0m22:46:59.518721 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_marts, now model.data_pipeline_project.mart_subscription_daily)
[0m22:46:59.519657 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.mart_subscription_daily
[0m22:46:59.531579 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.mart_subscription_daily"
[0m22:46:59.532517 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.mart_subscription_daily
[0m22:46:59.603550 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.mart_subscription_daily"
[0m22:46:59.606107 [debug] [Thread-1 (]: On model.data_pipeline_project.mart_subscription_daily: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.mart_subscription_daily"} */

  
    

    create or replace table `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily`
      
    
    

    
    OPTIONS()
    as (
      with subscriptions as (
    -- Use the dimension table for clean start/end dates
    select 
        subscription_id,
        customer_id,
        -- CRITICAL FIX: Cast to DATE to match GENERATE_DATE_ARRAY requirements
        cast(start_date as date) as start_date, 
        cast(end_date as date) as end_date,
        status
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_subscription`
),

-- Generate date series for the period you have data
date_series as (
    select date_day
    from unnest(generate_date_array(
        -- FIX APPLIED HERE: Ensure MIN and MAX dates are calculated from CAST(DATE)
        (select min(start_date) from subscriptions),
        current_date(),
        interval 1 day
    )) as date_day
),

daily_metrics as (
    select
        ds.date_day as date,
        
        -- Active subscribers (on that specific date)
        count(case 
            -- Note: We use the CAST start/end dates from the CTE above
            when s.start_date <= ds.date_day 
            and (s.end_date is null or s.end_date > ds.date_day)
            and s.status = 'Active' -- If status is already clean in dim_subscription
            then 1 
        end) as active_subscribers,
        
        -- New subscribers (started on that date)
        count(case 
            when s.start_date = ds.date_day 
            then 1 
        end) as new_subscribers,
        
        -- Cancellations (ended on that date)
        count(case 
            when s.end_date = ds.date_day 
            then 1 
        end) as cancellations

    from date_series ds
    cross join subscriptions s
    group by ds.date_day
)

select * from daily_metrics
order by date
    );
  
[0m22:46:59.606729 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:47:00.775343 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:6c184d3e-370c-438b-8db0-d3992b9fedda&page=queryresults
[0m22:47:01.086026 [error] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:6c184d3e-370c-438b-8db0-d3992b9fedda&page=queryresults
[0m22:47:01.112902 [debug] [Thread-1 (]: Database Error in model mart_subscription_daily (models/marts/mart_subscription_daily.sql)
  Invalid cast from INT64 to DATE at [21:14]
  compiled code at target/run/data_pipeline_project/models/marts/mart_subscription_daily.sql
[0m22:47:01.115216 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '526aedd7-7e53-4418-881b-3a751f56ac27', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74e8d2500bf0>]}
[0m22:47:01.116407 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model analytics_staging_marts.mart_subscription_daily  [[31mERROR[0m in 1.60s]
[0m22:47:01.117514 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.mart_subscription_daily
[0m22:47:01.118885 [debug] [Thread-7 (]: Marking all children of 'model.data_pipeline_project.mart_subscription_daily' to be skipped because of status 'error'.  Reason: Database Error in model mart_subscription_daily (models/marts/mart_subscription_daily.sql)
  Invalid cast from INT64 to DATE at [21:14]
  compiled code at target/run/data_pipeline_project/models/marts/mart_subscription_daily.sql.
[0m22:47:01.123848 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:47:01.125788 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:47:01.126518 [debug] [MainThread]: Connection 'model.data_pipeline_project.mart_subscription_daily' was properly closed.
[0m22:47:01.126923 [debug] [MainThread]: Connection 'list_data-pipeline-project-474812_analytics_staging_staging' was properly closed.
[0m22:47:01.127339 [info ] [MainThread]: 
[0m22:47:01.127927 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 3.56 seconds (3.56s).
[0m22:47:01.128976 [debug] [MainThread]: Command end result
[0m22:47:01.193360 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m22:47:01.196764 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m22:47:01.204939 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m22:47:01.205859 [info ] [MainThread]: 
[0m22:47:01.207704 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m22:47:01.208626 [info ] [MainThread]: 
[0m22:47:01.209364 [error] [MainThread]: [31mFailure in model mart_subscription_daily (models/marts/mart_subscription_daily.sql)[0m
[0m22:47:01.210223 [error] [MainThread]:   Database Error in model mart_subscription_daily (models/marts/mart_subscription_daily.sql)
  Invalid cast from INT64 to DATE at [21:14]
  compiled code at target/run/data_pipeline_project/models/marts/mart_subscription_daily.sql
[0m22:47:01.211167 [info ] [MainThread]: 
[0m22:47:01.211896 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_project/models/marts/mart_subscription_daily.sql
[0m22:47:01.212323 [info ] [MainThread]: 
[0m22:47:01.212965 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m22:47:01.213783 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 17 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m22:47:01.215127 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 8.207131, "process_in_blocks": "0", "process_kernel_time": 0.973308, "process_mem_max_rss": "383604", "process_out_blocks": "4136", "process_user_time": 6.174585}
[0m22:47:01.215902 [debug] [MainThread]: Command `dbt run` failed at 22:47:01.215773 after 8.21 seconds
[0m22:47:01.216401 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74e8f36bb5f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74e8d25a6ab0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74e8d30647a0>]}
[0m22:47:01.217005 [debug] [MainThread]: Flushing usage events
[0m22:47:01.921994 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:49:44.803088 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7914722af8f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7914748e6ed0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x791472b5b9e0>]}


============================== 22:49:44.806724 | 0b3103b2-0539-4f86-88a3-758e805cc4ee ==============================
[0m22:49:44.806724 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m22:49:44.808030 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'debug': 'False', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'introspect': 'True', 'indirect_selection': 'eager', 'log_format': 'default', 'partial_parse': 'True', 'quiet': 'False', 'empty': 'None', 'profiles_dir': '/home/ecem/.dbt', 'use_experimental_parser': 'False', 'write_json': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'send_anonymous_usage_stats': 'True', 'no_print': 'None', 'log_cache_events': 'False', 'warn_error': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'invocation_command': 'dbt clean', 'static_parser': 'True'}
[0m22:49:44.940526 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0b3103b2-0539-4f86-88a3-758e805cc4ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79147237c830>]}
[0m22:49:44.968450 [debug] [MainThread]: Resource report: {"command_name": "clean", "command_success": true, "command_wall_clock_time": 0.2263347, "process_in_blocks": "0", "process_kernel_time": 0.209504, "process_mem_max_rss": "101692", "process_out_blocks": "16", "process_user_time": 1.532306}
[0m22:49:44.971203 [debug] [MainThread]: Command `dbt clean` succeeded at 22:49:44.970598 after 0.23 seconds
[0m22:49:44.972130 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x791471c4c5f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x791472d69e80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x791472d1ba40>]}
[0m22:49:44.972814 [debug] [MainThread]: Flushing usage events
[0m22:49:45.992550 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:49:51.459487 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x785759461ee0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78575a207950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x785759981130>]}


============================== 22:49:51.463720 | 02480d27-6318-4ed2-852f-fd765de59897 ==============================
[0m22:49:51.463720 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m22:49:51.464995 [debug] [MainThread]: running dbt with arguments {'quiet': 'False', 'indirect_selection': 'eager', 'write_json': 'True', 'log_format': 'default', 'use_experimental_parser': 'False', 'send_anonymous_usage_stats': 'True', 'target_path': 'None', 'cache_selected_only': 'False', 'use_colors': 'True', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'invocation_command': 'dbt run --select mart_subscription_daily --full-refresh', 'warn_error': 'None', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'profiles_dir': '/home/ecem/.dbt', 'empty': 'False', 'introspect': 'True', 'log_cache_events': 'False', 'static_parser': 'True', 'no_print': 'None', 'printer_width': '80', 'debug': 'False', 'partial_parse': 'True', 'fail_fast': 'False'}
[0m22:49:53.998699 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '02480d27-6318-4ed2-852f-fd765de59897', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78573a031b80>]}
[0m22:49:54.071344 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '02480d27-6318-4ed2-852f-fd765de59897', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78573a157aa0>]}
[0m22:49:54.074064 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m22:49:54.343568 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m22:49:54.345082 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m22:49:54.346614 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '02480d27-6318-4ed2-852f-fd765de59897', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x785739ea7e00>]}
[0m22:49:55.740860 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `relationships` defined on 'stg_orders' in
package 'data_pipeline_project' (models/staging/staging_schema.yml). Arguments
to generic tests should be nested under the `arguments` property.
[0m22:49:55.742920 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '02480d27-6318-4ed2-852f-fd765de59897', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x785739915880>]}
[0m22:49:56.096583 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '02480d27-6318-4ed2-852f-fd765de59897', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x785738f9c680>]}
[0m22:49:56.216447 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m22:49:56.219052 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m22:49:56.238782 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '02480d27-6318-4ed2-852f-fd765de59897', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x785738d86900>]}
[0m22:49:56.239601 [info ] [MainThread]: Found 18 models, 35 data tests, 10 sources, 508 macros
[0m22:49:56.240129 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '02480d27-6318-4ed2-852f-fd765de59897', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x785738f9f7d0>]}
[0m22:49:56.242818 [info ] [MainThread]: 
[0m22:49:56.243689 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m22:49:56.244239 [info ] [MainThread]: 
[0m22:49:56.245235 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m22:49:56.247576 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812'
[0m22:49:56.248815 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:49:57.089749 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812, now list_data-pipeline-project-474812_analytics_staging_staging)
[0m22:49:57.095067 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_marts'
[0m22:49:57.097138 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:49:57.101187 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:49:58.096894 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '02480d27-6318-4ed2-852f-fd765de59897', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x785739e707d0>]}
[0m22:49:58.097646 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:49:58.105538 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.mart_subscription_daily
[0m22:49:58.106941 [info ] [Thread-1 (]: 1 of 1 START sql table model analytics_staging_marts.mart_subscription_daily ... [RUN]
[0m22:49:58.107860 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_marts, now model.data_pipeline_project.mart_subscription_daily)
[0m22:49:58.108484 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.mart_subscription_daily
[0m22:49:58.118285 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.mart_subscription_daily"
[0m22:49:58.119678 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.mart_subscription_daily
[0m22:49:58.188868 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.mart_subscription_daily"
[0m22:49:58.190057 [debug] [Thread-1 (]: On model.data_pipeline_project.mart_subscription_daily: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.mart_subscription_daily"} */

  
    

    create or replace table `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily`
      
    
    

    
    OPTIONS()
    as (
      

with subscriptions as (
    -- Use the dimension table for clean start/end dates
    select 
        subscription_id,
        customer_id,
        -- CRITICAL FIX: Cast to DATE. We must assume the source column
        -- can be cast, even if it's currently an unsupported type in the dim.
        cast(start_date as date) as start_date, 
        cast(end_date as date) as end_date,
        status
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_subscription`
),

-- Generate date series for the period you have data
date_series as (
    select date_day
    from unnest(generate_date_array(
        -- FIX: Use a subquery to correctly resolve the aggregated minimum date,
        -- ensuring the correct DATE type is used for GENERATE_DATE_ARRAY.
        (select min(start_date) from subscriptions),
        current_date(),
        interval 1 day
    )) as date_day
),

daily_metrics as (
    select
        ds.date_day as date,
        
        -- Active subscribers (on that specific date)
        count(s.subscription_id) filter (
            where s.start_date <= ds.date_day 
            and (s.end_date is null or s.end_date > ds.date_day)
            and s.status = 'Active'
        ) as active_subscribers,
        
        -- New subscribers (started on that date)
        count(s.subscription_id) filter (
            where s.start_date = ds.date_day 
        ) as new_subscribers,
        
        -- Cancellations (ended on that date)
        count(s.subscription_id) filter (
            where s.end_date = ds.date_day 
        ) as cancellations

    from date_series ds
    cross join subscriptions s
    group by ds.date_day
)

select * from daily_metrics
order by date
    );
  
[0m22:49:58.190758 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:49:59.109050 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:0d551e70-85eb-4e9e-a303-9287cf50994d&page=queryresults
[0m22:49:59.109970 [error] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:0d551e70-85eb-4e9e-a303-9287cf50994d&page=queryresults
[0m22:49:59.117562 [debug] [Thread-1 (]: Database Error in model mart_subscription_daily (models/marts/mart_subscription_daily.sql)
  Syntax error: Expected ")" but got "(" at [46:41]
  compiled code at target/run/data_pipeline_project/models/marts/mart_subscription_daily.sql
[0m22:49:59.120884 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '02480d27-6318-4ed2-852f-fd765de59897', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x785738c46f30>]}
[0m22:49:59.122053 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model analytics_staging_marts.mart_subscription_daily  [[31mERROR[0m in 1.01s]
[0m22:49:59.123259 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.mart_subscription_daily
[0m22:49:59.124090 [debug] [Thread-7 (]: Marking all children of 'model.data_pipeline_project.mart_subscription_daily' to be skipped because of status 'error'.  Reason: Database Error in model mart_subscription_daily (models/marts/mart_subscription_daily.sql)
  Syntax error: Expected ")" but got "(" at [46:41]
  compiled code at target/run/data_pipeline_project/models/marts/mart_subscription_daily.sql.
[0m22:49:59.128580 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:49:59.137069 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:49:59.137621 [debug] [MainThread]: Connection 'list_data-pipeline-project-474812_analytics_staging_staging' was properly closed.
[0m22:49:59.138350 [debug] [MainThread]: Connection 'model.data_pipeline_project.mart_subscription_daily' was properly closed.
[0m22:49:59.138797 [info ] [MainThread]: 
[0m22:49:59.140081 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 2.89 seconds (2.89s).
[0m22:49:59.141141 [debug] [MainThread]: Command end result
[0m22:49:59.185066 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m22:49:59.192204 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m22:49:59.199155 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m22:49:59.199641 [info ] [MainThread]: 
[0m22:49:59.200255 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m22:49:59.200991 [info ] [MainThread]: 
[0m22:49:59.201920 [error] [MainThread]: [31mFailure in model mart_subscription_daily (models/marts/mart_subscription_daily.sql)[0m
[0m22:49:59.202546 [error] [MainThread]:   Database Error in model mart_subscription_daily (models/marts/mart_subscription_daily.sql)
  Syntax error: Expected ")" but got "(" at [46:41]
  compiled code at target/run/data_pipeline_project/models/marts/mart_subscription_daily.sql
[0m22:49:59.203069 [info ] [MainThread]: 
[0m22:49:59.203637 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_project/models/marts/mart_subscription_daily.sql
[0m22:49:59.204144 [info ] [MainThread]: 
[0m22:49:59.204702 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m22:49:59.205978 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 17 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m22:49:59.208078 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 7.8184385, "process_in_blocks": "0", "process_kernel_time": 1.180137, "process_mem_max_rss": "383820", "process_out_blocks": "4144", "process_user_time": 6.445991}
[0m22:49:59.208696 [debug] [MainThread]: Command `dbt run` failed at 22:49:59.208586 after 7.82 seconds
[0m22:49:59.209371 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78575a326120>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7857398858b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7857593cbfe0>]}
[0m22:49:59.210059 [debug] [MainThread]: Flushing usage events
[0m22:49:59.898134 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:51:00.440084 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76e1b98757c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76e1b8d20a40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76e1b96a77a0>]}


============================== 22:51:00.447197 | 845c8fb1-abcb-437a-9e4c-67ffc4fcc79b ==============================
[0m22:51:00.447197 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m22:51:00.453060 [debug] [MainThread]: running dbt with arguments {'target_path': 'None', 'use_colors': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'no_print': 'None', 'quiet': 'False', 'send_anonymous_usage_stats': 'True', 'invocation_command': 'dbt clean', 'partial_parse': 'True', 'static_parser': 'True', 'empty': 'None', 'indirect_selection': 'eager', 'version_check': 'True', 'debug': 'False', 'log_cache_events': 'False', 'log_format': 'default', 'introspect': 'True', 'fail_fast': 'False', 'cache_selected_only': 'False', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'write_json': 'True', 'profiles_dir': '/home/ecem/.dbt', 'printer_width': '80', 'warn_error': 'None', 'use_experimental_parser': 'False'}
[0m22:51:00.590234 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '845c8fb1-abcb-437a-9e4c-67ffc4fcc79b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76e1b87d1be0>]}
[0m22:51:00.613999 [debug] [MainThread]: Resource report: {"command_name": "clean", "command_success": true, "command_wall_clock_time": 0.23577084, "process_in_blocks": "0", "process_kernel_time": 0.261404, "process_mem_max_rss": "101572", "process_out_blocks": "16", "process_user_time": 1.45742}
[0m22:51:00.614916 [debug] [MainThread]: Command `dbt clean` succeeded at 22:51:00.614556 after 0.24 seconds
[0m22:51:00.615759 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76e1b87d1f40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76e1b86afad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76e1bb210800>]}
[0m22:51:00.616292 [debug] [MainThread]: Flushing usage events
[0m22:51:01.550903 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:51:05.875523 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e922a586570>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e922a2c93d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e922a1c0980>]}


============================== 22:51:05.880238 | 6da789b8-2253-4ce1-bb16-1b72b6903547 ==============================
[0m22:51:05.880238 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m22:51:05.881125 [debug] [MainThread]: running dbt with arguments {'use_colors': 'True', 'version_check': 'True', 'send_anonymous_usage_stats': 'True', 'profiles_dir': '/home/ecem/.dbt', 'introspect': 'True', 'cache_selected_only': 'False', 'no_print': 'None', 'static_parser': 'True', 'invocation_command': 'dbt run --select mart_subscription_daily --full-refresh', 'indirect_selection': 'eager', 'fail_fast': 'False', 'target_path': 'None', 'use_experimental_parser': 'False', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'empty': 'False', 'printer_width': '80', 'partial_parse': 'True', 'warn_error': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_cache_events': 'False', 'debug': 'False', 'write_json': 'True', 'log_format': 'default'}
[0m22:51:08.141783 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6da789b8-2253-4ce1-bb16-1b72b6903547', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e920ad332c0>]}
[0m22:51:08.275689 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6da789b8-2253-4ce1-bb16-1b72b6903547', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e920a7d57f0>]}
[0m22:51:08.277719 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m22:51:08.521481 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m22:51:08.525765 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m22:51:08.526785 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '6da789b8-2253-4ce1-bb16-1b72b6903547', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e9229dec2f0>]}
[0m22:51:09.860939 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `relationships` defined on 'stg_orders' in
package 'data_pipeline_project' (models/staging/staging_schema.yml). Arguments
to generic tests should be nested under the `arguments` property.
[0m22:51:09.862245 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '6da789b8-2253-4ce1-bb16-1b72b6903547', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e9209f451c0>]}
[0m22:51:10.198937 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6da789b8-2253-4ce1-bb16-1b72b6903547', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e920951c980>]}
[0m22:51:10.314041 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m22:51:10.316424 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m22:51:10.335249 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6da789b8-2253-4ce1-bb16-1b72b6903547', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e92094b3bc0>]}
[0m22:51:10.337405 [info ] [MainThread]: Found 18 models, 35 data tests, 10 sources, 508 macros
[0m22:51:10.338189 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6da789b8-2253-4ce1-bb16-1b72b6903547', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e922b74a900>]}
[0m22:51:10.340617 [info ] [MainThread]: 
[0m22:51:10.341186 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m22:51:10.341819 [info ] [MainThread]: 
[0m22:51:10.342522 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m22:51:10.344178 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812'
[0m22:51:10.345123 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:51:11.260214 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812, now list_data-pipeline-project-474812_analytics_staging_staging)
[0m22:51:11.261903 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:51:11.308732 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_marts'
[0m22:51:11.310729 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:51:12.226740 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6da789b8-2253-4ce1-bb16-1b72b6903547', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e92094b7500>]}
[0m22:51:12.227487 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:51:12.236636 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.mart_subscription_daily
[0m22:51:12.237806 [info ] [Thread-1 (]: 1 of 1 START sql table model analytics_staging_marts.mart_subscription_daily ... [RUN]
[0m22:51:12.238948 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_marts, now model.data_pipeline_project.mart_subscription_daily)
[0m22:51:12.240041 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.mart_subscription_daily
[0m22:51:12.248911 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.mart_subscription_daily"
[0m22:51:12.253054 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.mart_subscription_daily
[0m22:51:12.310613 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.mart_subscription_daily"
[0m22:51:12.313261 [debug] [Thread-1 (]: On model.data_pipeline_project.mart_subscription_daily: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.mart_subscription_daily"} */

  
    

    create or replace table `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily`
      
    
    

    
    OPTIONS()
    as (
      

with subscriptions as (
    -- Use the dimension table for clean start/end dates
    select 
        subscription_id,
        customer_id,
        -- CRITICAL FIX: Cast to DATE to match GENERATE_DATE_ARRAY requirements
        cast(start_date as date) as start_date, 
        cast(end_date as date) as end_date,
        status
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_subscription`
),

-- Generate date series for the period you have data
date_series as (
    select date_day
    from unnest(generate_date_array(
        -- Ensure MIN date is calculated from the CAST(DATE) column
        (select min(start_date) from subscriptions),
        current_date(),
        interval 1 day
    )) as date_day
),

daily_metrics as (
    select
        ds.date_day as date,
        
        -- Active subscribers (on that specific date) - Using standard COUNT(CASE WHEN...)
        count(case 
            when s.start_date <= ds.date_day 
            and (s.end_date is null or s.end_date > ds.date_day)
            and s.status = 'Active'
            then 1 
        end) as active_subscribers,
        
        -- New subscribers (started on that date) - Using standard COUNT(CASE WHEN...)
        count(case 
            when s.start_date = ds.date_day 
            then 1 
        end) as new_subscribers,
        
        -- Cancellations (ended on that date) - Using standard COUNT(CASE WHEN...)
        count(case 
            when s.end_date = ds.date_day 
            then 1 
        end) as cancellations

    from date_series ds
    cross join subscriptions s
    group by ds.date_day
)

select * from daily_metrics
order by date
    );
  
[0m22:51:12.315006 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:51:13.435894 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:7a1099ed-6350-4095-ab34-b62a602bc1a5&page=queryresults
[0m22:51:13.845394 [error] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:7a1099ed-6350-4095-ab34-b62a602bc1a5&page=queryresults
[0m22:51:13.853901 [debug] [Thread-1 (]: Database Error in model mart_subscription_daily (models/marts/mart_subscription_daily.sql)
  Invalid cast from INT64 to DATE at [23:14]
  compiled code at target/run/data_pipeline_project/models/marts/mart_subscription_daily.sql
[0m22:51:13.856782 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6da789b8-2253-4ce1-bb16-1b72b6903547', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e92095f1610>]}
[0m22:51:13.857860 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model analytics_staging_marts.mart_subscription_daily  [[31mERROR[0m in 1.62s]
[0m22:51:13.858995 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.mart_subscription_daily
[0m22:51:13.860004 [debug] [Thread-7 (]: Marking all children of 'model.data_pipeline_project.mart_subscription_daily' to be skipped because of status 'error'.  Reason: Database Error in model mart_subscription_daily (models/marts/mart_subscription_daily.sql)
  Invalid cast from INT64 to DATE at [23:14]
  compiled code at target/run/data_pipeline_project/models/marts/mart_subscription_daily.sql.
[0m22:51:13.865086 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:51:13.866966 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:51:13.867364 [debug] [MainThread]: Connection 'list_data-pipeline-project-474812_analytics_staging_staging' was properly closed.
[0m22:51:13.871624 [debug] [MainThread]: Connection 'model.data_pipeline_project.mart_subscription_daily' was properly closed.
[0m22:51:13.872111 [info ] [MainThread]: 
[0m22:51:13.873677 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 3.53 seconds (3.53s).
[0m22:51:13.875621 [debug] [MainThread]: Command end result
[0m22:51:13.964671 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m22:51:13.974096 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m22:51:13.983748 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m22:51:13.984744 [info ] [MainThread]: 
[0m22:51:13.985801 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m22:51:13.986854 [info ] [MainThread]: 
[0m22:51:13.987491 [error] [MainThread]: [31mFailure in model mart_subscription_daily (models/marts/mart_subscription_daily.sql)[0m
[0m22:51:13.988207 [error] [MainThread]:   Database Error in model mart_subscription_daily (models/marts/mart_subscription_daily.sql)
  Invalid cast from INT64 to DATE at [23:14]
  compiled code at target/run/data_pipeline_project/models/marts/mart_subscription_daily.sql
[0m22:51:13.988825 [info ] [MainThread]: 
[0m22:51:13.989267 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_project/models/marts/mart_subscription_daily.sql
[0m22:51:13.989930 [info ] [MainThread]: 
[0m22:51:13.990411 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m22:51:13.991289 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 17 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m22:51:13.993119 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 8.172136, "process_in_blocks": "0", "process_kernel_time": 1.223909, "process_mem_max_rss": "384088", "process_out_blocks": "4136", "process_user_time": 5.896763}
[0m22:51:13.993755 [debug] [MainThread]: Command `dbt run` failed at 22:51:13.993642 after 8.17 seconds
[0m22:51:13.994977 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e922bef1fa0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e922a9a9a30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e92094dedb0>]}
[0m22:51:13.995513 [debug] [MainThread]: Flushing usage events
[0m22:51:14.704261 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:51:56.828353 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6384ed4f20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f63845b5730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f63868db350>]}


============================== 22:51:56.834763 | 4ddb9f48-30bb-4122-a567-aa189f547510 ==============================
[0m22:51:56.834763 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m22:51:56.835845 [debug] [MainThread]: running dbt with arguments {'write_json': 'True', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'introspect': 'True', 'invocation_command': 'dbt clean', 'fail_fast': 'False', 'static_parser': 'True', 'no_print': 'None', 'debug': 'False', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'version_check': 'True', 'send_anonymous_usage_stats': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'printer_width': '80', 'quiet': 'False', 'target_path': 'None', 'log_format': 'default', 'use_experimental_parser': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'warn_error': 'None', 'profiles_dir': '/home/ecem/.dbt', 'use_colors': 'True', 'empty': 'None'}
[0m22:51:56.947335 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4ddb9f48-30bb-4122-a567-aa189f547510', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6383de7bf0>]}
[0m22:51:56.973387 [debug] [MainThread]: Resource report: {"command_name": "clean", "command_success": true, "command_wall_clock_time": 0.20401031, "process_in_blocks": "0", "process_kernel_time": 0.274436, "process_mem_max_rss": "101408", "process_out_blocks": "16", "process_user_time": 1.426347}
[0m22:51:56.974947 [debug] [MainThread]: Command `dbt clean` succeeded at 22:51:56.974716 after 0.21 seconds
[0m22:51:56.975567 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f63842d1af0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f63847b11f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6383cb3500>]}
[0m22:51:56.975988 [debug] [MainThread]: Flushing usage events
[0m22:51:57.679532 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:52:03.762479 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6cb74c97c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6cb7b5c560>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6cb7b5c7a0>]}


============================== 22:52:03.775822 | 277d17af-65cd-4d53-9f49-dc6101b9a4bb ==============================
[0m22:52:03.775822 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m22:52:03.781404 [debug] [MainThread]: running dbt with arguments {'no_print': 'None', 'printer_width': '80', 'fail_fast': 'False', 'log_cache_events': 'False', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'use_colors': 'True', 'use_experimental_parser': 'False', 'warn_error': 'None', 'indirect_selection': 'eager', 'introspect': 'True', 'log_format': 'default', 'profiles_dir': '/home/ecem/.dbt', 'quiet': 'False', 'static_parser': 'True', 'cache_selected_only': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'version_check': 'True', 'invocation_command': 'dbt run --select mart_subscription_daily --full-refresh', 'send_anonymous_usage_stats': 'True', 'target_path': 'None', 'debug': 'False', 'empty': 'False', 'partial_parse': 'True', 'write_json': 'True'}
[0m22:52:06.050240 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '277d17af-65cd-4d53-9f49-dc6101b9a4bb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6cb6d29520>]}
[0m22:52:06.114706 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '277d17af-65cd-4d53-9f49-dc6101b9a4bb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6cb7769e80>]}
[0m22:52:06.115522 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m22:52:06.378669 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m22:52:06.380044 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m22:52:06.380611 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '277d17af-65cd-4d53-9f49-dc6101b9a4bb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6cb7c2dfd0>]}
[0m22:52:07.724632 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `relationships` defined on 'stg_orders' in
package 'data_pipeline_project' (models/staging/staging_schema.yml). Arguments
to generic tests should be nested under the `arguments` property.
[0m22:52:07.725741 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '277d17af-65cd-4d53-9f49-dc6101b9a4bb', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6c96909190>]}
[0m22:52:08.060105 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '277d17af-65cd-4d53-9f49-dc6101b9a4bb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6c969d5af0>]}
[0m22:52:08.169049 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m22:52:08.171669 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m22:52:08.191116 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '277d17af-65cd-4d53-9f49-dc6101b9a4bb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6c967c2a80>]}
[0m22:52:08.191879 [info ] [MainThread]: Found 18 models, 35 data tests, 10 sources, 508 macros
[0m22:52:08.192477 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '277d17af-65cd-4d53-9f49-dc6101b9a4bb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6c969ec1d0>]}
[0m22:52:08.194893 [info ] [MainThread]: 
[0m22:52:08.195487 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m22:52:08.196462 [info ] [MainThread]: 
[0m22:52:08.198913 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m22:52:08.201006 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812'
[0m22:52:08.202217 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:52:09.284322 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812, now list_data-pipeline-project-474812_analytics_staging_staging)
[0m22:52:09.289074 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:52:09.297773 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_marts'
[0m22:52:09.299834 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:52:10.333800 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '277d17af-65cd-4d53-9f49-dc6101b9a4bb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6c9721b830>]}
[0m22:52:10.334863 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:52:10.346972 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.mart_subscription_daily
[0m22:52:10.347932 [info ] [Thread-1 (]: 1 of 1 START sql table model analytics_staging_marts.mart_subscription_daily ... [RUN]
[0m22:52:10.348922 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_marts, now model.data_pipeline_project.mart_subscription_daily)
[0m22:52:10.349459 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.mart_subscription_daily
[0m22:52:10.358904 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.mart_subscription_daily"
[0m22:52:10.359878 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.mart_subscription_daily
[0m22:52:10.410266 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.mart_subscription_daily"
[0m22:52:10.411896 [debug] [Thread-1 (]: On model.data_pipeline_project.mart_subscription_daily: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.mart_subscription_daily"} */

  
    

    create or replace table `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily`
      
    
    

    
    OPTIONS()
    as (
      

with subscriptions as (
    -- Use the dimension table, applying the Unix Timestamp conversion fix
    select 
        subscription_id,
        customer_id,
        -- FINAL FIX: Convert INT64 (assumed Unix Timestamp in seconds) to DATE
        date(timestamp_seconds(start_date)) as start_date, 
        -- Apply the same conversion to the end date
        date(timestamp_seconds(end_date)) as end_date,
        status
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_subscription`
),

-- Generate date series for the period you have data
date_series as (
    select date_day
    from unnest(generate_date_array(
        -- This now correctly resolves to a DATE type
        (select min(start_date) from subscriptions),
        current_date(),
        interval 1 day
    )) as date_day
),

daily_metrics as (
    select
        ds.date_day as date,
        
        -- Active subscribers (using standard COUNT(CASE WHEN...) syntax)
        count(case 
            when s.start_date <= ds.date_day 
            and (s.end_date is null or s.end_date > ds.date_day)
            and s.status = 'Active'
            then 1 
        end) as active_subscribers,
        
        -- New subscribers
        count(case 
            when s.start_date = ds.date_day 
            then 1 
        end) as new_subscribers,
        
        -- Cancellations
        count(case 
            when s.end_date = ds.date_day 
            then 1 
        end) as cancellations

    from date_series ds
    cross join subscriptions s
    group by ds.date_day
)

select * from daily_metrics
order by date
    );
  
[0m22:52:10.413087 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:52:11.654861 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:3392edc3-334a-480d-99e7-95a56075aae3&page=queryresults
[0m22:52:11.967394 [error] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:3392edc3-334a-480d-99e7-95a56075aae3&page=queryresults
[0m22:52:11.979714 [debug] [Thread-1 (]: Database Error in model mart_subscription_daily (models/marts/mart_subscription_daily.sql)
  No matching signature for function TIMESTAMP_SECONDS
    Argument types: TIMESTAMP
    Signature: TIMESTAMP_SECONDS(INT64)
      Argument 1: Unable to coerce type TIMESTAMP to expected type INT64 at [22:14]
  compiled code at target/run/data_pipeline_project/models/marts/mart_subscription_daily.sql
[0m22:52:11.982723 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '277d17af-65cd-4d53-9f49-dc6101b9a4bb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6c969e3110>]}
[0m22:52:11.983900 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model analytics_staging_marts.mart_subscription_daily  [[31mERROR[0m in 1.63s]
[0m22:52:11.985033 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.mart_subscription_daily
[0m22:52:11.985958 [debug] [Thread-7 (]: Marking all children of 'model.data_pipeline_project.mart_subscription_daily' to be skipped because of status 'error'.  Reason: Database Error in model mart_subscription_daily (models/marts/mart_subscription_daily.sql)
  No matching signature for function TIMESTAMP_SECONDS
    Argument types: TIMESTAMP
    Signature: TIMESTAMP_SECONDS(INT64)
      Argument 1: Unable to coerce type TIMESTAMP to expected type INT64 at [22:14]
  compiled code at target/run/data_pipeline_project/models/marts/mart_subscription_daily.sql.
[0m22:52:11.993267 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:52:11.994885 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:52:11.995469 [debug] [MainThread]: Connection 'list_data-pipeline-project-474812_analytics_staging_staging' was properly closed.
[0m22:52:11.996115 [debug] [MainThread]: Connection 'model.data_pipeline_project.mart_subscription_daily' was properly closed.
[0m22:52:11.998432 [info ] [MainThread]: 
[0m22:52:11.999330 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 3.80 seconds (3.80s).
[0m22:52:12.000089 [debug] [MainThread]: Command end result
[0m22:52:12.066278 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m22:52:12.070414 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m22:52:12.081033 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m22:52:12.081710 [info ] [MainThread]: 
[0m22:52:12.083099 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m22:52:12.083877 [info ] [MainThread]: 
[0m22:52:12.086992 [error] [MainThread]: [31mFailure in model mart_subscription_daily (models/marts/mart_subscription_daily.sql)[0m
[0m22:52:12.090891 [error] [MainThread]:   Database Error in model mart_subscription_daily (models/marts/mart_subscription_daily.sql)
  No matching signature for function TIMESTAMP_SECONDS
    Argument types: TIMESTAMP
    Signature: TIMESTAMP_SECONDS(INT64)
      Argument 1: Unable to coerce type TIMESTAMP to expected type INT64 at [22:14]
  compiled code at target/run/data_pipeline_project/models/marts/mart_subscription_daily.sql
[0m22:52:12.092193 [info ] [MainThread]: 
[0m22:52:12.093110 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_project/models/marts/mart_subscription_daily.sql
[0m22:52:12.093987 [info ] [MainThread]: 
[0m22:52:12.095027 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m22:52:12.096207 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 17 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m22:52:12.099465 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 8.41034, "process_in_blocks": "0", "process_kernel_time": 1.389787, "process_mem_max_rss": "384020", "process_out_blocks": "4136", "process_user_time": 5.678586}
[0m22:52:12.100340 [debug] [MainThread]: Command `dbt run` failed at 22:52:12.100209 after 8.41 seconds
[0m22:52:12.101079 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6cb6e14fb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6cb7bb19d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6c9778a390>]}
[0m22:52:12.101800 [debug] [MainThread]: Flushing usage events
[0m22:52:12.978255 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:54:40.394506 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72940c1ec830>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72940ba25130>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72940ba273e0>]}


============================== 22:54:40.399155 | 8d50eb2b-9bb6-4b75-970e-184406451f0f ==============================
[0m22:54:40.399155 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m22:54:40.402043 [debug] [MainThread]: running dbt with arguments {'indirect_selection': 'eager', 'printer_width': '80', 'cache_selected_only': 'False', 'log_cache_events': 'False', 'static_parser': 'True', 'use_experimental_parser': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'write_json': 'True', 'use_colors': 'True', 'send_anonymous_usage_stats': 'True', 'partial_parse': 'True', 'quiet': 'False', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'log_format': 'default', 'introspect': 'True', 'profiles_dir': '/home/ecem/.dbt', 'warn_error': 'None', 'no_print': 'None', 'invocation_command': 'dbt clean', 'fail_fast': 'False', 'empty': 'None', 'version_check': 'True', 'debug': 'False'}
[0m22:54:40.550417 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8d50eb2b-9bb6-4b75-970e-184406451f0f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72940bc8c9b0>]}
[0m22:54:40.573596 [debug] [MainThread]: Resource report: {"command_name": "clean", "command_success": true, "command_wall_clock_time": 0.23317768, "process_in_blocks": "0", "process_kernel_time": 0.175026, "process_mem_max_rss": "101724", "process_out_blocks": "16", "process_user_time": 1.546502}
[0m22:54:40.574179 [debug] [MainThread]: Command `dbt clean` succeeded at 22:54:40.574048 after 0.23 seconds
[0m22:54:40.574603 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72940b12fc50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72940b0d9c10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72940b043500>]}
[0m22:54:40.575017 [debug] [MainThread]: Flushing usage events
[0m22:54:41.498054 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:54:52.583982 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x745399319610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7453993463f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74539951ade0>]}


============================== 22:54:52.590071 | 80b9e9f3-ac9c-4ca6-a5e2-6c8b9e5fe7fe ==============================
[0m22:54:52.590071 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m22:54:52.593123 [debug] [MainThread]: running dbt with arguments {'debug': 'False', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'log_format': 'default', 'warn_error': 'None', 'send_anonymous_usage_stats': 'True', 'printer_width': '80', 'static_parser': 'True', 'cache_selected_only': 'False', 'target_path': 'None', 'write_json': 'True', 'fail_fast': 'False', 'quiet': 'False', 'version_check': 'True', 'use_experimental_parser': 'False', 'profiles_dir': '/home/ecem/.dbt', 'use_colors': 'True', 'no_print': 'None', 'partial_parse': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'empty': 'False', 'invocation_command': 'dbt run --select mart_subscription_daily --full-refresh', 'introspect': 'True'}
[0m22:54:54.837906 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '80b9e9f3-ac9c-4ca6-a5e2-6c8b9e5fe7fe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74539b223c80>]}
[0m22:54:54.951343 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '80b9e9f3-ac9c-4ca6-a5e2-6c8b9e5fe7fe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x745379f1d340>]}
[0m22:54:54.953213 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m22:54:55.185269 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m22:54:55.187313 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m22:54:55.189528 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '80b9e9f3-ac9c-4ca6-a5e2-6c8b9e5fe7fe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7453795e9370>]}
[0m22:54:56.504765 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `relationships` defined on 'stg_orders' in
package 'data_pipeline_project' (models/staging/staging_schema.yml). Arguments
to generic tests should be nested under the `arguments` property.
[0m22:54:56.512663 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '80b9e9f3-ac9c-4ca6-a5e2-6c8b9e5fe7fe', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x745378e23fb0>]}
[0m22:54:56.847971 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '80b9e9f3-ac9c-4ca6-a5e2-6c8b9e5fe7fe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7453785d1dc0>]}
[0m22:54:56.969417 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m22:54:56.974655 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m22:54:56.995001 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '80b9e9f3-ac9c-4ca6-a5e2-6c8b9e5fe7fe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7453783c65d0>]}
[0m22:54:56.995813 [info ] [MainThread]: Found 18 models, 35 data tests, 10 sources, 508 macros
[0m22:54:56.996487 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '80b9e9f3-ac9c-4ca6-a5e2-6c8b9e5fe7fe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74537834d970>]}
[0m22:54:56.998688 [info ] [MainThread]: 
[0m22:54:56.999470 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m22:54:57.000207 [info ] [MainThread]: 
[0m22:54:57.001259 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m22:54:57.003017 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812'
[0m22:54:57.003740 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:54:58.104478 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812, now list_data-pipeline-project-474812_analytics_staging_marts)
[0m22:54:58.110442 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:54:58.115890 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_staging'
[0m22:54:58.177951 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:54:59.372737 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '80b9e9f3-ac9c-4ca6-a5e2-6c8b9e5fe7fe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7453783b2660>]}
[0m22:54:59.374081 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:54:59.388097 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.mart_subscription_daily
[0m22:54:59.389460 [info ] [Thread-1 (]: 1 of 1 START sql table model analytics_staging_marts.mart_subscription_daily ... [RUN]
[0m22:54:59.390597 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now model.data_pipeline_project.mart_subscription_daily)
[0m22:54:59.391200 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.mart_subscription_daily
[0m22:54:59.401929 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.mart_subscription_daily"
[0m22:54:59.403631 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.mart_subscription_daily
[0m22:54:59.453768 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.mart_subscription_daily"
[0m22:54:59.456673 [debug] [Thread-1 (]: On model.data_pipeline_project.mart_subscription_daily: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.mart_subscription_daily"} */

  
    

    create or replace table `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily`
      
    
    

    
    OPTIONS()
    as (
      with subscriptions as (
    -- Use the dimension table for clean start/end dates
    select 
        subscription_id,
        customer_id,
        -- DEFINITIVE FIX: Just convert the existing TIMESTAMP/DATETIME to a DATE.
        cast(start_date as date) as start_date, 
        cast(end_date as date) as end_date,
        status
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_subscription`
),

-- Generate date series for the period you have data
date_series as (
    select date_day
    from unnest(generate_date_array(
        -- This correctly uses the MIN value of the new DATE column
        (select min(start_date) from subscriptions),
        current_date(),
        interval 1 day
    )) as date_day
),

daily_metrics as (
    select
        ds.date_day as date,
        
        -- Active subscribers (using standard COUNT(CASE WHEN...) syntax)
        count(case 
            when s.start_date <= ds.date_day 
            and (s.end_date is null or s.end_date > ds.date_day)
            and s.status = 'Active'
            then 1 
        end) as active_subscribers,
        
        -- New subscribers
        count(case 
            when s.start_date = ds.date_day 
            then 1 
        end) as new_subscribers,
        
        -- Cancellations
        count(case 
            when s.end_date = ds.date_day 
            then 1 
        end) as cancellations

    from date_series ds
    cross join subscriptions s
    group by ds.date_day
)

select * from daily_metrics
order by date
    );
  
[0m22:54:59.458318 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:55:00.928683 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:eb56284a-992c-4354-83a1-2da34843f80c&page=queryresults
[0m22:55:01.227865 [error] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:eb56284a-992c-4354-83a1-2da34843f80c&page=queryresults
[0m22:55:01.251980 [debug] [Thread-1 (]: Database Error in model mart_subscription_daily (models/marts/mart_subscription_daily.sql)
  Invalid cast from INT64 to DATE at [21:14]
  compiled code at target/run/data_pipeline_project/models/marts/mart_subscription_daily.sql
[0m22:55:01.255591 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '80b9e9f3-ac9c-4ca6-a5e2-6c8b9e5fe7fe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7453785891f0>]}
[0m22:55:01.261325 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model analytics_staging_marts.mart_subscription_daily  [[31mERROR[0m in 1.86s]
[0m22:55:01.263189 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.mart_subscription_daily
[0m22:55:01.266824 [debug] [Thread-7 (]: Marking all children of 'model.data_pipeline_project.mart_subscription_daily' to be skipped because of status 'error'.  Reason: Database Error in model mart_subscription_daily (models/marts/mart_subscription_daily.sql)
  Invalid cast from INT64 to DATE at [21:14]
  compiled code at target/run/data_pipeline_project/models/marts/mart_subscription_daily.sql.
[0m22:55:01.272255 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:55:01.275864 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:55:01.278131 [debug] [MainThread]: Connection 'list_data-pipeline-project-474812_analytics_staging_marts' was properly closed.
[0m22:55:01.278708 [debug] [MainThread]: Connection 'model.data_pipeline_project.mart_subscription_daily' was properly closed.
[0m22:55:01.279092 [info ] [MainThread]: 
[0m22:55:01.282092 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 4.28 seconds (4.28s).
[0m22:55:01.284170 [debug] [MainThread]: Command end result
[0m22:55:01.356210 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m22:55:01.362791 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m22:55:01.369633 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m22:55:01.370341 [info ] [MainThread]: 
[0m22:55:01.371341 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m22:55:01.372276 [info ] [MainThread]: 
[0m22:55:01.373290 [error] [MainThread]: [31mFailure in model mart_subscription_daily (models/marts/mart_subscription_daily.sql)[0m
[0m22:55:01.374053 [error] [MainThread]:   Database Error in model mart_subscription_daily (models/marts/mart_subscription_daily.sql)
  Invalid cast from INT64 to DATE at [21:14]
  compiled code at target/run/data_pipeline_project/models/marts/mart_subscription_daily.sql
[0m22:55:01.375190 [info ] [MainThread]: 
[0m22:55:01.376340 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_project/models/marts/mart_subscription_daily.sql
[0m22:55:01.377439 [info ] [MainThread]: 
[0m22:55:01.378522 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m22:55:01.380029 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 17 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m22:55:01.384179 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 8.874676, "process_in_blocks": "0", "process_kernel_time": 1.508142, "process_mem_max_rss": "383656", "process_out_blocks": "4136", "process_user_time": 5.719521}
[0m22:55:01.385047 [debug] [MainThread]: Command `dbt run` failed at 22:55:01.384782 after 8.88 seconds
[0m22:55:01.385772 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7453989cddc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74537849e6f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7453998a9b50>]}
[0m22:55:01.386291 [debug] [MainThread]: Flushing usage events
[0m22:55:02.159825 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:56:58.850590 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77d271256060>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77d2733f3c50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77d2715fd520>]}


============================== 22:56:58.855248 | 55a1d2dd-ba2c-4edf-8ddc-f4911c1f3c0d ==============================
[0m22:56:58.855248 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m22:56:58.856715 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'partial_parse': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'printer_width': '80', 'introspect': 'True', 'profiles_dir': '/home/ecem/.dbt', 'debug': 'False', 'empty': 'None', 'write_json': 'True', 'invocation_command': 'dbt clean', 'log_format': 'default', 'no_print': 'None', 'indirect_selection': 'eager', 'send_anonymous_usage_stats': 'True', 'log_cache_events': 'False', 'use_experimental_parser': 'False', 'cache_selected_only': 'False', 'target_path': 'None', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'version_check': 'True', 'quiet': 'False', 'warn_error': 'None'}
[0m22:56:59.006591 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '55a1d2dd-ba2c-4edf-8ddc-f4911c1f3c0d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77d27128f260>]}
[0m22:56:59.028789 [debug] [MainThread]: Resource report: {"command_name": "clean", "command_success": true, "command_wall_clock_time": 0.23419696, "process_in_blocks": "0", "process_kernel_time": 0.270603, "process_mem_max_rss": "101764", "process_out_blocks": "16", "process_user_time": 1.421569}
[0m22:56:59.029443 [debug] [MainThread]: Command `dbt clean` succeeded at 22:56:59.029332 after 0.24 seconds
[0m22:56:59.029890 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77d270b2be60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77d270b73cb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77d272d50ec0>]}
[0m22:56:59.030314 [debug] [MainThread]: Flushing usage events
[0m22:57:00.000218 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:57:04.781450 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b6943b40d40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b6943cd1640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b6945dfd7f0>]}


============================== 22:57:04.786165 | 851e9a58-9381-4469-b477-4becb4b56a93 ==============================
[0m22:57:04.786165 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m22:57:04.787043 [debug] [MainThread]: running dbt with arguments {'send_anonymous_usage_stats': 'True', 'empty': 'False', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_experimental_parser': 'False', 'introspect': 'True', 'write_json': 'True', 'printer_width': '80', 'warn_error': 'None', 'debug': 'False', 'invocation_command': 'dbt run --select mart_subscription_daily --full-refresh', 'static_parser': 'True', 'version_check': 'True', 'quiet': 'False', 'indirect_selection': 'eager', 'profiles_dir': '/home/ecem/.dbt', 'log_cache_events': 'False', 'no_print': 'None', 'cache_selected_only': 'False', 'partial_parse': 'True', 'log_format': 'default', 'use_colors': 'True', 'fail_fast': 'False'}
[0m22:57:07.124121 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '851e9a58-9381-4469-b477-4becb4b56a93', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b69240639b0>]}
[0m22:57:07.199739 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '851e9a58-9381-4469-b477-4becb4b56a93', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b6923f0ce00>]}
[0m22:57:07.201410 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m22:57:07.442077 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m22:57:07.442967 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m22:57:07.443980 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '851e9a58-9381-4469-b477-4becb4b56a93', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b6923ec7560>]}
[0m22:57:08.804163 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `relationships` defined on 'stg_orders' in
package 'data_pipeline_project' (models/staging/staging_schema.yml). Arguments
to generic tests should be nested under the `arguments` property.
[0m22:57:08.806213 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '851e9a58-9381-4469-b477-4becb4b56a93', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b6923729d90>]}
[0m22:57:09.136173 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '851e9a58-9381-4469-b477-4becb4b56a93', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b6922dec620>]}
[0m22:57:09.271660 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m22:57:09.273861 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m22:57:09.296082 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '851e9a58-9381-4469-b477-4becb4b56a93', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b6922cce210>]}
[0m22:57:09.297065 [info ] [MainThread]: Found 18 models, 35 data tests, 10 sources, 508 macros
[0m22:57:09.297996 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '851e9a58-9381-4469-b477-4becb4b56a93', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b692379ca70>]}
[0m22:57:09.300320 [info ] [MainThread]: 
[0m22:57:09.301285 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m22:57:09.302227 [info ] [MainThread]: 
[0m22:57:09.303607 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m22:57:09.304933 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812'
[0m22:57:09.305520 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:57:10.361782 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_staging'
[0m22:57:10.363017 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812, now list_data-pipeline-project-474812_analytics_staging_marts)
[0m22:57:10.364099 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:57:10.367618 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:57:11.329175 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '851e9a58-9381-4469-b477-4becb4b56a93', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b6922d10bf0>]}
[0m22:57:11.330065 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:57:11.340740 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.mart_subscription_daily
[0m22:57:11.341857 [info ] [Thread-1 (]: 1 of 1 START sql table model analytics_staging_marts.mart_subscription_daily ... [RUN]
[0m22:57:11.343035 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now model.data_pipeline_project.mart_subscription_daily)
[0m22:57:11.343569 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.mart_subscription_daily
[0m22:57:11.353245 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.mart_subscription_daily"
[0m22:57:11.354885 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.mart_subscription_daily
[0m22:57:11.410781 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.mart_subscription_daily"
[0m22:57:11.412271 [debug] [Thread-1 (]: On model.data_pipeline_project.mart_subscription_daily: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.mart_subscription_daily"} */

  
    

    create or replace table `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily`
      
    
    

    
    OPTIONS()
    as (
      with subscriptions as (
    -- Use the dimension table for clean start/end dates
    select 
        subscription_id,
        customer_id,
        -- FINAL FIX: Convert the raw INT64 (assumed Unix Milliseconds) to DATE
        date(timestamp_millis(start_date)) as start_date, 
        date(timestamp_millis(end_date)) as end_date,
        status
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_subscription`
),

-- Generate date series for the period you have data
date_series as (
    select date_day
    from unnest(generate_date_array(
        -- This correctly uses the MIN value of the new DATE column
        (select min(start_date) from subscriptions),
        current_date(),
        interval 1 day
    )) as date_day
),

daily_metrics as (
    select
        ds.date_day as date,
        
        -- Active subscribers (using standard COUNT(CASE WHEN...) syntax)
        count(case 
            when s.start_date <= ds.date_day 
            and (s.end_date is null or s.end_date > ds.date_day)
            and s.status = 'Active'
            then 1 
        end) as active_subscribers,
        
        -- New subscribers
        count(case 
            when s.start_date = ds.date_day 
            then 1 
        end) as new_subscribers,
        
        -- Cancellations
        count(case 
            when s.end_date = ds.date_day 
            then 1 
        end) as cancellations

    from date_series ds
    cross join subscriptions s
    group by ds.date_day
)

select * from daily_metrics
order by date
    );
  
[0m22:57:11.413175 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:57:12.658591 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:1b88791f-d6e8-494a-8481-01ef7ade3094&page=queryresults
[0m22:57:12.930747 [error] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:1b88791f-d6e8-494a-8481-01ef7ade3094&page=queryresults
[0m22:57:12.940171 [debug] [Thread-1 (]: Database Error in model mart_subscription_daily (models/marts/mart_subscription_daily.sql)
  No matching signature for function TIMESTAMP_MILLIS
    Argument types: TIMESTAMP
    Signature: TIMESTAMP_MILLIS(INT64)
      Argument 1: Unable to coerce type TIMESTAMP to expected type INT64 at [20:14]
  compiled code at target/run/data_pipeline_project/models/marts/mart_subscription_daily.sql
[0m22:57:12.944752 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '851e9a58-9381-4469-b477-4becb4b56a93', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b69459859d0>]}
[0m22:57:12.945613 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model analytics_staging_marts.mart_subscription_daily  [[31mERROR[0m in 1.60s]
[0m22:57:12.947506 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.mart_subscription_daily
[0m22:57:12.949013 [debug] [Thread-7 (]: Marking all children of 'model.data_pipeline_project.mart_subscription_daily' to be skipped because of status 'error'.  Reason: Database Error in model mart_subscription_daily (models/marts/mart_subscription_daily.sql)
  No matching signature for function TIMESTAMP_MILLIS
    Argument types: TIMESTAMP
    Signature: TIMESTAMP_MILLIS(INT64)
      Argument 1: Unable to coerce type TIMESTAMP to expected type INT64 at [20:14]
  compiled code at target/run/data_pipeline_project/models/marts/mart_subscription_daily.sql.
[0m22:57:12.954694 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:57:12.957386 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:57:12.957842 [debug] [MainThread]: Connection 'list_data-pipeline-project-474812_analytics_staging_marts' was properly closed.
[0m22:57:12.958178 [debug] [MainThread]: Connection 'model.data_pipeline_project.mart_subscription_daily' was properly closed.
[0m22:57:12.958580 [info ] [MainThread]: 
[0m22:57:12.959187 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 3.66 seconds (3.66s).
[0m22:57:12.960442 [debug] [MainThread]: Command end result
[0m22:57:12.990966 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m22:57:12.993494 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m22:57:13.008728 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m22:57:13.009308 [info ] [MainThread]: 
[0m22:57:13.010147 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m22:57:13.011078 [info ] [MainThread]: 
[0m22:57:13.017405 [error] [MainThread]: [31mFailure in model mart_subscription_daily (models/marts/mart_subscription_daily.sql)[0m
[0m22:57:13.018813 [error] [MainThread]:   Database Error in model mart_subscription_daily (models/marts/mart_subscription_daily.sql)
  No matching signature for function TIMESTAMP_MILLIS
    Argument types: TIMESTAMP
    Signature: TIMESTAMP_MILLIS(INT64)
      Argument 1: Unable to coerce type TIMESTAMP to expected type INT64 at [20:14]
  compiled code at target/run/data_pipeline_project/models/marts/mart_subscription_daily.sql
[0m22:57:13.020018 [info ] [MainThread]: 
[0m22:57:13.021117 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_project/models/marts/mart_subscription_daily.sql
[0m22:57:13.022022 [info ] [MainThread]: 
[0m22:57:13.022768 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m22:57:13.024107 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 17 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m22:57:13.027470 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 8.302028, "process_in_blocks": "0", "process_kernel_time": 1.158259, "process_mem_max_rss": "384156", "process_out_blocks": "4136", "process_user_time": 6.11485}
[0m22:57:13.030896 [debug] [MainThread]: Command `dbt run` failed at 22:57:13.029286 after 8.30 seconds
[0m22:57:13.032522 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b6943fd7e90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b6922da6570>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b6922eafa40>]}
[0m22:57:13.033893 [debug] [MainThread]: Flushing usage events
[0m22:57:13.736187 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:58:45.225125 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x708b5a1eaf00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x708b5a292780>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x708b57bdb650>]}


============================== 22:58:45.236976 | 6e9f8a6b-b80e-491a-a034-b02882e73ada ==============================
[0m22:58:45.236976 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m22:58:45.239980 [debug] [MainThread]: running dbt with arguments {'cache_selected_only': 'False', 'empty': 'False', 'fail_fast': 'False', 'profiles_dir': '/home/ecem/.dbt', 'version_check': 'True', 'debug': 'False', 'no_print': 'None', 'static_parser': 'True', 'use_colors': 'True', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'printer_width': '80', 'warn_error': 'None', 'indirect_selection': 'eager', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'quiet': 'False', 'invocation_command': 'dbt run --select mart_subscription_daily --full-refresh', 'target_path': 'None', 'send_anonymous_usage_stats': 'True', 'use_experimental_parser': 'False', 'log_format': 'default', 'partial_parse': 'True'}
[0m22:58:47.507738 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6e9f8a6b-b80e-491a-a034-b02882e73ada', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x708b38ecffe0>]}
[0m22:58:47.574491 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6e9f8a6b-b80e-491a-a034-b02882e73ada', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x708b59e6e390>]}
[0m22:58:47.582170 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m22:58:47.955894 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m22:58:48.254509 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m22:58:48.255389 [debug] [MainThread]: Partial parsing: updated file: data_pipeline_project://models/marts/mart_subscription_daily.sql
[0m22:58:48.547796 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6e9f8a6b-b80e-491a-a034-b02882e73ada', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x708b37cf75c0>]}
[0m22:58:48.655912 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m22:58:48.657966 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m22:58:48.673073 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6e9f8a6b-b80e-491a-a034-b02882e73ada', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x708b373018e0>]}
[0m22:58:48.673859 [info ] [MainThread]: Found 18 models, 35 data tests, 10 sources, 508 macros
[0m22:58:48.674750 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6e9f8a6b-b80e-491a-a034-b02882e73ada', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x708b37c49f10>]}
[0m22:58:48.679856 [info ] [MainThread]: 
[0m22:58:48.683750 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m22:58:48.684755 [info ] [MainThread]: 
[0m22:58:48.685598 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m22:58:48.687450 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812'
[0m22:58:48.688608 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:58:49.642794 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812, now list_data-pipeline-project-474812_analytics_staging_staging)
[0m22:58:49.643818 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:58:49.677522 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_marts'
[0m22:58:49.679050 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:58:50.565285 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6e9f8a6b-b80e-491a-a034-b02882e73ada', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x708b577c4110>]}
[0m22:58:50.566229 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:58:50.574660 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.mart_subscription_daily
[0m22:58:50.575670 [info ] [Thread-1 (]: 1 of 1 START sql table model analytics_staging_marts.mart_subscription_daily ... [RUN]
[0m22:58:50.576642 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_marts, now model.data_pipeline_project.mart_subscription_daily)
[0m22:58:50.577196 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.mart_subscription_daily
[0m22:58:50.587263 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.mart_subscription_daily"
[0m22:58:50.588578 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.mart_subscription_daily
[0m22:58:50.641088 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.mart_subscription_daily"
[0m22:58:50.643335 [debug] [Thread-1 (]: On model.data_pipeline_project.mart_subscription_daily: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.mart_subscription_daily"} */

  
    

    create or replace table `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily`
      
    
    

    
    OPTIONS()
    as (
      with subscriptions as (
    -- Use the dimension table for clean start/end dates
    select 
        subscription_id,
        customer_id,
        -- FIX: Columns are already TIMESTAMP, just cast to DATE
        date(start_date) as start_date,
        date(end_date) as end_date,
        status
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_subscription`
),

-- Generate date series for the period you have data
date_series as (
    select date_day
    from unnest(generate_date_array(
        (select min(start_date) from subscriptions),
        current_date(),
        interval 1 day
    )) as date_day
),

daily_metrics as (
    select
        ds.date_day as date,
        
        -- Active subscribers
        count(case 
            when s.start_date <= ds.date_day 
            and (s.end_date is null or s.end_date > ds.date_day)
            and s.status = 'Active'
            then 1 
        end) as active_subscribers,
        
        -- New subscribers
        count(case 
            when s.start_date = ds.date_day 
            then 1 
        end) as new_subscribers,
        
        -- Cancellations
        count(case 
            when s.end_date = ds.date_day 
            then 1 
        end) as cancellations

    from date_series ds
    cross join subscriptions s
    group by ds.date_day
)

select * from daily_metrics
order by date
    );
  
[0m22:58:50.644267 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:58:51.804306 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:dc0f9df0-6274-42e0-931c-5a3195a96238&page=queryresults
[0m22:58:52.089136 [error] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:dc0f9df0-6274-42e0-931c-5a3195a96238&page=queryresults
[0m22:58:52.097245 [debug] [Thread-1 (]: Database Error in model mart_subscription_daily (models/marts/mart_subscription_daily.sql)
  No matching signature for function DATE
    Argument types: INT64
    Signature: DATE(TIMESTAMP, [STRING])
      Argument 1: Unable to coerce type INT64 to expected type TIMESTAMP
    Signature: DATE(DATETIME)
      Argument 1: Unable to coerce type INT64 to expected type DATETIME
    Signature: DATE(INT64, INT64, INT64)
      Signature requires at least 3 arguments, found 1 argument
    Signature: DATE(DATE)
      Argument 1: Unable to coerce type INT64 to expected type DATE
    Signature: DATE(STRING)
      Argument 1: Unable to coerce type INT64 to expected type STRING at [21:9]
  compiled code at target/run/data_pipeline_project/models/marts/mart_subscription_daily.sql
[0m22:58:52.099138 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6e9f8a6b-b80e-491a-a034-b02882e73ada', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x708b5864bcb0>]}
[0m22:58:52.099916 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model analytics_staging_marts.mart_subscription_daily  [[31mERROR[0m in 1.52s]
[0m22:58:52.103997 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.mart_subscription_daily
[0m22:58:52.105293 [debug] [Thread-7 (]: Marking all children of 'model.data_pipeline_project.mart_subscription_daily' to be skipped because of status 'error'.  Reason: Database Error in model mart_subscription_daily (models/marts/mart_subscription_daily.sql)
  No matching signature for function DATE
    Argument types: INT64
    Signature: DATE(TIMESTAMP, [STRING])
      Argument 1: Unable to coerce type INT64 to expected type TIMESTAMP
    Signature: DATE(DATETIME)
      Argument 1: Unable to coerce type INT64 to expected type DATETIME
    Signature: DATE(INT64, INT64, INT64)
      Signature requires at least 3 arguments, found 1 argument
    Signature: DATE(DATE)
      Argument 1: Unable to coerce type INT64 to expected type DATE
    Signature: DATE(STRING)
      Argument 1: Unable to coerce type INT64 to expected type STRING at [21:9]
  compiled code at target/run/data_pipeline_project/models/marts/mart_subscription_daily.sql.
[0m22:58:52.112319 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:58:52.114774 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:58:52.115468 [debug] [MainThread]: Connection 'list_data-pipeline-project-474812_analytics_staging_staging' was properly closed.
[0m22:58:52.116113 [debug] [MainThread]: Connection 'model.data_pipeline_project.mart_subscription_daily' was properly closed.
[0m22:58:52.119075 [info ] [MainThread]: 
[0m22:58:52.119518 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 3.43 seconds (3.43s).
[0m22:58:52.120674 [debug] [MainThread]: Command end result
[0m22:58:52.168644 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m22:58:52.173194 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m22:58:52.189085 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m22:58:52.190292 [info ] [MainThread]: 
[0m22:58:52.191671 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m22:58:52.193140 [info ] [MainThread]: 
[0m22:58:52.194382 [error] [MainThread]: [31mFailure in model mart_subscription_daily (models/marts/mart_subscription_daily.sql)[0m
[0m22:58:52.198443 [error] [MainThread]:   Database Error in model mart_subscription_daily (models/marts/mart_subscription_daily.sql)
  No matching signature for function DATE
    Argument types: INT64
    Signature: DATE(TIMESTAMP, [STRING])
      Argument 1: Unable to coerce type INT64 to expected type TIMESTAMP
    Signature: DATE(DATETIME)
      Argument 1: Unable to coerce type INT64 to expected type DATETIME
    Signature: DATE(INT64, INT64, INT64)
      Signature requires at least 3 arguments, found 1 argument
    Signature: DATE(DATE)
      Argument 1: Unable to coerce type INT64 to expected type DATE
    Signature: DATE(STRING)
      Argument 1: Unable to coerce type INT64 to expected type STRING at [21:9]
  compiled code at target/run/data_pipeline_project/models/marts/mart_subscription_daily.sql
[0m22:58:52.200580 [info ] [MainThread]: 
[0m22:58:52.201537 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_project/models/marts/mart_subscription_daily.sql
[0m22:58:52.202167 [info ] [MainThread]: 
[0m22:58:52.202840 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m22:58:52.208794 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 7.036815, "process_in_blocks": "0", "process_kernel_time": 1.490738, "process_mem_max_rss": "379332", "process_out_blocks": "4168", "process_user_time": 4.394787}
[0m22:58:52.209414 [debug] [MainThread]: Command `dbt run` failed at 22:58:52.209311 after 7.04 seconds
[0m22:58:52.209877 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x708b578a8830>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x708b58671bb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x708b37d508c0>]}
[0m22:58:52.211540 [debug] [MainThread]: Flushing usage events
[0m22:58:53.195040 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m23:05:14.475899 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x705483a00dd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x705483dc8980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x705483a001d0>]}


============================== 23:05:14.479718 | eb1b40d1-8188-411b-8c54-4a097a9a2772 ==============================
[0m23:05:14.479718 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m23:05:14.480720 [debug] [MainThread]: running dbt with arguments {'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'target_path': 'None', 'cache_selected_only': 'False', 'version_check': 'True', 'use_colors': 'True', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'indirect_selection': 'eager', 'no_print': 'None', 'debug': 'False', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'invocation_command': 'dbt clean', 'profiles_dir': '/home/ecem/.dbt', 'introspect': 'True', 'warn_error': 'None', 'static_parser': 'True', 'quiet': 'False', 'send_anonymous_usage_stats': 'True', 'fail_fast': 'False', 'use_experimental_parser': 'False', 'empty': 'None', 'log_format': 'default'}
[0m23:05:14.634057 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'eb1b40d1-8188-411b-8c54-4a097a9a2772', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7054837db6b0>]}
[0m23:05:14.654674 [debug] [MainThread]: Resource report: {"command_name": "clean", "command_success": true, "command_wall_clock_time": 0.26008853, "process_in_blocks": "0", "process_kernel_time": 0.23603, "process_mem_max_rss": "101528", "process_out_blocks": "8", "process_user_time": 1.49767}
[0m23:05:14.658973 [debug] [MainThread]: Command `dbt clean` succeeded at 23:05:14.658704 after 0.26 seconds
[0m23:05:14.659961 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70548406f230>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7054836b4230>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x705483a001a0>]}
[0m23:05:14.660457 [debug] [MainThread]: Flushing usage events
[0m23:05:15.600368 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m23:05:31.014938 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e39cdfc0800>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e39ce84f1d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e39cdfc2a20>]}


============================== 23:05:31.018053 | d9c2b676-1bb3-4c07-be95-ce5e76548d1f ==============================
[0m23:05:31.018053 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m23:05:31.019122 [debug] [MainThread]: running dbt with arguments {'use_colors': 'True', 'empty': 'False', 'write_json': 'True', 'version_check': 'True', 'cache_selected_only': 'False', 'debug': 'False', 'target_path': 'None', 'invocation_command': 'dbt run --select dim_subscription', 'indirect_selection': 'eager', 'log_format': 'default', 'partial_parse': 'True', 'static_parser': 'True', 'profiles_dir': '/home/ecem/.dbt', 'send_anonymous_usage_stats': 'True', 'use_experimental_parser': 'False', 'warn_error': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'no_print': 'None', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'introspect': 'True', 'fail_fast': 'False', 'printer_width': '80', 'log_cache_events': 'False'}
[0m23:05:33.451896 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd9c2b676-1bb3-4c07-be95-ce5e76548d1f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e39ae010650>]}
[0m23:05:33.519715 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd9c2b676-1bb3-4c07-be95-ce5e76548d1f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e39ae45f380>]}
[0m23:05:33.522683 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m23:05:33.754351 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m23:05:33.755415 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m23:05:33.755927 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'd9c2b676-1bb3-4c07-be95-ce5e76548d1f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e39ae0b71d0>]}
[0m23:05:35.097892 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `relationships` defined on 'stg_orders' in
package 'data_pipeline_project' (models/staging/staging_schema.yml). Arguments
to generic tests should be nested under the `arguments` property.
[0m23:05:35.099869 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': 'd9c2b676-1bb3-4c07-be95-ce5e76548d1f', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e39adbd5880>]}
[0m23:05:35.447083 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd9c2b676-1bb3-4c07-be95-ce5e76548d1f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e39ad26fc80>]}
[0m23:05:35.564209 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m23:05:35.570182 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m23:05:35.590245 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd9c2b676-1bb3-4c07-be95-ce5e76548d1f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e39ad096b10>]}
[0m23:05:35.591130 [info ] [MainThread]: Found 18 models, 35 data tests, 10 sources, 508 macros
[0m23:05:35.591707 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd9c2b676-1bb3-4c07-be95-ce5e76548d1f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e39adbd4290>]}
[0m23:05:35.594298 [info ] [MainThread]: 
[0m23:05:35.595155 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m23:05:35.595968 [info ] [MainThread]: 
[0m23:05:35.596881 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m23:05:35.598400 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812'
[0m23:05:35.599332 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:05:36.670471 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812, now list_data-pipeline-project-474812_analytics_staging_marts)
[0m23:05:36.673582 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_staging'
[0m23:05:36.675802 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:05:36.677376 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:05:37.740536 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd9c2b676-1bb3-4c07-be95-ce5e76548d1f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e39ad08a660>]}
[0m23:05:37.741828 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:05:37.752801 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.dim_subscription
[0m23:05:37.754111 [info ] [Thread-1 (]: 1 of 1 START sql table model analytics_staging_marts.dim_subscription .......... [RUN]
[0m23:05:37.755079 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_marts, now model.data_pipeline_project.dim_subscription)
[0m23:05:37.755639 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.dim_subscription
[0m23:05:37.765890 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.dim_subscription"
[0m23:05:37.767210 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.dim_subscription
[0m23:05:37.783219 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m23:05:38.728883 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.dim_subscription"
[0m23:05:38.733133 [debug] [Thread-1 (]: On model.data_pipeline_project.dim_subscription: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.dim_subscription"} */

  
    

    create or replace table `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_subscription`
      
    
    

    
    OPTIONS()
    as (
      
with subscriptions as (
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_subscriptions`
),

final_parsing as (
    select
        subscription_id,
        user_id as customer_id, 
        start_date,  
        case
            when is_active = FALSE then 'Cancelled'
            else 'Active'
        end as status,
        -- Business logic: end_date only exists upon explicit cancellation
        case
            when is_active = FALSE then cast(NULL as timestamp)  -- Would be cancellation date
            else cast(NULL as timestamp)                         -- Active = no end date
        end as end_date

        from subscriptions
)

select * from final_parsing
    );
  
[0m23:05:39.507799 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:518fe104-1e00-4712-a8e6-e4e6d5a19b64&page=queryresults
[0m23:05:41.546815 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd9c2b676-1bb3-4c07-be95-ce5e76548d1f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e39ce5479b0>]}
[0m23:05:41.547715 [info ] [Thread-1 (]: 1 of 1 OK created sql table model analytics_staging_marts.dim_subscription ..... [[32mCREATE TABLE (1.5k rows, 112.0 KiB processed)[0m in 3.79s]
[0m23:05:41.549809 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.dim_subscription
[0m23:05:41.554339 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m23:05:41.557678 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:05:41.558219 [debug] [MainThread]: Connection 'model.data_pipeline_project.dim_subscription' was properly closed.
[0m23:05:41.558820 [debug] [MainThread]: Connection 'list_data-pipeline-project-474812_analytics_staging_staging' was properly closed.
[0m23:05:41.559414 [info ] [MainThread]: 
[0m23:05:41.560299 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 5.96 seconds (5.96s).
[0m23:05:41.561549 [debug] [MainThread]: Command end result
[0m23:05:41.617933 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m23:05:41.626461 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m23:05:41.645177 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m23:05:41.646398 [info ] [MainThread]: 
[0m23:05:41.648300 [info ] [MainThread]: [32mCompleted successfully[0m
[0m23:05:41.651031 [info ] [MainThread]: 
[0m23:05:41.651660 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m23:05:41.652453 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 17 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m23:05:41.657266 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 10.694138, "process_in_blocks": "0", "process_kernel_time": 1.133205, "process_mem_max_rss": "387592", "process_out_blocks": "4120", "process_user_time": 6.244136}
[0m23:05:41.660204 [debug] [MainThread]: Command `dbt run` succeeded at 23:05:41.660069 after 10.70 seconds
[0m23:05:41.662618 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e39cdfc2a20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e39ad16ec60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e39ad1fc5f0>]}
[0m23:05:41.663244 [debug] [MainThread]: Flushing usage events
[0m23:05:42.477365 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m23:07:34.162317 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x704283e319a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7042820157c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7042816d7080>]}


============================== 23:07:34.166777 | a67bf6ca-8e10-4926-97d2-0aefad755311 ==============================
[0m23:07:34.166777 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m23:07:34.168125 [debug] [MainThread]: running dbt with arguments {'log_cache_events': 'False', 'introspect': 'True', 'fail_fast': 'False', 'no_print': 'None', 'cache_selected_only': 'False', 'use_experimental_parser': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_colors': 'True', 'warn_error': 'None', 'write_json': 'True', 'version_check': 'True', 'invocation_command': 'dbt run --select mart_subscription_daily', 'printer_width': '80', 'target_path': 'None', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'debug': 'False', 'log_format': 'default', 'partial_parse': 'True', 'quiet': 'False', 'indirect_selection': 'eager', 'send_anonymous_usage_stats': 'True', 'profiles_dir': '/home/ecem/.dbt', 'empty': 'False', 'static_parser': 'True'}
[0m23:07:36.564481 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a67bf6ca-8e10-4926-97d2-0aefad755311', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7042631c8470>]}
[0m23:07:36.631603 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a67bf6ca-8e10-4926-97d2-0aefad755311', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x704261d4dd60>]}
[0m23:07:36.634772 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m23:07:36.885664 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m23:07:37.150602 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m23:07:37.152169 [debug] [MainThread]: Partial parsing: updated file: data_pipeline_project://models/marts/mart_subscription_daily.sql
[0m23:07:37.455396 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a67bf6ca-8e10-4926-97d2-0aefad755311', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x704261a56750>]}
[0m23:07:37.570899 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m23:07:37.573809 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m23:07:37.593711 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a67bf6ca-8e10-4926-97d2-0aefad755311', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70426174ba70>]}
[0m23:07:37.594821 [info ] [MainThread]: Found 18 models, 35 data tests, 10 sources, 508 macros
[0m23:07:37.595593 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a67bf6ca-8e10-4926-97d2-0aefad755311', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x704261797890>]}
[0m23:07:37.597763 [info ] [MainThread]: 
[0m23:07:37.598432 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m23:07:37.599307 [info ] [MainThread]: 
[0m23:07:37.600900 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m23:07:37.602608 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812'
[0m23:07:37.603336 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:07:38.519814 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812, now list_data-pipeline-project-474812_analytics_staging_marts)
[0m23:07:38.520806 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:07:38.555022 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_staging'
[0m23:07:38.556030 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:07:39.463759 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a67bf6ca-8e10-4926-97d2-0aefad755311', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7042616f9fd0>]}
[0m23:07:39.464667 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:07:39.472258 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.mart_subscription_daily
[0m23:07:39.473127 [info ] [Thread-1 (]: 1 of 1 START sql table model analytics_staging_marts.mart_subscription_daily ... [RUN]
[0m23:07:39.473790 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now model.data_pipeline_project.mart_subscription_daily)
[0m23:07:39.474734 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.mart_subscription_daily
[0m23:07:39.484826 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.mart_subscription_daily"
[0m23:07:39.486751 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.mart_subscription_daily
[0m23:07:39.568929 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.mart_subscription_daily"
[0m23:07:39.570031 [debug] [Thread-1 (]: On model.data_pipeline_project.mart_subscription_daily: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.mart_subscription_daily"} */

  
    

    create or replace table `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily`
      
    
    

    
    OPTIONS()
    as (
      

with subscriptions as (
    select 
        subscription_id,
        customer_id,
        -- start_date is already TIMESTAMP → just cast to DATE
        date(start_date) as start_date,
        -- end_date is now TIMESTAMP (after your fix) → safe to cast to DATE
        date(end_date) as end_date,
        status
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_subscription`
),

-- Generate a full series of dates from the first subscription to today
date_series as (
    select date_day
    from unnest(generate_date_array(
        (select min(start_date) from subscriptions),
        current_date(),
        interval 1 day
    )) as date_day
),

-- Daily metrics calculation
daily_metrics as (
    select
        ds.date_day as date,

        -- Active subscribers
        count(case 
            when s.start_date <= ds.date_day 
            and (s.end_date is null or s.end_date > ds.date_day)
            and s.status = 'Active'
            then 1 
        end) as active_subscribers,

        -- New subscribers
        count(case 
            when s.start_date = ds.date_day 
            then 1 
        end) as new_subscribers,

        -- Cancellations
        count(case 
            when s.end_date = ds.date_day 
            then 1 
        end) as cancellations

    from date_series ds
    cross join subscriptions s
    group by ds.date_day
)

select * from daily_metrics
order by date
    );
  
[0m23:07:39.571458 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m23:07:40.708609 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:08d22f49-a0ec-4b8f-9bf5-4c58b03b4e3e&page=queryresults
[0m23:07:43.227845 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a67bf6ca-8e10-4926-97d2-0aefad755311', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x704260ab7e30>]}
[0m23:07:43.229900 [info ] [Thread-1 (]: 1 of 1 OK created sql table model analytics_staging_marts.mart_subscription_daily  [[32mCREATE TABLE (27.0 rows, 23.3 KiB processed)[0m in 3.75s]
[0m23:07:43.230970 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.mart_subscription_daily
[0m23:07:43.235233 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m23:07:43.237006 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:07:43.237484 [debug] [MainThread]: Connection 'list_data-pipeline-project-474812_analytics_staging_marts' was properly closed.
[0m23:07:43.238263 [debug] [MainThread]: Connection 'model.data_pipeline_project.mart_subscription_daily' was properly closed.
[0m23:07:43.239022 [info ] [MainThread]: 
[0m23:07:43.239861 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 5.64 seconds (5.64s).
[0m23:07:43.241135 [debug] [MainThread]: Command end result
[0m23:07:43.295519 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m23:07:43.301848 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m23:07:43.309314 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m23:07:43.310343 [info ] [MainThread]: 
[0m23:07:43.312615 [info ] [MainThread]: [32mCompleted successfully[0m
[0m23:07:43.313084 [info ] [MainThread]: 
[0m23:07:43.313493 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m23:07:43.315077 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 9.210612, "process_in_blocks": "0", "process_kernel_time": 1.255914, "process_mem_max_rss": "383312", "process_out_blocks": "4168", "process_user_time": 4.987628}
[0m23:07:43.315803 [debug] [MainThread]: Command `dbt run` succeeded at 23:07:43.315643 after 9.21 seconds
[0m23:07:43.316286 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70428101b9e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x704261794650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7042600d9c70>]}
[0m23:07:43.316752 [debug] [MainThread]: Flushing usage events
[0m23:07:44.266921 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:14:27.409319 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7875fdbcc5c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7875ffe8a3c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7875fd827710>]}


============================== 10:14:27.416090 | 10cdc033-6745-4d24-83bf-42b3e9d50704 ==============================
[0m10:14:27.416090 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m10:14:27.418809 [debug] [MainThread]: running dbt with arguments {'send_anonymous_usage_stats': 'True', 'introspect': 'True', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'write_json': 'True', 'warn_error': 'None', 'no_print': 'None', 'cache_selected_only': 'False', 'quiet': 'False', 'log_cache_events': 'False', 'empty': 'False', 'invocation_command': 'dbt run --select mart_revenue_daiy', 'fail_fast': 'False', 'partial_parse': 'True', 'log_format': 'default', 'version_check': 'True', 'use_experimental_parser': 'False', 'indirect_selection': 'eager', 'debug': 'False', 'profiles_dir': '/home/ecem/.dbt', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'use_colors': 'True', 'static_parser': 'True'}
[0m10:14:31.414960 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '10cdc033-6745-4d24-83bf-42b3e9d50704', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7875dd844440>]}
[0m10:14:31.522962 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '10cdc033-6745-4d24-83bf-42b3e9d50704', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7875ff59e240>]}
[0m10:14:31.524648 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m10:14:31.883825 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m10:14:32.388128 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
[0m10:14:32.389285 [debug] [MainThread]: Partial parsing: added file: data_pipeline_project://models/marts/mart_revenue_daily.sql
[0m10:14:32.747805 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '10cdc033-6745-4d24-83bf-42b3e9d50704', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7875dd3fb5f0>]}
[0m10:14:32.862653 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m10:14:32.871209 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m10:14:32.898079 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '10cdc033-6745-4d24-83bf-42b3e9d50704', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7875dd553080>]}
[0m10:14:32.898955 [info ] [MainThread]: Found 19 models, 35 data tests, 10 sources, 508 macros
[0m10:14:32.899999 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '10cdc033-6745-4d24-83bf-42b3e9d50704', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7875dca20950>]}
[0m10:14:32.901269 [warn ] [MainThread]: The selection criterion 'mart_revenue_daiy' does not match any enabled nodes
[0m10:14:32.903261 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m10:14:32.904161 [debug] [MainThread]: Command end result
[0m10:14:32.948910 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m10:14:32.956584 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m10:14:32.961582 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m10:14:32.963619 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 5.6451416, "process_in_blocks": "0", "process_kernel_time": 1.620165, "process_mem_max_rss": "372436", "process_out_blocks": "4144", "process_user_time": 6.05547}
[0m10:14:32.965058 [debug] [MainThread]: Command `dbt run` succeeded at 10:14:32.964929 after 5.65 seconds
[0m10:14:32.965913 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7875fd320d40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7875dd46c320>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7875fcf2f980>]}
[0m10:14:32.966440 [debug] [MainThread]: Flushing usage events
[0m10:14:34.446766 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:15:59.501432 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71f21adf0a40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71f2194af980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71f21af5a570>]}


============================== 10:15:59.505051 | 9a016ade-d4a4-428a-8f0b-184401022b51 ==============================
[0m10:15:59.505051 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m10:15:59.506276 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'log_format': 'default', 'fail_fast': 'False', 'indirect_selection': 'eager', 'cache_selected_only': 'False', 'log_cache_events': 'False', 'quiet': 'False', 'warn_error': 'None', 'static_parser': 'True', 'profiles_dir': '/home/ecem/.dbt', 'introspect': 'True', 'empty': 'False', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'invocation_command': 'dbt run --select mart_revenue_daily', 'use_colors': 'True', 'use_experimental_parser': 'False', 'debug': 'False', 'partial_parse': 'True', 'no_print': 'None', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'send_anonymous_usage_stats': 'True', 'version_check': 'True', 'write_json': 'True'}
[0m10:16:04.051883 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9a016ade-d4a4-428a-8f0b-184401022b51', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71f1fa921ac0>]}
[0m10:16:04.138090 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9a016ade-d4a4-428a-8f0b-184401022b51', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71f1f92e61b0>]}
[0m10:16:04.139324 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m10:16:04.415479 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m10:16:04.766407 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m10:16:04.767099 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m10:16:04.815911 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9a016ade-d4a4-428a-8f0b-184401022b51', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71f1f8e1e900>]}
[0m10:16:04.930056 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m10:16:04.942076 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m10:16:04.965926 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9a016ade-d4a4-428a-8f0b-184401022b51', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71f1f8c8aa50>]}
[0m10:16:04.966955 [info ] [MainThread]: Found 19 models, 35 data tests, 10 sources, 508 macros
[0m10:16:04.967566 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9a016ade-d4a4-428a-8f0b-184401022b51', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71f1f8ddd610>]}
[0m10:16:04.969764 [info ] [MainThread]: 
[0m10:16:04.970931 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m10:16:04.972302 [info ] [MainThread]: 
[0m10:16:04.973779 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m10:16:04.975649 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812'
[0m10:16:04.977049 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:16:06.184632 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812, now list_data-pipeline-project-474812_analytics_staging_marts)
[0m10:16:06.186910 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_staging'
[0m10:16:06.187984 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m10:16:06.189107 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:16:07.249482 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9a016ade-d4a4-428a-8f0b-184401022b51', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71f21a805940>]}
[0m10:16:07.254257 [debug] [MainThread]: Opening a new connection, currently in state init
[0m10:16:07.279655 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.mart_revenue_daily
[0m10:16:07.282157 [info ] [Thread-1 (]: 1 of 1 START sql table model analytics_staging_marts.mart_revenue_daily ........ [RUN]
[0m10:16:07.284279 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now model.data_pipeline_project.mart_revenue_daily)
[0m10:16:07.285377 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.mart_revenue_daily
[0m10:16:07.302678 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.mart_revenue_daily"
[0m10:16:07.308330 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.mart_revenue_daily
[0m10:16:07.380143 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.mart_revenue_daily"
[0m10:16:07.383063 [debug] [Thread-1 (]: On model.data_pipeline_project.mart_revenue_daily: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.mart_revenue_daily"} */

  
    

    create or replace table `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_revenue_daily`
      
    
    

    
    OPTIONS()
    as (
      with daily_revenue as (
    -- 1. Combine Revenue from fct_order and Delivery Status from fct_shipment
    select
        -- Group by Delivery Date, as revenue is only recognized when the product is delivered.
        cast(fs.delivered_at as date) as date,
        
        -- FIX: Sum the REVENUE for only the delivered orders.
        sum(fo.items_total) as daily_revenue,
        
        -- Count the number of distinct delivered orders.
        count(distinct fo.order_id) as delivered_orders
        
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_order` fo
    inner join `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_shipment` fs
        on fo.order_id = fs.order_id
    -- Filter for delivered orders only (Business Logic)
    where fs.latest_status = 'DELIVERED' 
    group by 1
)

select * from daily_revenue
    );
  
[0m10:16:07.384069 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m10:16:08.732351 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:e66516e0-793e-4b47-ba78-ff24dffeb5cb&page=queryresults
[0m10:16:11.436803 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9a016ade-d4a4-428a-8f0b-184401022b51', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71f1f8a40200>]}
[0m10:16:11.438867 [info ] [Thread-1 (]: 1 of 1 OK created sql table model analytics_staging_marts.mart_revenue_daily ... [[32mCREATE TABLE (8.0 rows, 166.7 KiB processed)[0m in 4.15s]
[0m10:16:11.440688 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.mart_revenue_daily
[0m10:16:11.448495 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m10:16:11.451596 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:16:11.452183 [debug] [MainThread]: Connection 'list_data-pipeline-project-474812_analytics_staging_marts' was properly closed.
[0m10:16:11.452908 [debug] [MainThread]: Connection 'model.data_pipeline_project.mart_revenue_daily' was properly closed.
[0m10:16:11.453602 [info ] [MainThread]: 
[0m10:16:11.454401 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 6.48 seconds (6.48s).
[0m10:16:11.456272 [debug] [MainThread]: Command end result
[0m10:16:11.502645 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m10:16:11.511671 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m10:16:11.518358 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m10:16:11.519021 [info ] [MainThread]: 
[0m10:16:11.521125 [info ] [MainThread]: [32mCompleted successfully[0m
[0m10:16:11.522513 [info ] [MainThread]: 
[0m10:16:11.523537 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m10:16:11.525403 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 12.085197, "process_in_blocks": "0", "process_kernel_time": 1.58763, "process_mem_max_rss": "380360", "process_out_blocks": "2888", "process_user_time": 7.039461}
[0m10:16:11.526410 [debug] [MainThread]: Command `dbt run` succeeded at 10:16:11.526290 after 12.09 seconds
[0m10:16:11.527470 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71f218a1f1d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71f2194ad190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71f2194adac0>]}
[0m10:16:11.528149 [debug] [MainThread]: Flushing usage events
[0m10:16:12.607916 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:28:43.153677 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71ed7d7396d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71ed7d1a79e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71ed7d03d9a0>]}


============================== 10:28:43.158297 | 7cc92886-91c4-423c-9787-e72ef9073df2 ==============================
[0m10:28:43.158297 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m10:28:43.160457 [debug] [MainThread]: running dbt with arguments {'cache_selected_only': 'False', 'static_parser': 'True', 'printer_width': '80', 'warn_error': 'None', 'debug': 'False', 'empty': 'None', 'quiet': 'False', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'introspect': 'True', 'no_print': 'None', 'version_check': 'True', 'target_path': 'None', 'partial_parse': 'True', 'profiles_dir': '/home/ecem/.dbt', 'log_format': 'default', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'invocation_command': 'dbt clean', 'log_cache_events': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_colors': 'True', 'use_experimental_parser': 'False', 'write_json': 'True', 'fail_fast': 'False'}
[0m10:28:43.326529 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7cc92886-91c4-423c-9787-e72ef9073df2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71ed7c6bb980>]}
[0m10:28:43.341294 [debug] [MainThread]: Resource report: {"command_name": "clean", "command_success": true, "command_wall_clock_time": 0.25273636, "process_in_blocks": "0", "process_kernel_time": 0.246548, "process_mem_max_rss": "101648", "process_out_blocks": "16", "process_user_time": 1.641402}
[0m10:28:43.341854 [debug] [MainThread]: Command `dbt clean` succeeded at 10:28:43.341748 after 0.25 seconds
[0m10:28:43.342330 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71ed7cf6fb30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71ed7ce9fec0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71ed7c665f40>]}
[0m10:28:43.342787 [debug] [MainThread]: Flushing usage events
[0m10:28:44.479504 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:28:51.224085 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7657cad19f40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7657ca65b8c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7657ca311af0>]}


============================== 10:28:51.253956 | ed5bbc8e-170c-4fa7-b042-1069350cf0f3 ==============================
[0m10:28:51.253956 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m10:28:51.256234 [debug] [MainThread]: running dbt with arguments {'fail_fast': 'False', 'write_json': 'True', 'invocation_command': 'dbt run --select mart_revenue_daily', 'introspect': 'True', 'printer_width': '80', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'no_print': 'None', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'debug': 'False', 'warn_error': 'None', 'quiet': 'False', 'empty': 'False', 'profiles_dir': '/home/ecem/.dbt', 'target_path': 'None', 'cache_selected_only': 'False', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_cache_events': 'False', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m10:28:55.524963 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ed5bbc8e-170c-4fa7-b042-1069350cf0f3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7657ca581880>]}
[0m10:28:55.605938 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ed5bbc8e-170c-4fa7-b042-1069350cf0f3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7657aac1d4c0>]}
[0m10:28:55.608221 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m10:28:55.905422 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m10:28:55.907020 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m10:28:55.908400 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'ed5bbc8e-170c-4fa7-b042-1069350cf0f3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7657aa9309e0>]}
[0m10:28:57.289417 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `relationships` defined on 'stg_orders' in
package 'data_pipeline_project' (models/staging/staging_schema.yml). Arguments
to generic tests should be nested under the `arguments` property.
[0m10:28:57.290425 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': 'ed5bbc8e-170c-4fa7-b042-1069350cf0f3', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7657a9c120f0>]}
[0m10:28:57.662201 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ed5bbc8e-170c-4fa7-b042-1069350cf0f3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7657a9cccbc0>]}
[0m10:28:57.788795 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m10:28:57.791841 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m10:28:57.808157 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ed5bbc8e-170c-4fa7-b042-1069350cf0f3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7657a9a52000>]}
[0m10:28:57.808893 [info ] [MainThread]: Found 19 models, 35 data tests, 10 sources, 508 macros
[0m10:28:57.809698 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ed5bbc8e-170c-4fa7-b042-1069350cf0f3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7657a9c3b650>]}
[0m10:28:57.811952 [info ] [MainThread]: 
[0m10:28:57.812731 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m10:28:57.813434 [info ] [MainThread]: 
[0m10:28:57.814284 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m10:28:57.815923 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812'
[0m10:28:57.817314 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:28:58.891698 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812, now list_data-pipeline-project-474812_analytics_staging_staging)
[0m10:28:58.893388 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m10:28:58.925428 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_marts'
[0m10:28:58.927954 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:28:59.828831 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ed5bbc8e-170c-4fa7-b042-1069350cf0f3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7657aa9346e0>]}
[0m10:28:59.829590 [debug] [MainThread]: Opening a new connection, currently in state init
[0m10:28:59.839735 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.mart_revenue_daily
[0m10:28:59.840928 [info ] [Thread-1 (]: 1 of 1 START sql table model analytics_staging_marts.mart_revenue_daily ........ [RUN]
[0m10:28:59.842115 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now model.data_pipeline_project.mart_revenue_daily)
[0m10:28:59.843200 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.mart_revenue_daily
[0m10:28:59.850749 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.mart_revenue_daily"
[0m10:28:59.853590 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.mart_revenue_daily
[0m10:28:59.878770 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m10:29:00.813695 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.mart_revenue_daily"
[0m10:29:00.816971 [debug] [Thread-1 (]: On model.data_pipeline_project.mart_revenue_daily: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.mart_revenue_daily"} */

  
    

    create or replace table `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_revenue_daily`
      
    
    

    
    OPTIONS()
    as (
      with daily_revenue as (
    select
        -- Use created_at as fallback when delivered_at is NULL
        coalesce(cast(fs.delivered_at as date), cast(fo.order_date as date)) as date,
        
        sum(fo.items_total) as daily_revenue,
        count(distinct fo.order_id) as delivered_orders
        
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_order` fo
    inner join `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_shipment` fs
        on fo.order_id = fs.order_id
    where fs.latest_status = 'DELIVERED' 
    group by 1
)

select * from daily_revenue
order by date
    );
  
[0m10:29:01.662920 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:92e87a1c-634d-46b9-8d4c-6a9b7a02e8e8&page=queryresults
[0m10:29:04.592260 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ed5bbc8e-170c-4fa7-b042-1069350cf0f3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7657aaa4a270>]}
[0m10:29:04.593823 [info ] [Thread-1 (]: 1 of 1 OK created sql table model analytics_staging_marts.mart_revenue_daily ... [[32mCREATE TABLE (10.0 rows, 180.7 KiB processed)[0m in 4.75s]
[0m10:29:04.594926 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.mart_revenue_daily
[0m10:29:04.598721 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m10:29:04.602921 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:29:04.603409 [debug] [MainThread]: Connection 'model.data_pipeline_project.mart_revenue_daily' was properly closed.
[0m10:29:04.603970 [debug] [MainThread]: Connection 'list_data-pipeline-project-474812_analytics_staging_marts' was properly closed.
[0m10:29:04.604364 [info ] [MainThread]: 
[0m10:29:04.605741 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 6.79 seconds (6.79s).
[0m10:29:04.607405 [debug] [MainThread]: Command end result
[0m10:29:04.682697 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m10:29:04.696831 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m10:29:04.708323 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m10:29:04.708761 [info ] [MainThread]: 
[0m10:29:04.709520 [info ] [MainThread]: [32mCompleted successfully[0m
[0m10:29:04.710866 [info ] [MainThread]: 
[0m10:29:04.711540 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m10:29:04.712411 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 17 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m10:29:04.713760 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 13.547042, "process_in_blocks": "0", "process_kernel_time": 1.801916, "process_mem_max_rss": "387436", "process_out_blocks": "4136", "process_user_time": 7.613096}
[0m10:29:04.715227 [debug] [MainThread]: Command `dbt run` succeeded at 10:29:04.715038 after 13.55 seconds
[0m10:29:04.719968 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7657ca6e2240>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7657a9bc3620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7657aa5df410>]}
[0m10:29:04.720506 [debug] [MainThread]: Flushing usage events
[0m10:29:05.442147 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:40:17.411593 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7adfc39d50a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7adfc47c2c00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7adfc46b2450>]}


============================== 10:40:17.415286 | 443b2f5f-6f6c-4aaf-aeb7-3e6712be74eb ==============================
[0m10:40:17.415286 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m10:40:17.416232 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'partial_parse': 'True', 'static_parser': 'True', 'log_format': 'default', 'cache_selected_only': 'False', 'write_json': 'True', 'invocation_command': 'dbt test --select int_shipments_decomposed', 'no_print': 'None', 'use_experimental_parser': 'False', 'debug': 'False', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'empty': 'None', 'profiles_dir': '/home/ecem/.dbt', 'target_path': 'None', 'quiet': 'False', 'indirect_selection': 'eager', 'version_check': 'True', 'send_anonymous_usage_stats': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'warn_error': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'use_colors': 'True'}
[0m10:40:21.437541 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '443b2f5f-6f6c-4aaf-aeb7-3e6712be74eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7adfa2ed30e0>]}
[0m10:40:21.501295 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '443b2f5f-6f6c-4aaf-aeb7-3e6712be74eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7adfa29824e0>]}
[0m10:40:21.502248 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m10:40:21.745436 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m10:40:22.013601 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m10:40:22.014976 [debug] [MainThread]: Partial parsing: updated file: data_pipeline_project://models/staging/staging_schema.yml
[0m10:40:22.086113 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `not_null` defined on
'int_shipments_decomposed' in package 'data_pipeline_project'
(models/staging/staging_schema.yml). Arguments to generic tests should be nested
under the `arguments` property.
[0m10:40:22.086754 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '443b2f5f-6f6c-4aaf-aeb7-3e6712be74eb', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7adfa24fa000>]}
[0m10:40:22.316967 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '443b2f5f-6f6c-4aaf-aeb7-3e6712be74eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7adfa204e180>]}
[0m10:40:22.436472 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m10:40:22.439905 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m10:40:22.485037 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '443b2f5f-6f6c-4aaf-aeb7-3e6712be74eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7adfa1ff7a70>]}
[0m10:40:22.485616 [info ] [MainThread]: Found 19 models, 38 data tests, 10 sources, 508 macros
[0m10:40:22.488283 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '443b2f5f-6f6c-4aaf-aeb7-3e6712be74eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7adfa23a9e20>]}
[0m10:40:22.490512 [info ] [MainThread]: 
[0m10:40:22.491256 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m10:40:22.492475 [info ] [MainThread]: 
[0m10:40:22.493618 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m10:40:22.500475 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_marts'
[0m10:40:22.501207 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:40:22.538207 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_staging'
[0m10:40:22.538863 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:40:23.805551 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '443b2f5f-6f6c-4aaf-aeb7-3e6712be74eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7adfa1ff7770>]}
[0m10:40:23.809097 [debug] [MainThread]: Opening a new connection, currently in state init
[0m10:40:23.822775 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_int_shipments_decomposed_carrier.604ed2370a
[0m10:40:23.824803 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b
[0m10:40:23.826825 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31
[0m10:40:23.825932 [info ] [Thread-1 (]: 1 of 3 START test not_null_int_shipments_decomposed_carrier .................... [RUN]
[0m10:40:23.828437 [info ] [Thread-2 (]: 2 of 3 START test not_null_int_shipments_decomposed_delivered_at ............... [RUN]
[0m10:40:23.830056 [info ] [Thread-3 (]: 3 of 3 START test not_null_int_shipments_decomposed_latest_status .............. [RUN]
[0m10:40:23.831350 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now test.data_pipeline_project.not_null_int_shipments_decomposed_carrier.604ed2370a)
[0m10:40:23.832338 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_marts, now test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b)
[0m10:40:23.833860 [debug] [Thread-3 (]: Acquiring new bigquery connection 'test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31'
[0m10:40:23.837523 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31
[0m10:40:23.836668 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b
[0m10:40:23.835646 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_int_shipments_decomposed_carrier.604ed2370a
[0m10:40:23.888387 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_int_shipments_decomposed_carrier.604ed2370a"
[0m10:40:23.890454 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b"
[0m10:40:23.894215 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31"
[0m10:40:23.896208 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_int_shipments_decomposed_carrier.604ed2370a
[0m10:40:23.912475 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b
[0m10:40:23.933239 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31
[0m10:40:23.946808 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_int_shipments_decomposed_carrier.604ed2370a"
[0m10:40:23.949764 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b"
[0m10:40:23.951007 [debug] [Thread-1 (]: On test.data_pipeline_project.not_null_int_shipments_decomposed_carrier.604ed2370a: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_int_shipments_decomposed_carrier.604ed2370a"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select carrier
from `data-pipeline-project-474812`.`analytics_staging_staging`.`int_shipments_decomposed`
where carrier is null



  
  
      
    ) dbt_internal_test
[0m10:40:23.953774 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31"
[0m10:40:23.955595 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m10:40:23.957495 [debug] [Thread-3 (]: On test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select latest_status
from `data-pipeline-project-474812`.`analytics_staging_staging`.`int_shipments_decomposed`
where latest_status is null



  
  
      
    ) dbt_internal_test
[0m10:40:23.961114 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m10:40:24.027837 [debug] [Thread-2 (]: On test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select delivered_at
from (select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`int_shipments_decomposed` where latest_status = 'DELIVERED') dbt_subquery
where delivered_at is null



  
  
      
    ) dbt_internal_test
[0m10:40:24.030529 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m10:40:25.502492 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:90fe4472-8ae1-4ef5-8750-c069bda08409&page=queryresults
[0m10:40:25.514948 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:d319f241-fc7f-482a-a875-d75af3868218&page=queryresults
[0m10:40:25.606088 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:d01305c8-b64b-4a2a-8d77-f0cd7f9298da&page=queryresults
[0m10:40:26.447238 [info ] [Thread-2 (]: 2 of 3 PASS not_null_int_shipments_decomposed_delivered_at ..................... [[32mPASS[0m in 2.61s]
[0m10:40:26.451876 [warn ] [Thread-1 (]: 1 of 3 WARN 27 not_null_int_shipments_decomposed_carrier ....................... [[33mWARN 27[0m in 2.62s]
[0m10:40:26.455437 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_int_shipments_decomposed_carrier.604ed2370a
[0m10:40:26.453849 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b
[0m10:40:26.507192 [info ] [Thread-3 (]: 3 of 3 PASS not_null_int_shipments_decomposed_latest_status .................... [[32mPASS[0m in 2.67s]
[0m10:40:26.508367 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31
[0m10:40:26.513576 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m10:40:26.516137 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:40:26.517973 [debug] [MainThread]: Connection 'test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b' was properly closed.
[0m10:40:26.519135 [debug] [MainThread]: Connection 'test.data_pipeline_project.not_null_int_shipments_decomposed_carrier.604ed2370a' was properly closed.
[0m10:40:26.519864 [debug] [MainThread]: Connection 'test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31' was properly closed.
[0m10:40:26.520418 [info ] [MainThread]: 
[0m10:40:26.521367 [info ] [MainThread]: Finished running 3 data tests in 0 hours 0 minutes and 4.03 seconds (4.03s).
[0m10:40:26.527330 [debug] [MainThread]: Command end result
[0m10:40:26.585699 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m10:40:26.591864 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m10:40:26.603688 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m10:40:26.604629 [info ] [MainThread]: 
[0m10:40:26.605341 [info ] [MainThread]: [33mCompleted with 1 warning:[0m
[0m10:40:26.606023 [info ] [MainThread]: 
[0m10:40:26.606818 [warn ] [MainThread]: [33mWarning in test not_null_int_shipments_decomposed_carrier (models/staging/staging_schema.yml)[0m
[0m10:40:26.607445 [warn ] [MainThread]: Got 27 results, configured to warn if != 0
[0m10:40:26.608083 [info ] [MainThread]: 
[0m10:40:26.608667 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_project/models/staging/staging_schema.yml/not_null_int_shipments_decomposed_carrier.sql
[0m10:40:26.609389 [info ] [MainThread]: 
[0m10:40:26.610090 [info ] [MainThread]: Done. PASS=2 WARN=1 ERROR=0 SKIP=0 NO-OP=0 TOTAL=3
[0m10:40:26.611067 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 2 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m10:40:26.612811 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": true, "command_wall_clock_time": 9.267709, "process_in_blocks": "1128", "process_kernel_time": 1.420435, "process_mem_max_rss": "385972", "process_out_blocks": "4280", "process_user_time": 7.828589}
[0m10:40:26.614161 [debug] [MainThread]: Command `dbt test` succeeded at 10:40:26.614005 after 9.27 seconds
[0m10:40:26.614970 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7adfc3f8a750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7adfa1ed6090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7adfc2b07500>]}
[0m10:40:26.615965 [debug] [MainThread]: Flushing usage events
[0m10:40:27.603332 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:43:41.153535 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7140a3b04e90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7140a2a478c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7140a48526f0>]}


============================== 10:43:41.160156 | aa3608bb-a851-4858-9b6f-325c2bdb57da ==============================
[0m10:43:41.160156 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m10:43:41.161954 [debug] [MainThread]: running dbt with arguments {'empty': 'None', 'send_anonymous_usage_stats': 'True', 'debug': 'False', 'invocation_command': 'dbt test --select int_shipments_decomposed', 'write_json': 'True', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'fail_fast': 'False', 'static_parser': 'True', 'use_experimental_parser': 'False', 'indirect_selection': 'eager', 'partial_parse': 'True', 'cache_selected_only': 'False', 'quiet': 'False', 'printer_width': '80', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'no_print': 'None', 'log_format': 'default', 'profiles_dir': '/home/ecem/.dbt', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'introspect': 'True', 'log_cache_events': 'False', 'target_path': 'None'}
[0m10:43:44.051362 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'aa3608bb-a851-4858-9b6f-325c2bdb57da', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x714084b52270>]}
[0m10:43:44.123362 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'aa3608bb-a851-4858-9b6f-325c2bdb57da', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7140a44362a0>]}
[0m10:43:44.124885 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m10:43:44.368967 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m10:43:44.652986 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m10:43:44.654782 [debug] [MainThread]: Partial parsing: updated file: data_pipeline_project://models/staging/staging_schema.yml
[0m10:43:44.927222 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `not_null` defined on
'int_shipments_decomposed' in package 'data_pipeline_project'
(models/staging/staging_schema.yml). Arguments to generic tests should be nested
under the `arguments` property.
[0m10:43:44.928329 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': 'aa3608bb-a851-4858-9b6f-325c2bdb57da', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x714082cada00>]}
[0m10:43:45.137544 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'aa3608bb-a851-4858-9b6f-325c2bdb57da', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71408175e360>]}
[0m10:43:45.244639 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m10:43:45.247980 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m10:43:45.279640 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'aa3608bb-a851-4858-9b6f-325c2bdb57da', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71408154bb30>]}
[0m10:43:45.280797 [info ] [MainThread]: Found 19 models, 37 data tests, 10 sources, 508 macros
[0m10:43:45.281380 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'aa3608bb-a851-4858-9b6f-325c2bdb57da', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7140817f38c0>]}
[0m10:43:45.284216 [info ] [MainThread]: 
[0m10:43:45.285178 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m10:43:45.286006 [info ] [MainThread]: 
[0m10:43:45.287101 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m10:43:45.294769 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_marts'
[0m10:43:45.299322 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:43:45.296621 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_staging'
[0m10:43:45.336282 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:43:46.340504 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'aa3608bb-a851-4858-9b6f-325c2bdb57da', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7140816171d0>]}
[0m10:43:46.341662 [debug] [MainThread]: Opening a new connection, currently in state init
[0m10:43:46.351303 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b
[0m10:43:46.352172 [info ] [Thread-1 (]: 1 of 2 START test not_null_int_shipments_decomposed_delivered_at ............... [RUN]
[0m10:43:46.354311 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b)
[0m10:43:46.355116 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b
[0m10:43:46.356467 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31
[0m10:43:46.363479 [info ] [Thread-2 (]: 2 of 2 START test not_null_int_shipments_decomposed_latest_status .............. [RUN]
[0m10:43:46.365028 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_marts, now test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31)
[0m10:43:46.365873 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31
[0m10:43:46.394344 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b"
[0m10:43:46.391444 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31"
[0m10:43:46.397320 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b
[0m10:43:46.404147 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31
[0m10:43:46.477961 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b"
[0m10:43:46.495343 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31"
[0m10:43:46.496909 [debug] [Thread-2 (]: On test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select latest_status
from `data-pipeline-project-474812`.`analytics_staging_staging`.`int_shipments_decomposed`
where latest_status is null



  
  
      
    ) dbt_internal_test
[0m10:43:46.497513 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m10:43:46.540028 [debug] [Thread-1 (]: On test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select delivered_at
from `data-pipeline-project-474812`.`analytics_staging_staging`.`int_shipments_decomposed`
where delivered_at is null



  
  
      
    ) dbt_internal_test
[0m10:43:46.542606 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m10:43:47.810421 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:e9973aa1-6264-4e5c-89e4-feb4f5adc228&page=queryresults
[0m10:43:48.149390 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:d7f04883-bfe5-4db5-b407-5e067dff29be&page=queryresults
[0m10:43:48.750140 [info ] [Thread-2 (]: 2 of 2 PASS not_null_int_shipments_decomposed_latest_status .................... [[32mPASS[0m in 2.39s]
[0m10:43:48.751538 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31
[0m10:43:49.057237 [warn ] [Thread-1 (]: 1 of 2 WARN 303 not_null_int_shipments_decomposed_delivered_at ................. [[33mWARN 303[0m in 2.70s]
[0m10:43:49.058748 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b
[0m10:43:49.062375 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m10:43:49.064451 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:43:49.065154 [debug] [MainThread]: Connection 'test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31' was properly closed.
[0m10:43:49.065675 [debug] [MainThread]: Connection 'test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b' was properly closed.
[0m10:43:49.066217 [info ] [MainThread]: 
[0m10:43:49.067135 [info ] [MainThread]: Finished running 2 data tests in 0 hours 0 minutes and 3.78 seconds (3.78s).
[0m10:43:49.068601 [debug] [MainThread]: Command end result
[0m10:43:49.119341 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m10:43:49.128728 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m10:43:49.153837 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m10:43:49.156529 [info ] [MainThread]: 
[0m10:43:49.158896 [info ] [MainThread]: [33mCompleted with 1 warning:[0m
[0m10:43:49.159627 [info ] [MainThread]: 
[0m10:43:49.160940 [warn ] [MainThread]: [33mWarning in test not_null_int_shipments_decomposed_delivered_at (models/staging/staging_schema.yml)[0m
[0m10:43:49.161766 [warn ] [MainThread]: Got 303 results, configured to warn if != 0
[0m10:43:49.162766 [info ] [MainThread]: 
[0m10:43:49.163484 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_project/models/staging/staging_schema.yml/not_null_int_shipments_decomposed_delivered_at.sql
[0m10:43:49.166193 [info ] [MainThread]: 
[0m10:43:49.169436 [info ] [MainThread]: Done. PASS=1 WARN=1 ERROR=0 SKIP=0 NO-OP=0 TOTAL=2
[0m10:43:49.172687 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m10:43:49.178140 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": true, "command_wall_clock_time": 8.088596, "process_in_blocks": "0", "process_kernel_time": 1.177774, "process_mem_max_rss": "387160", "process_out_blocks": "4248", "process_user_time": 6.522909}
[0m10:43:49.184129 [debug] [MainThread]: Command `dbt test` succeeded at 10:43:49.181618 after 8.09 seconds
[0m10:43:49.186647 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7140a1ce5be0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7140816174a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x714081617140>]}
[0m10:43:49.190085 [debug] [MainThread]: Flushing usage events
[0m10:43:50.189494 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:05:21.619260 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a18e553d310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a18e6edb170>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a18e536b8c0>]}


============================== 11:05:21.622378 | 61d55789-9cad-48ef-84f3-a2420d5373d7 ==============================
[0m11:05:21.622378 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m11:05:21.623828 [debug] [MainThread]: running dbt with arguments {'cache_selected_only': 'False', 'version_check': 'True', 'use_colors': 'True', 'indirect_selection': 'eager', 'warn_error': 'None', 'debug': 'False', 'target_path': 'None', 'invocation_command': 'dbt run --select mart_acquisition_efficiency', 'log_format': 'default', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_experimental_parser': 'False', 'profiles_dir': '/home/ecem/.dbt', 'introspect': 'True', 'log_cache_events': 'False', 'no_print': 'None', 'partial_parse': 'True', 'static_parser': 'True', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'send_anonymous_usage_stats': 'True', 'printer_width': '80', 'empty': 'False', 'write_json': 'True', 'fail_fast': 'False', 'quiet': 'False'}
[0m11:05:24.233888 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '61d55789-9cad-48ef-84f3-a2420d5373d7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a18e6ccbce0>]}
[0m11:05:24.299407 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '61d55789-9cad-48ef-84f3-a2420d5373d7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a18c554a360>]}
[0m11:05:24.300884 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m11:05:24.548608 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m11:05:24.834816 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
[0m11:05:24.836468 [debug] [MainThread]: Partial parsing: added file: data_pipeline_project://models/marts/mart_acquisition_efficiency.sql
[0m11:05:25.260439 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '61d55789-9cad-48ef-84f3-a2420d5373d7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a18b7fc1df0>]}
[0m11:05:25.503190 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m11:05:25.514324 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m11:05:25.595118 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '61d55789-9cad-48ef-84f3-a2420d5373d7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a18c4bca150>]}
[0m11:05:25.598211 [info ] [MainThread]: Found 20 models, 37 data tests, 10 sources, 508 macros
[0m11:05:25.601357 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '61d55789-9cad-48ef-84f3-a2420d5373d7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a18c4a336b0>]}
[0m11:05:25.609830 [info ] [MainThread]: 
[0m11:05:25.613012 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m11:05:25.619107 [info ] [MainThread]: 
[0m11:05:25.622676 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m11:05:25.629722 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812'
[0m11:05:25.638866 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:05:26.816233 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812, now list_data-pipeline-project-474812_analytics_staging_staging)
[0m11:05:26.818462 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:05:26.819951 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_marts'
[0m11:05:26.855079 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:05:27.735440 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '61d55789-9cad-48ef-84f3-a2420d5373d7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a18c4951ee0>]}
[0m11:05:27.736232 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:05:27.744621 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.mart_acquisition_efficiency
[0m11:05:27.745735 [info ] [Thread-1 (]: 1 of 1 START sql table model analytics_staging_marts.mart_acquisition_efficiency  [RUN]
[0m11:05:27.747173 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_marts, now model.data_pipeline_project.mart_acquisition_efficiency)
[0m11:05:27.747934 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.mart_acquisition_efficiency
[0m11:05:27.758049 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.mart_acquisition_efficiency"
[0m11:05:27.758918 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.mart_acquisition_efficiency
[0m11:05:27.819759 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.mart_acquisition_efficiency"
[0m11:05:27.821399 [debug] [Thread-1 (]: On model.data_pipeline_project.mart_acquisition_efficiency: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.mart_acquisition_efficiency"} */

  
    

    create or replace table `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_acquisition_efficiency`
      
    
    

    
    OPTIONS()
    as (
      

with daily_spend as (
    -- 1. Aggregate daily marketing spend
    select
        cast(date as date) as date,
        sum(spend_try) as total_marketing_spend
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_marketing_spend`
    group by 1
),

daily_new_subscribers as (
    -- 2. Pull daily new subscribers from the subscription mart
    select
        date,
        new_subscribers
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily`
),

final as (
    select
        -- Join date (ensures we get all dates from either spend or subscribers)
        coalesce(ds.date, dns.date) as date,

        -- Metrics used for calculation
        coalesce(ds.total_marketing_spend, 0) as total_marketing_spend,
        coalesce(dns.new_subscribers, 0) as new_subscribers,

        -- CALCULATE CAC: Spend / New Subscribers
        -- NULLIF prevents division by zero if there were 0 new subscribers on a given day.
        case 
            when coalesce(dns.new_subscribers, 0) > 0 then 
                coalesce(ds.total_marketing_spend, 0) / dns.new_subscribers
            else NULL -- Use NULL when there is no acquisition event (or cost is zero)
        end as daily_customer_acquisition_cost
        
    from daily_spend ds
    full outer join daily_new_subscribers dns
        on ds.date = dns.date
    where 
        -- Only include rows where we have either spend or new subscribers (to avoid empty dates)
        coalesce(ds.total_marketing_spend, 0) > 0 OR coalesce(dns.new_subscribers, 0) > 0
    order by 1
)

select * from final
    );
  
[0m11:05:27.822434 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:05:29.442637 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:4b877981-86bd-4bcd-8767-38e38b80fab1&page=queryresults
[0m11:05:32.082451 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '61d55789-9cad-48ef-84f3-a2420d5373d7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a18c7c1c680>]}
[0m11:05:32.083767 [info ] [Thread-1 (]: 1 of 1 OK created sql table model analytics_staging_marts.mart_acquisition_efficiency  [[32mCREATE TABLE (11.0 rows, 1008.0 Bytes processed)[0m in 4.33s]
[0m11:05:32.085079 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.mart_acquisition_efficiency
[0m11:05:32.088368 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m11:05:32.090310 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:05:32.091073 [debug] [MainThread]: Connection 'list_data-pipeline-project-474812_analytics_staging_staging' was properly closed.
[0m11:05:32.091749 [debug] [MainThread]: Connection 'model.data_pipeline_project.mart_acquisition_efficiency' was properly closed.
[0m11:05:32.092328 [info ] [MainThread]: 
[0m11:05:32.093159 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 6.47 seconds (6.47s).
[0m11:05:32.095064 [debug] [MainThread]: Command end result
[0m11:05:32.143174 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m11:05:32.147402 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m11:05:32.163494 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m11:05:32.164984 [info ] [MainThread]: 
[0m11:05:32.167962 [info ] [MainThread]: [32mCompleted successfully[0m
[0m11:05:32.169531 [info ] [MainThread]: 
[0m11:05:32.172018 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m11:05:32.175345 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 10.654027, "process_in_blocks": "0", "process_kernel_time": 1.093352, "process_mem_max_rss": "383584", "process_out_blocks": "4256", "process_user_time": 5.586037}
[0m11:05:32.176483 [debug] [MainThread]: Command `dbt run` succeeded at 11:05:32.176337 after 10.66 seconds
[0m11:05:32.177644 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a18e49bdaf0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a18e5368860>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a18e5369a30>]}
[0m11:05:32.179046 [debug] [MainThread]: Flushing usage events
[0m11:05:33.725378 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:09:01.621411 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f46e17358b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f46e07a1190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f46e07a0470>]}


============================== 11:09:01.625960 | bfc44a6e-7f49-4f24-ad87-4402661ab167 ==============================
[0m11:09:01.625960 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m11:09:01.627131 [debug] [MainThread]: running dbt with arguments {'no_print': 'None', 'version_check': 'True', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'write_json': 'True', 'profiles_dir': '/home/ecem/.dbt', 'send_anonymous_usage_stats': 'True', 'debug': 'False', 'warn_error': 'None', 'target_path': 'None', 'use_colors': 'True', 'log_format': 'default', 'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_experimental_parser': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'quiet': 'False', 'empty': 'None', 'static_parser': 'True', 'printer_width': '80', 'invocation_command': 'dbt clean', 'log_cache_events': 'False', 'indirect_selection': 'eager', 'partial_parse': 'True'}
[0m11:09:01.782056 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'bfc44a6e-7f49-4f24-ad87-4402661ab167', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f46e0558050>]}
[0m11:09:01.802154 [debug] [MainThread]: Resource report: {"command_name": "clean", "command_success": true, "command_wall_clock_time": 0.23999235, "process_in_blocks": "0", "process_kernel_time": 0.257831, "process_mem_max_rss": "101444", "process_out_blocks": "8", "process_user_time": 1.475745}
[0m11:09:01.803394 [debug] [MainThread]: Command `dbt clean` succeeded at 11:09:01.803239 after 0.24 seconds
[0m11:09:01.803910 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f46e06b7e00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f46e05dbcb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f46e05ac050>]}
[0m11:09:01.805074 [debug] [MainThread]: Flushing usage events
[0m11:09:02.789440 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:09:06.628032 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7700b16f10d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7700af064a10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7700af13de80>]}


============================== 11:09:06.631387 | d54ec9df-c26b-48f7-84fb-f8a90ce1a6e0 ==============================
[0m11:09:06.631387 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m11:09:06.632608 [debug] [MainThread]: running dbt with arguments {'debug': 'False', 'write_json': 'True', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'target_path': 'None', 'send_anonymous_usage_stats': 'True', 'static_parser': 'True', 'introspect': 'True', 'printer_width': '80', 'empty': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'cache_selected_only': 'False', 'use_colors': 'True', 'quiet': 'False', 'no_print': 'None', 'use_experimental_parser': 'False', 'log_cache_events': 'False', 'fail_fast': 'False', 'warn_error': 'None', 'invocation_command': 'dbt run --select mart_acquisition_efficiency', 'profiles_dir': '/home/ecem/.dbt', 'indirect_selection': 'eager', 'version_check': 'True', 'partial_parse': 'True', 'log_format': 'default'}
[0m11:09:08.993627 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd54ec9df-c26b-48f7-84fb-f8a90ce1a6e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77008f92e840>]}
[0m11:09:09.082847 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd54ec9df-c26b-48f7-84fb-f8a90ce1a6e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77008f505970>]}
[0m11:09:09.085352 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m11:09:09.344823 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m11:09:09.345885 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m11:09:09.346802 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'd54ec9df-c26b-48f7-84fb-f8a90ce1a6e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77008f21dc70>]}
[0m11:09:10.731875 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `relationships` defined on 'stg_orders' in
package 'data_pipeline_project' (models/staging/staging_schema.yml). Arguments
to generic tests should be nested under the `arguments` property.
[0m11:09:10.734402 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': 'd54ec9df-c26b-48f7-84fb-f8a90ce1a6e0', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77008e5b5220>]}
[0m11:09:11.077740 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd54ec9df-c26b-48f7-84fb-f8a90ce1a6e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77008e330380>]}
[0m11:09:11.183687 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m11:09:11.185797 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m11:09:11.203970 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd54ec9df-c26b-48f7-84fb-f8a90ce1a6e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77008e3a0200>]}
[0m11:09:11.204622 [info ] [MainThread]: Found 20 models, 37 data tests, 10 sources, 508 macros
[0m11:09:11.206086 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd54ec9df-c26b-48f7-84fb-f8a90ce1a6e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77008e4dfda0>]}
[0m11:09:11.209186 [info ] [MainThread]: 
[0m11:09:11.210069 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m11:09:11.210886 [info ] [MainThread]: 
[0m11:09:11.211564 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m11:09:11.213017 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812'
[0m11:09:11.213648 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:09:12.401676 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812, now list_data-pipeline-project-474812_analytics_staging_marts)
[0m11:09:12.402498 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:09:12.433813 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_staging'
[0m11:09:12.435121 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:09:14.033937 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd54ec9df-c26b-48f7-84fb-f8a90ce1a6e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77008e3a2e70>]}
[0m11:09:14.034675 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:09:14.043120 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.mart_acquisition_efficiency
[0m11:09:14.044328 [info ] [Thread-1 (]: 1 of 1 START sql table model analytics_staging_marts.mart_acquisition_efficiency  [RUN]
[0m11:09:14.045247 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_marts, now model.data_pipeline_project.mart_acquisition_efficiency)
[0m11:09:14.045989 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.mart_acquisition_efficiency
[0m11:09:14.055910 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.mart_acquisition_efficiency"
[0m11:09:14.058032 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.mart_acquisition_efficiency
[0m11:09:14.078319 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:09:14.970958 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.mart_acquisition_efficiency"
[0m11:09:14.972908 [debug] [Thread-1 (]: On model.data_pipeline_project.mart_acquisition_efficiency: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.mart_acquisition_efficiency"} */

  
    

    create or replace table `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_acquisition_efficiency`
      
    
    

    
    OPTIONS()
    as (
      

with daily_spend as (
    -- 1. Aggregate daily marketing spend
    select
        cast(date as date) as date,
        sum(spend_try) as total_marketing_spend
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_marketing_spend`
    group by 1
),

daily_new_subscribers as (
    -- 2. Pull daily new subscribers from the subscription mart
    select
        date,
        new_subscribers
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily`
),

final as (
    select
        -- Join date (ensures we get all dates from either spend or subscribers)
        coalesce(ds.date, dns.date) as date,

        -- Metrics used for calculation
        coalesce(ds.total_marketing_spend, 0) as total_marketing_spend,
        coalesce(dns.new_subscribers, 0) as new_subscribers,

        -- CALCULATE CAC: Spend / New Subscribers
        -- FIX: Apply ROUND to two decimal places for cleaner reporting.
        case 
            when coalesce(dns.new_subscribers, 0) > 0 then 
                round(coalesce(ds.total_marketing_spend, 0) / dns.new_subscribers, 2)
            else NULL -- Use NULL when there is no acquisition event (or cost is zero)
        end as daily_customer_acquisition_cost
        
    from daily_spend ds
    full outer join daily_new_subscribers dns
        on ds.date = dns.date
    where 
        -- Only include rows where we have either spend or new subscribers (to avoid empty dates)
        coalesce(ds.total_marketing_spend, 0) > 0 OR coalesce(dns.new_subscribers, 0) > 0
    order by 1
)

select * from final

with daily_spend as (
    -- 1. Aggregate daily marketing spend
    select
        cast(date as date) as date,
        sum(spend_try) as total_marketing_spend
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_marketing_spend`
    group by 1
),

daily_new_subscribers as (
    -- 2. Pull daily new subscribers from the subscription mart
    select
        date,
        new_subscribers
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily`
),

final as (
    select
        -- Join date (ensures we get all dates from either spend or subscribers)
        coalesce(ds.date, dns.date) as date,

        -- Metrics used for calculation
        coalesce(ds.total_marketing_spend, 0) as total_marketing_spend,
        coalesce(dns.new_subscribers, 0) as new_subscribers,

        -- CALCULATE CAC: Spend / New Subscribers
        -- FIX: Apply ROUND to two decimal places for cleaner reporting.
        case 
            when coalesce(dns.new_subscribers, 0) > 0 then 
                round(coalesce(ds.total_marketing_spend, 0) / dns.new_subscribers, 2)
            else NULL -- Use NULL when there is no acquisition event (or cost is zero)
        end as daily_customer_acquisition_cost
        
    from daily_spend ds
    full outer join daily_new_subscribers dns
        on ds.date = dns.date
    where 
        -- Only include rows where we have either spend or new subscribers (to avoid empty dates)
        coalesce(ds.total_marketing_spend, 0) > 0 OR coalesce(dns.new_subscribers, 0) > 0
    order by 1
)

select * from final

with daily_spend as (
    -- 1. Aggregate daily marketing spend
    select
        cast(date as date) as date,
        sum(spend_try) as total_marketing_spend
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_marketing_spend`
    group by 1
),

daily_new_subscribers as (
    -- 2. Pull daily new subscribers from the subscription mart
    select
        date,
        new_subscribers
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily`
),

final as (
    select
        -- Join date (ensures we get all dates from either spend or subscribers)
        coalesce(ds.date, dns.date) as date,

        -- Metrics used for calculation
        coalesce(ds.total_marketing_spend, 0) as total_marketing_spend,
        coalesce(dns.new_subscribers, 0) as new_subscribers,

        -- CALCULATE CAC: Spend / New Subscribers
        -- FIX: Apply ROUND to two decimal places for cleaner reporting.
        case 
            when coalesce(dns.new_subscribers, 0) > 0 then 
                round(coalesce(ds.total_marketing_spend, 0) / dns.new_subscribers, 2)
            else NULL -- Use NULL when there is no acquisition event (or cost is zero)
        end as daily_customer_acquisition_cost
        
    from daily_spend ds
    full outer join daily_new_subscribers dns
        on ds.date = dns.date
    where 
        -- Only include rows where we have either spend or new subscribers (to avoid empty dates)
        coalesce(ds.total_marketing_spend, 0) > 0 OR coalesce(dns.new_subscribers, 0) > 0
    order by 1
)

select * from final

with daily_spend as (
    -- 1. Aggregate daily marketing spend
    select
        cast(date as date) as date,
        sum(spend_try) as total_marketing_spend
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_marketing_spend`
    group by 1
),

daily_new_subscribers as (
    -- 2. Pull daily new subscribers from the subscription mart
    select
        date,
        new_subscribers
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily`
),

final as (
    select
        -- Join date (ensures we get all dates from either spend or subscribers)
        coalesce(ds.date, dns.date) as date,

        -- Metrics used for calculation
        coalesce(ds.total_marketing_spend, 0) as total_marketing_spend,
        coalesce(dns.new_subscribers, 0) as new_subscribers,

        -- CALCULATE CAC: Spend / New Subscribers
        -- FIX: Apply ROUND to two decimal places for cleaner reporting.
        case 
            when coalesce(dns.new_subscribers, 0) > 0 then 
                round(coalesce(ds.total_marketing_spend, 0) / dns.new_subscribers, 2)
            else NULL -- Use NULL when there is no acquisition event (or cost is zero)
        end as daily_customer_acquisition_cost
        
    from daily_spend ds
    full outer join daily_new_subscribers dns
        on ds.date = dns.date
    where 
        -- Only include rows where we have either spend or new subscribers (to avoid empty dates)
        coalesce(ds.total_marketing_spend, 0) > 0 OR coalesce(dns.new_subscribers, 0) > 0
    order by 1
)

select * from final
    );
  
[0m11:09:15.345220 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:e402eded-7bd3-45f1-93a9-7d51a1b30dcb&page=queryresults
[0m11:09:15.346127 [error] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:e402eded-7bd3-45f1-93a9-7d51a1b30dcb&page=queryresults
[0m11:09:15.355403 [debug] [Thread-1 (]: Database Error in model mart_acquisition_efficiency (models/marts/mart_acquisition_efficiency.sql)
  Syntax error: Expected keyword OFFSET but got identifier "daily_spend" at [61:6]
  compiled code at target/run/data_pipeline_project/models/marts/mart_acquisition_efficiency.sql
[0m11:09:15.357869 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd54ec9df-c26b-48f7-84fb-f8a90ce1a6e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77008e5d5610>]}
[0m11:09:15.358915 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model analytics_staging_marts.mart_acquisition_efficiency  [[31mERROR[0m in 1.31s]
[0m11:09:15.360137 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.mart_acquisition_efficiency
[0m11:09:15.361160 [debug] [Thread-7 (]: Marking all children of 'model.data_pipeline_project.mart_acquisition_efficiency' to be skipped because of status 'error'.  Reason: Database Error in model mart_acquisition_efficiency (models/marts/mart_acquisition_efficiency.sql)
  Syntax error: Expected keyword OFFSET but got identifier "daily_spend" at [61:6]
  compiled code at target/run/data_pipeline_project/models/marts/mart_acquisition_efficiency.sql.
[0m11:09:15.367229 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m11:09:15.369280 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:09:15.369748 [debug] [MainThread]: Connection 'model.data_pipeline_project.mart_acquisition_efficiency' was properly closed.
[0m11:09:15.370191 [debug] [MainThread]: Connection 'list_data-pipeline-project-474812_analytics_staging_staging' was properly closed.
[0m11:09:15.370661 [info ] [MainThread]: 
[0m11:09:15.371629 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 4.16 seconds (4.16s).
[0m11:09:15.373247 [debug] [MainThread]: Command end result
[0m11:09:15.443278 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m11:09:15.447646 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m11:09:15.459548 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m11:09:15.460356 [info ] [MainThread]: 
[0m11:09:15.461593 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m11:09:15.462432 [info ] [MainThread]: 
[0m11:09:15.463476 [error] [MainThread]: [31mFailure in model mart_acquisition_efficiency (models/marts/mart_acquisition_efficiency.sql)[0m
[0m11:09:15.468995 [error] [MainThread]:   Database Error in model mart_acquisition_efficiency (models/marts/mart_acquisition_efficiency.sql)
  Syntax error: Expected keyword OFFSET but got identifier "daily_spend" at [61:6]
  compiled code at target/run/data_pipeline_project/models/marts/mart_acquisition_efficiency.sql
[0m11:09:15.470244 [info ] [MainThread]: 
[0m11:09:15.472363 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_project/models/marts/mart_acquisition_efficiency.sql
[0m11:09:15.476128 [info ] [MainThread]: 
[0m11:09:15.491083 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m11:09:15.492156 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 18 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m11:09:15.509263 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 8.934489, "process_in_blocks": "0", "process_kernel_time": 1.104437, "process_mem_max_rss": "384912", "process_out_blocks": "4304", "process_user_time": 6.196257}
[0m11:09:15.511008 [debug] [MainThread]: Command `dbt run` failed at 11:09:15.510851 after 8.95 seconds
[0m11:09:15.515061 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7700ae97dcd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77008f090cb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77008f0d5280>]}
[0m11:09:15.515888 [debug] [MainThread]: Flushing usage events
[0m11:09:16.250071 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:09:56.071403 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x725d6bdaaae0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x725d6a975550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x725d6aa57500>]}


============================== 11:09:56.074939 | b2390206-cd08-4219-93d4-3b268c53f590 ==============================
[0m11:09:56.074939 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m11:09:56.075988 [debug] [MainThread]: running dbt with arguments {'warn_error': 'None', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'invocation_command': 'dbt run --select mart_acquisition_efficiency', 'log_cache_events': 'False', 'quiet': 'False', 'write_json': 'True', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_format': 'default', 'printer_width': '80', 'fail_fast': 'False', 'use_experimental_parser': 'False', 'use_colors': 'True', 'static_parser': 'True', 'empty': 'False', 'introspect': 'True', 'indirect_selection': 'eager', 'version_check': 'True', 'debug': 'False', 'cache_selected_only': 'False', 'target_path': 'None', 'profiles_dir': '/home/ecem/.dbt'}
[0m11:09:58.456583 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b2390206-cd08-4219-93d4-3b268c53f590', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x725d4a8617c0>]}
[0m11:09:58.520248 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b2390206-cd08-4219-93d4-3b268c53f590', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x725d6c6f20f0>]}
[0m11:09:58.521019 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m11:09:58.800518 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m11:09:59.075934 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m11:09:59.077111 [debug] [MainThread]: Partial parsing: updated file: data_pipeline_project://models/marts/mart_acquisition_efficiency.sql
[0m11:09:59.371023 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b2390206-cd08-4219-93d4-3b268c53f590', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x725d49a9c6e0>]}
[0m11:09:59.491343 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m11:09:59.494011 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m11:09:59.515799 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b2390206-cd08-4219-93d4-3b268c53f590', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x725d4a5634d0>]}
[0m11:09:59.516806 [info ] [MainThread]: Found 20 models, 37 data tests, 10 sources, 508 macros
[0m11:09:59.517535 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b2390206-cd08-4219-93d4-3b268c53f590', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x725d49a39df0>]}
[0m11:09:59.519900 [info ] [MainThread]: 
[0m11:09:59.520917 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m11:09:59.521511 [info ] [MainThread]: 
[0m11:09:59.523043 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m11:09:59.524675 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812'
[0m11:09:59.525856 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:10:00.436959 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812, now list_data-pipeline-project-474812_analytics_staging_marts)
[0m11:10:00.437933 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:10:00.472069 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_staging'
[0m11:10:00.473291 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:10:01.433204 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b2390206-cd08-4219-93d4-3b268c53f590', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x725d49a9fef0>]}
[0m11:10:01.434172 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:10:01.442786 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.mart_acquisition_efficiency
[0m11:10:01.444040 [info ] [Thread-1 (]: 1 of 1 START sql table model analytics_staging_marts.mart_acquisition_efficiency  [RUN]
[0m11:10:01.444936 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now model.data_pipeline_project.mart_acquisition_efficiency)
[0m11:10:01.445432 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.mart_acquisition_efficiency
[0m11:10:01.453483 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.mart_acquisition_efficiency"
[0m11:10:01.455786 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.mart_acquisition_efficiency
[0m11:10:01.476436 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:10:02.402299 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.mart_acquisition_efficiency"
[0m11:10:02.404316 [debug] [Thread-1 (]: On model.data_pipeline_project.mart_acquisition_efficiency: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.mart_acquisition_efficiency"} */

  
    

    create or replace table `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_acquisition_efficiency`
      
    
    

    
    OPTIONS()
    as (
      

with daily_spend as (
    -- 1. Aggregate daily marketing spend
    select
        cast(date as date) as date,
        sum(spend_try) as total_marketing_spend
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_marketing_spend`
    group by 1
),

daily_new_subscribers as (
    -- 2. Pull daily new subscribers from the subscription mart
    select
        date,
        new_subscribers
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily`
),

final as (
    select
        -- Join date (ensures we get all dates from either spend or subscribers)
        coalesce(ds.date, dns.date) as date,

        -- Metrics used for calculation
        coalesce(ds.total_marketing_spend, 0) as total_marketing_spend,
        coalesce(dns.new_subscribers, 0) as new_subscribers,

        -- CALCULATE CAC: Spend / New Subscribers
        -- FIX: Apply ROUND to two decimal places for cleaner reporting.
        case 
            when coalesce(dns.new_subscribers, 0) > 0 then 
                round(coalesce(ds.total_marketing_spend, 0) / dns.new_subscribers, 2)
            else NULL -- Use NULL when there is no acquisition event (or cost is zero)
        end as daily_customer_acquisition_cost
        
    from daily_spend ds
    full outer join daily_new_subscribers dns
        on ds.date = dns.date
    where 
        -- Only include rows where we have either spend or new subscribers (to avoid empty dates)
        coalesce(ds.total_marketing_spend, 0) > 0 OR coalesce(dns.new_subscribers, 0) > 0
    order by 1
)

select * from final
    );
  
[0m11:10:03.081519 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:b7fa9555-6735-411f-adc6-988237d0963f&page=queryresults
[0m11:10:05.924320 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b2390206-cd08-4219-93d4-3b268c53f590', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x725d4d66c8f0>]}
[0m11:10:05.925402 [info ] [Thread-1 (]: 1 of 1 OK created sql table model analytics_staging_marts.mart_acquisition_efficiency  [[32mCREATE TABLE (11.0 rows, 1008.0 Bytes processed)[0m in 4.48s]
[0m11:10:05.927118 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.mart_acquisition_efficiency
[0m11:10:05.930027 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m11:10:05.932406 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:10:05.933435 [debug] [MainThread]: Connection 'list_data-pipeline-project-474812_analytics_staging_marts' was properly closed.
[0m11:10:05.934284 [debug] [MainThread]: Connection 'model.data_pipeline_project.mart_acquisition_efficiency' was properly closed.
[0m11:10:05.934911 [info ] [MainThread]: 
[0m11:10:05.935380 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 6.41 seconds (6.41s).
[0m11:10:05.936654 [debug] [MainThread]: Command end result
[0m11:10:05.984355 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m11:10:05.988815 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m11:10:05.999332 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m11:10:06.002494 [info ] [MainThread]: 
[0m11:10:06.003378 [info ] [MainThread]: [32mCompleted successfully[0m
[0m11:10:06.004142 [info ] [MainThread]: 
[0m11:10:06.004751 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m11:10:06.006365 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 9.99532, "process_in_blocks": "0", "process_kernel_time": 1.132878, "process_mem_max_rss": "383184", "process_out_blocks": "4256", "process_user_time": 5.04955}
[0m11:10:06.007074 [debug] [MainThread]: Command `dbt run` succeeded at 11:10:06.006957 after 10.00 seconds
[0m11:10:06.007519 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x725d6a284950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x725d4a9f88c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x725d4a389eb0>]}
[0m11:10:06.007937 [debug] [MainThread]: Flushing usage events
[0m11:10:07.014040 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:08:05.023810 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x796ea23c36e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x796ea41f2fc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x796ea2267740>]}


============================== 12:08:05.027249 | 773fabd4-3ae8-4293-a013-3fd4547b38df ==============================
[0m12:08:05.027249 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m12:08:05.028716 [debug] [MainThread]: running dbt with arguments {'log_cache_events': 'False', 'debug': 'False', 'warn_error': 'None', 'cache_selected_only': 'False', 'use_colors': 'True', 'version_check': 'True', 'no_print': 'None', 'indirect_selection': 'eager', 'introspect': 'True', 'quiet': 'False', 'printer_width': '80', 'log_format': 'default', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'use_experimental_parser': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'write_json': 'True', 'profiles_dir': '/home/ecem/.dbt', 'send_anonymous_usage_stats': 'True', 'invocation_command': 'dbt run --select mart_kpis_latest', 'target_path': 'None', 'empty': 'False', 'static_parser': 'True', 'partial_parse': 'True', 'fail_fast': 'False'}
[0m12:08:07.474816 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '773fabd4-3ae8-4293-a013-3fd4547b38df', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x796e8203c5c0>]}
[0m12:08:07.540546 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '773fabd4-3ae8-4293-a013-3fd4547b38df', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x796e822e3380>]}
[0m12:08:07.545127 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m12:08:07.746765 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m12:08:08.077010 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
[0m12:08:08.077929 [debug] [MainThread]: Partial parsing: added file: data_pipeline_project://models/marts/mart_kpis_latest.sql
[0m12:08:08.383766 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '773fabd4-3ae8-4293-a013-3fd4547b38df', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x796e81281010>]}
[0m12:08:08.501668 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m12:08:08.504513 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m12:08:08.524276 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '773fabd4-3ae8-4293-a013-3fd4547b38df', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x796e8123c770>]}
[0m12:08:08.525189 [info ] [MainThread]: Found 21 models, 37 data tests, 10 sources, 508 macros
[0m12:08:08.525881 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '773fabd4-3ae8-4293-a013-3fd4547b38df', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x796e812315b0>]}
[0m12:08:08.528382 [info ] [MainThread]: 
[0m12:08:08.529200 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m12:08:08.529733 [info ] [MainThread]: 
[0m12:08:08.530428 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m12:08:08.532006 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812'
[0m12:08:08.532666 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:08:09.584492 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812, now list_data-pipeline-project-474812_analytics_staging_marts)
[0m12:08:09.586642 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_staging'
[0m12:08:09.587637 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:08:09.589316 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:08:10.573373 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '773fabd4-3ae8-4293-a013-3fd4547b38df', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x796e81f27290>]}
[0m12:08:10.574563 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:08:10.589908 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.mart_kpis_latest
[0m12:08:10.591482 [info ] [Thread-1 (]: 1 of 1 START sql table model analytics_staging_marts.mart_kpis_latest .......... [RUN]
[0m12:08:10.592394 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now model.data_pipeline_project.mart_kpis_latest)
[0m12:08:10.593098 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.mart_kpis_latest
[0m12:08:10.603215 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.mart_kpis_latest"
[0m12:08:10.604269 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.mart_kpis_latest
[0m12:08:10.663378 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.mart_kpis_latest"
[0m12:08:10.664532 [debug] [Thread-1 (]: On model.data_pipeline_project.mart_kpis_latest: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.mart_kpis_latest"} */

  
    

    create or replace table `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_kpis_latest`
      
    
    

    
    OPTIONS()
    as (
      with snapshot_kpis as (

    select
        -- Anchor the entire report to the latest date available in the subscription mart
        max(date) as latest_date,
        
        -- 1. LATEST ACTIVE SUBSCRIBERS (Active on the Max Date)
        -- We use MAX(CASE) over the entire dataset to pull the single value for the latest date
        max(case 
            when date = max(date) over () 
            then active_subscribers 
            else NULL 
        end) as active_subscribers_latest
        
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily`
),

-- CTE: Calculate the Average Revenue Per Active Subscriber (ARPAS) over the latest 30 days.
-- This dynamic calculation is used for MRR and Payback.
monthly_revenue_metrics as (
    select
        -- Total gross revenue from delivered orders over the last 30 days
        sum(mr.daily_gross_revenue) as total_gross_revenue_30d,
        -- Total sum of active subscribers across those 30 days
        sum(msd.active_subscribers) as total_active_subs_30d
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_revenue_daily` mr
    inner join `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily` msd
        on mr.date = msd.date
    where mr.date >= date_sub(
        (select max(date) from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily`), 
        interval 30 day
    )
),

-- Final assembly with lookback calculations
select
    t1.latest_date,
    t1.active_subscribers_latest,

    -- 2. AVERAGE REVENUE PER ACTIVE SUB (ARPAS)
    -- ARPAS = (Total Gross Revenue / Total Active Subscribers) * 30 days. This is the new 'price' proxy.
    round(
        (t2.total_gross_revenue_30d / nullif(t2.total_active_subs_30d, 0)) * 30
    , 2) as monthly_revenue_per_active_sub,


    -- 3. MONTHLY RECURRING REVENUE (MRR) - Uses the new calculated ARPAS
    -- MRR = Latest Active Subscribers * ARPAS
    round(
        t1.active_subscribers_latest * (t2.total_gross_revenue_30d / nullif(t2.total_active_subs_30d, 0)) * 30
    , 2) as monthly_recurring_revenue,

    -- 4. MONTHLY CHURN RATE % (Over the last 30 days)
    -- Formula: (Total Cancellations in 30d / Active Subs 30d Ago) * 100
    round(
        (
            -- Total cancellations in the last 30 days
            (select sum(cancellations) from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily` where date >= date_sub(t1.latest_date, interval 30 day))
            / 
            -- Active subscribers 30 days ago (the base for the churn rate)
            nullif((select active_subscribers from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily` where date = date_sub(t1.latest_date, interval 30 day)), 0)
        ) * 100
        , 2
    ) as monthly_churn_rate_pct,

    -- 5. CAC PAYBACK PERIOD (in Months) - Uses the new calculated ARPAS as the price
    -- Formula: Payback = Average 30d CAC / ARPAS
    round(
        (
            -- Average CAC over the last 30 days
            (select avg(daily_customer_acquisition_cost) from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_acquisition_efficiency` where date >= date_sub(t1.latest_date, interval 30 day))
            / 
            -- Monthly Revenue Per Active Sub (ARPAS)
            nullif(
                (t2.total_gross_revenue_30d / nullif(t2.total_active_subs_30d, 0)) * 30
                , 0
            )
        )
        , 2
    ) as cac_payback_months

from snapshot_kpis t1
cross join monthly_revenue_metrics t2
    );
  
[0m12:08:10.665234 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:08:11.593571 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:007780ab-a637-4881-b707-8fe58a7b7ddb&page=queryresults
[0m12:08:11.594538 [error] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:007780ab-a637-4881-b707-8fe58a7b7ddb&page=queryresults
[0m12:08:11.607532 [debug] [Thread-1 (]: Database Error in model mart_kpis_latest (models/marts/mart_kpis_latest.sql)
  Syntax error: Trailing comma after the WITH clause before the main query is not allowed at [49:1]
  compiled code at target/run/data_pipeline_project/models/marts/mart_kpis_latest.sql
[0m12:08:11.610137 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '773fabd4-3ae8-4293-a013-3fd4547b38df', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x796e81107770>]}
[0m12:08:11.611345 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model analytics_staging_marts.mart_kpis_latest . [[31mERROR[0m in 1.02s]
[0m12:08:11.612633 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.mart_kpis_latest
[0m12:08:11.613458 [debug] [Thread-7 (]: Marking all children of 'model.data_pipeline_project.mart_kpis_latest' to be skipped because of status 'error'.  Reason: Database Error in model mart_kpis_latest (models/marts/mart_kpis_latest.sql)
  Syntax error: Trailing comma after the WITH clause before the main query is not allowed at [49:1]
  compiled code at target/run/data_pipeline_project/models/marts/mart_kpis_latest.sql.
[0m12:08:11.620200 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m12:08:11.622733 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:08:11.623235 [debug] [MainThread]: Connection 'list_data-pipeline-project-474812_analytics_staging_marts' was properly closed.
[0m12:08:11.623958 [debug] [MainThread]: Connection 'model.data_pipeline_project.mart_kpis_latest' was properly closed.
[0m12:08:11.624959 [info ] [MainThread]: 
[0m12:08:11.625486 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 3.09 seconds (3.09s).
[0m12:08:11.626452 [debug] [MainThread]: Command end result
[0m12:08:11.699201 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m12:08:11.703173 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m12:08:11.717773 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m12:08:11.720129 [info ] [MainThread]: 
[0m12:08:11.720942 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m12:08:11.721652 [info ] [MainThread]: 
[0m12:08:11.722480 [error] [MainThread]: [31mFailure in model mart_kpis_latest (models/marts/mart_kpis_latest.sql)[0m
[0m12:08:11.723385 [error] [MainThread]:   Database Error in model mart_kpis_latest (models/marts/mart_kpis_latest.sql)
  Syntax error: Trailing comma after the WITH clause before the main query is not allowed at [49:1]
  compiled code at target/run/data_pipeline_project/models/marts/mart_kpis_latest.sql
[0m12:08:11.724720 [info ] [MainThread]: 
[0m12:08:11.726079 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_project/models/marts/mart_kpis_latest.sql
[0m12:08:11.727037 [info ] [MainThread]: 
[0m12:08:11.727978 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m12:08:11.729960 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 6.7664223, "process_in_blocks": "0", "process_kernel_time": 1.135131, "process_mem_max_rss": "379616", "process_out_blocks": "4312", "process_user_time": 5.153344}
[0m12:08:11.731089 [debug] [MainThread]: Command `dbt run` failed at 12:08:11.730961 after 6.77 seconds
[0m12:08:11.731550 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x796ea201d190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x796e81283bc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x796e8216c650>]}
[0m12:08:11.732300 [debug] [MainThread]: Flushing usage events
[0m12:08:12.693594 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:09:28.626232 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70649b01b8c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70649b1f3830>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70649ae81eb0>]}


============================== 12:09:28.630819 | 74a89802-4cf3-43f6-9fb8-5d7fc0e3129d ==============================
[0m12:09:28.630819 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m12:09:28.633927 [debug] [MainThread]: running dbt with arguments {'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'indirect_selection': 'eager', 'send_anonymous_usage_stats': 'True', 'log_cache_events': 'False', 'warn_error': 'None', 'use_colors': 'True', 'version_check': 'True', 'target_path': 'None', 'no_print': 'None', 'debug': 'False', 'static_parser': 'True', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'use_experimental_parser': 'False', 'invocation_command': 'dbt run --select mart_kpis_latest', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'empty': 'False', 'fail_fast': 'False', 'quiet': 'False', 'introspect': 'True', 'log_format': 'default', 'profiles_dir': '/home/ecem/.dbt'}
[0m12:09:30.979522 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '74a89802-4cf3-43f6-9fb8-5d7fc0e3129d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70647b1b0d70>]}
[0m12:09:31.046372 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '74a89802-4cf3-43f6-9fb8-5d7fc0e3129d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70647b3f3ad0>]}
[0m12:09:31.047639 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m12:09:31.278457 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m12:09:31.560386 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m12:09:31.563300 [debug] [MainThread]: Partial parsing: updated file: data_pipeline_project://models/marts/mart_kpis_latest.sql
[0m12:09:31.859133 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '74a89802-4cf3-43f6-9fb8-5d7fc0e3129d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70647a284350>]}
[0m12:09:31.974752 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m12:09:31.977627 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m12:09:31.998149 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '74a89802-4cf3-43f6-9fb8-5d7fc0e3129d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70647adfd100>]}
[0m12:09:31.999155 [info ] [MainThread]: Found 21 models, 37 data tests, 10 sources, 508 macros
[0m12:09:31.999958 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '74a89802-4cf3-43f6-9fb8-5d7fc0e3129d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70647a287b00>]}
[0m12:09:32.002184 [info ] [MainThread]: 
[0m12:09:32.003199 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m12:09:32.004013 [info ] [MainThread]: 
[0m12:09:32.004853 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m12:09:32.006738 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812'
[0m12:09:32.008416 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:09:32.970484 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812, now list_data-pipeline-project-474812_analytics_staging_marts)
[0m12:09:32.971782 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:09:33.005935 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_staging'
[0m12:09:33.006953 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:09:34.059004 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '74a89802-4cf3-43f6-9fb8-5d7fc0e3129d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70647a287b00>]}
[0m12:09:34.061231 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:09:34.076568 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.mart_kpis_latest
[0m12:09:34.078273 [info ] [Thread-1 (]: 1 of 1 START sql table model analytics_staging_marts.kpis_latest ............... [RUN]
[0m12:09:34.079025 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_marts, now model.data_pipeline_project.mart_kpis_latest)
[0m12:09:34.080021 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.mart_kpis_latest
[0m12:09:34.094813 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.mart_kpis_latest"
[0m12:09:34.096353 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.mart_kpis_latest
[0m12:09:34.160354 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.mart_kpis_latest"
[0m12:09:34.162494 [debug] [Thread-1 (]: On model.data_pipeline_project.mart_kpis_latest: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.mart_kpis_latest"} */

  
    

    create or replace table `data-pipeline-project-474812`.`analytics_staging_marts`.`kpis_latest`
      
    
    

    
    OPTIONS()
    as (
      

with snapshot_kpis as (

    select
        -- Anchor the entire report to the latest date available in the subscription mart
        max(date) as latest_date,
        
        -- 1. LATEST ACTIVE SUBSCRIBERS (Active on the Max Date)
        -- We use MAX(CASE) over the entire dataset to pull the single value for the latest date
        max(case 
            when date = max(date) over () 
            then active_subscribers 
            else NULL 
        end) as active_subscribers_latest
        
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily`
),

-- CTE: Calculate the Average Revenue Per Active Subscriber (ARPAS) over the latest 30 days.
-- This dynamic calculation is used for MRR and Payback.
monthly_revenue_metrics as (
    select
        -- Total gross revenue from delivered orders over the last 30 days
        sum(mr.daily_gross_revenue) as total_gross_revenue_30d,
        -- Total sum of active subscribers across those 30 days
        sum(msd.active_subscribers) as total_active_subs_30d
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_revenue_daily` mr
    inner join `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily` msd
        on mr.date = msd.date
    where mr.date >= date_sub(
        (select max(date) from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily`), 
        interval 30 day
    )
)
-- Removed the trailing comma here that was causing the error.

-- Final assembly with lookback calculations
select
    t1.latest_date,
    t1.active_subscribers_latest,

    -- 2. AVERAGE REVENUE PER ACTIVE SUB (ARPAS)
    -- ARPAS = (Total Gross Revenue / Total Active Subscribers) * 30 days. This is the new 'price' proxy.
    round(
        (t2.total_gross_revenue_30d / nullif(t2.total_active_subs_30d, 0)) * 30
    , 2) as monthly_revenue_per_active_sub,


    -- 3. MONTHLY RECURRING REVENUE (MRR) - Uses the new calculated ARPAS
    -- MRR = Latest Active Subscribers * ARPAS
    round(
        t1.active_subscribers_latest * (t2.total_gross_revenue_30d / nullif(t2.total_active_subs_30d, 0)) * 30
    , 2) as monthly_recurring_revenue,

    -- 4. MONTHLY CHURN RATE % (Over the last 30 days)
    -- Formula: (Total Cancellations in 30d / Active Subs 30d Ago) * 100
    round(
        (
            -- Total cancellations in the last 30 days
            (select sum(cancellations) from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily` where date >= date_sub(t1.latest_date, interval 30 day))
            / 
            -- Active subscribers 30 days ago (the base for the churn rate)
            nullif((select active_subscribers from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily` where date = date_sub(t1.latest_date, interval 30 day)), 0)
        ) * 100
        , 2
    ) as monthly_churn_rate_pct,

    -- 5. CAC PAYBACK PERIOD (in Months) - Uses the new calculated ARPAS as the price
    -- Formula: Payback = Average 30d CAC / ARPAS
    round(
        (
            -- Average CAC over the last 30 days
            (select avg(daily_customer_acquisition_cost) from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_acquisition_efficiency` where date >= date_sub(t1.latest_date, interval 30 day))
            / 
            -- Monthly Revenue Per Active Sub (ARPAS)
            nullif(
                (t2.total_gross_revenue_30d / nullif(t2.total_active_subs_30d, 0)) * 30
                , 0
            )
        )
        , 2
    ) as cac_payback_months

from snapshot_kpis t1
cross join monthly_revenue_metrics t2
    );
  
[0m12:09:34.163764 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:09:35.382510 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:8aa8c38b-9522-45b1-8d75-4147db95f2d4&page=queryresults
[0m12:09:35.688105 [error] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:8aa8c38b-9522-45b1-8d75-4147db95f2d4&page=queryresults
[0m12:09:35.705188 [debug] [Thread-1 (]: Database Error in model mart_kpis_latest (models/marts/mart_kpis_latest.sql)
  Analytic functions cannot be arguments to aggregate functions at [24:9]
  compiled code at target/run/data_pipeline_project/models/marts/mart_kpis_latest.sql
[0m12:09:35.708640 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '74a89802-4cf3-43f6-9fb8-5d7fc0e3129d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70647a229eb0>]}
[0m12:09:35.710279 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model analytics_staging_marts.kpis_latest ...... [[31mERROR[0m in 1.63s]
[0m12:09:35.712289 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.mart_kpis_latest
[0m12:09:35.716480 [debug] [Thread-7 (]: Marking all children of 'model.data_pipeline_project.mart_kpis_latest' to be skipped because of status 'error'.  Reason: Database Error in model mart_kpis_latest (models/marts/mart_kpis_latest.sql)
  Analytic functions cannot be arguments to aggregate functions at [24:9]
  compiled code at target/run/data_pipeline_project/models/marts/mart_kpis_latest.sql.
[0m12:09:35.723630 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m12:09:35.725463 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:09:35.728167 [debug] [MainThread]: Connection 'model.data_pipeline_project.mart_kpis_latest' was properly closed.
[0m12:09:35.729194 [debug] [MainThread]: Connection 'list_data-pipeline-project-474812_analytics_staging_staging' was properly closed.
[0m12:09:35.729946 [info ] [MainThread]: 
[0m12:09:35.731690 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 3.73 seconds (3.73s).
[0m12:09:35.732508 [debug] [MainThread]: Command end result
[0m12:09:35.785175 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m12:09:35.790246 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m12:09:35.800980 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m12:09:35.801914 [info ] [MainThread]: 
[0m12:09:35.802624 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m12:09:35.803820 [info ] [MainThread]: 
[0m12:09:35.804702 [error] [MainThread]: [31mFailure in model mart_kpis_latest (models/marts/mart_kpis_latest.sql)[0m
[0m12:09:35.805369 [error] [MainThread]:   Database Error in model mart_kpis_latest (models/marts/mart_kpis_latest.sql)
  Analytic functions cannot be arguments to aggregate functions at [24:9]
  compiled code at target/run/data_pipeline_project/models/marts/mart_kpis_latest.sql
[0m12:09:35.806223 [info ] [MainThread]: 
[0m12:09:35.806754 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_project/models/marts/mart_kpis_latest.sql
[0m12:09:35.809328 [info ] [MainThread]: 
[0m12:09:35.810884 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m12:09:35.814742 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 7.244961, "process_in_blocks": "0", "process_kernel_time": 1.260484, "process_mem_max_rss": "379576", "process_out_blocks": "4312", "process_user_time": 4.855701}
[0m12:09:35.815945 [debug] [MainThread]: Command `dbt run` failed at 12:09:35.815812 after 7.25 seconds
[0m12:09:35.819547 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70649b629850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70647adfda60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70647ab3f890>]}
[0m12:09:35.823168 [debug] [MainThread]: Flushing usage events
[0m12:09:36.753795 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:12:42.580905 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b16c0e19010>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b16c127adb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b16c1229d00>]}


============================== 12:12:42.598207 | 312dadb3-24fe-4c4f-ac3b-3b7e4e92ab71 ==============================
[0m12:12:42.598207 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m12:12:42.607242 [debug] [MainThread]: running dbt with arguments {'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'partial_parse': 'True', 'no_print': 'None', 'write_json': 'True', 'empty': 'False', 'cache_selected_only': 'False', 'warn_error': 'None', 'profiles_dir': '/home/ecem/.dbt', 'printer_width': '80', 'use_experimental_parser': 'False', 'invocation_command': 'dbt run --select mart_kpis_latest', 'target_path': 'None', 'use_colors': 'True', 'quiet': 'False', 'debug': 'False', 'static_parser': 'True', 'introspect': 'True', 'indirect_selection': 'eager', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'version_check': 'True', 'send_anonymous_usage_stats': 'True', 'log_format': 'default', 'fail_fast': 'False', 'log_cache_events': 'False'}
[0m12:12:44.906288 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '312dadb3-24fe-4c4f-ac3b-3b7e4e92ab71', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b16a3c17530>]}
[0m12:12:44.994196 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '312dadb3-24fe-4c4f-ac3b-3b7e4e92ab71', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b16a2be5b20>]}
[0m12:12:44.995319 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m12:12:45.256616 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m12:12:45.523582 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m12:12:45.524830 [debug] [MainThread]: Partial parsing: updated file: data_pipeline_project://models/marts/mart_kpis_latest.sql
[0m12:12:45.815660 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '312dadb3-24fe-4c4f-ac3b-3b7e4e92ab71', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b16a0ea9f70>]}
[0m12:12:45.927762 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m12:12:45.930529 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m12:12:45.946673 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '312dadb3-24fe-4c4f-ac3b-3b7e4e92ab71', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b1693f4ea80>]}
[0m12:12:45.947273 [info ] [MainThread]: Found 21 models, 37 data tests, 10 sources, 508 macros
[0m12:12:45.947835 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '312dadb3-24fe-4c4f-ac3b-3b7e4e92ab71', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b1693f29790>]}
[0m12:12:45.949755 [info ] [MainThread]: 
[0m12:12:45.950321 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m12:12:45.950885 [info ] [MainThread]: 
[0m12:12:45.951579 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m12:12:45.953003 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812'
[0m12:12:45.954179 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:12:46.962688 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812, now list_data-pipeline-project-474812_analytics_staging_marts)
[0m12:12:46.964211 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_staging'
[0m12:12:46.965400 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:12:46.966893 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:12:47.938051 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '312dadb3-24fe-4c4f-ac3b-3b7e4e92ab71', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b1693f4ea80>]}
[0m12:12:47.939044 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:12:47.946729 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.mart_kpis_latest
[0m12:12:47.947893 [info ] [Thread-1 (]: 1 of 1 START sql table model analytics_staging_marts.kpis_latest ............... [RUN]
[0m12:12:47.948898 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now model.data_pipeline_project.mart_kpis_latest)
[0m12:12:47.949358 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.mart_kpis_latest
[0m12:12:47.959065 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.mart_kpis_latest"
[0m12:12:47.962086 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.mart_kpis_latest
[0m12:12:48.024383 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.mart_kpis_latest"
[0m12:12:48.028856 [debug] [Thread-1 (]: On model.data_pipeline_project.mart_kpis_latest: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.mart_kpis_latest"} */

  
    

    create or replace table `data-pipeline-project-474812`.`analytics_staging_marts`.`kpis_latest`
      
    
    

    
    OPTIONS()
    as (
      

with latest_subscription_data as (
    -- Get the latest date and active subscribers in separate steps
    select 
        max(date) as latest_date
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily`
),

snapshot_kpis as (
    select
        lsd.latest_date,
        -- Get active subscribers for the latest date
        (select active_subscribers 
         from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily` 
         where date = lsd.latest_date) as active_subscribers_latest
    from latest_subscription_data lsd
),

-- CTE: Calculate the Average Revenue Per Active Subscriber (ARPAS) over the latest 30 days.
monthly_revenue_metrics as (
    select
        -- Total gross revenue from delivered orders over the last 30 days
        sum(mr.daily_revenue) as total_gross_revenue_30d,
        -- Total sum of active subscribers across those 30 days
        sum(msd.active_subscribers) as total_active_subs_30d
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_revenue_daily` mr
    inner join `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily` msd
        on mr.date = msd.date
    where mr.date >= date_sub(
        (select max(date) from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily`), 
        interval 30 day
    )
)

-- Final assembly with lookback calculations
select
    t1.latest_date,
    t1.active_subscribers_latest,

    -- 2. AVERAGE REVENUE PER ACTIVE SUB (ARPAS)
    round(
        (t2.total_gross_revenue_30d / nullif(t2.total_active_subs_30d, 0)) * 30
    , 2) as monthly_revenue_per_active_sub,

    -- 3. MONTHLY RECURRING REVENUE (MRR)
    round(
        t1.active_subscribers_latest * (t2.total_gross_revenue_30d / nullif(t2.total_active_subs_30d, 0)) * 30
    , 2) as monthly_recurring_revenue,

    -- 4. MONTHLY CHURN RATE %
    round(
        (
            -- Total cancellations in the last 30 days
            (select sum(cancellations) from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily` where date >= date_sub(t1.latest_date, interval 30 day))
            / 
            -- Active subscribers 30 days ago
            nullif((select active_subscribers from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily` where date = date_sub(t1.latest_date, interval 30 day)), 0)
        ) * 100
        , 2
    ) as monthly_churn_rate_pct,

    -- 5. CAC PAYBACK PERIOD
    round(
        (
            -- Average CAC over the last 30 days
            (select avg(daily_customer_acquisition_cost) from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_acquisition_efficiency` where date >= date_sub(t1.latest_date, interval 30 day))
            / 
            -- Monthly Revenue Per Active Sub (ARPAS)
            nullif(
                (t2.total_gross_revenue_30d / nullif(t2.total_active_subs_30d, 0)) * 30
                , 0
            )
        )
        , 2
    ) as cac_payback_months

from snapshot_kpis t1
cross join monthly_revenue_metrics t2
    );
  
[0m12:12:48.031462 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:12:49.248195 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:87bc5e2e-1752-454d-9820-10c40a9c927f&page=queryresults
[0m12:12:51.880292 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '312dadb3-24fe-4c4f-ac3b-3b7e4e92ab71', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b16c13875f0>]}
[0m12:12:51.881559 [info ] [Thread-1 (]: 1 of 1 OK created sql table model analytics_staging_marts.kpis_latest .......... [[32mCREATE TABLE (1.0 rows, 1.1 KiB processed)[0m in 3.93s]
[0m12:12:51.882824 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.mart_kpis_latest
[0m12:12:51.885851 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m12:12:51.888018 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:12:51.888763 [debug] [MainThread]: Connection 'list_data-pipeline-project-474812_analytics_staging_marts' was properly closed.
[0m12:12:51.889512 [debug] [MainThread]: Connection 'model.data_pipeline_project.mart_kpis_latest' was properly closed.
[0m12:12:51.890161 [info ] [MainThread]: 
[0m12:12:51.891640 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 5.94 seconds (5.94s).
[0m12:12:51.892654 [debug] [MainThread]: Command end result
[0m12:12:51.955896 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m12:12:51.964549 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m12:12:51.987143 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m12:12:51.987867 [info ] [MainThread]: 
[0m12:12:51.988582 [info ] [MainThread]: [32mCompleted successfully[0m
[0m12:12:51.989283 [info ] [MainThread]: 
[0m12:12:51.989947 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m12:12:51.991118 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 9.483849, "process_in_blocks": "0", "process_kernel_time": 1.502062, "process_mem_max_rss": "383128", "process_out_blocks": "4296", "process_user_time": 4.292717}
[0m12:12:51.991746 [debug] [MainThread]: Command `dbt run` succeeded at 12:12:51.991601 after 9.48 seconds
[0m12:12:51.992343 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b16c0523f80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b16c13a5940>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b16a0ba9eb0>]}
[0m12:12:51.994028 [debug] [MainThread]: Flushing usage events
[0m12:12:52.984283 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:55:19.305922 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1627ce84a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1628892660>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f162647e4e0>]}


============================== 12:55:19.310610 | 1a0da51d-1bc3-4d3a-bcde-af326a00e7b2 ==============================
[0m12:55:19.310610 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m12:55:19.311951 [debug] [MainThread]: running dbt with arguments {'use_colors': 'True', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'target_path': 'None', 'static_parser': 'True', 'log_format': 'default', 'warn_error': 'None', 'use_experimental_parser': 'False', 'fail_fast': 'False', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'version_check': 'True', 'introspect': 'True', 'profiles_dir': '/home/ecem/.dbt', 'partial_parse': 'True', 'invocation_command': 'dbt test --select int_facts_marts_schema', 'cache_selected_only': 'False', 'write_json': 'True', 'no_print': 'None', 'empty': 'None', 'printer_width': '80', 'quiet': 'False', 'debug': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])'}
[0m12:55:21.698453 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1a0da51d-1bc3-4d3a-bcde-af326a00e7b2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f162859bd10>]}
[0m12:55:21.767899 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '1a0da51d-1bc3-4d3a-bcde-af326a00e7b2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f16069e45c0>]}
[0m12:55:21.769077 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m12:55:22.031370 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m12:55:22.307075 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 1 files changed.
[0m12:55:22.308891 [debug] [MainThread]: Partial parsing: added file: data_pipeline_project://models/intermediate/int_facts_marts_schema.yml
[0m12:55:22.309702 [debug] [MainThread]: Partial parsing: updated file: data_pipeline_project://models/marts/mart_kpis_latest.sql
[0m12:55:22.582211 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'int_subscriptions_parsed' in the 'models' section of file 'models/intermediate/int_facts_marts_schema.yml'
[0m12:55:22.705386 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `relationships` defined on
'int_subscriptions_parsed' in package 'data_pipeline_project'
(models/intermediate/int_facts_marts_schema.yml). Arguments to generic tests
should be nested under the `arguments` property.
[0m12:55:22.706014 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '1a0da51d-1bc3-4d3a-bcde-af326a00e7b2', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f16060fd880>]}
[0m12:55:22.855931 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.data_pipeline_project.unique_int_subscriptions_parsed_subscription_id.69766961b1' (models/intermediate/int_facts_marts_schema.yml) depends on a node named 'int_subscriptions_parsed' in package '' which was not found
[0m12:55:22.856671 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.data_pipeline_project.not_null_int_subscriptions_parsed_subscription_id.01203fb859' (models/intermediate/int_facts_marts_schema.yml) depends on a node named 'int_subscriptions_parsed' in package '' which was not found
[0m12:55:22.858926 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.data_pipeline_project.not_null_int_subscriptions_parsed_user_id.675e06afdf' (models/intermediate/int_facts_marts_schema.yml) depends on a node named 'int_subscriptions_parsed' in package '' which was not found
[0m12:55:22.862071 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.data_pipeline_project.relationships_int_subscriptions_parsed_user_id__user_id__ref_stg_users_.536d58fe6b' (models/intermediate/int_facts_marts_schema.yml) depends on a node named 'int_subscriptions_parsed' in package '' which was not found
[0m12:55:22.863554 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.data_pipeline_project.not_null_int_subscriptions_parsed_start_date.1892adf117' (models/intermediate/int_facts_marts_schema.yml) depends on a node named 'int_subscriptions_parsed' in package '' which was not found
[0m12:55:22.864439 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.data_pipeline_project.relationships_dim_subscription_user_id__customer_id__ref_dim_customers_.35f0a793d3' (models/intermediate/int_facts_marts_schema.yml) depends on a node named 'dim_customers' in package '' which was not found
[0m12:55:22.976334 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1a0da51d-1bc3-4d3a-bcde-af326a00e7b2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f16060fd310>]}
[0m12:55:23.106209 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m12:55:23.109711 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m12:55:23.145782 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1a0da51d-1bc3-4d3a-bcde-af326a00e7b2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f16057c2240>]}
[0m12:55:23.146611 [info ] [MainThread]: Found 21 models, 70 data tests, 10 sources, 508 macros
[0m12:55:23.147521 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1a0da51d-1bc3-4d3a-bcde-af326a00e7b2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f16060ffb00>]}
[0m12:55:23.148957 [warn ] [MainThread]: The selection criterion 'int_facts_marts_schema' does not match any enabled nodes
[0m12:55:23.150995 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m12:55:23.152176 [debug] [MainThread]: Command end result
[0m12:55:23.196276 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m12:55:23.198674 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m12:55:23.202870 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m12:55:23.204142 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 13 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m12:55:23.206188 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": true, "command_wall_clock_time": 3.9633589, "process_in_blocks": "0", "process_kernel_time": 0.880042, "process_mem_max_rss": "377784", "process_out_blocks": "4864", "process_user_time": 4.900684}
[0m12:55:23.207564 [debug] [MainThread]: Command `dbt test` succeeded at 12:55:23.207339 after 3.96 seconds
[0m12:55:23.208099 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1625cc6780>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1605752780>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1605632c60>]}
[0m12:55:23.208913 [debug] [MainThread]: Flushing usage events
[0m12:55:24.238056 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:03:19.568957 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b941b781340>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b941bf48a70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b941b4c5280>]}


============================== 13:03:19.575072 | fe4613c2-59bc-43df-8dd8-9f02d994cdd8 ==============================
[0m13:03:19.575072 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m13:03:19.575735 [debug] [MainThread]: running dbt with arguments {'fail_fast': 'False', 'send_anonymous_usage_stats': 'True', 'no_print': 'None', 'use_experimental_parser': 'False', 'version_check': 'True', 'invocation_command': 'dbt clean', 'use_colors': 'True', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_cache_events': 'False', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'debug': 'False', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'profiles_dir': '/home/ecem/.dbt', 'empty': 'None', 'printer_width': '80', 'indirect_selection': 'eager', 'log_format': 'default', 'warn_error': 'None', 'partial_parse': 'True', 'cache_selected_only': 'False', 'write_json': 'True'}
[0m13:03:19.723256 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'fe4613c2-59bc-43df-8dd8-9f02d994cdd8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b941acfc140>]}
[0m13:03:19.743996 [debug] [MainThread]: Resource report: {"command_name": "clean", "command_success": true, "command_wall_clock_time": 0.2590214, "process_in_blocks": "0", "process_kernel_time": 0.376237, "process_mem_max_rss": "101680", "process_out_blocks": "8", "process_user_time": 1.31341}
[0m13:03:19.744606 [debug] [MainThread]: Command `dbt clean` succeeded at 13:03:19.744497 after 0.26 seconds
[0m13:03:19.745036 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b941abebcb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b941cb1c6b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b941b087080>]}
[0m13:03:19.745481 [debug] [MainThread]: Flushing usage events
[0m13:03:20.653121 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:03:54.844515 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x799b5685d2e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x799b55f971a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x799b59781ca0>]}


============================== 13:03:54.850784 | ee91cc15-7378-4598-9ecc-49be78a9e275 ==============================
[0m13:03:54.850784 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m13:03:54.852574 [debug] [MainThread]: running dbt with arguments {'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'indirect_selection': 'eager', 'target_path': 'None', 'printer_width': '80', 'empty': 'None', 'use_colors': 'True', 'warn_error': 'None', 'log_format': 'default', 'static_parser': 'True', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'introspect': 'True', 'quiet': 'False', 'invocation_command': 'dbt test --select int_facts_marts_schema', 'profiles_dir': '/home/ecem/.dbt', 'version_check': 'True', 'send_anonymous_usage_stats': 'True', 'debug': 'False', 'fail_fast': 'False', 'log_cache_events': 'False', 'use_experimental_parser': 'False', 'no_print': 'None'}
[0m13:03:57.177375 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ee91cc15-7378-4598-9ecc-49be78a9e275', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x799b36870e90>]}
[0m13:03:57.247761 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ee91cc15-7378-4598-9ecc-49be78a9e275', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x799b58672510>]}
[0m13:03:57.250916 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m13:03:57.636500 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m13:03:57.638436 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m13:03:57.639068 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'ee91cc15-7378-4598-9ecc-49be78a9e275', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x799b36849d00>]}
[0m13:03:59.024919 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `relationships` defined on 'stg_orders' in
package 'data_pipeline_project' (models/staging/staging_schema.yml). Arguments
to generic tests should be nested under the `arguments` property.
[0m13:03:59.026425 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': 'ee91cc15-7378-4598-9ecc-49be78a9e275', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x799b369b1730>]}
[0m13:03:59.355965 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.data_pipeline_project.relationships_dim_subscription_user_id__customer_id__ref_dim_customers_.35f0a793d3' (models/intermediate/int_facts_marts_schema.yml) depends on a node named 'dim_customers' in package '' which was not found
[0m13:03:59.501609 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ee91cc15-7378-4598-9ecc-49be78a9e275', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x799b3684a720>]}
[0m13:03:59.644712 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m13:03:59.647302 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m13:03:59.679503 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ee91cc15-7378-4598-9ecc-49be78a9e275', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x799b359ccb90>]}
[0m13:03:59.680324 [info ] [MainThread]: Found 21 models, 70 data tests, 10 sources, 508 macros
[0m13:03:59.680911 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ee91cc15-7378-4598-9ecc-49be78a9e275', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x799b358532c0>]}
[0m13:03:59.682126 [warn ] [MainThread]: The selection criterion 'int_facts_marts_schema' does not match any enabled nodes
[0m13:03:59.683922 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m13:03:59.685010 [debug] [MainThread]: Command end result
[0m13:03:59.733644 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m13:03:59.737332 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m13:03:59.742582 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m13:03:59.743417 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 30 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m13:03:59.746338 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": true, "command_wall_clock_time": 4.9980836, "process_in_blocks": "0", "process_kernel_time": 1.32402, "process_mem_max_rss": "379472", "process_out_blocks": "4768", "process_user_time": 5.643978}
[0m13:03:59.747215 [debug] [MainThread]: Command `dbt test` succeeded at 13:03:59.747099 after 5.00 seconds
[0m13:03:59.748589 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x799b567939e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x799b359cc110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x799b35a23560>]}
[0m13:03:59.750267 [debug] [MainThread]: Flushing usage events
[0m13:04:00.456907 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:58:33.960384 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c693bec1340>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c693c5513d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c693e0d6990>]}


============================== 13:58:33.965416 | f17d9030-7861-4348-a8e2-ba9004588d5a ==============================
[0m13:58:33.965416 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m13:58:33.969854 [debug] [MainThread]: running dbt with arguments {'log_cache_events': 'False', 'empty': 'None', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'profiles_dir': '/home/ecem/.dbt', 'use_experimental_parser': 'False', 'write_json': 'True', 'use_colors': 'True', 'debug': 'False', 'send_anonymous_usage_stats': 'True', 'partial_parse': 'True', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'fail_fast': 'False', 'invocation_command': 'dbt test --select int_facts_marts_schema', 'version_check': 'True', 'cache_selected_only': 'False', 'no_print': 'None', 'printer_width': '80', 'warn_error': 'None', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'quiet': 'False', 'indirect_selection': 'eager'}
[0m13:58:36.418956 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f17d9030-7861-4348-a8e2-ba9004588d5a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c691c13c410>]}
[0m13:58:36.486321 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f17d9030-7861-4348-a8e2-ba9004588d5a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c693b795a60>]}
[0m13:58:36.488522 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m13:58:36.743732 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m13:58:37.024588 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m13:58:37.026681 [debug] [MainThread]: Partial parsing: updated file: data_pipeline_project://models/intermediate/int_facts_marts_schema.yml
[0m13:58:37.388113 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `relationships` defined on 'dim_subscription'
in package 'data_pipeline_project'
(models/intermediate/int_facts_marts_schema.yml). Arguments to generic tests
should be nested under the `arguments` property.
[0m13:58:37.392450 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': 'f17d9030-7861-4348-a8e2-ba9004588d5a', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c691bb31a00>]}
[0m13:58:37.525030 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f17d9030-7861-4348-a8e2-ba9004588d5a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c691b2835c0>]}
[0m13:58:37.650368 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m13:58:37.652913 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m13:58:37.687689 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f17d9030-7861-4348-a8e2-ba9004588d5a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c691b033e00>]}
[0m13:58:37.688637 [info ] [MainThread]: Found 21 models, 71 data tests, 10 sources, 508 macros
[0m13:58:37.689470 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f17d9030-7861-4348-a8e2-ba9004588d5a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c691bd90410>]}
[0m13:58:37.690666 [warn ] [MainThread]: The selection criterion 'int_facts_marts_schema' does not match any enabled nodes
[0m13:58:37.692767 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m13:58:37.693907 [debug] [MainThread]: Command end result
[0m13:58:37.739453 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m13:58:37.742058 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m13:58:37.747283 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m13:58:37.748258 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m13:58:37.749727 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": true, "command_wall_clock_time": 3.8541121, "process_in_blocks": "0", "process_kernel_time": 0.847686, "process_mem_max_rss": "377872", "process_out_blocks": "4816", "process_user_time": 4.986816}
[0m13:58:37.750810 [debug] [MainThread]: Command `dbt test` succeeded at 13:58:37.750661 after 3.86 seconds
[0m13:58:37.751817 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c693e1ef080>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c691bb04fb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c691bc33e30>]}
[0m13:58:37.752473 [debug] [MainThread]: Flushing usage events
[0m13:58:38.665861 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:03:37.780304 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a427606b380>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a4276a42d50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a4276ead730>]}


============================== 14:03:37.788164 | ca0c86a3-8c41-4ee7-a66a-82205fcba2d0 ==============================
[0m14:03:37.788164 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m14:03:37.789084 [debug] [MainThread]: running dbt with arguments {'target_path': 'None', 'cache_selected_only': 'False', 'version_check': 'True', 'send_anonymous_usage_stats': 'True', 'printer_width': '80', 'debug': 'False', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'partial_parse': 'True', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'use_colors': 'True', 'indirect_selection': 'eager', 'empty': 'None', 'quiet': 'False', 'use_experimental_parser': 'False', 'warn_error': 'None', 'write_json': 'True', 'fail_fast': 'False', 'log_cache_events': 'False', 'log_format': 'default', 'introspect': 'True', 'profiles_dir': '/home/ecem/.dbt', 'invocation_command': 'dbt clean', 'no_print': 'None'}
[0m14:03:37.939269 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ca0c86a3-8c41-4ee7-a66a-82205fcba2d0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a4275fceae0>]}
[0m14:03:37.952734 [debug] [MainThread]: Resource report: {"command_name": "clean", "command_success": true, "command_wall_clock_time": 0.23620512, "process_in_blocks": "0", "process_kernel_time": 0.1843, "process_mem_max_rss": "101752", "process_out_blocks": "8", "process_user_time": 1.53233}
[0m14:03:37.953549 [debug] [MainThread]: Command `dbt clean` succeeded at 14:03:37.953445 after 0.24 seconds
[0m14:03:37.954143 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a427609f0b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a427606a180>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a4278b1d8e0>]}
[0m14:03:37.954613 [debug] [MainThread]: Flushing usage events
[0m14:03:38.941937 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:03:46.367888 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e5e91925460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e5e917c55b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e5e93a66c90>]}


============================== 14:03:46.370773 | eaee592d-6eb3-4049-a2ff-de3b72a20277 ==============================
[0m14:03:46.370773 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m14:03:46.372284 [debug] [MainThread]: running dbt with arguments {'invocation_command': 'dbt test --select int_facts_marts_schema', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'printer_width': '80', 'use_colors': 'True', 'empty': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'write_json': 'True', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'log_cache_events': 'False', 'quiet': 'False', 'warn_error': 'None', 'static_parser': 'True', 'debug': 'False', 'log_format': 'default', 'fail_fast': 'False', 'no_print': 'None', 'use_experimental_parser': 'False', 'introspect': 'True', 'target_path': 'None', 'profiles_dir': '/home/ecem/.dbt', 'version_check': 'True', 'indirect_selection': 'eager', 'cache_selected_only': 'False'}
[0m14:03:48.682768 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'eaee592d-6eb3-4049-a2ff-de3b72a20277', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e5e93813920>]}
[0m14:03:48.746056 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'eaee592d-6eb3-4049-a2ff-de3b72a20277', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e5e71c17800>]}
[0m14:03:48.747314 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m14:03:48.996013 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m14:03:48.997441 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m14:03:48.998133 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'eaee592d-6eb3-4049-a2ff-de3b72a20277', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e5e7183d280>]}
[0m14:03:50.367099 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `relationships` defined on 'stg_orders' in
package 'data_pipeline_project' (models/staging/staging_schema.yml). Arguments
to generic tests should be nested under the `arguments` property.
[0m14:03:50.367848 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': 'eaee592d-6eb3-4049-a2ff-de3b72a20277', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e5e70afc6e0>]}
[0m14:03:50.813314 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'eaee592d-6eb3-4049-a2ff-de3b72a20277', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e5e7098a510>]}
[0m14:03:50.947398 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m14:03:50.949927 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m14:03:50.983999 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'eaee592d-6eb3-4049-a2ff-de3b72a20277', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e5e7097d6d0>]}
[0m14:03:50.984930 [info ] [MainThread]: Found 21 models, 71 data tests, 10 sources, 508 macros
[0m14:03:50.985917 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'eaee592d-6eb3-4049-a2ff-de3b72a20277', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e5e70a76960>]}
[0m14:03:50.987589 [warn ] [MainThread]: The selection criterion 'int_facts_marts_schema' does not match any enabled nodes
[0m14:03:50.989968 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m14:03:50.991004 [debug] [MainThread]: Command end result
[0m14:03:51.032024 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m14:03:51.034349 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m14:03:51.037153 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m14:03:51.037782 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 27 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m14:03:51.040051 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": true, "command_wall_clock_time": 4.728809, "process_in_blocks": "0", "process_kernel_time": 1.057077, "process_mem_max_rss": "379392", "process_out_blocks": "4760", "process_user_time": 5.44363}
[0m14:03:51.040632 [debug] [MainThread]: Command `dbt test` succeeded at 14:03:51.040523 after 4.73 seconds
[0m14:03:51.041512 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e5e919bd940>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e5e91eb4b30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e5e91eb5b20>]}
[0m14:03:51.042244 [debug] [MainThread]: Flushing usage events
[0m14:03:51.771444 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:08:27.331960 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e757e6d4b00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e75805c9e50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e757e4d9040>]}


============================== 14:08:27.338602 | 7eb2d7c1-adf4-467a-9e8a-2eda530202f2 ==============================
[0m14:08:27.338602 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m14:08:27.344708 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'debug': 'False', 'empty': 'None', 'printer_width': '80', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'invocation_command': 'dbt ls --resource-type model --select intermediate', 'introspect': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'profiles_dir': '/home/ecem/.dbt', 'target_path': 'None', 'indirect_selection': 'eager', 'quiet': 'False', 'static_parser': 'True', 'log_format': 'default', 'warn_error': 'None', 'write_json': 'True', 'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'send_anonymous_usage_stats': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'no_print': 'None', 'cache_selected_only': 'False'}
[0m14:08:29.687215 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7eb2d7c1-adf4-467a-9e8a-2eda530202f2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e755e9d76e0>]}
[0m14:08:29.754807 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7eb2d7c1-adf4-467a-9e8a-2eda530202f2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e755e847ef0>]}
[0m14:08:29.756033 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m14:08:29.993775 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m14:08:30.269759 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:08:30.270806 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:08:30.316657 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7eb2d7c1-adf4-467a-9e8a-2eda530202f2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e755e7b62d0>]}
[0m14:08:30.425529 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m14:08:30.428172 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m14:08:30.490876 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7eb2d7c1-adf4-467a-9e8a-2eda530202f2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e755e17be00>]}
[0m14:08:30.492000 [info ] [MainThread]: Found 21 models, 71 data tests, 10 sources, 508 macros
[0m14:08:30.492568 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7eb2d7c1-adf4-467a-9e8a-2eda530202f2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e755e7ea480>]}
[0m14:08:30.493899 [info ] [MainThread]: data_pipeline_project.intermediate.int_orders_decomposed
[0m14:08:30.494547 [info ] [MainThread]: data_pipeline_project.intermediate.int_shipments_decomposed
[0m14:08:30.495889 [debug] [MainThread]: Resource report: {"command_name": "list", "command_success": true, "command_wall_clock_time": 3.219271, "process_in_blocks": "24", "process_kernel_time": 0.83663, "process_mem_max_rss": "370800", "process_out_blocks": "1840", "process_user_time": 4.15185}
[0m14:08:30.496848 [debug] [MainThread]: Command `dbt ls` succeeded at 14:08:30.496614 after 3.22 seconds
[0m14:08:30.497342 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e757e63f830>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e755ecbf1a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e755e0c3b00>]}
[0m14:08:30.498111 [debug] [MainThread]: Flushing usage events
[0m14:08:31.439579 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:15:01.842830 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x735835593380>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x735834afc4a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7358360d6570>]}


============================== 14:15:01.845947 | 05b769a5-f39d-47e7-860d-0c032e2b5d97 ==============================
[0m14:15:01.845947 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m14:15:01.856721 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'empty': 'None', 'no_print': 'None', 'target_path': 'None', 'indirect_selection': 'eager', 'profiles_dir': '/home/ecem/.dbt', 'quiet': 'False', 'cache_selected_only': 'False', 'invocation_command': 'dbt test', 'log_cache_events': 'False', 'send_anonymous_usage_stats': 'True', 'use_colors': 'True', 'version_check': 'True', 'warn_error': 'None', 'log_format': 'default', 'fail_fast': 'False', 'static_parser': 'True', 'partial_parse': 'True', 'debug': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'introspect': 'True', 'write_json': 'True', 'use_experimental_parser': 'False', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs'}
[0m14:15:04.247934 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '05b769a5-f39d-47e7-860d-0c032e2b5d97', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7358164521e0>]}
[0m14:15:04.309734 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '05b769a5-f39d-47e7-860d-0c032e2b5d97', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7358159b4170>]}
[0m14:15:04.310680 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m14:15:04.551057 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m14:15:04.835111 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:15:04.835617 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:15:04.894346 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '05b769a5-f39d-47e7-860d-0c032e2b5d97', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x735813fdd520>]}
[0m14:15:05.003140 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m14:15:05.005625 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m14:15:05.035116 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '05b769a5-f39d-47e7-860d-0c032e2b5d97', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x735813b109b0>]}
[0m14:15:05.036023 [info ] [MainThread]: Found 21 models, 71 data tests, 10 sources, 508 macros
[0m14:15:05.036807 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '05b769a5-f39d-47e7-860d-0c032e2b5d97', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x735814002630>]}
[0m14:15:05.041663 [info ] [MainThread]: 
[0m14:15:05.042841 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:15:05.043243 [info ] [MainThread]: 
[0m14:15:05.044218 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m14:15:05.061837 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_staging'
[0m14:15:05.062390 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:15:05.105977 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_marts'
[0m14:15:05.107092 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:15:06.285872 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '05b769a5-f39d-47e7-860d-0c032e2b5d97', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x735835eae2d0>]}
[0m14:15:06.286554 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:15:06.295922 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.accepted_range_fct_marketing_spend_spend_try__0.29ff739b28
[0m14:15:06.296699 [info ] [Thread-1 (]: 1 of 71 START test accepted_range_fct_marketing_spend_spend_try__0 ............. [RUN]
[0m14:15:06.297834 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_marts, now test.data_pipeline_project.accepted_range_fct_marketing_spend_spend_try__0.29ff739b28)
[0m14:15:06.298364 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.accepted_range_fct_marketing_spend_spend_try__0.29ff739b28
[0m14:15:06.305185 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.accepted_range_fct_order_discount_total__0.5e17f8aa8a
[0m14:15:06.310022 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.accepted_range_fct_order_net_revenue__0.49d8f40cb7
[0m14:15:06.317197 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0.494a0c0ebc
[0m14:15:06.307026 [info ] [Thread-2 (]: 2 of 71 START test accepted_range_fct_order_discount_total__0 .................. [RUN]
[0m14:15:06.331405 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now test.data_pipeline_project.accepted_range_fct_order_discount_total__0.5e17f8aa8a)
[0m14:15:06.332371 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.accepted_range_fct_order_discount_total__0.5e17f8aa8a
[0m14:15:06.314154 [info ] [Thread-3 (]: 3 of 71 START test accepted_range_fct_order_net_revenue__0 ..................... [RUN]
[0m14:15:06.338156 [debug] [Thread-3 (]: Acquiring new bigquery connection 'test.data_pipeline_project.accepted_range_fct_order_net_revenue__0.49d8f40cb7'
[0m14:15:06.317746 [info ] [Thread-4 (]: 4 of 71 START test accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0  [RUN]
[0m14:15:06.339899 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.accepted_range_fct_order_net_revenue__0.49d8f40cb7
[0m14:15:06.341776 [debug] [Thread-4 (]: Acquiring new bigquery connection 'test.data_pipeline_project.accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0.494a0c0ebc'
[0m14:15:06.347303 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0.494a0c0ebc
[0m14:15:06.361770 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:15:06.367272 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:15:06.368153 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m14:15:06.369755 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m14:15:06.402896 [debug] [Thread-2 (]: Compilation Error in test accepted_range_fct_order_discount_total__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:15:06.405459 [debug] [Thread-3 (]: Compilation Error in test accepted_range_fct_order_net_revenue__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:15:06.409223 [debug] [Thread-1 (]: Compilation Error in test accepted_range_fct_marketing_spend_spend_try__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:15:06.407879 [error] [Thread-2 (]: 2 of 71 ERROR accepted_range_fct_order_discount_total__0 ....................... [[31mERROR[0m in 0.08s]
[0m14:15:06.413441 [debug] [Thread-4 (]: Compilation Error in test accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:15:06.410168 [error] [Thread-3 (]: 3 of 71 ERROR accepted_range_fct_order_net_revenue__0 .......................... [[31mERROR[0m in 0.07s]
[0m14:15:06.420131 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.accepted_range_fct_order_net_revenue__0.49d8f40cb7
[0m14:15:06.417341 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.accepted_range_fct_order_discount_total__0.5e17f8aa8a
[0m14:15:06.414666 [error] [Thread-1 (]: 1 of 71 ERROR accepted_range_fct_marketing_spend_spend_try__0 .................. [[31mERROR[0m in 0.12s]
[0m14:15:06.425305 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.accepted_range_fct_marketing_spend_spend_try__0.29ff739b28
[0m14:15:06.422613 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.accepted_range_mart_kpis_latest_monthly_recurring_revenue__0.b1fd398ff0
[0m14:15:06.418557 [error] [Thread-4 (]: 4 of 71 ERROR accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0  [[31mERROR[0m in 0.08s]
[0m14:15:06.420774 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.accepted_range_mart_kpis_latest_cac_payback_months__0.b5860838e1
[0m14:15:06.426636 [debug] [Thread-7 (]: Marking all children of 'test.data_pipeline_project.accepted_range_fct_order_net_revenue__0.49d8f40cb7' to be skipped because of status 'error'.  Reason: Compilation Error in test accepted_range_fct_order_net_revenue__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps"..
[0m14:15:06.432031 [debug] [Thread-7 (]: Marking all children of 'test.data_pipeline_project.accepted_range_fct_order_discount_total__0.5e17f8aa8a' to be skipped because of status 'error'.  Reason: Compilation Error in test accepted_range_fct_order_discount_total__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps"..
[0m14:15:06.429071 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0.494a0c0ebc
[0m14:15:06.428240 [info ] [Thread-2 (]: 6 of 71 START test accepted_range_mart_kpis_latest_monthly_recurring_revenue__0  [RUN]
[0m14:15:06.426040 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.accepted_range_mart_revenue_daily_daily_revenue__0.12a5734f28
[0m14:15:06.430278 [info ] [Thread-3 (]: 5 of 71 START test accepted_range_mart_kpis_latest_cac_payback_months__0 ....... [RUN]
[0m14:15:06.439263 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.accepted_range_fct_order_net_revenue__0.49d8f40cb7, now test.data_pipeline_project.accepted_range_mart_kpis_latest_cac_payback_months__0.b5860838e1)
[0m14:15:06.440228 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.accepted_range_mart_kpis_latest_cac_payback_months__0.b5860838e1
[0m14:15:06.436381 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.accepted_range_fct_order_discount_total__0.5e17f8aa8a, now test.data_pipeline_project.accepted_range_mart_kpis_latest_monthly_recurring_revenue__0.b1fd398ff0)
[0m14:15:06.434352 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.accepted_range_mart_subscription_daily_active_subscribers__0.5d06041ada
[0m14:15:06.433415 [debug] [Thread-7 (]: Marking all children of 'test.data_pipeline_project.accepted_range_fct_marketing_spend_spend_try__0.29ff739b28' to be skipped because of status 'error'.  Reason: Compilation Error in test accepted_range_fct_marketing_spend_spend_try__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps"..
[0m14:15:06.450731 [debug] [Thread-7 (]: Marking all children of 'test.data_pipeline_project.accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0.494a0c0ebc' to be skipped because of status 'error'.  Reason: Compilation Error in test accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps"..
[0m14:15:06.449758 [info ] [Thread-4 (]: 8 of 71 START test accepted_range_mart_subscription_daily_active_subscribers__0  [RUN]
[0m14:15:06.448062 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.accepted_range_mart_kpis_latest_monthly_recurring_revenue__0.b1fd398ff0
[0m14:15:06.452797 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0.494a0c0ebc, now test.data_pipeline_project.accepted_range_mart_subscription_daily_active_subscribers__0.5d06041ada)
[0m14:15:06.438351 [info ] [Thread-1 (]: 7 of 71 START test accepted_range_mart_revenue_daily_daily_revenue__0 .......... [RUN]
[0m14:15:06.465962 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.accepted_range_fct_marketing_spend_spend_try__0.29ff739b28, now test.data_pipeline_project.accepted_range_mart_revenue_daily_daily_revenue__0.12a5734f28)
[0m14:15:06.466662 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.accepted_range_mart_revenue_daily_daily_revenue__0.12a5734f28
[0m14:15:06.446936 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:15:06.463783 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.accepted_range_mart_subscription_daily_active_subscribers__0.5d06041ada
[0m14:15:06.472117 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:15:06.481300 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:15:06.495624 [debug] [Thread-3 (]: Compilation Error in test accepted_range_mart_kpis_latest_cac_payback_months__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:15:06.498365 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:15:06.501037 [error] [Thread-3 (]: 5 of 71 ERROR accepted_range_mart_kpis_latest_cac_payback_months__0 ............ [[31mERROR[0m in 0.06s]
[0m14:15:06.521885 [debug] [Thread-2 (]: Compilation Error in test accepted_range_mart_kpis_latest_monthly_recurring_revenue__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:15:06.523942 [debug] [Thread-1 (]: Compilation Error in test accepted_range_mart_revenue_daily_daily_revenue__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:15:06.527702 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.accepted_range_mart_kpis_latest_cac_payback_months__0.b5860838e1
[0m14:15:06.530659 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.accepted_range_mart_subscription_daily_new_subscribers__0.f410e5c26f
[0m14:15:06.526857 [debug] [Thread-4 (]: Compilation Error in test accepted_range_mart_subscription_daily_active_subscribers__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:15:06.531133 [debug] [Thread-7 (]: Marking all children of 'test.data_pipeline_project.accepted_range_mart_kpis_latest_cac_payback_months__0.b5860838e1' to be skipped because of status 'error'.  Reason: Compilation Error in test accepted_range_mart_kpis_latest_cac_payback_months__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps"..
[0m14:15:06.528945 [error] [Thread-2 (]: 6 of 71 ERROR accepted_range_mart_kpis_latest_monthly_recurring_revenue__0 ..... [[31mERROR[0m in 0.09s]
[0m14:15:06.534299 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.accepted_range_mart_kpis_latest_monthly_recurring_revenue__0.b1fd398ff0
[0m14:15:06.529949 [error] [Thread-1 (]: 7 of 71 ERROR accepted_range_mart_revenue_daily_daily_revenue__0 ............... [[31mERROR[0m in 0.06s]
[0m14:15:06.531638 [info ] [Thread-3 (]: 9 of 71 START test accepted_range_mart_subscription_daily_new_subscribers__0 ... [RUN]
[0m14:15:06.543234 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.accepted_range_mart_kpis_latest_cac_payback_months__0.b5860838e1, now test.data_pipeline_project.accepted_range_mart_subscription_daily_new_subscribers__0.f410e5c26f)
[0m14:15:06.540952 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc
[0m14:15:06.541601 [debug] [Thread-7 (]: Marking all children of 'test.data_pipeline_project.accepted_range_mart_kpis_latest_monthly_recurring_revenue__0.b1fd398ff0' to be skipped because of status 'error'.  Reason: Compilation Error in test accepted_range_mart_kpis_latest_monthly_recurring_revenue__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps"..
[0m14:15:06.542486 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.accepted_range_mart_revenue_daily_daily_revenue__0.12a5734f28
[0m14:15:06.543728 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.accepted_range_mart_subscription_daily_new_subscribers__0.f410e5c26f
[0m14:15:06.532159 [error] [Thread-4 (]: 8 of 71 ERROR accepted_range_mart_subscription_daily_active_subscribers__0 ..... [[31mERROR[0m in 0.08s]
[0m14:15:06.546339 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723
[0m14:15:06.547249 [debug] [Thread-7 (]: Marking all children of 'test.data_pipeline_project.accepted_range_mart_revenue_daily_daily_revenue__0.12a5734f28' to be skipped because of status 'error'.  Reason: Compilation Error in test accepted_range_mart_revenue_daily_daily_revenue__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps"..
[0m14:15:06.544579 [info ] [Thread-2 (]: 10 of 71 START test not_null_dim_customer_customer_id .......................... [RUN]
[0m14:15:06.554997 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.accepted_range_mart_subscription_daily_active_subscribers__0.5d06041ada
[0m14:15:06.561928 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:15:06.563470 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.accepted_range_mart_kpis_latest_monthly_recurring_revenue__0.b1fd398ff0, now test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc)
[0m14:15:06.568651 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6
[0m14:15:06.556295 [info ] [Thread-1 (]: 11 of 71 START test not_null_dim_subscription_customer_id ...................... [RUN]
[0m14:15:06.574012 [debug] [Thread-7 (]: Marking all children of 'test.data_pipeline_project.accepted_range_mart_subscription_daily_active_subscribers__0.5d06041ada' to be skipped because of status 'error'.  Reason: Compilation Error in test accepted_range_mart_subscription_daily_active_subscribers__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps"..
[0m14:15:06.583523 [info ] [Thread-4 (]: 12 of 71 START test not_null_dim_subscription_subscription_id .................. [RUN]
[0m14:15:06.582868 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc
[0m14:15:06.584472 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.accepted_range_mart_revenue_daily_daily_revenue__0.12a5734f28, now test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723)
[0m14:15:06.580700 [debug] [Thread-3 (]: Compilation Error in test accepted_range_mart_subscription_daily_new_subscribers__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:15:06.587191 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.accepted_range_mart_subscription_daily_active_subscribers__0.5d06041ada, now test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6)
[0m14:15:06.613832 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6
[0m14:15:06.607387 [error] [Thread-3 (]: 9 of 71 ERROR accepted_range_mart_subscription_daily_new_subscribers__0 ........ [[31mERROR[0m in 0.06s]
[0m14:15:06.612471 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc"
[0m14:15:06.596829 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723
[0m14:15:06.618180 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6"
[0m14:15:06.621631 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.accepted_range_mart_subscription_daily_new_subscribers__0.f410e5c26f
[0m14:15:06.630448 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc
[0m14:15:06.638473 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719
[0m14:15:06.676274 [info ] [Thread-3 (]: 13 of 71 START test not_null_fct_marketing_spend_date .......................... [RUN]
[0m14:15:06.641720 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723"
[0m14:15:06.649317 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6
[0m14:15:06.639419 [debug] [Thread-7 (]: Marking all children of 'test.data_pipeline_project.accepted_range_mart_subscription_daily_new_subscribers__0.f410e5c26f' to be skipped because of status 'error'.  Reason: Compilation Error in test accepted_range_mart_subscription_daily_new_subscribers__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps"..
[0m14:15:06.678269 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.accepted_range_mart_subscription_daily_new_subscribers__0.f410e5c26f, now test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719)
[0m14:15:06.709935 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723
[0m14:15:06.717748 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc"
[0m14:15:06.732988 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719
[0m14:15:06.736439 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6"
[0m14:15:06.745604 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723"
[0m14:15:06.770831 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719"
[0m14:15:06.783713 [debug] [Thread-4 (]: On test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select subscription_id
from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_subscription`
where subscription_id is null



  
  
      
    ) dbt_internal_test
[0m14:15:06.776566 [debug] [Thread-2 (]: On test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select customer_id
from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_customer`
where customer_id is null



  
  
      
    ) dbt_internal_test
[0m14:15:06.801036 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:15:06.791537 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719
[0m14:15:06.799201 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:15:06.789544 [debug] [Thread-1 (]: On test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select customer_id
from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_subscription`
where customer_id is null



  
  
      
    ) dbt_internal_test
[0m14:15:06.899260 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719"
[0m14:15:06.959766 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:15:06.976020 [debug] [Thread-3 (]: On test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select date
from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_marketing_spend`
where date is null



  
  
      
    ) dbt_internal_test
[0m14:15:07.019800 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:15:08.198193 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:ae52d334-6554-48ce-a782-6577e662693e&page=queryresults
[0m14:15:08.459604 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:db67a4ba-3ad2-431a-b7a1-f0a53e222ce7&page=queryresults
[0m14:15:08.562229 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:7c444944-c5fb-4c88-9cd2-4b38915233d6&page=queryresults
[0m14:15:08.649937 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:b2bbd007-a7de-460c-ac07-8db713d1fec9&page=queryresults
[0m14:15:09.394045 [info ] [Thread-2 (]: 10 of 71 PASS not_null_dim_customer_customer_id ................................ [[32mPASS[0m in 2.83s]
[0m14:15:09.397085 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc
[0m14:15:09.398291 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d
[0m14:15:09.399412 [info ] [Thread-2 (]: 14 of 71 START test not_null_fct_marketing_spend_spend_try ..................... [RUN]
[0m14:15:09.401519 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc, now test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d)
[0m14:15:09.403369 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d
[0m14:15:09.412406 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d"
[0m14:15:09.414356 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d
[0m14:15:09.426277 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d"
[0m14:15:09.429430 [info ] [Thread-1 (]: 11 of 71 PASS not_null_dim_subscription_customer_id ............................ [[32mPASS[0m in 2.84s]
[0m14:15:09.432034 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723
[0m14:15:09.432715 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff
[0m14:15:09.433489 [info ] [Thread-1 (]: 15 of 71 START test not_null_fct_order_customer_id ............................. [RUN]
[0m14:15:09.434914 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723, now test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff)
[0m14:15:09.435579 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff
[0m14:15:09.437220 [debug] [Thread-2 (]: On test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select spend_try
from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_marketing_spend`
where spend_try is null



  
  
      
    ) dbt_internal_test
[0m14:15:09.441617 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff"
[0m14:15:09.443238 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:15:09.496715 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff
[0m14:15:09.510469 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff"
[0m14:15:09.564133 [debug] [Thread-1 (]: On test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select customer_id
from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_order`
where customer_id is null



  
  
      
    ) dbt_internal_test
[0m14:15:09.565141 [info ] [Thread-3 (]: 13 of 71 PASS not_null_fct_marketing_spend_date ................................ [[32mPASS[0m in 2.86s]
[0m14:15:09.593776 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719
[0m14:15:09.609433 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:15:09.597365 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea
[0m14:15:09.647601 [info ] [Thread-3 (]: 16 of 71 START test not_null_fct_order_discount_total .......................... [RUN]
[0m14:15:09.716565 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719, now test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea)
[0m14:15:09.723483 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea
[0m14:15:09.721502 [info ] [Thread-4 (]: 12 of 71 PASS not_null_dim_subscription_subscription_id ........................ [[32mPASS[0m in 3.13s]
[0m14:15:09.734597 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6
[0m14:15:09.735464 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e
[0m14:15:09.740770 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea"
[0m14:15:09.743450 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea
[0m14:15:09.747331 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea"
[0m14:15:09.742473 [info ] [Thread-4 (]: 17 of 71 START test not_null_fct_order_net_revenue ............................. [RUN]
[0m14:15:09.749441 [debug] [Thread-3 (]: On test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select discount_total
from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_order`
where discount_total is null



  
  
      
    ) dbt_internal_test
[0m14:15:09.754736 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6, now test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e)
[0m14:15:09.756246 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:15:09.757272 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e
[0m14:15:09.853819 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e"
[0m14:15:09.857722 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e
[0m14:15:09.866060 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e"
[0m14:15:09.867167 [debug] [Thread-4 (]: On test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select net_revenue
from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_order`
where net_revenue is null



  
  
      
    ) dbt_internal_test
[0m14:15:09.872023 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:15:10.997821 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:02ac7385-2023-4ea6-8b36-663e5a7c3516&page=queryresults
[0m14:15:11.020420 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:c1fafbc4-ad0e-479d-b675-02fc1ffbcc1f&page=queryresults
[0m14:15:11.108467 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:970abc06-e962-4cce-9b34-e3be5cdda6dd&page=queryresults
[0m14:15:11.269878 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:dce0c76d-d8b3-4376-8f92-e707452b42dd&page=queryresults
[0m14:15:11.881293 [info ] [Thread-1 (]: 15 of 71 PASS not_null_fct_order_customer_id ................................... [[32mPASS[0m in 2.44s]
[0m14:15:11.888197 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff
[0m14:15:11.891188 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def
[0m14:15:11.894220 [info ] [Thread-1 (]: 18 of 71 START test not_null_fct_order_order_id ................................ [RUN]
[0m14:15:11.897136 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff, now test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def)
[0m14:15:11.899240 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def
[0m14:15:11.910106 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def"
[0m14:15:11.919293 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def
[0m14:15:11.929929 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def"
[0m14:15:11.932141 [debug] [Thread-1 (]: On test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select order_id
from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_order`
where order_id is null



  
  
      
    ) dbt_internal_test
[0m14:15:11.933096 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:15:11.989673 [info ] [Thread-2 (]: 14 of 71 PASS not_null_fct_marketing_spend_spend_try ........................... [[32mPASS[0m in 2.58s]
[0m14:15:12.011293 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d
[0m14:15:12.015907 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_fct_shipment_order_id.8b34a6bb78
[0m14:15:12.020100 [info ] [Thread-2 (]: 19 of 71 START test not_null_fct_shipment_order_id ............................. [RUN]
[0m14:15:12.038337 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d, now test.data_pipeline_project.not_null_fct_shipment_order_id.8b34a6bb78)
[0m14:15:12.048494 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_fct_shipment_order_id.8b34a6bb78
[0m14:15:12.159657 [info ] [Thread-4 (]: 17 of 71 PASS not_null_fct_order_net_revenue ................................... [[32mPASS[0m in 2.40s]
[0m14:15:12.165207 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_fct_shipment_order_id.8b34a6bb78"
[0m14:15:12.170535 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e
[0m14:15:12.171850 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_fct_shipment_order_id.8b34a6bb78
[0m14:15:12.177180 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d
[0m14:15:12.186378 [info ] [Thread-4 (]: 20 of 71 START test not_null_fct_shipment_shipment_id .......................... [RUN]
[0m14:15:12.188534 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e, now test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d)
[0m14:15:12.188991 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d
[0m14:15:12.185117 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_fct_shipment_order_id.8b34a6bb78"
[0m14:15:12.198149 [debug] [Thread-2 (]: On test.data_pipeline_project.not_null_fct_shipment_order_id.8b34a6bb78: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_fct_shipment_order_id.8b34a6bb78"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select order_id
from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_shipment`
where order_id is null



  
  
      
    ) dbt_internal_test
[0m14:15:12.202331 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:15:12.289277 [info ] [Thread-3 (]: 16 of 71 PASS not_null_fct_order_discount_total ................................ [[32mPASS[0m in 2.57s]
[0m14:15:12.292677 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea
[0m14:15:12.300562 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d"
[0m14:15:12.302346 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b
[0m14:15:12.304440 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d
[0m14:15:12.306199 [info ] [Thread-3 (]: 21 of 71 START test not_null_int_shipments_decomposed_delivered_at ............. [RUN]
[0m14:15:12.319481 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d"
[0m14:15:12.322860 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea, now test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b)
[0m14:15:12.325156 [debug] [Thread-4 (]: On test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select shipment_id
from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_shipment`
where shipment_id is null



  
  
      
    ) dbt_internal_test
[0m14:15:12.327332 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b
[0m14:15:12.331279 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:15:12.349291 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b"
[0m14:15:12.407284 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b
[0m14:15:12.439202 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b"
[0m14:15:12.440816 [debug] [Thread-3 (]: On test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select delivered_at
from `data-pipeline-project-474812`.`analytics_staging_staging`.`int_shipments_decomposed`
where delivered_at is null



  
  
      
    ) dbt_internal_test
[0m14:15:12.442357 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:15:13.419701 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:6921c9f7-8587-4ca6-8d6b-87f9d57240ce&page=queryresults
[0m14:15:13.743421 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:dd3286bb-6d8c-4dc8-81a5-307dce6f20c2&page=queryresults
[0m14:15:13.999124 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:e34e39e2-541e-4661-81e1-840394f61bc2&page=queryresults
[0m14:15:14.212718 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:a6ecaaeb-1e4b-4ddd-8d1f-e1be4699712e&page=queryresults
[0m14:15:14.517726 [info ] [Thread-1 (]: 18 of 71 PASS not_null_fct_order_order_id ...................................... [[32mPASS[0m in 2.62s]
[0m14:15:14.519140 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def
[0m14:15:14.520156 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31
[0m14:15:14.520966 [info ] [Thread-1 (]: 22 of 71 START test not_null_int_shipments_decomposed_latest_status ............ [RUN]
[0m14:15:14.521998 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def, now test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31)
[0m14:15:14.522578 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31
[0m14:15:14.531409 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31"
[0m14:15:14.534243 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31
[0m14:15:14.540465 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31"
[0m14:15:14.551620 [debug] [Thread-1 (]: On test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select latest_status
from `data-pipeline-project-474812`.`analytics_staging_staging`.`int_shipments_decomposed`
where latest_status is null



  
  
      
    ) dbt_internal_test
[0m14:15:14.552851 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:15:14.752857 [info ] [Thread-4 (]: 20 of 71 PASS not_null_fct_shipment_shipment_id ................................ [[32mPASS[0m in 2.55s]
[0m14:15:14.773627 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d
[0m14:15:14.779287 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b
[0m14:15:14.781499 [info ] [Thread-4 (]: 23 of 71 START test not_null_mart_acquisition_efficiency_date .................. [RUN]
[0m14:15:14.785229 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d, now test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b)
[0m14:15:14.786074 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b
[0m14:15:14.795424 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b"
[0m14:15:14.800362 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b
[0m14:15:14.805270 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b"
[0m14:15:14.812613 [debug] [Thread-4 (]: On test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select date
from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_acquisition_efficiency`
where date is null



  
  
      
    ) dbt_internal_test
[0m14:15:14.818214 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:15:14.952427 [warn ] [Thread-3 (]: 21 of 71 WARN 303 not_null_int_shipments_decomposed_delivered_at ............... [[33mWARN 303[0m in 2.63s]
[0m14:15:14.954571 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b
[0m14:15:14.956017 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23
[0m14:15:14.958087 [info ] [Thread-3 (]: 24 of 71 START test not_null_mart_revenue_daily_date ........................... [RUN]
[0m14:15:14.959979 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b, now test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23)
[0m14:15:14.960736 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23
[0m14:15:14.993321 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23"
[0m14:15:15.002944 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23
[0m14:15:15.011621 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23"
[0m14:15:15.018077 [debug] [Thread-3 (]: On test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select date
from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_revenue_daily`
where date is null



  
  
      
    ) dbt_internal_test
[0m14:15:15.023048 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:15:15.192416 [info ] [Thread-2 (]: 19 of 71 PASS not_null_fct_shipment_order_id ................................... [[32mPASS[0m in 3.16s]
[0m14:15:15.194239 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_fct_shipment_order_id.8b34a6bb78
[0m14:15:15.197287 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322
[0m14:15:15.199734 [info ] [Thread-2 (]: 25 of 71 START test not_null_mart_subscription_daily_date ...................... [RUN]
[0m14:15:15.202619 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_fct_shipment_order_id.8b34a6bb78, now test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322)
[0m14:15:15.207722 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322
[0m14:15:15.217314 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322"
[0m14:15:15.230525 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322
[0m14:15:15.234185 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322"
[0m14:15:15.235926 [debug] [Thread-2 (]: On test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select date
from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily`
where date is null



  
  
      
    ) dbt_internal_test
[0m14:15:15.236966 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:15:16.036257 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:c4fa66f5-8507-40b6-bb59-54fb60274a61&page=queryresults
[0m14:15:16.301536 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:ceb5a6e2-153b-45df-abb1-d2e89d801e56&page=queryresults
[0m14:15:16.390063 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:b5de9a3c-0202-4bfb-a515-1be4a4d747f4&page=queryresults
[0m14:15:16.467283 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:1b3acb60-ebf4-4713-9c53-0f9df526b878&page=queryresults
[0m14:15:16.917180 [info ] [Thread-1 (]: 22 of 71 PASS not_null_int_shipments_decomposed_latest_status .................. [[32mPASS[0m in 2.39s]
[0m14:15:16.919086 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31
[0m14:15:16.919787 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501
[0m14:15:16.920850 [info ] [Thread-1 (]: 26 of 71 START test not_null_stg_addresses_address_id .......................... [RUN]
[0m14:15:16.921740 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31, now test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501)
[0m14:15:16.922528 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501
[0m14:15:16.928149 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501"
[0m14:15:16.929380 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501
[0m14:15:16.941318 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501"
[0m14:15:16.944604 [debug] [Thread-1 (]: On test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select address_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_addresses`
where address_id is null



  
  
      
    ) dbt_internal_test
[0m14:15:16.945364 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:15:17.261516 [info ] [Thread-4 (]: 23 of 71 PASS not_null_mart_acquisition_efficiency_date ........................ [[32mPASS[0m in 2.48s]
[0m14:15:17.273178 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b
[0m14:15:17.275959 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3
[0m14:15:17.277206 [info ] [Thread-4 (]: 27 of 71 START test not_null_stg_addresses_user_id ............................. [RUN]
[0m14:15:17.279215 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b, now test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3)
[0m14:15:17.280435 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3
[0m14:15:17.287376 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3"
[0m14:15:17.289628 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3
[0m14:15:17.294259 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3"
[0m14:15:17.295311 [debug] [Thread-4 (]: On test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_addresses`
where user_id is null



  
  
      
    ) dbt_internal_test
[0m14:15:17.296403 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:15:17.352035 [info ] [Thread-3 (]: 24 of 71 PASS not_null_mart_revenue_daily_date ................................. [[32mPASS[0m in 2.39s]
[0m14:15:17.353519 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23
[0m14:15:17.354804 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2
[0m14:15:17.355444 [info ] [Thread-3 (]: 28 of 71 START test not_null_stg_countries_country_id .......................... [RUN]
[0m14:15:17.358920 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23, now test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2)
[0m14:15:17.360367 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2
[0m14:15:17.385592 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2"
[0m14:15:17.393499 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2
[0m14:15:17.407413 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2"
[0m14:15:17.411512 [debug] [Thread-3 (]: On test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select country_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_countries`
where country_id is null



  
  
      
    ) dbt_internal_test
[0m14:15:17.413415 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:15:17.524893 [info ] [Thread-2 (]: 25 of 71 PASS not_null_mart_subscription_daily_date ............................ [[32mPASS[0m in 2.31s]
[0m14:15:17.528380 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322
[0m14:15:17.529581 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64
[0m14:15:17.539225 [info ] [Thread-2 (]: 29 of 71 START test not_null_stg_orders_order_id ............................... [RUN]
[0m14:15:17.540438 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322, now test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64)
[0m14:15:17.541439 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64
[0m14:15:17.549738 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64"
[0m14:15:17.561392 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64
[0m14:15:17.579854 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64"
[0m14:15:17.584284 [debug] [Thread-2 (]: On test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select order_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_orders`
where order_id is null



  
  
      
    ) dbt_internal_test
[0m14:15:17.585072 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:15:18.721455 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:f04efaa8-6417-45c4-a58c-41a188e06d98&page=queryresults
[0m14:15:18.976128 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:20c5b723-823f-488d-b329-3cf99288b7f0&page=queryresults
[0m14:15:19.067206 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:ba43a673-229b-4005-b1aa-f7479490b6a2&page=queryresults
[0m14:15:19.247762 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:aaeb97cc-d377-4349-813b-de693f7d407b&page=queryresults
[0m14:15:19.622185 [info ] [Thread-1 (]: 26 of 71 PASS not_null_stg_addresses_address_id ................................ [[32mPASS[0m in 2.70s]
[0m14:15:19.624755 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501
[0m14:15:19.625957 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209
[0m14:15:19.626895 [info ] [Thread-1 (]: 30 of 71 START test not_null_stg_orders_user_id ................................ [RUN]
[0m14:15:19.631412 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501, now test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209)
[0m14:15:19.632451 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209
[0m14:15:19.650212 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209"
[0m14:15:19.651471 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209
[0m14:15:19.659473 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209"
[0m14:15:19.661208 [debug] [Thread-1 (]: On test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_orders`
where user_id is null



  
  
      
    ) dbt_internal_test
[0m14:15:19.662216 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:15:19.932738 [info ] [Thread-4 (]: 27 of 71 PASS not_null_stg_addresses_user_id ................................... [[32mPASS[0m in 2.65s]
[0m14:15:19.936247 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3
[0m14:15:19.937456 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380
[0m14:15:19.938298 [info ] [Thread-4 (]: 31 of 71 START test not_null_stg_shipments_shipment_id ......................... [RUN]
[0m14:15:19.939483 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3, now test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380)
[0m14:15:19.942044 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380
[0m14:15:19.952464 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380"
[0m14:15:19.954152 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380
[0m14:15:19.958339 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380"
[0m14:15:19.959923 [debug] [Thread-4 (]: On test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select shipment_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_shipments`
where shipment_id is null



  
  
      
    ) dbt_internal_test
[0m14:15:19.961000 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:15:20.024167 [info ] [Thread-2 (]: 29 of 71 PASS not_null_stg_orders_order_id ..................................... [[32mPASS[0m in 2.48s]
[0m14:15:20.026538 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64
[0m14:15:20.027544 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852
[0m14:15:20.028623 [info ] [Thread-2 (]: 32 of 71 START test not_null_stg_shipments_user_id ............................. [RUN]
[0m14:15:20.034528 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64, now test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852)
[0m14:15:20.037399 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852
[0m14:15:20.054382 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852"
[0m14:15:20.055901 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852
[0m14:15:20.065204 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852"
[0m14:15:20.067244 [debug] [Thread-2 (]: On test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_shipments`
where user_id is null



  
  
      
    ) dbt_internal_test
[0m14:15:20.073946 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:15:20.414754 [info ] [Thread-3 (]: 28 of 71 PASS not_null_stg_countries_country_id ................................ [[32mPASS[0m in 3.05s]
[0m14:15:20.421657 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2
[0m14:15:20.423495 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8
[0m14:15:20.426613 [info ] [Thread-3 (]: 33 of 71 START test not_null_stg_subscriptions_start_date ...................... [RUN]
[0m14:15:20.428099 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2, now test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8)
[0m14:15:20.428687 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8
[0m14:15:20.434103 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8"
[0m14:15:20.436352 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8
[0m14:15:20.442013 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8"
[0m14:15:20.446232 [debug] [Thread-3 (]: On test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select start_date
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_subscriptions`
where start_date is null



  
  
      
    ) dbt_internal_test
[0m14:15:20.451462 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:15:21.320373 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:15e37991-4e5c-47bf-b0fe-54b671c0fd8b&page=queryresults
[0m14:15:21.452380 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:e1504e0a-52ff-43a1-b887-aabe6f30c478&page=queryresults
[0m14:15:21.716660 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:1dbcd5bd-9bda-4321-9f92-dd392fd7dea7&page=queryresults
[0m14:15:21.896903 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:455c1435-12ea-4d4a-959c-2f8cdff63a26&page=queryresults
[0m14:15:22.179848 [info ] [Thread-1 (]: 30 of 71 PASS not_null_stg_orders_user_id ...................................... [[32mPASS[0m in 2.55s]
[0m14:15:22.182441 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209
[0m14:15:22.184057 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m14:15:22.185484 [info ] [Thread-1 (]: 34 of 71 START test not_null_stg_subscriptions_subscription_id ................. [RUN]
[0m14:15:22.186731 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209, now test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91)
[0m14:15:22.187681 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m14:15:22.194181 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91"
[0m14:15:22.195992 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m14:15:22.201217 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91"
[0m14:15:22.208162 [debug] [Thread-1 (]: On test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select subscription_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_subscriptions`
where subscription_id is null



  
  
      
    ) dbt_internal_test
[0m14:15:22.211598 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:15:22.502222 [info ] [Thread-4 (]: 31 of 71 PASS not_null_stg_shipments_shipment_id ............................... [[32mPASS[0m in 2.56s]
[0m14:15:22.504185 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380
[0m14:15:22.505951 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2
[0m14:15:22.506704 [info ] [Thread-4 (]: 35 of 71 START test not_null_stg_subscriptions_user_id ......................... [RUN]
[0m14:15:22.508286 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380, now test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2)
[0m14:15:22.511084 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2
[0m14:15:22.522943 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2"
[0m14:15:22.525439 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2
[0m14:15:22.534193 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2"
[0m14:15:22.535279 [debug] [Thread-4 (]: On test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_subscriptions`
where user_id is null



  
  
      
    ) dbt_internal_test
[0m14:15:22.535935 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:15:22.618800 [info ] [Thread-2 (]: 32 of 71 PASS not_null_stg_shipments_user_id ................................... [[32mPASS[0m in 2.58s]
[0m14:15:22.620480 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852
[0m14:15:22.621459 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25
[0m14:15:22.623520 [info ] [Thread-2 (]: 36 of 71 START test not_null_stg_users_created_at .............................. [RUN]
[0m14:15:22.624983 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852, now test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25)
[0m14:15:22.626240 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25
[0m14:15:22.643023 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25"
[0m14:15:22.648948 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25
[0m14:15:22.665497 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25"
[0m14:15:22.668007 [debug] [Thread-2 (]: On test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select created_at
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
where created_at is null



  
  
      
    ) dbt_internal_test
[0m14:15:22.668614 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:15:22.797011 [info ] [Thread-3 (]: 33 of 71 PASS not_null_stg_subscriptions_start_date ............................ [[32mPASS[0m in 2.37s]
[0m14:15:22.800925 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8
[0m14:15:22.804409 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77
[0m14:15:22.806201 [info ] [Thread-3 (]: 37 of 71 START test not_null_stg_users_user_id ................................. [RUN]
[0m14:15:22.807141 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8, now test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77)
[0m14:15:22.807603 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77
[0m14:15:22.813675 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77"
[0m14:15:22.823329 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77
[0m14:15:22.834677 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77"
[0m14:15:22.837467 [debug] [Thread-3 (]: On test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
where user_id is null



  
  
      
    ) dbt_internal_test
[0m14:15:22.838281 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:15:23.911965 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:56ed34d5-841e-4f22-b015-fe6ef112d07d&page=queryresults
[0m14:15:24.155412 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:939325de-23d8-4b62-9322-31ae9fab32ff&page=queryresults
[0m14:15:24.175763 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:51b62eb7-2822-4a49-b82f-15a951b9da78&page=queryresults
[0m14:15:24.511651 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:97f11c62-57a1-4742-94f2-813a893a558b&page=queryresults
[0m14:15:24.814170 [info ] [Thread-1 (]: 34 of 71 PASS not_null_stg_subscriptions_subscription_id ....................... [[32mPASS[0m in 2.63s]
[0m14:15:24.819346 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m14:15:24.823472 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29
[0m14:15:24.827285 [info ] [Thread-1 (]: 38 of 71 START test relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_  [RUN]
[0m14:15:24.828727 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91, now test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29)
[0m14:15:24.830512 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29
[0m14:15:24.873295 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29"
[0m14:15:24.875177 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29
[0m14:15:24.879218 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29"
[0m14:15:24.884809 [debug] [Thread-1 (]: On test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select customer_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_subscription`
    where customer_id is not null
),

parent as (
    select customer_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_customer`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m14:15:24.886100 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:15:25.051699 [info ] [Thread-4 (]: 35 of 71 PASS not_null_stg_subscriptions_user_id ............................... [[32mPASS[0m in 2.54s]
[0m14:15:25.054105 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2
[0m14:15:25.056747 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85
[0m14:15:25.064618 [info ] [Thread-4 (]: 39 of 71 START test relationships_fct_order_customer_id__customer_id__ref_dim_customer_  [RUN]
[0m14:15:25.072241 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2, now test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85)
[0m14:15:25.075776 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85
[0m14:15:25.100086 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85"
[0m14:15:25.108225 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85
[0m14:15:25.126134 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85"
[0m14:15:25.131312 [debug] [Thread-4 (]: On test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select customer_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_order`
    where customer_id is not null
),

parent as (
    select customer_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_customer`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m14:15:25.132445 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:15:25.216533 [info ] [Thread-2 (]: 36 of 71 PASS not_null_stg_users_created_at .................................... [[32mPASS[0m in 2.59s]
[0m14:15:25.220876 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25
[0m14:15:25.222130 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.relationships_fct_shipment_order_id__order_id__ref_fct_order_.6096f95e39
[0m14:15:25.223558 [info ] [Thread-2 (]: 40 of 71 START test relationships_fct_shipment_order_id__order_id__ref_fct_order_  [RUN]
[0m14:15:25.225030 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25, now test.data_pipeline_project.relationships_fct_shipment_order_id__order_id__ref_fct_order_.6096f95e39)
[0m14:15:25.225783 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.relationships_fct_shipment_order_id__order_id__ref_fct_order_.6096f95e39
[0m14:15:25.238840 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_fct_shipment_order_id__order_id__ref_fct_order_.6096f95e39"
[0m14:15:25.240144 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.relationships_fct_shipment_order_id__order_id__ref_fct_order_.6096f95e39
[0m14:15:25.246076 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_fct_shipment_order_id__order_id__ref_fct_order_.6096f95e39"
[0m14:15:25.250060 [debug] [Thread-2 (]: On test.data_pipeline_project.relationships_fct_shipment_order_id__order_id__ref_fct_order_.6096f95e39: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_fct_shipment_order_id__order_id__ref_fct_order_.6096f95e39"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select order_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_shipment`
    where order_id is not null
),

parent as (
    select order_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_order`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m14:15:25.251196 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:15:25.403515 [info ] [Thread-3 (]: 37 of 71 PASS not_null_stg_users_user_id ....................................... [[32mPASS[0m in 2.60s]
[0m14:15:25.405719 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77
[0m14:15:25.406759 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781
[0m14:15:25.408293 [info ] [Thread-3 (]: 41 of 71 START test relationships_stg_addresses_user_id__user_id__ref_stg_users_  [RUN]
[0m14:15:25.410677 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77, now test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781)
[0m14:15:25.411481 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781
[0m14:15:25.422091 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781"
[0m14:15:25.423174 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781
[0m14:15:25.445243 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781"
[0m14:15:25.448709 [debug] [Thread-3 (]: On test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select user_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_addresses`
    where user_id is not null
),

parent as (
    select user_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m14:15:25.451378 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:15:26.331735 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:2e233912-e773-4bbd-96a0-d37d909d586b&page=queryresults
[0m14:15:26.619122 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:631382f5-07db-4335-a873-bbf1ca82a459&page=queryresults
[0m14:15:26.727820 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:b4420de3-434f-4bd2-8507-df7d0c9fe648&page=queryresults
[0m14:15:26.959173 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:3e3effb2-5928-484f-a859-298d50ced023&page=queryresults
[0m14:15:27.352580 [info ] [Thread-1 (]: 38 of 71 PASS relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_  [[32mPASS[0m in 2.52s]
[0m14:15:27.354302 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29
[0m14:15:27.355263 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737
[0m14:15:27.356216 [info ] [Thread-1 (]: 42 of 71 START test relationships_stg_orders_user_id__user_id__ref_stg_users_ .. [RUN]
[0m14:15:27.357162 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29, now test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737)
[0m14:15:27.358122 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737
[0m14:15:27.367441 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737"
[0m14:15:27.368849 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737
[0m14:15:27.372870 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737"
[0m14:15:27.374132 [debug] [Thread-1 (]: On test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select user_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_orders`
    where user_id is not null
),

parent as (
    select user_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m14:15:27.375058 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:15:27.580916 [info ] [Thread-4 (]: 39 of 71 PASS relationships_fct_order_customer_id__customer_id__ref_dim_customer_  [[32mPASS[0m in 2.50s]
[0m14:15:27.585189 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85
[0m14:15:27.588290 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8
[0m14:15:27.590928 [info ] [Thread-4 (]: 43 of 71 START test relationships_stg_shipments_user_id__user_id__ref_stg_users_  [RUN]
[0m14:15:27.601375 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85, now test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8)
[0m14:15:27.603330 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8
[0m14:15:27.612039 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8"
[0m14:15:27.613096 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8
[0m14:15:27.618756 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8"
[0m14:15:27.620566 [debug] [Thread-4 (]: On test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select user_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_shipments`
    where user_id is not null
),

parent as (
    select user_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m14:15:27.621424 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:15:27.670927 [error] [Thread-2 (]: 40 of 71 FAIL 476 relationships_fct_shipment_order_id__order_id__ref_fct_order_  [[31mFAIL 476[0m in 2.45s]
[0m14:15:27.676464 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.relationships_fct_shipment_order_id__order_id__ref_fct_order_.6096f95e39
[0m14:15:27.679838 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m14:15:27.682358 [info ] [Thread-2 (]: 44 of 71 START test relationships_stg_subscriptions_user_id__user_id__ref_stg_users_  [RUN]
[0m14:15:27.686380 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_fct_shipment_order_id__order_id__ref_fct_order_.6096f95e39, now test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e)
[0m14:15:27.690396 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m14:15:27.701865 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e"
[0m14:15:27.703617 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m14:15:27.709923 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e"
[0m14:15:27.712413 [debug] [Thread-2 (]: On test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select user_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_subscriptions`
    where user_id is not null
),

parent as (
    select user_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m14:15:27.715015 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:15:27.859262 [info ] [Thread-3 (]: 41 of 71 PASS relationships_stg_addresses_user_id__user_id__ref_stg_users_ ..... [[32mPASS[0m in 2.45s]
[0m14:15:27.861648 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781
[0m14:15:27.863324 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6
[0m14:15:27.863961 [info ] [Thread-3 (]: 45 of 71 START test source_not_null_raw_data_addresses__id ..................... [RUN]
[0m14:15:27.864836 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781, now test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6)
[0m14:15:27.865383 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6
[0m14:15:27.878677 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6"
[0m14:15:27.884350 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6
[0m14:15:27.887958 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6"
[0m14:15:27.898926 [debug] [Thread-3 (]: On test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`addresses`
where _id is null



  
  
      
    ) dbt_internal_test
[0m14:15:27.899838 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:15:29.016270 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:4b72503e-fcd9-4c23-ae2b-e275d9932df3&page=queryresults
[0m14:15:29.094688 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:a96cfb21-42b1-4d34-9829-29b6b3e002bb&page=queryresults
[0m14:15:29.110343 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:469be001-303c-4aec-9034-8df25dc79c7c&page=queryresults
[0m14:15:29.194629 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:416fbb2e-e802-46dc-86f8-05600d9a91e3&page=queryresults
[0m14:15:29.926580 [info ] [Thread-1 (]: 42 of 71 PASS relationships_stg_orders_user_id__user_id__ref_stg_users_ ........ [[32mPASS[0m in 2.57s]
[0m14:15:29.928741 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737
[0m14:15:29.929968 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc
[0m14:15:29.930769 [info ] [Thread-1 (]: 46 of 71 START test source_not_null_raw_data_cities__id ........................ [RUN]
[0m14:15:29.932400 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737, now test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc)
[0m14:15:29.933425 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc
[0m14:15:29.945112 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc"
[0m14:15:29.947259 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc
[0m14:15:29.958332 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc"
[0m14:15:29.966336 [debug] [Thread-1 (]: On test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`cities`
where _id is null



  
  
      
    ) dbt_internal_test
[0m14:15:29.969052 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:15:30.080285 [info ] [Thread-4 (]: 43 of 71 PASS relationships_stg_shipments_user_id__user_id__ref_stg_users_ ..... [[32mPASS[0m in 2.47s]
[0m14:15:30.088587 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8
[0m14:15:30.093366 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd
[0m14:15:30.097642 [info ] [Thread-4 (]: 47 of 71 START test source_not_null_raw_data_countries__id ..................... [RUN]
[0m14:15:30.138105 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8, now test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd)
[0m14:15:30.177810 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd
[0m14:15:30.176087 [info ] [Thread-3 (]: 45 of 71 PASS source_not_null_raw_data_addresses__id ........................... [[32mPASS[0m in 2.31s]
[0m14:15:30.189484 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6
[0m14:15:30.197360 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd"
[0m14:15:30.205364 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be
[0m14:15:30.215755 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd
[0m14:15:30.216460 [info ] [Thread-2 (]: 44 of 71 PASS relationships_stg_subscriptions_user_id__user_id__ref_stg_users_ . [[32mPASS[0m in 2.52s]
[0m14:15:30.218990 [info ] [Thread-3 (]: 48 of 71 START test source_not_null_raw_data_neighborhoods__id ................. [RUN]
[0m14:15:30.224064 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd"
[0m14:15:30.230973 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6, now test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be)
[0m14:15:30.228560 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m14:15:30.232318 [debug] [Thread-4 (]: On test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`countries`
where _id is null



  
  
      
    ) dbt_internal_test
[0m14:15:30.235378 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878
[0m14:15:30.233995 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be
[0m14:15:30.237046 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:15:30.238659 [info ] [Thread-2 (]: 49 of 71 START test source_not_null_raw_data_orders__id ........................ [RUN]
[0m14:15:30.329162 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e, now test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878)
[0m14:15:30.337957 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878
[0m14:15:30.332484 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be"
[0m14:15:30.346979 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be
[0m14:15:30.379035 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878"
[0m14:15:30.381028 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be"
[0m14:15:30.382125 [debug] [Thread-3 (]: On test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`neighborhoods`
where _id is null



  
  
      
    ) dbt_internal_test
[0m14:15:30.385683 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:15:30.428663 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878
[0m14:15:30.454125 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878"
[0m14:15:30.456410 [debug] [Thread-2 (]: On test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`orders`
where _id is null



  
  
      
    ) dbt_internal_test
[0m14:15:30.458024 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:15:31.512875 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:79737b27-290c-489c-a5df-540b9751297d&page=queryresults
[0m14:15:31.727360 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:da280bb9-a480-4748-b746-efd3af9598b5&page=queryresults
[0m14:15:31.898736 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:309161ca-6569-4f99-a9f1-0ca1a7f20133&page=queryresults
[0m14:15:31.914044 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:25dba646-283a-42dc-a380-8ba1785f8902&page=queryresults
[0m14:15:32.405648 [info ] [Thread-1 (]: 46 of 71 PASS source_not_null_raw_data_cities__id .............................. [[32mPASS[0m in 2.47s]
[0m14:15:32.407036 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc
[0m14:15:32.408064 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f
[0m14:15:32.408735 [info ] [Thread-1 (]: 50 of 71 START test source_not_null_raw_data_orders__user ...................... [RUN]
[0m14:15:32.409541 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc, now test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f)
[0m14:15:32.410385 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f
[0m14:15:32.415850 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f"
[0m14:15:32.418329 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f
[0m14:15:32.422570 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f"
[0m14:15:32.424040 [debug] [Thread-1 (]: On test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _user
from `data-pipeline-project-474812`.`raw_data`.`orders`
where _user is null



  
  
      
    ) dbt_internal_test
[0m14:15:32.424984 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:15:32.643256 [info ] [Thread-4 (]: 47 of 71 PASS source_not_null_raw_data_countries__id ........................... [[32mPASS[0m in 2.51s]
[0m14:15:32.646185 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd
[0m14:15:32.647596 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2
[0m14:15:32.648632 [info ] [Thread-4 (]: 51 of 71 START test source_not_null_raw_data_shipments__id ..................... [RUN]
[0m14:15:32.653467 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd, now test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2)
[0m14:15:32.654697 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2
[0m14:15:32.665082 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2"
[0m14:15:32.666162 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2
[0m14:15:32.670271 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2"
[0m14:15:32.671285 [debug] [Thread-4 (]: On test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`shipments`
where _id is null



  
  
      
    ) dbt_internal_test
[0m14:15:32.671979 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:15:32.829129 [info ] [Thread-3 (]: 48 of 71 PASS source_not_null_raw_data_neighborhoods__id ....................... [[32mPASS[0m in 2.59s]
[0m14:15:32.830768 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be
[0m14:15:32.835235 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1
[0m14:15:32.837660 [info ] [Thread-3 (]: 52 of 71 START test source_not_null_raw_data_shipments__order .................. [RUN]
[0m14:15:32.840053 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be, now test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1)
[0m14:15:32.840687 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1
[0m14:15:32.850290 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1"
[0m14:15:32.852121 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1
[0m14:15:32.864442 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1"
[0m14:15:32.871744 [debug] [Thread-3 (]: On test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _order
from `data-pipeline-project-474812`.`raw_data`.`shipments`
where _order is null



  
  
      
    ) dbt_internal_test
[0m14:15:32.877423 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:15:32.941154 [info ] [Thread-2 (]: 49 of 71 PASS source_not_null_raw_data_orders__id .............................. [[32mPASS[0m in 2.61s]
[0m14:15:32.954500 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878
[0m14:15:32.960382 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98
[0m14:15:32.964560 [info ] [Thread-2 (]: 53 of 71 START test source_not_null_raw_data_shipments__user ................... [RUN]
[0m14:15:32.970275 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878, now test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98)
[0m14:15:32.972320 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98
[0m14:15:32.992688 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98"
[0m14:15:32.996111 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98
[0m14:15:33.002919 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98"
[0m14:15:33.005199 [debug] [Thread-2 (]: On test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _user
from `data-pipeline-project-474812`.`raw_data`.`shipments`
where _user is null



  
  
      
    ) dbt_internal_test
[0m14:15:33.009002 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:15:33.979226 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:d9401a5f-1e87-4562-a53b-7cbd38d75bc1&page=queryresults
[0m14:15:34.002457 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:beac9523-0532-4735-9b71-ba9d1bab643f&page=queryresults
[0m14:15:34.276641 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:6c3c8dc0-a334-4e62-872e-635149e25197&page=queryresults
[0m14:15:34.878160 [info ] [Thread-4 (]: 51 of 71 PASS source_not_null_raw_data_shipments__id ........................... [[32mPASS[0m in 2.23s]
[0m14:15:34.879374 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2
[0m14:15:34.881271 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86
[0m14:15:34.882361 [info ] [Thread-4 (]: 54 of 71 START test source_not_null_raw_data_states__id ........................ [RUN]
[0m14:15:34.883497 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2, now test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86)
[0m14:15:34.884995 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86
[0m14:15:34.895071 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86"
[0m14:15:34.896334 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86
[0m14:15:34.900666 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86"
[0m14:15:34.902200 [debug] [Thread-4 (]: On test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`states`
where _id is null



  
  
      
    ) dbt_internal_test
[0m14:15:34.904492 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:15:34.961581 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:9259e788-c30e-41f4-bc34-b7307c97c95f&page=queryresults
[0m14:15:34.958783 [info ] [Thread-1 (]: 50 of 71 PASS source_not_null_raw_data_orders__user ............................ [[32mPASS[0m in 2.55s]
[0m14:15:34.967412 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f
[0m14:15:34.968434 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a
[0m14:15:34.969956 [info ] [Thread-1 (]: 55 of 71 START test source_not_null_raw_data_subscriptions__id ................. [RUN]
[0m14:15:34.974304 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f, now test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a)
[0m14:15:34.975933 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a
[0m14:15:35.012207 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a"
[0m14:15:35.018029 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a
[0m14:15:35.033499 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a"
[0m14:15:35.038138 [debug] [Thread-1 (]: On test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`subscriptions`
where _id is null



  
  
      
    ) dbt_internal_test
[0m14:15:35.041559 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:15:35.131453 [info ] [Thread-3 (]: 52 of 71 PASS source_not_null_raw_data_shipments__order ........................ [[32mPASS[0m in 2.29s]
[0m14:15:35.133126 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1
[0m14:15:35.137534 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6
[0m14:15:35.139724 [info ] [Thread-3 (]: 56 of 71 START test source_not_null_raw_data_subscriptions__user ............... [RUN]
[0m14:15:35.142432 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1, now test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6)
[0m14:15:35.143018 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6
[0m14:15:35.165287 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6"
[0m14:15:35.171132 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6
[0m14:15:35.181394 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6"
[0m14:15:35.185641 [debug] [Thread-3 (]: On test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _user
from `data-pipeline-project-474812`.`raw_data`.`subscriptions`
where _user is null



  
  
      
    ) dbt_internal_test
[0m14:15:35.187357 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:15:35.767835 [info ] [Thread-2 (]: 53 of 71 PASS source_not_null_raw_data_shipments__user ......................... [[32mPASS[0m in 2.79s]
[0m14:15:35.772295 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98
[0m14:15:35.776116 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b
[0m14:15:35.779340 [info ] [Thread-2 (]: 57 of 71 START test source_not_null_raw_data_users__id ......................... [RUN]
[0m14:15:35.781598 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98, now test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b)
[0m14:15:35.785415 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b
[0m14:15:35.811713 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b"
[0m14:15:35.815624 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b
[0m14:15:35.847565 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b"
[0m14:15:35.852301 [debug] [Thread-2 (]: On test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`users`
where _id is null



  
  
      
    ) dbt_internal_test
[0m14:15:35.854583 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:15:36.441668 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:0c649709-94f6-45e0-998f-fea644f3e0a8&page=queryresults
[0m14:15:36.601086 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:a0c8300c-ca9d-48c2-9dac-914873570444&page=queryresults
[0m14:15:36.621242 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:1e94e120-c956-4bb3-b94c-974906c763f5&page=queryresults
[0m14:15:37.198540 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:a396c464-8788-4ca6-9c3f-5b61d21ebcf5&page=queryresults
[0m14:15:37.288579 [info ] [Thread-4 (]: 54 of 71 PASS source_not_null_raw_data_states__id .............................. [[32mPASS[0m in 2.40s]
[0m14:15:37.295708 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86
[0m14:15:37.299195 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1
[0m14:15:37.307081 [info ] [Thread-4 (]: 58 of 71 START test unique_dim_customer_customer_id ............................ [RUN]
[0m14:15:37.316037 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86, now test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1)
[0m14:15:37.321985 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1
[0m14:15:37.361869 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1"
[0m14:15:37.368744 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1
[0m14:15:37.392221 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1"
[0m14:15:37.400028 [debug] [Thread-4 (]: On test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select customer_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_customer`
  where customer_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:15:37.405463 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:15:37.502531 [info ] [Thread-1 (]: 55 of 71 PASS source_not_null_raw_data_subscriptions__id ....................... [[32mPASS[0m in 2.53s]
[0m14:15:37.504959 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a
[0m14:15:37.505923 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469
[0m14:15:37.508670 [info ] [Thread-1 (]: 59 of 71 START test unique_dim_subscription_subscription_id .................... [RUN]
[0m14:15:37.511494 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a, now test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469)
[0m14:15:37.526186 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469
[0m14:15:37.543778 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469"
[0m14:15:37.549088 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469
[0m14:15:37.561231 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469"
[0m14:15:37.565267 [info ] [Thread-3 (]: 56 of 71 PASS source_not_null_raw_data_subscriptions__user ..................... [[32mPASS[0m in 2.42s]
[0m14:15:37.566645 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6
[0m14:15:37.567968 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.unique_fct_marketing_spend_date.e93d255723
[0m14:15:37.571498 [debug] [Thread-1 (]: On test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select subscription_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_subscription`
  where subscription_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:15:37.573879 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:15:37.569846 [info ] [Thread-3 (]: 60 of 71 START test unique_fct_marketing_spend_date ............................ [RUN]
[0m14:15:37.626716 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6, now test.data_pipeline_project.unique_fct_marketing_spend_date.e93d255723)
[0m14:15:37.639537 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.unique_fct_marketing_spend_date.e93d255723
[0m14:15:37.662915 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.unique_fct_marketing_spend_date.e93d255723"
[0m14:15:37.665213 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.unique_fct_marketing_spend_date.e93d255723
[0m14:15:37.673522 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.unique_fct_marketing_spend_date.e93d255723"
[0m14:15:37.675539 [debug] [Thread-3 (]: On test.data_pipeline_project.unique_fct_marketing_spend_date.e93d255723: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_fct_marketing_spend_date.e93d255723"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select date as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_marketing_spend`
  where date is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:15:37.676566 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:15:38.228219 [info ] [Thread-2 (]: 57 of 71 PASS source_not_null_raw_data_users__id ............................... [[32mPASS[0m in 2.45s]
[0m14:15:38.234210 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b
[0m14:15:38.237166 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.unique_fct_order_order_id.653705d6a5
[0m14:15:38.241639 [info ] [Thread-2 (]: 61 of 71 START test unique_fct_order_order_id .................................. [RUN]
[0m14:15:38.245307 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b, now test.data_pipeline_project.unique_fct_order_order_id.653705d6a5)
[0m14:15:38.250706 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.unique_fct_order_order_id.653705d6a5
[0m14:15:38.263229 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.unique_fct_order_order_id.653705d6a5"
[0m14:15:38.264123 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.unique_fct_order_order_id.653705d6a5
[0m14:15:38.271494 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.unique_fct_order_order_id.653705d6a5"
[0m14:15:38.278949 [debug] [Thread-2 (]: On test.data_pipeline_project.unique_fct_order_order_id.653705d6a5: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_fct_order_order_id.653705d6a5"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select order_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_order`
  where order_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:15:38.283545 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:15:38.829574 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:7804be2d-6548-4eec-b76b-1673559a81ca&page=queryresults
[0m14:15:38.926503 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:78be8b56-9cc8-4597-a6dc-4f0e77b3ef83&page=queryresults
[0m14:15:39.099441 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:9c54e512-ca8a-4c1d-b0ad-1e12507231a4&page=queryresults
[0m14:15:39.609474 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:11b0330a-0f64-4a43-b9c6-2b9054d91a3b&page=queryresults
[0m14:15:39.691604 [error] [Thread-4 (]: 58 of 71 FAIL 16 unique_dim_customer_customer_id ............................... [[31mFAIL 16[0m in 2.38s]
[0m14:15:39.693528 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1
[0m14:15:39.694856 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3
[0m14:15:39.696027 [info ] [Thread-4 (]: 62 of 71 START test unique_fct_shipment_shipment_id ............................ [RUN]
[0m14:15:39.697233 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1, now test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3)
[0m14:15:39.698196 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3
[0m14:15:39.707284 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3"
[0m14:15:39.709054 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3
[0m14:15:39.714405 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3"
[0m14:15:39.715982 [debug] [Thread-4 (]: On test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select shipment_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_shipment`
  where shipment_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:15:39.716987 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:15:39.870408 [info ] [Thread-1 (]: 59 of 71 PASS unique_dim_subscription_subscription_id .......................... [[32mPASS[0m in 2.36s]
[0m14:15:39.875711 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469
[0m14:15:39.877445 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd
[0m14:15:39.884989 [info ] [Thread-1 (]: 63 of 71 START test unique_mart_acquisition_efficiency_date .................... [RUN]
[0m14:15:39.886344 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469, now test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd)
[0m14:15:39.890767 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd
[0m14:15:39.971861 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd"
[0m14:15:39.985091 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd
[0m14:15:39.987471 [error] [Thread-3 (]: 60 of 71 FAIL 8 unique_fct_marketing_spend_date ................................ [[31mFAIL 8[0m in 2.36s]
[0m14:15:40.001777 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd"
[0m14:15:40.008206 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.unique_fct_marketing_spend_date.e93d255723
[0m14:15:40.013896 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8
[0m14:15:40.011896 [debug] [Thread-1 (]: On test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select date as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_acquisition_efficiency`
  where date is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:15:40.015990 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:15:40.015054 [info ] [Thread-3 (]: 64 of 71 START test unique_mart_revenue_daily_date ............................. [RUN]
[0m14:15:40.083612 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_fct_marketing_spend_date.e93d255723, now test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8)
[0m14:15:40.086271 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8
[0m14:15:40.094704 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8"
[0m14:15:40.096149 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8
[0m14:15:40.103447 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8"
[0m14:15:40.106006 [debug] [Thread-3 (]: On test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select date as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_revenue_daily`
  where date is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:15:40.107387 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:15:40.527134 [info ] [Thread-2 (]: 61 of 71 PASS unique_fct_order_order_id ........................................ [[32mPASS[0m in 2.28s]
[0m14:15:40.529299 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.unique_fct_order_order_id.653705d6a5
[0m14:15:40.530358 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c
[0m14:15:40.531640 [info ] [Thread-2 (]: 65 of 71 START test unique_mart_subscription_daily_date ........................ [RUN]
[0m14:15:40.532830 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_fct_order_order_id.653705d6a5, now test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c)
[0m14:15:40.533399 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c
[0m14:15:40.538295 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c"
[0m14:15:40.541550 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c
[0m14:15:40.545677 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c"
[0m14:15:40.547648 [debug] [Thread-2 (]: On test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select date as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily`
  where date is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:15:40.548866 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:15:41.148971 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:23d53335-ee7d-49bd-877b-c9062c03e012&page=queryresults
[0m14:15:41.343534 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:f72f2436-f212-4a9e-b436-159b0d27ab76&page=queryresults
[0m14:15:41.517453 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:dbcec420-4253-4edf-bfb8-fe871dbf4f60&page=queryresults
[0m14:15:41.718079 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:38013549-66b0-4131-8e55-bf27730d4cd1&page=queryresults
[0m14:15:42.036926 [info ] [Thread-4 (]: 62 of 71 PASS unique_fct_shipment_shipment_id .................................. [[32mPASS[0m in 2.34s]
[0m14:15:42.038516 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3
[0m14:15:42.040056 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00
[0m14:15:42.041109 [info ] [Thread-4 (]: 66 of 71 START test unique_stg_addresses_address_id ............................ [RUN]
[0m14:15:42.043888 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3, now test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00)
[0m14:15:42.045075 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00
[0m14:15:42.057296 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00"
[0m14:15:42.058573 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00
[0m14:15:42.063097 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00"
[0m14:15:42.068513 [debug] [Thread-4 (]: On test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select address_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_addresses`
  where address_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:15:42.070636 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:15:42.261495 [info ] [Thread-1 (]: 63 of 71 PASS unique_mart_acquisition_efficiency_date .......................... [[32mPASS[0m in 2.35s]
[0m14:15:42.263953 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd
[0m14:15:42.268486 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.unique_stg_countries_country_id.8679936442
[0m14:15:42.270038 [info ] [Thread-1 (]: 67 of 71 START test unique_stg_countries_country_id ............................ [RUN]
[0m14:15:42.271874 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd, now test.data_pipeline_project.unique_stg_countries_country_id.8679936442)
[0m14:15:42.272915 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.unique_stg_countries_country_id.8679936442
[0m14:15:42.281659 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.unique_stg_countries_country_id.8679936442"
[0m14:15:42.283152 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.unique_stg_countries_country_id.8679936442
[0m14:15:42.299691 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.unique_stg_countries_country_id.8679936442"
[0m14:15:42.301123 [debug] [Thread-1 (]: On test.data_pipeline_project.unique_stg_countries_country_id.8679936442: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_stg_countries_country_id.8679936442"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select country_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_countries`
  where country_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:15:42.301720 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:15:42.430710 [info ] [Thread-3 (]: 64 of 71 PASS unique_mart_revenue_daily_date ................................... [[32mPASS[0m in 2.35s]
[0m14:15:42.432394 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8
[0m14:15:42.433293 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a
[0m14:15:42.440021 [info ] [Thread-3 (]: 68 of 71 START test unique_stg_orders_order_id ................................. [RUN]
[0m14:15:42.441454 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8, now test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a)
[0m14:15:42.442357 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a
[0m14:15:42.451928 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a"
[0m14:15:42.457931 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a
[0m14:15:42.468246 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a"
[0m14:15:42.469634 [debug] [Thread-3 (]: On test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select order_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_orders`
  where order_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:15:42.471442 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:15:42.740306 [info ] [Thread-2 (]: 65 of 71 PASS unique_mart_subscription_daily_date .............................. [[32mPASS[0m in 2.21s]
[0m14:15:42.745706 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c
[0m14:15:42.747316 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c
[0m14:15:42.749310 [info ] [Thread-2 (]: 69 of 71 START test unique_stg_shipments_shipment_id ........................... [RUN]
[0m14:15:42.752788 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c, now test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c)
[0m14:15:42.755612 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c
[0m14:15:42.763879 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c"
[0m14:15:42.768420 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c
[0m14:15:42.775998 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c"
[0m14:15:42.777637 [debug] [Thread-2 (]: On test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select shipment_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_shipments`
  where shipment_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:15:42.779192 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:15:43.685897 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:6a796765-64df-4cfd-96e2-5bfd00f35c2f&page=queryresults
[0m14:15:43.860300 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:3e578516-3f5f-44d8-961d-b53340aecff9&page=queryresults
[0m14:15:43.957102 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:42131910-1738-40bd-b813-a92892b848b8&page=queryresults
[0m14:15:44.269667 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:5fcd9a88-a69f-4059-a177-b60bf2d5b809&page=queryresults
[0m14:15:44.531761 [info ] [Thread-4 (]: 66 of 71 PASS unique_stg_addresses_address_id .................................. [[32mPASS[0m in 2.49s]
[0m14:15:44.533495 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00
[0m14:15:44.536182 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m14:15:44.537173 [info ] [Thread-4 (]: 70 of 71 START test unique_stg_subscriptions_subscription_id ................... [RUN]
[0m14:15:44.539202 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00, now test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e)
[0m14:15:44.540392 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m14:15:44.551882 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e"
[0m14:15:44.553657 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m14:15:44.559545 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e"
[0m14:15:44.563908 [debug] [Thread-4 (]: On test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select subscription_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_subscriptions`
  where subscription_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:15:44.564790 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:15:44.751548 [info ] [Thread-3 (]: 68 of 71 PASS unique_stg_orders_order_id ....................................... [[32mPASS[0m in 2.31s]
[0m14:15:44.754666 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a
[0m14:15:44.758457 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b
[0m14:15:44.760190 [info ] [Thread-3 (]: 71 of 71 START test unique_stg_users_user_id ................................... [RUN]
[0m14:15:44.761853 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a, now test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b)
[0m14:15:44.764975 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b
[0m14:15:44.832263 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b"
[0m14:15:44.837086 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b
[0m14:15:44.889224 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b"
[0m14:15:44.895466 [debug] [Thread-3 (]: On test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select user_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
  where user_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:15:44.898696 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:15:44.892268 [info ] [Thread-1 (]: 67 of 71 PASS unique_stg_countries_country_id .................................. [[32mPASS[0m in 2.62s]
[0m14:15:44.966271 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.unique_stg_countries_country_id.8679936442
[0m14:15:45.266946 [info ] [Thread-2 (]: 69 of 71 PASS unique_stg_shipments_shipment_id ................................. [[32mPASS[0m in 2.51s]
[0m14:15:45.271720 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c
[0m14:15:45.965545 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:6122bbed-59db-4d0e-99ce-5d1324d96630&page=queryresults
[0m14:15:46.498534 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:26c643ba-2e96-467a-86d2-f376891252c8&page=queryresults
[0m14:15:46.786634 [info ] [Thread-4 (]: 70 of 71 PASS unique_stg_subscriptions_subscription_id ......................... [[32mPASS[0m in 2.25s]
[0m14:15:46.793904 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m14:15:47.352012 [info ] [Thread-3 (]: 71 of 71 PASS unique_stg_users_user_id ......................................... [[32mPASS[0m in 2.59s]
[0m14:15:47.355211 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b
[0m14:15:47.363320 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:15:47.366417 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:15:47.367163 [debug] [MainThread]: Connection 'test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c' was properly closed.
[0m14:15:47.367828 [debug] [MainThread]: Connection 'test.data_pipeline_project.unique_stg_countries_country_id.8679936442' was properly closed.
[0m14:15:47.369172 [debug] [MainThread]: Connection 'test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b' was properly closed.
[0m14:15:47.370307 [debug] [MainThread]: Connection 'test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e' was properly closed.
[0m14:15:47.371189 [info ] [MainThread]: 
[0m14:15:47.372231 [info ] [MainThread]: Finished running 71 data tests in 0 hours 0 minutes and 42.33 seconds (42.33s).
[0m14:15:47.404615 [debug] [MainThread]: Command end result
[0m14:15:47.592922 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m14:15:47.600574 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m14:15:47.622142 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m14:15:47.623068 [info ] [MainThread]: 
[0m14:15:47.624374 [info ] [MainThread]: [31mCompleted with 12 errors, 0 partial successes, and 1 warning:[0m
[0m14:15:47.625253 [info ] [MainThread]: 
[0m14:15:47.626218 [error] [MainThread]: [31mFailure in test accepted_range_fct_order_net_revenue__0 (models/intermediate/int_facts_marts_schema.yml)[0m
[0m14:15:47.627187 [error] [MainThread]:   Compilation Error in test accepted_range_fct_order_net_revenue__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:15:47.628216 [info ] [MainThread]: 
[0m14:15:47.629195 [error] [MainThread]: [31mFailure in test accepted_range_fct_order_discount_total__0 (models/intermediate/int_facts_marts_schema.yml)[0m
[0m14:15:47.630377 [error] [MainThread]:   Compilation Error in test accepted_range_fct_order_discount_total__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:15:47.631720 [info ] [MainThread]: 
[0m14:15:47.633053 [error] [MainThread]: [31mFailure in test accepted_range_fct_marketing_spend_spend_try__0 (models/intermediate/int_facts_marts_schema.yml)[0m
[0m14:15:47.634577 [error] [MainThread]:   Compilation Error in test accepted_range_fct_marketing_spend_spend_try__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:15:47.636036 [info ] [MainThread]: 
[0m14:15:47.636655 [error] [MainThread]: [31mFailure in test accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0 (models/intermediate/int_facts_marts_schema.yml)[0m
[0m14:15:47.637450 [error] [MainThread]:   Compilation Error in test accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:15:47.638472 [info ] [MainThread]: 
[0m14:15:47.639416 [error] [MainThread]: [31mFailure in test accepted_range_mart_kpis_latest_cac_payback_months__0 (models/intermediate/int_facts_marts_schema.yml)[0m
[0m14:15:47.640330 [error] [MainThread]:   Compilation Error in test accepted_range_mart_kpis_latest_cac_payback_months__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:15:47.641485 [info ] [MainThread]: 
[0m14:15:47.642429 [error] [MainThread]: [31mFailure in test accepted_range_mart_kpis_latest_monthly_recurring_revenue__0 (models/intermediate/int_facts_marts_schema.yml)[0m
[0m14:15:47.643474 [error] [MainThread]:   Compilation Error in test accepted_range_mart_kpis_latest_monthly_recurring_revenue__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:15:47.645318 [info ] [MainThread]: 
[0m14:15:47.646416 [error] [MainThread]: [31mFailure in test accepted_range_mart_revenue_daily_daily_revenue__0 (models/intermediate/int_facts_marts_schema.yml)[0m
[0m14:15:47.647509 [error] [MainThread]:   Compilation Error in test accepted_range_mart_revenue_daily_daily_revenue__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:15:47.648404 [info ] [MainThread]: 
[0m14:15:47.649562 [error] [MainThread]: [31mFailure in test accepted_range_mart_subscription_daily_active_subscribers__0 (models/intermediate/int_facts_marts_schema.yml)[0m
[0m14:15:47.653888 [error] [MainThread]:   Compilation Error in test accepted_range_mart_subscription_daily_active_subscribers__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:15:47.656533 [info ] [MainThread]: 
[0m14:15:47.657616 [error] [MainThread]: [31mFailure in test accepted_range_mart_subscription_daily_new_subscribers__0 (models/intermediate/int_facts_marts_schema.yml)[0m
[0m14:15:47.660540 [error] [MainThread]:   Compilation Error in test accepted_range_mart_subscription_daily_new_subscribers__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:15:47.661891 [info ] [MainThread]: 
[0m14:15:47.664424 [error] [MainThread]: [31mFailure in test relationships_fct_shipment_order_id__order_id__ref_fct_order_ (models/intermediate/int_facts_marts_schema.yml)[0m
[0m14:15:47.665707 [error] [MainThread]:   Got 476 results, configured to fail if != 0
[0m14:15:47.667844 [info ] [MainThread]: 
[0m14:15:47.670039 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_project/models/intermediate/int_facts_marts_schema.yml/relationships_fct_shipment_order_id__order_id__ref_fct_order_.sql
[0m14:15:47.672184 [info ] [MainThread]: 
[0m14:15:47.676154 [error] [MainThread]: [31mFailure in test unique_dim_customer_customer_id (models/intermediate/int_facts_marts_schema.yml)[0m
[0m14:15:47.678350 [error] [MainThread]:   Got 16 results, configured to fail if != 0
[0m14:15:47.679971 [info ] [MainThread]: 
[0m14:15:47.681476 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_project/models/intermediate/int_facts_marts_schema.yml/unique_dim_customer_customer_id.sql
[0m14:15:47.684553 [info ] [MainThread]: 
[0m14:15:47.687156 [error] [MainThread]: [31mFailure in test unique_fct_marketing_spend_date (models/intermediate/int_facts_marts_schema.yml)[0m
[0m14:15:47.688849 [error] [MainThread]:   Got 8 results, configured to fail if != 0
[0m14:15:47.690399 [info ] [MainThread]: 
[0m14:15:47.693003 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_project/models/intermediate/int_facts_marts_schema.yml/unique_fct_marketing_spend_date.sql
[0m14:15:47.694060 [info ] [MainThread]: 
[0m14:15:47.697231 [warn ] [MainThread]: [33mWarning in test not_null_int_shipments_decomposed_delivered_at (models/staging/staging_schema.yml)[0m
[0m14:15:47.698334 [warn ] [MainThread]: Got 303 results, configured to warn if != 0
[0m14:15:47.699325 [info ] [MainThread]: 
[0m14:15:47.700429 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_project/models/staging/staging_schema.yml/not_null_int_shipments_decomposed_delivered_at.sql
[0m14:15:47.701497 [info ] [MainThread]: 
[0m14:15:47.702359 [info ] [MainThread]: Done. PASS=58 WARN=1 ERROR=12 SKIP=0 NO-OP=0 TOTAL=71
[0m14:15:47.703728 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 45.91454, "process_in_blocks": "232", "process_kernel_time": 4.69067, "process_mem_max_rss": "394516", "process_out_blocks": "4936", "process_user_time": 16.41975}
[0m14:15:47.704897 [debug] [MainThread]: Command `dbt test` failed at 14:15:47.704433 after 45.92 seconds
[0m14:15:47.705916 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7358343785c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7357f3cedf40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7357f3c80380>]}
[0m14:15:47.706465 [debug] [MainThread]: Flushing usage events
[0m14:15:48.660602 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:27:18.341665 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73d32c2e7620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73d32bef8b60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73d32f5dbb60>]}


============================== 14:27:18.347063 | 5cb58f71-034a-43d2-8075-d103081ca0c4 ==============================
[0m14:27:18.347063 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m14:27:18.348575 [debug] [MainThread]: running dbt with arguments {'profiles_dir': '/home/ecem/.dbt', 'warn_error': 'None', 'cache_selected_only': 'False', 'log_format': 'default', 'static_parser': 'True', 'printer_width': '80', 'introspect': 'True', 'version_check': 'True', 'use_experimental_parser': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'write_json': 'True', 'invocation_command': 'dbt clean', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'debug': 'False', 'empty': 'None', 'quiet': 'False', 'partial_parse': 'True', 'target_path': 'None', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'send_anonymous_usage_stats': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'no_print': 'None'}
[0m14:27:18.508868 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5cb58f71-034a-43d2-8075-d103081ca0c4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73d32e09fe60>]}
[0m14:27:18.532224 [debug] [MainThread]: Resource report: {"command_name": "clean", "command_success": true, "command_wall_clock_time": 0.24914782, "process_in_blocks": "0", "process_kernel_time": 0.278457, "process_mem_max_rss": "101680", "process_out_blocks": "8", "process_user_time": 1.508309}
[0m14:27:18.532837 [debug] [MainThread]: Command `dbt clean` succeeded at 14:27:18.532734 after 0.25 seconds
[0m14:27:18.533247 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73d32bba67e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73d32f5dbb60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73d32bf7aff0>]}
[0m14:27:18.533688 [debug] [MainThread]: Flushing usage events
[0m14:27:19.558365 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:27:35.524249 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x718a70c57620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x718a70b80ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x718a70b813d0>]}


============================== 14:27:35.528214 | 1fd8b114-3ae5-49af-91e1-5a0c7499ecb2 ==============================
[0m14:27:35.528214 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m14:27:35.529192 [debug] [MainThread]: running dbt with arguments {'fail_fast': 'False', 'empty': 'None', 'target_path': 'None', 'log_cache_events': 'False', 'write_json': 'True', 'debug': 'False', 'version_check': 'True', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'indirect_selection': 'eager', 'warn_error': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'invocation_command': 'dbt test', 'no_print': 'None', 'use_experimental_parser': 'False', 'send_anonymous_usage_stats': 'True', 'profiles_dir': '/home/ecem/.dbt', 'introspect': 'True', 'use_colors': 'True', 'cache_selected_only': 'False', 'log_format': 'default', 'printer_width': '80', 'quiet': 'False', 'static_parser': 'True', 'partial_parse': 'True'}
[0m14:27:37.925174 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1fd8b114-3ae5-49af-91e1-5a0c7499ecb2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x718a50e3e4b0>]}
[0m14:27:37.997343 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '1fd8b114-3ae5-49af-91e1-5a0c7499ecb2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x718a50ed07a0>]}
[0m14:27:37.999477 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m14:27:38.234888 [error] [MainThread]: Encountered an error:
Compilation Error
  dbt found 1 package(s) specified in packages.yml, but only 0 package(s) installed in dbt_packages. Run "dbt deps" to install package dependencies.
[0m14:27:38.239065 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 2.7728899, "process_in_blocks": "0", "process_kernel_time": 0.770605, "process_mem_max_rss": "364436", "process_out_blocks": "8", "process_user_time": 3.67017}
[0m14:27:38.240107 [debug] [MainThread]: Command `dbt test` failed at 14:27:38.239990 after 2.77 seconds
[0m14:27:38.240598 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x718a70fe6e40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x718a50c13ec0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x718a50e0d3d0>]}
[0m14:27:38.241224 [debug] [MainThread]: Flushing usage events
[0m14:27:38.952852 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:28:13.044110 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x732bbc1ccd40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x732bbbe3c3b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x732bbbe3f6b0>]}


============================== 14:28:13.047852 | 8da72d4d-7ed9-43f4-a910-0b6b1206d866 ==============================
[0m14:28:13.047852 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m14:28:13.048852 [debug] [MainThread]: running dbt with arguments {'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'debug': 'False', 'empty': 'None', 'log_format': 'default', 'static_parser': 'True', 'version_check': 'True', 'profiles_dir': '/home/ecem/.dbt', 'printer_width': '80', 'invocation_command': 'dbt deps', 'use_experimental_parser': 'False', 'cache_selected_only': 'False', 'indirect_selection': 'eager', 'write_json': 'True', 'quiet': 'False', 'introspect': 'True', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'no_print': 'None', 'target_path': 'None'}
[0m14:28:13.201101 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8da72d4d-7ed9-43f4-a910-0b6b1206d866', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x732bbb0e6ae0>]}
[0m14:28:13.266345 [debug] [MainThread]: Set downloads directory='/tmp/dbt-downloads-cyoe28wk'
[0m14:28:13.267237 [debug] [MainThread]: Making package index registry request: GET https://hub.getdbt.com/api/v1/index.json
[0m14:28:13.850362 [debug] [MainThread]: Response from registry index: GET https://hub.getdbt.com/api/v1/index.json 200
[0m14:28:13.860082 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
[0m14:28:14.163192 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
[0m14:28:14.184736 [info ] [MainThread]: Updating lock file in file path: /home/ecem/Desktop/data-pipeline-project/dbt/package-lock.yml
[0m14:28:14.188704 [debug] [MainThread]: Set downloads directory='/tmp/dbt-downloads-8l6zltrw'
[0m14:28:14.195055 [info ] [MainThread]: Installing dbt-labs/dbt_utils
[0m14:28:14.879739 [info ] [MainThread]: Installed from version 1.3.1
[0m14:28:14.881816 [info ] [MainThread]: Up to date!
[0m14:28:14.884376 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': '8da72d4d-7ed9-43f4-a910-0b6b1206d866', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x732bbb6474a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x732bbb7505f0>]}
[0m14:28:14.890911 [debug] [MainThread]: Resource report: {"command_name": "deps", "command_success": true, "command_wall_clock_time": 1.9019939, "process_in_blocks": "176", "process_kernel_time": 0.288856, "process_mem_max_rss": "103708", "process_out_blocks": "2296", "process_user_time": 1.705182}
[0m14:28:14.893958 [debug] [MainThread]: Command `dbt deps` succeeded at 14:28:14.893544 after 1.91 seconds
[0m14:28:14.897014 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x732bbb777b00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x732bbba07470>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x732bbb7a7f80>]}
[0m14:28:14.898524 [debug] [MainThread]: Flushing usage events
[0m14:28:15.706785 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:28:48.341394 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x727ea5a43800>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x727ea59c9490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x727ea5ff1520>]}


============================== 14:28:48.344988 | 6d3eeee2-1939-4bd9-ad5e-3a82b4afb4ec ==============================
[0m14:28:48.344988 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m14:28:48.346896 [debug] [MainThread]: running dbt with arguments {'warn_error': 'None', 'profiles_dir': '/home/ecem/.dbt', 'debug': 'False', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'target_path': 'None', 'indirect_selection': 'eager', 'write_json': 'True', 'fail_fast': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'cache_selected_only': 'False', 'use_colors': 'True', 'invocation_command': 'dbt test', 'printer_width': '80', 'use_experimental_parser': 'False', 'empty': 'None', 'send_anonymous_usage_stats': 'True', 'introspect': 'True', 'quiet': 'False', 'static_parser': 'True', 'log_format': 'default', 'version_check': 'True', 'partial_parse': 'True', 'log_cache_events': 'False'}
[0m14:28:51.986293 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6d3eeee2-1939-4bd9-ad5e-3a82b4afb4ec', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x727e85a107d0>]}
[0m14:28:52.164670 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6d3eeee2-1939-4bd9-ad5e-3a82b4afb4ec', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x727ea50c2180>]}
[0m14:28:52.167798 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m14:28:52.574151 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m14:28:52.576148 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m14:28:52.578229 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '6d3eeee2-1939-4bd9-ad5e-3a82b4afb4ec', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x727ea50c0bf0>]}
[0m14:28:54.779837 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `relationships` defined on 'stg_orders' in
package 'data_pipeline_project' (models/staging/staging_schema.yml). Arguments
to generic tests should be nested under the `arguments` property.
[0m14:28:54.781680 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '6d3eeee2-1939-4bd9-ad5e-3a82b4afb4ec', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x727e85a11940>]}
[0m14:28:55.535440 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6d3eeee2-1939-4bd9-ad5e-3a82b4afb4ec', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x727e84a28170>]}
[0m14:28:55.819801 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m14:28:55.826751 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m14:28:55.917127 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6d3eeee2-1939-4bd9-ad5e-3a82b4afb4ec', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x727e84803f80>]}
[0m14:28:55.923349 [info ] [MainThread]: Found 21 models, 71 data tests, 10 sources, 624 macros
[0m14:28:55.927609 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6d3eeee2-1939-4bd9-ad5e-3a82b4afb4ec', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x727e8495c620>]}
[0m14:28:55.945549 [info ] [MainThread]: 
[0m14:28:55.947256 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:28:55.949219 [info ] [MainThread]: 
[0m14:28:55.951410 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m14:28:55.972733 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_marts'
[0m14:28:55.978968 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_staging'
[0m14:28:55.982834 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:28:55.990093 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:28:57.253504 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6d3eeee2-1939-4bd9-ad5e-3a82b4afb4ec', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x727e84a915b0>]}
[0m14:28:57.254499 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:28:57.276338 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.accepted_range_fct_marketing_spend_spend_try__0.29ff739b28
[0m14:28:57.281550 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.accepted_range_fct_order_discount_total__0.5e17f8aa8a
[0m14:28:57.287390 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.accepted_range_fct_order_net_revenue__0.49d8f40cb7
[0m14:28:57.291517 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0.494a0c0ebc
[0m14:28:57.284680 [info ] [Thread-1 (]: 1 of 71 START test accepted_range_fct_marketing_spend_spend_try__0 ............. [RUN]
[0m14:28:57.295630 [info ] [Thread-2 (]: 2 of 71 START test accepted_range_fct_order_discount_total__0 .................. [RUN]
[0m14:28:57.298511 [info ] [Thread-3 (]: 3 of 71 START test accepted_range_fct_order_net_revenue__0 ..................... [RUN]
[0m14:28:57.311468 [debug] [Thread-3 (]: Acquiring new bigquery connection 'test.data_pipeline_project.accepted_range_fct_order_net_revenue__0.49d8f40cb7'
[0m14:28:57.305743 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_marts, now test.data_pipeline_project.accepted_range_fct_marketing_spend_spend_try__0.29ff739b28)
[0m14:28:57.308981 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now test.data_pipeline_project.accepted_range_fct_order_discount_total__0.5e17f8aa8a)
[0m14:28:57.301666 [info ] [Thread-4 (]: 4 of 71 START test accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0  [RUN]
[0m14:28:57.314175 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.accepted_range_fct_order_net_revenue__0.49d8f40cb7
[0m14:28:57.315251 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.accepted_range_fct_marketing_spend_spend_try__0.29ff739b28
[0m14:28:57.316234 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.accepted_range_fct_order_discount_total__0.5e17f8aa8a
[0m14:28:57.317501 [debug] [Thread-4 (]: Acquiring new bigquery connection 'test.data_pipeline_project.accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0.494a0c0ebc'
[0m14:28:57.417641 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0.494a0c0ebc
[0m14:28:57.405905 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m14:28:57.413465 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:28:57.410703 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:28:57.460494 [debug] [Thread-2 (]: Compilation Error in test accepted_range_fct_order_discount_total__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:28:57.465689 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m14:28:57.479021 [debug] [Thread-1 (]: Compilation Error in test accepted_range_fct_marketing_spend_spend_try__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:28:57.482334 [debug] [Thread-3 (]: Compilation Error in test accepted_range_fct_order_net_revenue__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:28:57.489865 [error] [Thread-2 (]: 2 of 71 ERROR accepted_range_fct_order_discount_total__0 ....................... [[31mERROR[0m in 0.18s]
[0m14:28:57.521113 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.accepted_range_fct_order_discount_total__0.5e17f8aa8a
[0m14:28:57.515114 [debug] [Thread-4 (]: Compilation Error in test accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:28:57.506896 [error] [Thread-3 (]: 3 of 71 ERROR accepted_range_fct_order_net_revenue__0 .......................... [[31mERROR[0m in 0.20s]
[0m14:28:57.501946 [error] [Thread-1 (]: 1 of 71 ERROR accepted_range_fct_marketing_spend_spend_try__0 .................. [[31mERROR[0m in 0.20s]
[0m14:28:57.526790 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.accepted_range_fct_marketing_spend_spend_try__0.29ff739b28
[0m14:28:57.523570 [debug] [Thread-7 (]: Marking all children of 'test.data_pipeline_project.accepted_range_fct_order_discount_total__0.5e17f8aa8a' to be skipped because of status 'error'.  Reason: Compilation Error in test accepted_range_fct_order_discount_total__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps"..
[0m14:28:57.524503 [error] [Thread-4 (]: 4 of 71 ERROR accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0  [[31mERROR[0m in 0.21s]
[0m14:28:57.525963 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.accepted_range_fct_order_net_revenue__0.49d8f40cb7
[0m14:28:57.533401 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.accepted_range_mart_revenue_daily_daily_revenue__0.12a5734f28
[0m14:28:57.521867 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.accepted_range_mart_kpis_latest_cac_payback_months__0.b5860838e1
[0m14:28:57.530181 [debug] [Thread-7 (]: Marking all children of 'test.data_pipeline_project.accepted_range_fct_marketing_spend_spend_try__0.29ff739b28' to be skipped because of status 'error'.  Reason: Compilation Error in test accepted_range_fct_marketing_spend_spend_try__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps"..
[0m14:28:57.532122 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0.494a0c0ebc
[0m14:28:57.539199 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.accepted_range_mart_subscription_daily_active_subscribers__0.5d06041ada
[0m14:28:57.536323 [debug] [Thread-7 (]: Marking all children of 'test.data_pipeline_project.accepted_range_fct_order_net_revenue__0.49d8f40cb7' to be skipped because of status 'error'.  Reason: Compilation Error in test accepted_range_fct_order_net_revenue__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps"..
[0m14:28:57.534134 [info ] [Thread-3 (]: 7 of 71 START test accepted_range_mart_revenue_daily_daily_revenue__0 .......... [RUN]
[0m14:28:57.527441 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.accepted_range_mart_kpis_latest_monthly_recurring_revenue__0.b1fd398ff0
[0m14:28:57.534564 [info ] [Thread-2 (]: 5 of 71 START test accepted_range_mart_kpis_latest_cac_payback_months__0 ....... [RUN]
[0m14:28:57.541689 [debug] [Thread-7 (]: Marking all children of 'test.data_pipeline_project.accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0.494a0c0ebc' to be skipped because of status 'error'.  Reason: Compilation Error in test accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps"..
[0m14:28:57.539838 [info ] [Thread-4 (]: 8 of 71 START test accepted_range_mart_subscription_daily_active_subscribers__0  [RUN]
[0m14:28:57.545516 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0.494a0c0ebc, now test.data_pipeline_project.accepted_range_mart_subscription_daily_active_subscribers__0.5d06041ada)
[0m14:28:57.546071 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.accepted_range_mart_subscription_daily_active_subscribers__0.5d06041ada
[0m14:28:57.551062 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:28:57.543452 [info ] [Thread-1 (]: 6 of 71 START test accepted_range_mart_kpis_latest_monthly_recurring_revenue__0  [RUN]
[0m14:28:57.555982 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.accepted_range_fct_marketing_spend_spend_try__0.29ff739b28, now test.data_pipeline_project.accepted_range_mart_kpis_latest_monthly_recurring_revenue__0.b1fd398ff0)
[0m14:28:57.544131 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.accepted_range_fct_order_discount_total__0.5e17f8aa8a, now test.data_pipeline_project.accepted_range_mart_kpis_latest_cac_payback_months__0.b5860838e1)
[0m14:28:57.556518 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.accepted_range_mart_kpis_latest_monthly_recurring_revenue__0.b1fd398ff0
[0m14:28:57.542954 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.accepted_range_fct_order_net_revenue__0.49d8f40cb7, now test.data_pipeline_project.accepted_range_mart_revenue_daily_daily_revenue__0.12a5734f28)
[0m14:28:57.562905 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.accepted_range_mart_revenue_daily_daily_revenue__0.12a5734f28
[0m14:28:57.557324 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.accepted_range_mart_kpis_latest_cac_payback_months__0.b5860838e1
[0m14:28:57.566991 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:28:57.553668 [debug] [Thread-4 (]: Compilation Error in test accepted_range_mart_subscription_daily_active_subscribers__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:28:57.576374 [error] [Thread-4 (]: 8 of 71 ERROR accepted_range_mart_subscription_daily_active_subscribers__0 ..... [[31mERROR[0m in 0.03s]
[0m14:28:57.561956 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:28:57.590347 [debug] [Thread-1 (]: Compilation Error in test accepted_range_mart_kpis_latest_monthly_recurring_revenue__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:28:57.581240 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.accepted_range_mart_subscription_daily_active_subscribers__0.5d06041ada
[0m14:28:57.587351 [debug] [Thread-3 (]: Compilation Error in test accepted_range_mart_revenue_daily_daily_revenue__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:28:57.579604 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:28:57.592615 [error] [Thread-1 (]: 6 of 71 ERROR accepted_range_mart_kpis_latest_monthly_recurring_revenue__0 ..... [[31mERROR[0m in 0.04s]
[0m14:28:57.593881 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.accepted_range_mart_subscription_daily_new_subscribers__0.f410e5c26f
[0m14:28:57.603113 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.accepted_range_mart_kpis_latest_monthly_recurring_revenue__0.b1fd398ff0
[0m14:28:57.604807 [debug] [Thread-7 (]: Marking all children of 'test.data_pipeline_project.accepted_range_mart_subscription_daily_active_subscribers__0.5d06041ada' to be skipped because of status 'error'.  Reason: Compilation Error in test accepted_range_mart_subscription_daily_active_subscribers__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps"..
[0m14:28:57.595187 [error] [Thread-3 (]: 7 of 71 ERROR accepted_range_mart_revenue_daily_daily_revenue__0 ............... [[31mERROR[0m in 0.05s]
[0m14:28:57.608431 [info ] [Thread-4 (]: 9 of 71 START test accepted_range_mart_subscription_daily_new_subscribers__0 ... [RUN]
[0m14:28:57.612384 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc
[0m14:28:57.618269 [debug] [Thread-2 (]: Compilation Error in test accepted_range_mart_kpis_latest_cac_payback_months__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:28:57.621003 [debug] [Thread-7 (]: Marking all children of 'test.data_pipeline_project.accepted_range_mart_kpis_latest_monthly_recurring_revenue__0.b1fd398ff0' to be skipped because of status 'error'.  Reason: Compilation Error in test accepted_range_mart_kpis_latest_monthly_recurring_revenue__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps"..
[0m14:28:57.624705 [info ] [Thread-1 (]: 10 of 71 START test not_null_dim_customer_customer_id .......................... [RUN]
[0m14:28:57.622551 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.accepted_range_mart_revenue_daily_daily_revenue__0.12a5734f28
[0m14:28:57.627177 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.accepted_range_mart_kpis_latest_monthly_recurring_revenue__0.b1fd398ff0, now test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc)
[0m14:28:57.629467 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc
[0m14:28:57.628100 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723
[0m14:28:57.629072 [debug] [Thread-7 (]: Marking all children of 'test.data_pipeline_project.accepted_range_mart_revenue_daily_daily_revenue__0.12a5734f28' to be skipped because of status 'error'.  Reason: Compilation Error in test accepted_range_mart_revenue_daily_daily_revenue__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps"..
[0m14:28:57.623933 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.accepted_range_mart_subscription_daily_active_subscribers__0.5d06041ada, now test.data_pipeline_project.accepted_range_mart_subscription_daily_new_subscribers__0.f410e5c26f)
[0m14:28:57.625627 [error] [Thread-2 (]: 5 of 71 ERROR accepted_range_mart_kpis_latest_cac_payback_months__0 ............ [[31mERROR[0m in 0.08s]
[0m14:28:57.647298 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc"
[0m14:28:57.643233 [info ] [Thread-3 (]: 11 of 71 START test not_null_dim_subscription_customer_id ...................... [RUN]
[0m14:28:57.648820 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.accepted_range_mart_subscription_daily_new_subscribers__0.f410e5c26f
[0m14:28:57.650019 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.accepted_range_mart_kpis_latest_cac_payback_months__0.b5860838e1
[0m14:28:57.651591 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.accepted_range_mart_revenue_daily_daily_revenue__0.12a5734f28, now test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723)
[0m14:28:57.663338 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723
[0m14:28:57.661237 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:28:57.662486 [debug] [Thread-7 (]: Marking all children of 'test.data_pipeline_project.accepted_range_mart_kpis_latest_cac_payback_months__0.b5860838e1' to be skipped because of status 'error'.  Reason: Compilation Error in test accepted_range_mart_kpis_latest_cac_payback_months__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps"..
[0m14:28:57.662038 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc
[0m14:28:57.660038 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6
[0m14:28:57.673510 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723"
[0m14:28:57.699966 [debug] [Thread-4 (]: Compilation Error in test accepted_range_mart_subscription_daily_new_subscribers__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:28:57.717800 [info ] [Thread-2 (]: 12 of 71 START test not_null_dim_subscription_subscription_id .................. [RUN]
[0m14:28:57.743408 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.accepted_range_mart_kpis_latest_cac_payback_months__0.b5860838e1, now test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6)
[0m14:28:57.743952 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6
[0m14:28:57.734660 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723
[0m14:28:57.747850 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6"
[0m14:28:57.742579 [error] [Thread-4 (]: 9 of 71 ERROR accepted_range_mart_subscription_daily_new_subscribers__0 ........ [[31mERROR[0m in 0.12s]
[0m14:28:57.769221 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.accepted_range_mart_subscription_daily_new_subscribers__0.f410e5c26f
[0m14:28:57.771078 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc"
[0m14:28:57.779364 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723"
[0m14:28:57.780361 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719
[0m14:28:57.781815 [debug] [Thread-7 (]: Marking all children of 'test.data_pipeline_project.accepted_range_mart_subscription_daily_new_subscribers__0.f410e5c26f' to be skipped because of status 'error'.  Reason: Compilation Error in test accepted_range_mart_subscription_daily_new_subscribers__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps"..
[0m14:28:57.787634 [debug] [Thread-3 (]: On test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select customer_id
from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_subscription`
where customer_id is null



  
  
      
    ) dbt_internal_test
[0m14:28:57.784304 [info ] [Thread-4 (]: 13 of 71 START test not_null_fct_marketing_spend_date .......................... [RUN]
[0m14:28:57.782883 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6
[0m14:28:57.788285 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:28:57.785911 [debug] [Thread-1 (]: On test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select customer_id
from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_customer`
where customer_id is null



  
  
      
    ) dbt_internal_test
[0m14:28:57.789169 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.accepted_range_mart_subscription_daily_new_subscribers__0.f410e5c26f, now test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719)
[0m14:28:57.800904 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6"
[0m14:28:57.821609 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:28:57.861233 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719
[0m14:28:57.862912 [debug] [Thread-2 (]: On test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select subscription_id
from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_subscription`
where subscription_id is null



  
  
      
    ) dbt_internal_test
[0m14:28:57.919431 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719"
[0m14:28:57.928186 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:28:57.929267 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719
[0m14:28:58.016737 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719"
[0m14:28:58.023872 [debug] [Thread-4 (]: On test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select date
from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_marketing_spend`
where date is null



  
  
      
    ) dbt_internal_test
[0m14:28:58.025880 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:28:59.204743 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:1d083782-ecc9-4962-89f4-f9256a5e9cc7&page=queryresults
[0m14:28:59.232936 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:c3458241-9f55-42cd-b2a0-55a4787d057b&page=queryresults
[0m14:28:59.248582 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:78c5c8be-5951-4929-a450-2be02e7cd788&page=queryresults
[0m14:28:59.334897 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:6363361e-2619-43ee-96e0-9990e1d44231&page=queryresults
[0m14:29:00.242512 [info ] [Thread-3 (]: 11 of 71 PASS not_null_dim_subscription_customer_id ............................ [[32mPASS[0m in 2.59s]
[0m14:29:00.245822 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723
[0m14:29:00.246942 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d
[0m14:29:00.248483 [info ] [Thread-3 (]: 14 of 71 START test not_null_fct_marketing_spend_spend_try ..................... [RUN]
[0m14:29:00.255685 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723, now test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d)
[0m14:29:00.253531 [info ] [Thread-1 (]: 10 of 71 PASS not_null_dim_customer_customer_id ................................ [[32mPASS[0m in 2.62s]
[0m14:29:00.257334 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d
[0m14:29:00.258956 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc
[0m14:29:00.273262 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d"
[0m14:29:00.277132 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff
[0m14:29:00.279502 [info ] [Thread-2 (]: 12 of 71 PASS not_null_dim_subscription_subscription_id ........................ [[32mPASS[0m in 2.54s]
[0m14:29:00.281751 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d
[0m14:29:00.280762 [info ] [Thread-1 (]: 15 of 71 START test not_null_fct_order_customer_id ............................. [RUN]
[0m14:29:00.283413 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6
[0m14:29:00.290223 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc, now test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff)
[0m14:29:00.293699 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff
[0m14:29:00.291798 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea
[0m14:29:00.289337 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d"
[0m14:29:00.299371 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff"
[0m14:29:00.300085 [info ] [Thread-2 (]: 16 of 71 START test not_null_fct_order_discount_total .......................... [RUN]
[0m14:29:00.303655 [debug] [Thread-3 (]: On test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select spend_try
from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_marketing_spend`
where spend_try is null



  
  
      
    ) dbt_internal_test
[0m14:29:00.305958 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6, now test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea)
[0m14:29:00.309107 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:29:00.307768 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff
[0m14:29:00.310128 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea
[0m14:29:00.314165 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff"
[0m14:29:00.373641 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea"
[0m14:29:00.377474 [debug] [Thread-1 (]: On test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select customer_id
from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_order`
where customer_id is null



  
  
      
    ) dbt_internal_test
[0m14:29:00.382692 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:29:00.386621 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea
[0m14:29:00.517248 [info ] [Thread-4 (]: 13 of 71 PASS not_null_fct_marketing_spend_date ................................ [[32mPASS[0m in 2.73s]
[0m14:29:00.520846 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719
[0m14:29:00.508823 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea"
[0m14:29:00.521668 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e
[0m14:29:00.524738 [debug] [Thread-2 (]: On test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select discount_total
from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_order`
where discount_total is null



  
  
      
    ) dbt_internal_test
[0m14:29:00.531329 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:29:00.523335 [info ] [Thread-4 (]: 17 of 71 START test not_null_fct_order_net_revenue ............................. [RUN]
[0m14:29:00.542668 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719, now test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e)
[0m14:29:00.543839 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e
[0m14:29:00.611556 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e"
[0m14:29:00.615389 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e
[0m14:29:00.619771 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e"
[0m14:29:00.621251 [debug] [Thread-4 (]: On test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select net_revenue
from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_order`
where net_revenue is null



  
  
      
    ) dbt_internal_test
[0m14:29:00.621835 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:29:01.465685 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:a0396533-2203-4827-848b-08e74e722380&page=queryresults
[0m14:29:01.664234 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:26cd3628-fe34-4c61-9cb3-8df3b99c608c&page=queryresults
[0m14:29:01.871746 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:2ba1cb02-a22c-4a74-a94e-43e9c61460ff&page=queryresults
[0m14:29:02.287394 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:9e20e357-ce67-4081-be5f-5a2778cc9068&page=queryresults
[0m14:29:02.375243 [info ] [Thread-3 (]: 14 of 71 PASS not_null_fct_marketing_spend_spend_try ........................... [[32mPASS[0m in 2.12s]
[0m14:29:02.378761 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d
[0m14:29:02.383784 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def
[0m14:29:02.387448 [info ] [Thread-3 (]: 18 of 71 START test not_null_fct_order_order_id ................................ [RUN]
[0m14:29:02.391709 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d, now test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def)
[0m14:29:02.393636 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def
[0m14:29:02.409747 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def"
[0m14:29:02.411457 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def
[0m14:29:02.418988 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def"
[0m14:29:02.423115 [debug] [Thread-3 (]: On test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select order_id
from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_order`
where order_id is null



  
  
      
    ) dbt_internal_test
[0m14:29:02.423786 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:29:02.576286 [info ] [Thread-1 (]: 15 of 71 PASS not_null_fct_order_customer_id ................................... [[32mPASS[0m in 2.28s]
[0m14:29:02.594922 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff
[0m14:29:02.596362 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_fct_shipment_order_id.8b34a6bb78
[0m14:29:02.603161 [info ] [Thread-1 (]: 19 of 71 START test not_null_fct_shipment_order_id ............................. [RUN]
[0m14:29:02.621456 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff, now test.data_pipeline_project.not_null_fct_shipment_order_id.8b34a6bb78)
[0m14:29:02.622057 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_fct_shipment_order_id.8b34a6bb78
[0m14:29:02.626640 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_fct_shipment_order_id.8b34a6bb78"
[0m14:29:02.632590 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_fct_shipment_order_id.8b34a6bb78
[0m14:29:02.640055 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_fct_shipment_order_id.8b34a6bb78"
[0m14:29:02.641200 [debug] [Thread-1 (]: On test.data_pipeline_project.not_null_fct_shipment_order_id.8b34a6bb78: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_fct_shipment_order_id.8b34a6bb78"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select order_id
from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_shipment`
where order_id is null



  
  
      
    ) dbt_internal_test
[0m14:29:02.642662 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:29:02.778730 [info ] [Thread-4 (]: 17 of 71 PASS not_null_fct_order_net_revenue ................................... [[32mPASS[0m in 2.24s]
[0m14:29:02.780134 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e
[0m14:29:02.781179 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d
[0m14:29:02.781949 [info ] [Thread-4 (]: 20 of 71 START test not_null_fct_shipment_shipment_id .......................... [RUN]
[0m14:29:02.782884 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e, now test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d)
[0m14:29:02.783872 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d
[0m14:29:02.793090 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d"
[0m14:29:02.794276 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d
[0m14:29:02.806640 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d"
[0m14:29:02.809791 [debug] [Thread-4 (]: On test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select shipment_id
from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_shipment`
where shipment_id is null



  
  
      
    ) dbt_internal_test
[0m14:29:02.811242 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:29:03.503099 [info ] [Thread-2 (]: 16 of 71 PASS not_null_fct_order_discount_total ................................ [[32mPASS[0m in 3.19s]
[0m14:29:03.507364 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea
[0m14:29:03.514527 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b
[0m14:29:03.517271 [info ] [Thread-2 (]: 21 of 71 START test not_null_int_shipments_decomposed_delivered_at ............. [RUN]
[0m14:29:03.521337 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea, now test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b)
[0m14:29:03.522558 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b
[0m14:29:03.539769 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b"
[0m14:29:03.544378 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b
[0m14:29:03.556190 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b"
[0m14:29:03.558114 [debug] [Thread-2 (]: On test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select delivered_at
from `data-pipeline-project-474812`.`analytics_staging_staging`.`int_shipments_decomposed`
where delivered_at is null



  
  
      
    ) dbt_internal_test
[0m14:29:03.559041 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:29:03.885698 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:530a0bcd-f7a7-46f1-9294-9de89cc0f66b&page=queryresults
[0m14:29:03.931278 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:894a9a83-da17-40b1-8723-52ed2f476926&page=queryresults
[0m14:29:04.096378 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:ad4d500c-dcd2-4b4d-81e2-20f8fc664d3a&page=queryresults
[0m14:29:04.808612 [info ] [Thread-4 (]: 20 of 71 PASS not_null_fct_shipment_shipment_id ................................ [[32mPASS[0m in 2.02s]
[0m14:29:04.812293 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d
[0m14:29:04.813686 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31
[0m14:29:04.814706 [info ] [Thread-4 (]: 22 of 71 START test not_null_int_shipments_decomposed_latest_status ............ [RUN]
[0m14:29:04.817402 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d, now test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31)
[0m14:29:04.820249 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31
[0m14:29:04.842205 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31"
[0m14:29:04.843664 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31
[0m14:29:04.856526 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31"
[0m14:29:04.862175 [debug] [Thread-4 (]: On test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select latest_status
from `data-pipeline-project-474812`.`analytics_staging_staging`.`int_shipments_decomposed`
where latest_status is null



  
  
      
    ) dbt_internal_test
[0m14:29:04.865028 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:29:04.954568 [info ] [Thread-3 (]: 18 of 71 PASS not_null_fct_order_order_id ...................................... [[32mPASS[0m in 2.54s]
[0m14:29:04.969435 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def
[0m14:29:04.979958 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b
[0m14:29:04.988315 [info ] [Thread-3 (]: 23 of 71 START test not_null_mart_acquisition_efficiency_date .................. [RUN]
[0m14:29:05.010568 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def, now test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b)
[0m14:29:05.015737 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b
[0m14:29:05.035303 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:c72a4b2a-edb5-4ce1-9a48-5d103fecf9dd&page=queryresults
[0m14:29:05.057623 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b"
[0m14:29:05.063958 [info ] [Thread-1 (]: 19 of 71 PASS not_null_fct_shipment_order_id ................................... [[32mPASS[0m in 2.44s]
[0m14:29:05.076627 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_fct_shipment_order_id.8b34a6bb78
[0m14:29:05.077869 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b
[0m14:29:05.079670 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23
[0m14:29:05.092602 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b"
[0m14:29:05.087855 [info ] [Thread-1 (]: 24 of 71 START test not_null_mart_revenue_daily_date ........................... [RUN]
[0m14:29:05.096638 [debug] [Thread-3 (]: On test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select date
from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_acquisition_efficiency`
where date is null



  
  
      
    ) dbt_internal_test
[0m14:29:05.104968 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:29:05.103134 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_fct_shipment_order_id.8b34a6bb78, now test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23)
[0m14:29:05.157547 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23
[0m14:29:05.165117 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23"
[0m14:29:05.166506 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23
[0m14:29:05.170050 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23"
[0m14:29:05.171460 [debug] [Thread-1 (]: On test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select date
from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_revenue_daily`
where date is null



  
  
      
    ) dbt_internal_test
[0m14:29:05.172051 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:29:05.958632 [warn ] [Thread-2 (]: 21 of 71 WARN 303 not_null_int_shipments_decomposed_delivered_at ............... [[33mWARN 303[0m in 2.43s]
[0m14:29:05.963720 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b
[0m14:29:05.971013 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322
[0m14:29:05.972377 [info ] [Thread-2 (]: 25 of 71 START test not_null_mart_subscription_daily_date ...................... [RUN]
[0m14:29:05.974474 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b, now test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322)
[0m14:29:05.975808 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322
[0m14:29:06.049891 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322"
[0m14:29:06.064440 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322
[0m14:29:06.105138 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322"
[0m14:29:06.120042 [debug] [Thread-2 (]: On test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select date
from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily`
where date is null



  
  
      
    ) dbt_internal_test
[0m14:29:06.124840 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:29:06.225885 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:08cad3b6-1920-4016-93bc-774909ea95fc&page=queryresults
[0m14:29:06.261489 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:7353131c-4a30-407d-b3ff-88dae8439e4a&page=queryresults
[0m14:29:06.315915 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:66a4cca7-7afb-4c69-87fc-9c9fab91a733&page=queryresults
[0m14:29:07.141008 [info ] [Thread-4 (]: 22 of 71 PASS not_null_int_shipments_decomposed_latest_status .................. [[32mPASS[0m in 2.32s]
[0m14:29:07.144786 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31
[0m14:29:07.161489 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501
[0m14:29:07.165433 [info ] [Thread-4 (]: 26 of 71 START test not_null_stg_addresses_address_id .......................... [RUN]
[0m14:29:07.173624 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31, now test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501)
[0m14:29:07.186403 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501
[0m14:29:07.196419 [info ] [Thread-3 (]: 23 of 71 PASS not_null_mart_acquisition_efficiency_date ........................ [[32mPASS[0m in 2.18s]
[0m14:29:07.244994 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b
[0m14:29:07.246283 [info ] [Thread-1 (]: 24 of 71 PASS not_null_mart_revenue_daily_date ................................. [[32mPASS[0m in 2.13s]
[0m14:29:07.259066 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3
[0m14:29:07.264250 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23
[0m14:29:07.252504 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501"
[0m14:29:07.269773 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2
[0m14:29:07.273334 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501
[0m14:29:07.268066 [info ] [Thread-3 (]: 27 of 71 START test not_null_stg_addresses_user_id ............................. [RUN]
[0m14:29:07.291271 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b, now test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3)
[0m14:29:07.275197 [info ] [Thread-1 (]: 28 of 71 START test not_null_stg_countries_country_id .......................... [RUN]
[0m14:29:07.313149 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3
[0m14:29:07.303883 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501"
[0m14:29:07.323158 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23, now test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2)
[0m14:29:07.332305 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:8d613765-7f92-4c09-ac0d-6c055dbb07f9&page=queryresults
[0m14:29:07.338096 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2
[0m14:29:07.364334 [debug] [Thread-4 (]: On test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select address_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_addresses`
where address_id is null



  
  
      
    ) dbt_internal_test
[0m14:29:07.373704 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2"
[0m14:29:07.386511 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:29:07.384241 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3"
[0m14:29:07.389832 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2
[0m14:29:07.478409 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3
[0m14:29:07.505766 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2"
[0m14:29:07.511415 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3"
[0m14:29:07.515388 [debug] [Thread-1 (]: On test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select country_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_countries`
where country_id is null



  
  
      
    ) dbt_internal_test
[0m14:29:07.527436 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:29:07.523445 [debug] [Thread-3 (]: On test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_addresses`
where user_id is null



  
  
      
    ) dbt_internal_test
[0m14:29:07.537309 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:29:08.347134 [info ] [Thread-2 (]: 25 of 71 PASS not_null_mart_subscription_daily_date ............................ [[32mPASS[0m in 2.37s]
[0m14:29:08.351507 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322
[0m14:29:08.356735 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64
[0m14:29:08.360001 [info ] [Thread-2 (]: 29 of 71 START test not_null_stg_orders_order_id ............................... [RUN]
[0m14:29:08.363624 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322, now test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64)
[0m14:29:08.366584 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64
[0m14:29:08.394408 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64"
[0m14:29:08.399410 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64
[0m14:29:08.434423 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64"
[0m14:29:08.444051 [debug] [Thread-2 (]: On test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select order_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_orders`
where order_id is null



  
  
      
    ) dbt_internal_test
[0m14:29:08.454126 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:29:08.768579 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:2f724ad8-2cb0-49f6-99ba-d13a7bbc73b1&page=queryresults
[0m14:29:08.932014 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:a3af6a85-d2c0-4a41-85c0-d6330409678b&page=queryresults
[0m14:29:08.942742 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:15721e8c-f228-4589-8e36-d88bdea7768f&page=queryresults
[0m14:29:09.752380 [info ] [Thread-4 (]: 26 of 71 PASS not_null_stg_addresses_address_id ................................ [[32mPASS[0m in 2.58s]
[0m14:29:09.756883 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501
[0m14:29:09.759061 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:3c380449-deae-415c-9bfc-7a9ca9e18c12&page=queryresults
[0m14:29:09.760268 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209
[0m14:29:09.771942 [info ] [Thread-4 (]: 30 of 71 START test not_null_stg_orders_user_id ................................ [RUN]
[0m14:29:09.774936 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501, now test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209)
[0m14:29:09.777192 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209
[0m14:29:09.794897 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209"
[0m14:29:09.801717 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209
[0m14:29:09.812462 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209"
[0m14:29:09.814936 [debug] [Thread-4 (]: On test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_orders`
where user_id is null



  
  
      
    ) dbt_internal_test
[0m14:29:09.817699 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:29:09.927476 [info ] [Thread-1 (]: 28 of 71 PASS not_null_stg_countries_country_id ................................ [[32mPASS[0m in 2.60s]
[0m14:29:09.937508 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2
[0m14:29:09.943125 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380
[0m14:29:09.947145 [info ] [Thread-1 (]: 31 of 71 START test not_null_stg_shipments_shipment_id ......................... [RUN]
[0m14:29:09.974420 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2, now test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380)
[0m14:29:09.976193 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380
[0m14:29:09.987396 [info ] [Thread-3 (]: 27 of 71 PASS not_null_stg_addresses_user_id ................................... [[32mPASS[0m in 2.69s]
[0m14:29:10.000521 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380"
[0m14:29:10.007783 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3
[0m14:29:10.020767 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852
[0m14:29:10.026956 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380
[0m14:29:10.023017 [info ] [Thread-3 (]: 32 of 71 START test not_null_stg_shipments_user_id ............................. [RUN]
[0m14:29:10.037858 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380"
[0m14:29:10.039553 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3, now test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852)
[0m14:29:10.042109 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852
[0m14:29:10.053841 [debug] [Thread-1 (]: On test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select shipment_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_shipments`
where shipment_id is null



  
  
      
    ) dbt_internal_test
[0m14:29:10.059320 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852"
[0m14:29:10.061201 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:29:10.062166 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852
[0m14:29:10.118625 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852"
[0m14:29:10.124328 [debug] [Thread-3 (]: On test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_shipments`
where user_id is null



  
  
      
    ) dbt_internal_test
[0m14:29:10.125544 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:29:10.715337 [info ] [Thread-2 (]: 29 of 71 PASS not_null_stg_orders_order_id ..................................... [[32mPASS[0m in 2.35s]
[0m14:29:10.719589 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64
[0m14:29:10.721302 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8
[0m14:29:10.728416 [info ] [Thread-2 (]: 33 of 71 START test not_null_stg_subscriptions_start_date ...................... [RUN]
[0m14:29:10.732877 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64, now test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8)
[0m14:29:10.734831 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8
[0m14:29:10.758639 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8"
[0m14:29:10.767898 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8
[0m14:29:10.787472 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8"
[0m14:29:10.791763 [debug] [Thread-2 (]: On test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select start_date
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_subscriptions`
where start_date is null



  
  
      
    ) dbt_internal_test
[0m14:29:10.794140 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:29:11.248507 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:e4dff34e-f80c-49cc-99c9-9f31d1fd9461&page=queryresults
[0m14:29:11.356478 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:881ee013-45fe-46e6-bb85-0e5e8392aa42&page=queryresults
[0m14:29:11.433225 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:f1b9be0b-e1e6-4f8b-82e1-4cbbc3f89577&page=queryresults
[0m14:29:12.090342 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:4a5ff330-ba75-4983-b12f-979661eaa318&page=queryresults
[0m14:29:12.120887 [info ] [Thread-4 (]: 30 of 71 PASS not_null_stg_orders_user_id ...................................... [[32mPASS[0m in 2.34s]
[0m14:29:12.125273 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209
[0m14:29:12.126648 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m14:29:12.129510 [info ] [Thread-4 (]: 34 of 71 START test not_null_stg_subscriptions_subscription_id ................. [RUN]
[0m14:29:12.134499 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209, now test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91)
[0m14:29:12.137589 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m14:29:12.153685 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91"
[0m14:29:12.155931 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m14:29:12.161792 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91"
[0m14:29:12.166504 [debug] [Thread-4 (]: On test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select subscription_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_subscriptions`
where subscription_id is null



  
  
      
    ) dbt_internal_test
[0m14:29:12.169362 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:29:12.303362 [info ] [Thread-1 (]: 31 of 71 PASS not_null_stg_shipments_shipment_id ............................... [[32mPASS[0m in 2.33s]
[0m14:29:12.305803 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380
[0m14:29:12.307041 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2
[0m14:29:12.309677 [info ] [Thread-1 (]: 35 of 71 START test not_null_stg_subscriptions_user_id ......................... [RUN]
[0m14:29:12.311782 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380, now test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2)
[0m14:29:12.315997 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2
[0m14:29:12.336208 [info ] [Thread-3 (]: 32 of 71 PASS not_null_stg_shipments_user_id ................................... [[32mPASS[0m in 2.30s]
[0m14:29:12.346729 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852
[0m14:29:12.354168 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2"
[0m14:29:12.355024 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25
[0m14:29:12.356513 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2
[0m14:29:12.360498 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2"
[0m14:29:12.357863 [info ] [Thread-3 (]: 36 of 71 START test not_null_stg_users_created_at .............................. [RUN]
[0m14:29:12.365149 [debug] [Thread-1 (]: On test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_subscriptions`
where user_id is null



  
  
      
    ) dbt_internal_test
[0m14:29:12.367223 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:29:12.366599 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852, now test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25)
[0m14:29:12.423769 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25
[0m14:29:12.442234 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25"
[0m14:29:12.448956 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25
[0m14:29:12.455390 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25"
[0m14:29:12.456562 [debug] [Thread-3 (]: On test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select created_at
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
where created_at is null



  
  
      
    ) dbt_internal_test
[0m14:29:12.457395 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:29:13.084229 [info ] [Thread-2 (]: 33 of 71 PASS not_null_stg_subscriptions_start_date ............................ [[32mPASS[0m in 2.35s]
[0m14:29:13.086418 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8
[0m14:29:13.091378 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77
[0m14:29:13.099262 [info ] [Thread-2 (]: 37 of 71 START test not_null_stg_users_user_id ................................. [RUN]
[0m14:29:13.102145 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8, now test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77)
[0m14:29:13.104662 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77
[0m14:29:13.136095 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77"
[0m14:29:13.138355 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77
[0m14:29:13.156134 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77"
[0m14:29:13.159199 [debug] [Thread-2 (]: On test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
where user_id is null



  
  
      
    ) dbt_internal_test
[0m14:29:13.161393 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:29:13.464539 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:6398c33c-5b82-4bbc-ad14-705d071d013e&page=queryresults
[0m14:29:13.687547 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:afb75a30-be3b-4919-918f-7492704d63fa&page=queryresults
[0m14:29:13.710829 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:6c7e5452-f06e-4c1a-84b1-8048e3718f09&page=queryresults
[0m14:29:14.439241 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:4435eaf0-72b7-4141-aa58-4c856fac317a&page=queryresults
[0m14:29:14.577662 [info ] [Thread-4 (]: 34 of 71 PASS not_null_stg_subscriptions_subscription_id ....................... [[32mPASS[0m in 2.44s]
[0m14:29:14.580464 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m14:29:14.584177 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29
[0m14:29:14.585461 [info ] [Thread-4 (]: 38 of 71 START test relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_  [RUN]
[0m14:29:14.588344 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91, now test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29)
[0m14:29:14.589829 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29
[0m14:29:14.603972 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29"
[0m14:29:14.606410 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29
[0m14:29:14.622156 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29"
[0m14:29:14.634916 [debug] [Thread-4 (]: On test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select customer_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_subscription`
    where customer_id is not null
),

parent as (
    select customer_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_customer`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m14:29:14.647556 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:29:14.651687 [info ] [Thread-3 (]: 36 of 71 PASS not_null_stg_users_created_at .................................... [[32mPASS[0m in 2.28s]
[0m14:29:14.708418 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25
[0m14:29:14.709395 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85
[0m14:29:14.710304 [info ] [Thread-3 (]: 39 of 71 START test relationships_fct_order_customer_id__customer_id__ref_dim_customer_  [RUN]
[0m14:29:14.717304 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25, now test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85)
[0m14:29:14.718172 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85
[0m14:29:14.715617 [info ] [Thread-1 (]: 35 of 71 PASS not_null_stg_subscriptions_user_id ............................... [[32mPASS[0m in 2.40s]
[0m14:29:14.733958 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85"
[0m14:29:14.737916 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2
[0m14:29:14.748422 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.relationships_fct_shipment_order_id__order_id__ref_fct_order_.6096f95e39
[0m14:29:14.758248 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85
[0m14:29:14.753481 [info ] [Thread-1 (]: 40 of 71 START test relationships_fct_shipment_order_id__order_id__ref_fct_order_  [RUN]
[0m14:29:14.772094 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2, now test.data_pipeline_project.relationships_fct_shipment_order_id__order_id__ref_fct_order_.6096f95e39)
[0m14:29:14.774400 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.relationships_fct_shipment_order_id__order_id__ref_fct_order_.6096f95e39
[0m14:29:14.806302 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_fct_shipment_order_id__order_id__ref_fct_order_.6096f95e39"
[0m14:29:14.816492 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85"
[0m14:29:14.831387 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.relationships_fct_shipment_order_id__order_id__ref_fct_order_.6096f95e39
[0m14:29:14.835244 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_fct_shipment_order_id__order_id__ref_fct_order_.6096f95e39"
[0m14:29:14.836299 [debug] [Thread-1 (]: On test.data_pipeline_project.relationships_fct_shipment_order_id__order_id__ref_fct_order_.6096f95e39: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_fct_shipment_order_id__order_id__ref_fct_order_.6096f95e39"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select order_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_shipment`
    where order_id is not null
),

parent as (
    select order_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_order`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m14:29:14.838238 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:29:14.837580 [debug] [Thread-3 (]: On test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select customer_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_order`
    where customer_id is not null
),

parent as (
    select customer_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_customer`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m14:29:14.890396 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:29:15.371146 [info ] [Thread-2 (]: 37 of 71 PASS not_null_stg_users_user_id ....................................... [[32mPASS[0m in 2.26s]
[0m14:29:15.375990 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77
[0m14:29:15.377747 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781
[0m14:29:15.380000 [info ] [Thread-2 (]: 41 of 71 START test relationships_stg_addresses_user_id__user_id__ref_stg_users_  [RUN]
[0m14:29:15.381634 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77, now test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781)
[0m14:29:15.382999 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781
[0m14:29:15.398937 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781"
[0m14:29:15.401156 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781
[0m14:29:15.421556 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781"
[0m14:29:15.427330 [debug] [Thread-2 (]: On test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select user_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_addresses`
    where user_id is not null
),

parent as (
    select user_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m14:29:15.430233 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:29:15.767856 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:3305e550-b30c-4952-81f8-1f8851b0ff4a&page=queryresults
[0m14:29:15.990221 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:5c89a7e9-7924-4ac7-8aa4-2aea875f53a1&page=queryresults
[0m14:29:16.138698 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:7d1c60fd-3fe8-4582-bc81-d3e70379d3ef&page=queryresults
[0m14:29:16.753396 [info ] [Thread-4 (]: 38 of 71 PASS relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_  [[32mPASS[0m in 2.16s]
[0m14:29:16.756598 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29
[0m14:29:16.759598 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737
[0m14:29:16.766313 [info ] [Thread-4 (]: 42 of 71 START test relationships_stg_orders_user_id__user_id__ref_stg_users_ .. [RUN]
[0m14:29:16.769358 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29, now test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737)
[0m14:29:16.772059 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737
[0m14:29:16.826192 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737"
[0m14:29:16.830822 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737
[0m14:29:16.850198 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737"
[0m14:29:16.857428 [debug] [Thread-4 (]: On test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select user_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_orders`
    where user_id is not null
),

parent as (
    select user_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m14:29:16.864346 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:29:16.988977 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:a17d29b1-85a1-41ea-b556-8269a975f1ea&page=queryresults
[0m14:29:16.989913 [error] [Thread-1 (]: 40 of 71 FAIL 476 relationships_fct_shipment_order_id__order_id__ref_fct_order_  [[31mFAIL 476[0m in 2.21s]
[0m14:29:16.995477 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.relationships_fct_shipment_order_id__order_id__ref_fct_order_.6096f95e39
[0m14:29:17.001227 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8
[0m14:29:17.004443 [info ] [Thread-1 (]: 43 of 71 START test relationships_stg_shipments_user_id__user_id__ref_stg_users_  [RUN]
[0m14:29:17.007786 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_fct_shipment_order_id__order_id__ref_fct_order_.6096f95e39, now test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8)
[0m14:29:17.009153 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8
[0m14:29:17.028096 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8"
[0m14:29:17.032657 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8
[0m14:29:17.042190 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8"
[0m14:29:17.043939 [debug] [Thread-1 (]: On test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select user_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_shipments`
    where user_id is not null
),

parent as (
    select user_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m14:29:17.045757 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:29:17.100235 [info ] [Thread-3 (]: 39 of 71 PASS relationships_fct_order_customer_id__customer_id__ref_dim_customer_  [[32mPASS[0m in 2.38s]
[0m14:29:17.104009 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85
[0m14:29:17.104645 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m14:29:17.107189 [info ] [Thread-3 (]: 44 of 71 START test relationships_stg_subscriptions_user_id__user_id__ref_stg_users_  [RUN]
[0m14:29:17.109114 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85, now test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e)
[0m14:29:17.109628 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m14:29:17.132529 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e"
[0m14:29:17.133464 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m14:29:17.140574 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e"
[0m14:29:17.144566 [debug] [Thread-3 (]: On test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select user_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_subscriptions`
    where user_id is not null
),

parent as (
    select user_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m14:29:17.145320 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:29:17.857400 [info ] [Thread-2 (]: 41 of 71 PASS relationships_stg_addresses_user_id__user_id__ref_stg_users_ ..... [[32mPASS[0m in 2.47s]
[0m14:29:17.861224 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781
[0m14:29:17.865381 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6
[0m14:29:17.869050 [info ] [Thread-2 (]: 45 of 71 START test source_not_null_raw_data_addresses__id ..................... [RUN]
[0m14:29:17.871912 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781, now test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6)
[0m14:29:17.873693 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6
[0m14:29:17.888852 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6"
[0m14:29:17.890331 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6
[0m14:29:17.897716 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6"
[0m14:29:17.907807 [debug] [Thread-2 (]: On test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`addresses`
where _id is null



  
  
      
    ) dbt_internal_test
[0m14:29:17.910059 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:29:18.432207 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:0af0660c-db35-4eed-80b2-542378f181d4&page=queryresults
[0m14:29:18.512581 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:09e5d24d-d5fb-4022-9f32-e27536872cca&page=queryresults
[0m14:29:18.609309 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:6aaa2b31-b31b-45e2-b419-e0c2657c0e3e&page=queryresults
[0m14:29:19.098789 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:72fad884-c24b-4cd9-9ac2-b7ec2eba0ebd&page=queryresults
[0m14:29:19.442593 [info ] [Thread-1 (]: 43 of 71 PASS relationships_stg_shipments_user_id__user_id__ref_stg_users_ ..... [[32mPASS[0m in 2.43s]
[0m14:29:19.450907 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8
[0m14:29:19.457232 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc
[0m14:29:19.461108 [info ] [Thread-1 (]: 46 of 71 START test source_not_null_raw_data_cities__id ........................ [RUN]
[0m14:29:19.462610 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8, now test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc)
[0m14:29:19.463592 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc
[0m14:29:19.474061 [info ] [Thread-3 (]: 44 of 71 PASS relationships_stg_subscriptions_user_id__user_id__ref_stg_users_ . [[32mPASS[0m in 2.36s]
[0m14:29:19.479209 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m14:29:19.481356 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc"
[0m14:29:19.482380 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd
[0m14:29:19.486101 [info ] [Thread-3 (]: 47 of 71 START test source_not_null_raw_data_countries__id ..................... [RUN]
[0m14:29:19.486817 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e, now test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd)
[0m14:29:19.488354 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd
[0m14:29:19.494037 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd"
[0m14:29:19.488850 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc
[0m14:29:19.503654 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc"
[0m14:29:19.506507 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd
[0m14:29:19.509708 [debug] [Thread-1 (]: On test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`cities`
where _id is null



  
  
      
    ) dbt_internal_test
[0m14:29:19.516010 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd"
[0m14:29:19.517250 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:29:19.523716 [debug] [Thread-3 (]: On test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`countries`
where _id is null



  
  
      
    ) dbt_internal_test
[0m14:29:19.522822 [info ] [Thread-4 (]: 42 of 71 PASS relationships_stg_orders_user_id__user_id__ref_stg_users_ ........ [[32mPASS[0m in 2.75s]
[0m14:29:19.569354 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737
[0m14:29:19.569975 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be
[0m14:29:19.568037 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:29:19.570603 [info ] [Thread-4 (]: 48 of 71 START test source_not_null_raw_data_neighborhoods__id ................. [RUN]
[0m14:29:19.585566 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737, now test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be)
[0m14:29:19.604499 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be
[0m14:29:19.697233 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be"
[0m14:29:19.704194 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be
[0m14:29:19.714579 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be"
[0m14:29:19.719367 [debug] [Thread-4 (]: On test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`neighborhoods`
where _id is null



  
  
      
    ) dbt_internal_test
[0m14:29:19.721148 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:29:20.088042 [info ] [Thread-2 (]: 45 of 71 PASS source_not_null_raw_data_addresses__id ........................... [[32mPASS[0m in 2.22s]
[0m14:29:20.089130 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6
[0m14:29:20.090039 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878
[0m14:29:20.091092 [info ] [Thread-2 (]: 49 of 71 START test source_not_null_raw_data_orders__id ........................ [RUN]
[0m14:29:20.092085 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6, now test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878)
[0m14:29:20.092574 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878
[0m14:29:20.097371 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878"
[0m14:29:20.098148 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878
[0m14:29:20.101712 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878"
[0m14:29:20.103909 [debug] [Thread-2 (]: On test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`orders`
where _id is null



  
  
      
    ) dbt_internal_test
[0m14:29:20.104733 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:29:20.726398 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:a5a7e880-35b0-4f13-8ee6-84a04b243582&page=queryresults
[0m14:29:20.930691 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:5f730b67-1539-48a0-b5be-7e4b9cdefe02&page=queryresults
[0m14:29:20.945091 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:53182dea-7427-4693-b19c-133ad40694c8&page=queryresults
[0m14:29:21.122300 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:479bcddc-6a60-417e-9491-26418050b685&page=queryresults
[0m14:29:21.584808 [info ] [Thread-1 (]: 46 of 71 PASS source_not_null_raw_data_cities__id .............................. [[32mPASS[0m in 2.12s]
[0m14:29:21.589480 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc
[0m14:29:21.592445 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f
[0m14:29:21.594434 [info ] [Thread-1 (]: 50 of 71 START test source_not_null_raw_data_orders__user ...................... [RUN]
[0m14:29:21.596550 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc, now test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f)
[0m14:29:21.597610 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f
[0m14:29:21.607540 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f"
[0m14:29:21.613309 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f
[0m14:29:21.619013 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f"
[0m14:29:21.620139 [debug] [Thread-1 (]: On test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _user
from `data-pipeline-project-474812`.`raw_data`.`orders`
where _user is null



  
  
      
    ) dbt_internal_test
[0m14:29:21.620782 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:29:21.811799 [info ] [Thread-4 (]: 48 of 71 PASS source_not_null_raw_data_neighborhoods__id ....................... [[32mPASS[0m in 2.23s]
[0m14:29:21.813720 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be
[0m14:29:21.814297 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2
[0m14:29:21.814819 [info ] [Thread-4 (]: 51 of 71 START test source_not_null_raw_data_shipments__id ..................... [RUN]
[0m14:29:21.815356 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be, now test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2)
[0m14:29:21.815767 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2
[0m14:29:21.829688 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2"
[0m14:29:21.833265 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2
[0m14:29:21.843438 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2"
[0m14:29:21.864551 [debug] [Thread-4 (]: On test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`shipments`
where _id is null



  
  
      
    ) dbt_internal_test
[0m14:29:21.862846 [info ] [Thread-3 (]: 47 of 71 PASS source_not_null_raw_data_countries__id ........................... [[32mPASS[0m in 2.37s]
[0m14:29:21.866790 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:29:21.867613 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd
[0m14:29:21.904448 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1
[0m14:29:21.910074 [info ] [Thread-3 (]: 52 of 71 START test source_not_null_raw_data_shipments__order .................. [RUN]
[0m14:29:21.913061 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd, now test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1)
[0m14:29:21.913625 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1
[0m14:29:21.924408 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1"
[0m14:29:21.926359 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1
[0m14:29:21.930291 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1"
[0m14:29:21.931507 [debug] [Thread-3 (]: On test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _order
from `data-pipeline-project-474812`.`raw_data`.`shipments`
where _order is null



  
  
      
    ) dbt_internal_test
[0m14:29:21.932321 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:29:21.997514 [info ] [Thread-2 (]: 49 of 71 PASS source_not_null_raw_data_orders__id .............................. [[32mPASS[0m in 1.90s]
[0m14:29:21.999135 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878
[0m14:29:22.001416 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98
[0m14:29:22.004775 [info ] [Thread-2 (]: 53 of 71 START test source_not_null_raw_data_shipments__user ................... [RUN]
[0m14:29:22.007635 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878, now test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98)
[0m14:29:22.009411 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98
[0m14:29:22.016625 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98"
[0m14:29:22.019120 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98
[0m14:29:22.027315 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98"
[0m14:29:22.028125 [debug] [Thread-2 (]: On test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _user
from `data-pipeline-project-474812`.`raw_data`.`shipments`
where _user is null



  
  
      
    ) dbt_internal_test
[0m14:29:22.030911 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:29:22.820886 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:ee863303-6c97-4901-8b94-371c30b25116&page=queryresults
[0m14:29:22.892394 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:b996ff6b-3515-4454-9900-affc0cd24a24&page=queryresults
[0m14:29:22.967697 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:d4732839-c370-467b-bccb-681732c62ab0&page=queryresults
[0m14:29:23.193240 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:e933efaf-fda6-4b65-860a-0969d0987c8c&page=queryresults
[0m14:29:23.693191 [info ] [Thread-1 (]: 50 of 71 PASS source_not_null_raw_data_orders__user ............................ [[32mPASS[0m in 2.10s]
[0m14:29:23.695353 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f
[0m14:29:23.696011 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86
[0m14:29:23.697000 [info ] [Thread-1 (]: 54 of 71 START test source_not_null_raw_data_states__id ........................ [RUN]
[0m14:29:23.697749 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f, now test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86)
[0m14:29:23.698617 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86
[0m14:29:23.707796 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86"
[0m14:29:23.709410 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86
[0m14:29:23.712529 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86"
[0m14:29:23.714004 [debug] [Thread-1 (]: On test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`states`
where _id is null



  
  
      
    ) dbt_internal_test
[0m14:29:23.714894 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:29:23.785913 [info ] [Thread-4 (]: 51 of 71 PASS source_not_null_raw_data_shipments__id ........................... [[32mPASS[0m in 1.97s]
[0m14:29:23.790225 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2
[0m14:29:23.791799 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a
[0m14:29:23.793236 [info ] [Thread-4 (]: 55 of 71 START test source_not_null_raw_data_subscriptions__id ................. [RUN]
[0m14:29:23.794308 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2, now test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a)
[0m14:29:23.795143 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a
[0m14:29:23.804434 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a"
[0m14:29:23.805915 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a
[0m14:29:23.814600 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a"
[0m14:29:23.832104 [debug] [Thread-4 (]: On test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`subscriptions`
where _id is null



  
  
      
    ) dbt_internal_test
[0m14:29:23.833362 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:29:23.828654 [info ] [Thread-3 (]: 52 of 71 PASS source_not_null_raw_data_shipments__order ........................ [[32mPASS[0m in 1.91s]
[0m14:29:23.886518 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1
[0m14:29:23.895290 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6
[0m14:29:23.908338 [info ] [Thread-3 (]: 56 of 71 START test source_not_null_raw_data_subscriptions__user ............... [RUN]
[0m14:29:23.921679 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1, now test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6)
[0m14:29:23.926092 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6
[0m14:29:23.942127 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6"
[0m14:29:23.949512 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6
[0m14:29:23.956550 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6"
[0m14:29:23.958070 [debug] [Thread-3 (]: On test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _user
from `data-pipeline-project-474812`.`raw_data`.`subscriptions`
where _user is null



  
  
      
    ) dbt_internal_test
[0m14:29:23.960142 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:29:24.031576 [info ] [Thread-2 (]: 53 of 71 PASS source_not_null_raw_data_shipments__user ......................... [[32mPASS[0m in 2.02s]
[0m14:29:24.034000 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98
[0m14:29:24.034683 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b
[0m14:29:24.038200 [info ] [Thread-2 (]: 57 of 71 START test source_not_null_raw_data_users__id ......................... [RUN]
[0m14:29:24.039508 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98, now test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b)
[0m14:29:24.040265 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b
[0m14:29:24.045434 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b"
[0m14:29:24.054669 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b
[0m14:29:24.063036 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b"
[0m14:29:24.066206 [debug] [Thread-2 (]: On test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`users`
where _id is null



  
  
      
    ) dbt_internal_test
[0m14:29:24.067875 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:29:24.988277 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:a2af4831-6d0a-45f4-9977-8d189c235d69&page=queryresults
[0m14:29:25.099581 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:1b3a39fc-ce51-4597-b8ad-31e6f32bfa17&page=queryresults
[0m14:29:25.166023 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:d182915d-7301-4b8b-bf95-64a37e81666d&page=queryresults
[0m14:29:25.234937 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:b94b9a17-1aeb-42c5-bdf3-218e95321390&page=queryresults
[0m14:29:25.993093 [info ] [Thread-3 (]: 56 of 71 PASS source_not_null_raw_data_subscriptions__user ..................... [[32mPASS[0m in 2.07s]
[0m14:29:25.994416 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6
[0m14:29:25.996324 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1
[0m14:29:25.997691 [info ] [Thread-3 (]: 58 of 71 START test unique_dim_customer_customer_id ............................ [RUN]
[0m14:29:25.999344 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6, now test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1)
[0m14:29:26.000327 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1
[0m14:29:26.015137 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1"
[0m14:29:26.016151 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1
[0m14:29:26.387393 [info ] [Thread-1 (]: 54 of 71 PASS source_not_null_raw_data_states__id .............................. [[32mPASS[0m in 2.69s]
[0m14:29:26.404041 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1"
[0m14:29:26.407667 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86
[0m14:29:26.415362 [debug] [Thread-3 (]: On test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select customer_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_customer`
  where customer_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:29:26.416952 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469
[0m14:29:26.411633 [info ] [Thread-4 (]: 55 of 71 PASS source_not_null_raw_data_subscriptions__id ....................... [[32mPASS[0m in 2.62s]
[0m14:29:26.423059 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a
[0m14:29:26.413348 [info ] [Thread-2 (]: 57 of 71 PASS source_not_null_raw_data_users__id ............................... [[32mPASS[0m in 2.37s]
[0m14:29:26.419185 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:29:26.428755 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b
[0m14:29:26.421003 [info ] [Thread-1 (]: 59 of 71 START test unique_dim_subscription_subscription_id .................... [RUN]
[0m14:29:26.425512 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.unique_fct_marketing_spend_date.e93d255723
[0m14:29:26.490395 [info ] [Thread-4 (]: 60 of 71 START test unique_fct_marketing_spend_date ............................ [RUN]
[0m14:29:26.489388 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86, now test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469)
[0m14:29:26.487980 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.unique_fct_order_order_id.653705d6a5
[0m14:29:26.492056 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469
[0m14:29:26.491269 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a, now test.data_pipeline_project.unique_fct_marketing_spend_date.e93d255723)
[0m14:29:26.499165 [info ] [Thread-2 (]: 61 of 71 START test unique_fct_order_order_id .................................. [RUN]
[0m14:29:26.517786 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469"
[0m14:29:26.522443 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.unique_fct_marketing_spend_date.e93d255723
[0m14:29:26.523509 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b, now test.data_pipeline_project.unique_fct_order_order_id.653705d6a5)
[0m14:29:26.527954 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.unique_fct_marketing_spend_date.e93d255723"
[0m14:29:26.529208 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469
[0m14:29:26.530429 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.unique_fct_order_order_id.653705d6a5
[0m14:29:26.531212 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.unique_fct_marketing_spend_date.e93d255723
[0m14:29:26.538112 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469"
[0m14:29:26.543460 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.unique_fct_order_order_id.653705d6a5"
[0m14:29:26.561104 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.unique_fct_order_order_id.653705d6a5
[0m14:29:26.557171 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.unique_fct_marketing_spend_date.e93d255723"
[0m14:29:26.563579 [debug] [Thread-1 (]: On test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select subscription_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_subscription`
  where subscription_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:29:26.576102 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.unique_fct_order_order_id.653705d6a5"
[0m14:29:26.580451 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:29:26.582204 [debug] [Thread-4 (]: On test.data_pipeline_project.unique_fct_marketing_spend_date.e93d255723: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_fct_marketing_spend_date.e93d255723"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select date as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_marketing_spend`
  where date is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:29:26.633044 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:29:26.583422 [debug] [Thread-2 (]: On test.data_pipeline_project.unique_fct_order_order_id.653705d6a5: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_fct_order_order_id.653705d6a5"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select order_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_order`
  where order_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:29:26.683197 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:29:27.644247 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:ac320826-320b-4340-b8ac-0050fb608019&page=queryresults
[0m14:29:27.787226 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:d3dab86f-d6c6-4ee3-b185-95532d179d94&page=queryresults
[0m14:29:27.818683 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:73126ba0-2c4e-46ac-a930-0b86986ca032&page=queryresults
[0m14:29:27.828644 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:8c9855ec-d9ab-489e-90a5-8d4988df7697&page=queryresults
[0m14:29:28.542623 [error] [Thread-3 (]: 58 of 71 FAIL 16 unique_dim_customer_customer_id ............................... [[31mFAIL 16[0m in 2.54s]
[0m14:29:28.545274 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1
[0m14:29:28.547143 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3
[0m14:29:28.548588 [info ] [Thread-3 (]: 62 of 71 START test unique_fct_shipment_shipment_id ............................ [RUN]
[0m14:29:28.549952 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1, now test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3)
[0m14:29:28.551848 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3
[0m14:29:28.570782 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3"
[0m14:29:28.573977 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3
[0m14:29:28.580073 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3"
[0m14:29:28.581885 [debug] [Thread-3 (]: On test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select shipment_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_shipment`
  where shipment_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:29:28.583071 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:29:28.653484 [info ] [Thread-2 (]: 61 of 71 PASS unique_fct_order_order_id ........................................ [[32mPASS[0m in 2.13s]
[0m14:29:28.657315 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.unique_fct_order_order_id.653705d6a5
[0m14:29:28.666527 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd
[0m14:29:28.671481 [info ] [Thread-2 (]: 63 of 71 START test unique_mart_acquisition_efficiency_date .................... [RUN]
[0m14:29:28.679275 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_fct_order_order_id.653705d6a5, now test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd)
[0m14:29:28.681859 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd
[0m14:29:28.790794 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd"
[0m14:29:28.766652 [info ] [Thread-1 (]: 59 of 71 PASS unique_dim_subscription_subscription_id .......................... [[32mPASS[0m in 2.22s]
[0m14:29:28.806318 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469
[0m14:29:28.814146 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd
[0m14:29:28.803376 [error] [Thread-4 (]: 60 of 71 FAIL 8 unique_fct_marketing_spend_date ................................ [[31mFAIL 8[0m in 2.31s]
[0m14:29:28.817212 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8
[0m14:29:28.827996 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd"
[0m14:29:28.829489 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.unique_fct_marketing_spend_date.e93d255723
[0m14:29:28.832115 [debug] [Thread-2 (]: On test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select date as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_acquisition_efficiency`
  where date is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:29:28.830599 [info ] [Thread-1 (]: 64 of 71 START test unique_mart_revenue_daily_date ............................. [RUN]
[0m14:29:28.835179 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c
[0m14:29:28.837147 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:29:28.841524 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469, now test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8)
[0m14:29:28.852906 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8
[0m14:29:28.861067 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8"
[0m14:29:28.843113 [info ] [Thread-4 (]: 65 of 71 START test unique_mart_subscription_daily_date ........................ [RUN]
[0m14:29:28.912914 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_fct_marketing_spend_date.e93d255723, now test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c)
[0m14:29:28.915080 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c
[0m14:29:28.919797 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8
[0m14:29:28.941989 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8"
[0m14:29:28.946810 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c"
[0m14:29:28.951342 [debug] [Thread-1 (]: On test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select date as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_revenue_daily`
  where date is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:29:28.954825 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:29:28.970170 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c
[0m14:29:29.042387 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c"
[0m14:29:29.046701 [debug] [Thread-4 (]: On test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select date as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily`
  where date is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:29:29.049938 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:29:29.916791 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:854ce998-653a-4ebd-8c10-eb00e429970d&page=queryresults
[0m14:29:29.998557 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:ef46f56a-b5ea-4c51-b68a-7f6b89e96232&page=queryresults
[0m14:29:30.018604 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:98f3d951-885c-4bc6-93a9-42d7d182aef3&page=queryresults
[0m14:29:30.090927 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:913cf763-7e3f-4d92-831b-a45fee9e9484&page=queryresults
[0m14:29:30.813782 [info ] [Thread-3 (]: 62 of 71 PASS unique_fct_shipment_shipment_id .................................. [[32mPASS[0m in 2.26s]
[0m14:29:30.815239 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3
[0m14:29:30.816583 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00
[0m14:29:30.818487 [info ] [Thread-3 (]: 66 of 71 START test unique_stg_addresses_address_id ............................ [RUN]
[0m14:29:30.819994 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3, now test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00)
[0m14:29:30.820957 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00
[0m14:29:30.827175 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00"
[0m14:29:30.834453 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00
[0m14:29:30.843335 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00"
[0m14:29:30.844496 [debug] [Thread-3 (]: On test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select address_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_addresses`
  where address_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:29:30.845291 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:29:30.962930 [info ] [Thread-4 (]: 65 of 71 PASS unique_mart_subscription_daily_date .............................. [[32mPASS[0m in 2.05s]
[0m14:29:30.996737 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c
[0m14:29:30.983470 [info ] [Thread-1 (]: 64 of 71 PASS unique_mart_revenue_daily_date ................................... [[32mPASS[0m in 2.14s]
[0m14:29:31.006700 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.unique_stg_countries_country_id.8679936442
[0m14:29:31.013673 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8
[0m14:29:31.017249 [info ] [Thread-4 (]: 67 of 71 START test unique_stg_countries_country_id ............................ [RUN]
[0m14:29:31.078029 [info ] [Thread-2 (]: 63 of 71 PASS unique_mart_acquisition_efficiency_date .......................... [[32mPASS[0m in 2.38s]
[0m14:29:31.045897 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a
[0m14:29:31.081830 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c, now test.data_pipeline_project.unique_stg_countries_country_id.8679936442)
[0m14:29:31.084085 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd
[0m14:29:31.085822 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.unique_stg_countries_country_id.8679936442
[0m14:29:31.087330 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c
[0m14:29:31.085129 [info ] [Thread-1 (]: 68 of 71 START test unique_stg_orders_order_id ................................. [RUN]
[0m14:29:31.111599 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8, now test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a)
[0m14:29:31.112054 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a
[0m14:29:31.116195 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a"
[0m14:29:31.109958 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.unique_stg_countries_country_id.8679936442"
[0m14:29:31.110950 [info ] [Thread-2 (]: 69 of 71 START test unique_stg_shipments_shipment_id ........................... [RUN]
[0m14:29:31.122098 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd, now test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c)
[0m14:29:31.124945 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c
[0m14:29:31.131210 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c"
[0m14:29:31.132857 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.unique_stg_countries_country_id.8679936442
[0m14:29:31.134485 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a
[0m14:29:31.142858 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c
[0m14:29:31.142011 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.unique_stg_countries_country_id.8679936442"
[0m14:29:31.162291 [debug] [Thread-4 (]: On test.data_pipeline_project.unique_stg_countries_country_id.8679936442: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_stg_countries_country_id.8679936442"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select country_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_countries`
  where country_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:29:31.159246 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c"
[0m14:29:31.151915 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a"
[0m14:29:31.165946 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:29:31.167929 [debug] [Thread-2 (]: On test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select shipment_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_shipments`
  where shipment_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:29:31.168757 [debug] [Thread-1 (]: On test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select order_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_orders`
  where order_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:29:31.226617 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:29:31.230505 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:29:32.417200 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:b5d001f1-0402-4a4f-bf95-03ddf3d7e704&page=queryresults
[0m14:29:32.511523 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:937ef42b-5bd8-488f-ab67-cfc30cff3888&page=queryresults
[0m14:29:32.541398 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:86aa6381-4e3c-4f41-8d57-d160f629233c&page=queryresults
[0m14:29:32.790568 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:e1ff55f7-de5f-4a0d-b1ac-6b8a00bde928&page=queryresults
[0m14:29:33.344164 [info ] [Thread-3 (]: 66 of 71 PASS unique_stg_addresses_address_id .................................. [[32mPASS[0m in 2.52s]
[0m14:29:33.346053 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00
[0m14:29:33.347322 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m14:29:33.348130 [info ] [Thread-3 (]: 70 of 71 START test unique_stg_subscriptions_subscription_id ................... [RUN]
[0m14:29:33.349954 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00, now test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e)
[0m14:29:33.351933 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m14:29:33.368437 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e"
[0m14:29:33.369705 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m14:29:33.392744 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e"
[0m14:29:33.410021 [debug] [Thread-3 (]: On test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select subscription_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_subscriptions`
  where subscription_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:29:33.412165 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:29:33.409177 [info ] [Thread-2 (]: 69 of 71 PASS unique_stg_shipments_shipment_id ................................. [[32mPASS[0m in 2.29s]
[0m14:29:33.421203 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c
[0m14:29:33.416920 [info ] [Thread-1 (]: 68 of 71 PASS unique_stg_orders_order_id ....................................... [[32mPASS[0m in 2.30s]
[0m14:29:33.466201 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a
[0m14:29:33.459763 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b
[0m14:29:33.469865 [info ] [Thread-2 (]: 71 of 71 START test unique_stg_users_user_id ................................... [RUN]
[0m14:29:33.471284 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c, now test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b)
[0m14:29:33.473596 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b
[0m14:29:33.491852 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b"
[0m14:29:33.495819 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b
[0m14:29:33.522596 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b"
[0m14:29:33.534161 [debug] [Thread-2 (]: On test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select user_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
  where user_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:29:33.535731 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:29:33.684013 [info ] [Thread-4 (]: 67 of 71 PASS unique_stg_countries_country_id .................................. [[32mPASS[0m in 2.60s]
[0m14:29:33.685396 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.unique_stg_countries_country_id.8679936442
[0m14:29:34.732612 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:c4d68781-d5c0-474c-bde3-53d4de0f80c2&page=queryresults
[0m14:29:34.812864 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:7467e914-fd3d-4467-adcf-6f1daee77aeb&page=queryresults
[0m14:29:35.587226 [info ] [Thread-3 (]: 70 of 71 PASS unique_stg_subscriptions_subscription_id ......................... [[32mPASS[0m in 2.24s]
[0m14:29:35.589270 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m14:29:35.739183 [info ] [Thread-2 (]: 71 of 71 PASS unique_stg_users_user_id ......................................... [[32mPASS[0m in 2.26s]
[0m14:29:35.741980 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b
[0m14:29:35.755502 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:29:35.762598 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:29:35.763888 [debug] [MainThread]: Connection 'test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a' was properly closed.
[0m14:29:35.768050 [debug] [MainThread]: Connection 'test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b' was properly closed.
[0m14:29:35.770420 [debug] [MainThread]: Connection 'test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e' was properly closed.
[0m14:29:35.774976 [debug] [MainThread]: Connection 'test.data_pipeline_project.unique_stg_countries_country_id.8679936442' was properly closed.
[0m14:29:35.778816 [info ] [MainThread]: 
[0m14:29:35.783362 [info ] [MainThread]: Finished running 71 data tests in 0 hours 0 minutes and 39.83 seconds (39.83s).
[0m14:29:35.815653 [debug] [MainThread]: Command end result
[0m14:29:35.876688 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m14:29:35.891462 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m14:29:35.904516 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m14:29:35.905892 [info ] [MainThread]: 
[0m14:29:35.906741 [info ] [MainThread]: [31mCompleted with 12 errors, 0 partial successes, and 1 warning:[0m
[0m14:29:35.907503 [info ] [MainThread]: 
[0m14:29:35.908411 [error] [MainThread]: [31mFailure in test accepted_range_fct_order_discount_total__0 (models/intermediate/int_facts_marts_schema.yml)[0m
[0m14:29:35.909185 [error] [MainThread]:   Compilation Error in test accepted_range_fct_order_discount_total__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:29:35.910098 [info ] [MainThread]: 
[0m14:29:35.910677 [error] [MainThread]: [31mFailure in test accepted_range_fct_marketing_spend_spend_try__0 (models/intermediate/int_facts_marts_schema.yml)[0m
[0m14:29:35.911183 [error] [MainThread]:   Compilation Error in test accepted_range_fct_marketing_spend_spend_try__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:29:35.912156 [info ] [MainThread]: 
[0m14:29:35.912666 [error] [MainThread]: [31mFailure in test accepted_range_fct_order_net_revenue__0 (models/intermediate/int_facts_marts_schema.yml)[0m
[0m14:29:35.913153 [error] [MainThread]:   Compilation Error in test accepted_range_fct_order_net_revenue__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:29:35.913643 [info ] [MainThread]: 
[0m14:29:35.914142 [error] [MainThread]: [31mFailure in test accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0 (models/intermediate/int_facts_marts_schema.yml)[0m
[0m14:29:35.915153 [error] [MainThread]:   Compilation Error in test accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:29:35.915932 [info ] [MainThread]: 
[0m14:29:35.916470 [error] [MainThread]: [31mFailure in test accepted_range_mart_subscription_daily_active_subscribers__0 (models/intermediate/int_facts_marts_schema.yml)[0m
[0m14:29:35.916975 [error] [MainThread]:   Compilation Error in test accepted_range_mart_subscription_daily_active_subscribers__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:29:35.917673 [info ] [MainThread]: 
[0m14:29:35.918169 [error] [MainThread]: [31mFailure in test accepted_range_mart_kpis_latest_monthly_recurring_revenue__0 (models/intermediate/int_facts_marts_schema.yml)[0m
[0m14:29:35.919163 [error] [MainThread]:   Compilation Error in test accepted_range_mart_kpis_latest_monthly_recurring_revenue__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:29:35.920114 [info ] [MainThread]: 
[0m14:29:35.920915 [error] [MainThread]: [31mFailure in test accepted_range_mart_revenue_daily_daily_revenue__0 (models/intermediate/int_facts_marts_schema.yml)[0m
[0m14:29:35.921464 [error] [MainThread]:   Compilation Error in test accepted_range_mart_revenue_daily_daily_revenue__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:29:35.921972 [info ] [MainThread]: 
[0m14:29:35.922779 [error] [MainThread]: [31mFailure in test accepted_range_mart_kpis_latest_cac_payback_months__0 (models/intermediate/int_facts_marts_schema.yml)[0m
[0m14:29:35.924162 [error] [MainThread]:   Compilation Error in test accepted_range_mart_kpis_latest_cac_payback_months__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:29:35.925014 [info ] [MainThread]: 
[0m14:29:35.926099 [error] [MainThread]: [31mFailure in test accepted_range_mart_subscription_daily_new_subscribers__0 (models/intermediate/int_facts_marts_schema.yml)[0m
[0m14:29:35.926733 [error] [MainThread]:   Compilation Error in test accepted_range_mart_subscription_daily_new_subscribers__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:29:35.927201 [info ] [MainThread]: 
[0m14:29:35.927751 [error] [MainThread]: [31mFailure in test relationships_fct_shipment_order_id__order_id__ref_fct_order_ (models/intermediate/int_facts_marts_schema.yml)[0m
[0m14:29:35.928795 [error] [MainThread]:   Got 476 results, configured to fail if != 0
[0m14:29:35.930148 [info ] [MainThread]: 
[0m14:29:35.930906 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_project/models/intermediate/int_facts_marts_schema.yml/relationships_fct_shipment_order_id__order_id__ref_fct_order_.sql
[0m14:29:35.931518 [info ] [MainThread]: 
[0m14:29:35.932076 [error] [MainThread]: [31mFailure in test unique_dim_customer_customer_id (models/intermediate/int_facts_marts_schema.yml)[0m
[0m14:29:35.932580 [error] [MainThread]:   Got 16 results, configured to fail if != 0
[0m14:29:35.933353 [info ] [MainThread]: 
[0m14:29:35.933950 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_project/models/intermediate/int_facts_marts_schema.yml/unique_dim_customer_customer_id.sql
[0m14:29:35.934399 [info ] [MainThread]: 
[0m14:29:35.934887 [error] [MainThread]: [31mFailure in test unique_fct_marketing_spend_date (models/intermediate/int_facts_marts_schema.yml)[0m
[0m14:29:35.935400 [error] [MainThread]:   Got 8 results, configured to fail if != 0
[0m14:29:35.935866 [info ] [MainThread]: 
[0m14:29:35.936392 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_project/models/intermediate/int_facts_marts_schema.yml/unique_fct_marketing_spend_date.sql
[0m14:29:35.936861 [info ] [MainThread]: 
[0m14:29:35.937680 [warn ] [MainThread]: [33mWarning in test not_null_int_shipments_decomposed_delivered_at (models/staging/staging_schema.yml)[0m
[0m14:29:35.938502 [warn ] [MainThread]: Got 303 results, configured to warn if != 0
[0m14:29:35.938974 [info ] [MainThread]: 
[0m14:29:35.940136 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_project/models/staging/staging_schema.yml/not_null_int_shipments_decomposed_delivered_at.sql
[0m14:29:35.941283 [info ] [MainThread]: 
[0m14:29:35.941937 [info ] [MainThread]: Done. PASS=58 WARN=1 ERROR=12 SKIP=0 NO-OP=0 TOTAL=71
[0m14:29:35.943563 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 27 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m14:29:35.944672 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 47.656372, "process_in_blocks": "8", "process_kernel_time": 5.251092, "process_mem_max_rss": "401996", "process_out_blocks": "7096", "process_user_time": 20.851273}
[0m14:29:35.945727 [debug] [MainThread]: Command `dbt test` failed at 14:29:35.945614 after 47.66 seconds
[0m14:29:35.947203 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x727ea557dee0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x727e74656240>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x727ea59fffe0>]}
[0m14:29:35.948091 [debug] [MainThread]: Flushing usage events
[0m14:29:36.903265 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:32:23.898215 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x732132bc4b30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x732131b6b080>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73213490c470>]}


============================== 14:32:23.903279 | 8b4a9d65-8e10-44d2-8969-1a87535ce956 ==============================
[0m14:32:23.903279 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m14:32:23.905050 [debug] [MainThread]: running dbt with arguments {'target_path': 'None', 'no_print': 'None', 'static_parser': 'True', 'empty': 'None', 'partial_parse': 'True', 'fail_fast': 'False', 'profiles_dir': '/home/ecem/.dbt', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_experimental_parser': 'False', 'quiet': 'False', 'introspect': 'True', 'log_cache_events': 'False', 'warn_error': 'None', 'log_format': 'default', 'write_json': 'True', 'printer_width': '80', 'version_check': 'True', 'cache_selected_only': 'False', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'invocation_command': 'dbt deps', 'use_colors': 'True', 'send_anonymous_usage_stats': 'True', 'debug': 'False', 'indirect_selection': 'eager'}
[0m14:32:24.057161 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8b4a9d65-8e10-44d2-8969-1a87535ce956', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7321331dbb60>]}
[0m14:32:24.071377 [debug] [MainThread]: Set downloads directory='/tmp/dbt-downloads-bczjag41'
[0m14:32:24.072940 [debug] [MainThread]: Making package index registry request: GET https://hub.getdbt.com/api/v1/index.json
[0m14:32:24.544803 [debug] [MainThread]: Response from registry index: GET https://hub.getdbt.com/api/v1/index.json 200
[0m14:32:24.550070 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
[0m14:32:24.724564 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
[0m14:32:24.734077 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/calogica/dbt_expectations.json
[0m14:32:24.905748 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/calogica/dbt_expectations.json 200
[0m14:32:24.912095 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality
The `calogica/dbt_expectations` package is deprecated in favor of
`metaplane/dbt_expectations`. Please update your `packages.yml` configuration to
use `metaplane/dbt_expectations` instead.
[0m14:32:24.913012 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '8b4a9d65-8e10-44d2-8969-1a87535ce956', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73213078e450>]}
[0m14:32:24.916393 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/calogica/dbt_date.json
[0m14:32:25.090931 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/calogica/dbt_date.json 200
[0m14:32:25.113141 [info ] [MainThread]: Updating lock file in file path: /home/ecem/Desktop/data-pipeline-project/dbt/package-lock.yml
[0m14:32:25.138152 [debug] [MainThread]: Set downloads directory='/tmp/dbt-downloads-qwow5as0'
[0m14:32:25.147684 [info ] [MainThread]: Installing dbt-labs/dbt_utils
[0m14:32:25.810479 [info ] [MainThread]: Installed from version 1.3.1
[0m14:32:25.811193 [info ] [MainThread]: Up to date!
[0m14:32:25.811671 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': '8b4a9d65-8e10-44d2-8969-1a87535ce956', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7321307b89e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73213075c9e0>]}
[0m14:32:25.812107 [info ] [MainThread]: Installing calogica/dbt_expectations
[0m14:32:28.397677 [info ] [MainThread]: Installed from version 0.10.4
[0m14:32:28.398233 [info ] [MainThread]: Up to date!
[0m14:32:28.399114 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': '8b4a9d65-8e10-44d2-8969-1a87535ce956', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7321308c7920>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7321308c70e0>]}
[0m14:32:28.399590 [info ] [MainThread]: Installing calogica/dbt_date
[0m14:32:28.865403 [info ] [MainThread]: Installed from version 0.10.1
[0m14:32:28.866026 [info ] [MainThread]: Up to date!
[0m14:32:28.867061 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': '8b4a9d65-8e10-44d2-8969-1a87535ce956', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x732130dea030>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7321307c7c50>]}
[0m14:32:28.868453 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- PackageRedirectDeprecation: 2 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m14:32:28.869513 [debug] [MainThread]: Resource report: {"command_name": "deps", "command_success": true, "command_wall_clock_time": 5.027893, "process_in_blocks": "0", "process_kernel_time": 0.781617, "process_mem_max_rss": "110780", "process_out_blocks": "23120", "process_user_time": 2.066275}
[0m14:32:28.869999 [debug] [MainThread]: Command `dbt deps` succeeded at 14:32:28.869903 after 5.03 seconds
[0m14:32:28.870384 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7321309d3e00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7321308a68a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7321308a6ed0>]}
[0m14:32:28.870811 [debug] [MainThread]: Flushing usage events
[0m14:32:29.790411 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:32:59.957362 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c5c53fc0bc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c5c5570b440>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c5c54659b20>]}


============================== 14:32:59.960820 | 71be1228-b709-46be-9601-0144c9e5527b ==============================
[0m14:32:59.960820 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m14:32:59.963284 [debug] [MainThread]: running dbt with arguments {'indirect_selection': 'eager', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_cache_events': 'False', 'send_anonymous_usage_stats': 'True', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'version_check': 'True', 'cache_selected_only': 'False', 'empty': 'None', 'profiles_dir': '/home/ecem/.dbt', 'use_experimental_parser': 'False', 'printer_width': '80', 'warn_error': 'None', 'target_path': 'None', 'write_json': 'True', 'use_colors': 'True', 'fail_fast': 'False', 'debug': 'False', 'static_parser': 'True', 'quiet': 'False', 'introspect': 'True', 'partial_parse': 'True', 'invocation_command': 'dbt test', 'log_format': 'default'}
[0m14:33:02.344032 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '71be1228-b709-46be-9601-0144c9e5527b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c5c348277d0>]}
[0m14:33:02.411450 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '71be1228-b709-46be-9601-0144c9e5527b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c5c546af8c0>]}
[0m14:33:02.415979 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m14:33:02.689208 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m14:33:02.956363 [info ] [MainThread]: Unable to do partial parsing because a project dependency has been added
[0m14:33:02.957297 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '71be1228-b709-46be-9601-0144c9e5527b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c5c53f57860>]}
[0m14:33:04.682140 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `relationships` defined on 'stg_orders' in
package 'data_pipeline_project' (models/staging/staging_schema.yml). Arguments
to generic tests should be nested under the `arguments` property.
[0m14:33:04.687010 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '71be1228-b709-46be-9601-0144c9e5527b', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c5c33eb4050>]}
[0m14:33:05.164801 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '71be1228-b709-46be-9601-0144c9e5527b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c5c33087110>]}
[0m14:33:05.299693 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m14:33:05.302867 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m14:33:05.334297 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '71be1228-b709-46be-9601-0144c9e5527b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c5c331b5580>]}
[0m14:33:05.335110 [info ] [MainThread]: Found 21 models, 71 data tests, 10 sources, 891 macros
[0m14:33:05.335592 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '71be1228-b709-46be-9601-0144c9e5527b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c5c33048cb0>]}
[0m14:33:05.341807 [info ] [MainThread]: 
[0m14:33:05.342475 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:33:05.343177 [info ] [MainThread]: 
[0m14:33:05.344020 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m14:33:05.351253 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_marts'
[0m14:33:05.351968 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:33:05.418537 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_staging'
[0m14:33:05.423695 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:33:06.468541 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '71be1228-b709-46be-9601-0144c9e5527b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c5c33062d80>]}
[0m14:33:06.469361 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:33:06.477621 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.accepted_range_fct_marketing_spend_spend_try__0.29ff739b28
[0m14:33:06.478357 [info ] [Thread-1 (]: 1 of 71 START test accepted_range_fct_marketing_spend_spend_try__0 ............. [RUN]
[0m14:33:06.479966 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now test.data_pipeline_project.accepted_range_fct_marketing_spend_spend_try__0.29ff739b28)
[0m14:33:06.480512 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.accepted_range_fct_marketing_spend_spend_try__0.29ff739b28
[0m14:33:06.489786 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:33:06.498097 [debug] [Thread-1 (]: Compilation Error in test accepted_range_fct_marketing_spend_spend_try__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:33:06.500346 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.accepted_range_fct_order_discount_total__0.5e17f8aa8a
[0m14:33:06.501233 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.accepted_range_fct_order_net_revenue__0.49d8f40cb7
[0m14:33:06.502232 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0.494a0c0ebc
[0m14:33:06.508181 [error] [Thread-1 (]: 1 of 71 ERROR accepted_range_fct_marketing_spend_spend_try__0 .................. [[31mERROR[0m in 0.03s]
[0m14:33:06.509223 [info ] [Thread-2 (]: 2 of 71 START test accepted_range_fct_order_discount_total__0 .................. [RUN]
[0m14:33:06.512251 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.accepted_range_fct_marketing_spend_spend_try__0.29ff739b28
[0m14:33:06.509862 [info ] [Thread-3 (]: 3 of 71 START test accepted_range_fct_order_net_revenue__0 ..................... [RUN]
[0m14:33:06.510860 [info ] [Thread-4 (]: 4 of 71 START test accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0  [RUN]
[0m14:33:06.514148 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_marts, now test.data_pipeline_project.accepted_range_fct_order_discount_total__0.5e17f8aa8a)
[0m14:33:06.515090 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.accepted_range_mart_kpis_latest_cac_payback_months__0.b5860838e1
[0m14:33:06.516036 [debug] [Thread-3 (]: Acquiring new bigquery connection 'test.data_pipeline_project.accepted_range_fct_order_net_revenue__0.49d8f40cb7'
[0m14:33:06.516928 [debug] [Thread-4 (]: Acquiring new bigquery connection 'test.data_pipeline_project.accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0.494a0c0ebc'
[0m14:33:06.522597 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0.494a0c0ebc
[0m14:33:06.518108 [debug] [Thread-7 (]: Marking all children of 'test.data_pipeline_project.accepted_range_fct_marketing_spend_spend_try__0.29ff739b28' to be skipped because of status 'error'.  Reason: Compilation Error in test accepted_range_fct_marketing_spend_spend_try__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps"..
[0m14:33:06.520015 [info ] [Thread-1 (]: 5 of 71 START test accepted_range_mart_kpis_latest_cac_payback_months__0 ....... [RUN]
[0m14:33:06.520934 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.accepted_range_fct_order_net_revenue__0.49d8f40cb7
[0m14:33:06.519101 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.accepted_range_fct_order_discount_total__0.5e17f8aa8a
[0m14:33:06.537774 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m14:33:06.547310 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.accepted_range_fct_marketing_spend_spend_try__0.29ff739b28, now test.data_pipeline_project.accepted_range_mart_kpis_latest_cac_payback_months__0.b5860838e1)
[0m14:33:06.574467 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.accepted_range_mart_kpis_latest_cac_payback_months__0.b5860838e1
[0m14:33:06.568389 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:33:06.556685 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m14:33:06.573542 [debug] [Thread-4 (]: Compilation Error in test accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:33:06.578746 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:33:06.589566 [debug] [Thread-3 (]: Compilation Error in test accepted_range_fct_order_net_revenue__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:33:06.596542 [debug] [Thread-2 (]: Compilation Error in test accepted_range_fct_order_discount_total__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:33:06.602054 [debug] [Thread-1 (]: Compilation Error in test accepted_range_mart_kpis_latest_cac_payback_months__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:33:06.594577 [error] [Thread-4 (]: 4 of 71 ERROR accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0  [[31mERROR[0m in 0.08s]
[0m14:33:06.607262 [error] [Thread-3 (]: 3 of 71 ERROR accepted_range_fct_order_net_revenue__0 .......................... [[31mERROR[0m in 0.09s]
[0m14:33:06.609569 [error] [Thread-2 (]: 2 of 71 ERROR accepted_range_fct_order_discount_total__0 ....................... [[31mERROR[0m in 0.10s]
[0m14:33:06.618283 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0.494a0c0ebc
[0m14:33:06.620500 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.accepted_range_fct_order_net_revenue__0.49d8f40cb7
[0m14:33:06.624277 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.accepted_range_fct_order_discount_total__0.5e17f8aa8a
[0m14:33:06.619505 [error] [Thread-1 (]: 5 of 71 ERROR accepted_range_mart_kpis_latest_cac_payback_months__0 ............ [[31mERROR[0m in 0.06s]
[0m14:33:06.633433 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.accepted_range_mart_kpis_latest_monthly_recurring_revenue__0.b1fd398ff0
[0m14:33:06.634780 [debug] [Thread-7 (]: Marking all children of 'test.data_pipeline_project.accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0.494a0c0ebc' to be skipped because of status 'error'.  Reason: Compilation Error in test accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps"..
[0m14:33:06.635696 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.accepted_range_mart_revenue_daily_daily_revenue__0.12a5734f28
[0m14:33:06.636749 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.accepted_range_mart_subscription_daily_active_subscribers__0.5d06041ada
[0m14:33:06.641014 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.accepted_range_mart_kpis_latest_cac_payback_months__0.b5860838e1
[0m14:33:06.643795 [debug] [Thread-7 (]: Marking all children of 'test.data_pipeline_project.accepted_range_fct_order_net_revenue__0.49d8f40cb7' to be skipped because of status 'error'.  Reason: Compilation Error in test accepted_range_fct_order_net_revenue__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps"..
[0m14:33:06.642163 [info ] [Thread-4 (]: 6 of 71 START test accepted_range_mart_kpis_latest_monthly_recurring_revenue__0  [RUN]
[0m14:33:06.644792 [info ] [Thread-3 (]: 7 of 71 START test accepted_range_mart_revenue_daily_daily_revenue__0 .......... [RUN]
[0m14:33:06.651123 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.accepted_range_fct_order_net_revenue__0.49d8f40cb7, now test.data_pipeline_project.accepted_range_mart_revenue_daily_daily_revenue__0.12a5734f28)
[0m14:33:06.649077 [debug] [Thread-7 (]: Marking all children of 'test.data_pipeline_project.accepted_range_fct_order_discount_total__0.5e17f8aa8a' to be skipped because of status 'error'.  Reason: Compilation Error in test accepted_range_fct_order_discount_total__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps"..
[0m14:33:06.650256 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0.494a0c0ebc, now test.data_pipeline_project.accepted_range_mart_kpis_latest_monthly_recurring_revenue__0.b1fd398ff0)
[0m14:33:06.651693 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.accepted_range_mart_revenue_daily_daily_revenue__0.12a5734f28
[0m14:33:06.646549 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.accepted_range_mart_subscription_daily_new_subscribers__0.f410e5c26f
[0m14:33:06.645874 [info ] [Thread-2 (]: 8 of 71 START test accepted_range_mart_subscription_daily_active_subscribers__0  [RUN]
[0m14:33:06.653005 [debug] [Thread-7 (]: Marking all children of 'test.data_pipeline_project.accepted_range_mart_kpis_latest_cac_payback_months__0.b5860838e1' to be skipped because of status 'error'.  Reason: Compilation Error in test accepted_range_mart_kpis_latest_cac_payback_months__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps"..
[0m14:33:06.653795 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.accepted_range_mart_kpis_latest_monthly_recurring_revenue__0.b1fd398ff0
[0m14:33:06.679398 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:33:06.677436 [info ] [Thread-1 (]: 9 of 71 START test accepted_range_mart_subscription_daily_new_subscribers__0 ... [RUN]
[0m14:33:06.681705 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.accepted_range_fct_order_discount_total__0.5e17f8aa8a, now test.data_pipeline_project.accepted_range_mart_subscription_daily_active_subscribers__0.5d06041ada)
[0m14:33:06.687620 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:33:06.694924 [debug] [Thread-3 (]: Compilation Error in test accepted_range_mart_revenue_daily_daily_revenue__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:33:06.697982 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.accepted_range_mart_kpis_latest_cac_payback_months__0.b5860838e1, now test.data_pipeline_project.accepted_range_mart_subscription_daily_new_subscribers__0.f410e5c26f)
[0m14:33:06.699398 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.accepted_range_mart_subscription_daily_active_subscribers__0.5d06041ada
[0m14:33:06.705451 [debug] [Thread-4 (]: Compilation Error in test accepted_range_mart_kpis_latest_monthly_recurring_revenue__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:33:06.701172 [error] [Thread-3 (]: 7 of 71 ERROR accepted_range_mart_revenue_daily_daily_revenue__0 ............... [[31mERROR[0m in 0.05s]
[0m14:33:06.725478 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.accepted_range_mart_revenue_daily_daily_revenue__0.12a5734f28
[0m14:33:06.714357 [error] [Thread-4 (]: 6 of 71 ERROR accepted_range_mart_kpis_latest_monthly_recurring_revenue__0 ..... [[31mERROR[0m in 0.06s]
[0m14:33:06.723898 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:33:06.706378 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.accepted_range_mart_subscription_daily_new_subscribers__0.f410e5c26f
[0m14:33:06.727289 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc
[0m14:33:06.730923 [debug] [Thread-7 (]: Marking all children of 'test.data_pipeline_project.accepted_range_mart_revenue_daily_daily_revenue__0.12a5734f28' to be skipped because of status 'error'.  Reason: Compilation Error in test accepted_range_mart_revenue_daily_daily_revenue__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps"..
[0m14:33:06.732272 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.accepted_range_mart_kpis_latest_monthly_recurring_revenue__0.b1fd398ff0
[0m14:33:06.754434 [debug] [Thread-2 (]: Compilation Error in test accepted_range_mart_subscription_daily_active_subscribers__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:33:06.759316 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:33:06.763354 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723
[0m14:33:06.762355 [info ] [Thread-3 (]: 10 of 71 START test not_null_dim_customer_customer_id .......................... [RUN]
[0m14:33:06.764456 [debug] [Thread-7 (]: Marking all children of 'test.data_pipeline_project.accepted_range_mart_kpis_latest_monthly_recurring_revenue__0.b1fd398ff0' to be skipped because of status 'error'.  Reason: Compilation Error in test accepted_range_mart_kpis_latest_monthly_recurring_revenue__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps"..
[0m14:33:06.765246 [error] [Thread-2 (]: 8 of 71 ERROR accepted_range_mart_subscription_daily_active_subscribers__0 ..... [[31mERROR[0m in 0.08s]
[0m14:33:06.771973 [debug] [Thread-1 (]: Compilation Error in test accepted_range_mart_subscription_daily_new_subscribers__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:33:06.773507 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.accepted_range_mart_revenue_daily_daily_revenue__0.12a5734f28, now test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc)
[0m14:33:06.770644 [info ] [Thread-4 (]: 11 of 71 START test not_null_dim_subscription_customer_id ...................... [RUN]
[0m14:33:06.776040 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.accepted_range_mart_subscription_daily_active_subscribers__0.5d06041ada
[0m14:33:06.778100 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc
[0m14:33:06.779019 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.accepted_range_mart_kpis_latest_monthly_recurring_revenue__0.b1fd398ff0, now test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723)
[0m14:33:06.777317 [error] [Thread-1 (]: 9 of 71 ERROR accepted_range_mart_subscription_daily_new_subscribers__0 ........ [[31mERROR[0m in 0.08s]
[0m14:33:06.780069 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6
[0m14:33:06.786344 [debug] [Thread-7 (]: Marking all children of 'test.data_pipeline_project.accepted_range_mart_subscription_daily_active_subscribers__0.5d06041ada' to be skipped because of status 'error'.  Reason: Compilation Error in test accepted_range_mart_subscription_daily_active_subscribers__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps"..
[0m14:33:06.797079 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723
[0m14:33:06.798144 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.accepted_range_mart_subscription_daily_new_subscribers__0.f410e5c26f
[0m14:33:06.798918 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc"
[0m14:33:06.799764 [info ] [Thread-2 (]: 12 of 71 START test not_null_dim_subscription_subscription_id .................. [RUN]
[0m14:33:06.809109 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719
[0m14:33:06.811415 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723"
[0m14:33:06.812262 [debug] [Thread-7 (]: Marking all children of 'test.data_pipeline_project.accepted_range_mart_subscription_daily_new_subscribers__0.f410e5c26f' to be skipped because of status 'error'.  Reason: Compilation Error in test accepted_range_mart_subscription_daily_new_subscribers__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps"..
[0m14:33:06.813729 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc
[0m14:33:06.814535 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.accepted_range_mart_subscription_daily_active_subscribers__0.5d06041ada, now test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6)
[0m14:33:06.816366 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723
[0m14:33:06.815488 [info ] [Thread-1 (]: 13 of 71 START test not_null_fct_marketing_spend_date .......................... [RUN]
[0m14:33:06.826418 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6
[0m14:33:06.883627 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.accepted_range_mart_subscription_daily_new_subscribers__0.f410e5c26f, now test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719)
[0m14:33:06.894614 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc"
[0m14:33:06.913709 [debug] [Thread-3 (]: On test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select customer_id
from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_customer`
where customer_id is null



  
  
      
    ) dbt_internal_test
[0m14:33:06.897914 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723"
[0m14:33:06.912549 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719
[0m14:33:06.911675 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6"
[0m14:33:06.933003 [debug] [Thread-4 (]: On test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select customer_id
from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_subscription`
where customer_id is null



  
  
      
    ) dbt_internal_test
[0m14:33:06.933566 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:33:06.931526 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719"
[0m14:33:06.925115 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:33:06.938607 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6
[0m14:33:06.989034 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719
[0m14:33:07.009974 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6"
[0m14:33:07.048877 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719"
[0m14:33:07.052186 [debug] [Thread-2 (]: On test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select subscription_id
from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_subscription`
where subscription_id is null



  
  
      
    ) dbt_internal_test
[0m14:33:07.052973 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:33:07.088914 [debug] [Thread-1 (]: On test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select date
from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_marketing_spend`
where date is null



  
  
      
    ) dbt_internal_test
[0m14:33:07.091083 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:33:08.271590 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:6b9bffa4-5ae6-4b30-b95f-8952291e984a&page=queryresults
[0m14:33:08.343384 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:76c0600e-320f-4e9c-a26d-7846b45d0d5a&page=queryresults
[0m14:33:08.426404 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:f0fca5c3-b978-4eb0-8576-a5645a39eec3&page=queryresults
[0m14:33:08.461699 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:629e9186-b3a9-458a-b796-ed3c12c76983&page=queryresults
[0m14:33:09.276353 [info ] [Thread-1 (]: 13 of 71 PASS not_null_fct_marketing_spend_date ................................ [[32mPASS[0m in 2.39s]
[0m14:33:09.278094 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719
[0m14:33:09.280372 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d
[0m14:33:09.281260 [info ] [Thread-1 (]: 14 of 71 START test not_null_fct_marketing_spend_spend_try ..................... [RUN]
[0m14:33:09.282138 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719, now test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d)
[0m14:33:09.287969 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d
[0m14:33:09.287086 [info ] [Thread-3 (]: 10 of 71 PASS not_null_dim_customer_customer_id ................................ [[32mPASS[0m in 2.51s]
[0m14:33:09.306816 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d"
[0m14:33:09.308074 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc
[0m14:33:09.310558 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff
[0m14:33:09.313645 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d
[0m14:33:09.312629 [info ] [Thread-3 (]: 15 of 71 START test not_null_fct_order_customer_id ............................. [RUN]
[0m14:33:09.317235 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d"
[0m14:33:09.320239 [debug] [Thread-1 (]: On test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select spend_try
from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_marketing_spend`
where spend_try is null



  
  
      
    ) dbt_internal_test
[0m14:33:09.318882 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc, now test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff)
[0m14:33:09.320786 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:33:09.322642 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff
[0m14:33:09.360051 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff"
[0m14:33:09.372936 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff
[0m14:33:09.377299 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff"
[0m14:33:09.384484 [debug] [Thread-3 (]: On test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select customer_id
from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_order`
where customer_id is null



  
  
      
    ) dbt_internal_test
[0m14:33:09.386457 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:33:09.452783 [info ] [Thread-2 (]: 12 of 71 PASS not_null_dim_subscription_subscription_id ........................ [[32mPASS[0m in 2.64s]
[0m14:33:09.470505 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6
[0m14:33:09.471885 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea
[0m14:33:09.474712 [info ] [Thread-4 (]: 11 of 71 PASS not_null_dim_subscription_customer_id ............................ [[32mPASS[0m in 2.68s]
[0m14:33:09.477944 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723
[0m14:33:09.481497 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e
[0m14:33:09.475387 [info ] [Thread-2 (]: 16 of 71 START test not_null_fct_order_discount_total .......................... [RUN]
[0m14:33:09.484390 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6, now test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea)
[0m14:33:09.485062 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea
[0m14:33:09.493145 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea"
[0m14:33:09.482094 [info ] [Thread-4 (]: 17 of 71 START test not_null_fct_order_net_revenue ............................. [RUN]
[0m14:33:09.498173 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723, now test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e)
[0m14:33:09.498777 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e
[0m14:33:09.510430 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e"
[0m14:33:09.511308 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e
[0m14:33:09.518895 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e"
[0m14:33:09.519722 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea
[0m14:33:09.524014 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea"
[0m14:33:09.525503 [debug] [Thread-2 (]: On test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select discount_total
from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_order`
where discount_total is null



  
  
      
    ) dbt_internal_test
[0m14:33:09.526271 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:33:09.524610 [debug] [Thread-4 (]: On test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select net_revenue
from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_order`
where net_revenue is null



  
  
      
    ) dbt_internal_test
[0m14:33:09.563178 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:33:10.562630 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:d8e12dc9-481d-4275-bdb3-8c69aa3be628&page=queryresults
[0m14:33:10.651472 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:410e5b89-7a76-4c6c-9b0a-2d541addbcf4&page=queryresults
[0m14:33:10.799213 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:dca22497-8496-4f30-a16f-e3c646d72a4b&page=queryresults
[0m14:33:10.891523 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:c2046e20-cc5a-4cbb-b5a6-3eae880a8020&page=queryresults
[0m14:33:11.537546 [info ] [Thread-1 (]: 14 of 71 PASS not_null_fct_marketing_spend_spend_try ........................... [[32mPASS[0m in 2.25s]
[0m14:33:11.541566 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d
[0m14:33:11.542557 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def
[0m14:33:11.545067 [info ] [Thread-1 (]: 18 of 71 START test not_null_fct_order_order_id ................................ [RUN]
[0m14:33:11.548266 [info ] [Thread-3 (]: 15 of 71 PASS not_null_fct_order_customer_id ................................... [[32mPASS[0m in 2.23s]
[0m14:33:11.551904 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff
[0m14:33:11.549448 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d, now test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def)
[0m14:33:11.552598 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.not_null_fct_shipment_order_id.8b34a6bb78
[0m14:33:11.553689 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def
[0m14:33:11.559238 [info ] [Thread-3 (]: 19 of 71 START test not_null_fct_shipment_order_id ............................. [RUN]
[0m14:33:11.569188 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff, now test.data_pipeline_project.not_null_fct_shipment_order_id.8b34a6bb78)
[0m14:33:11.569753 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.not_null_fct_shipment_order_id.8b34a6bb78
[0m14:33:11.580584 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def"
[0m14:33:11.581301 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_fct_shipment_order_id.8b34a6bb78"
[0m14:33:11.586695 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def
[0m14:33:11.590125 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.not_null_fct_shipment_order_id.8b34a6bb78
[0m14:33:11.606759 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_fct_shipment_order_id.8b34a6bb78"
[0m14:33:11.608817 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def"
[0m14:33:11.614892 [debug] [Thread-1 (]: On test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select order_id
from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_order`
where order_id is null



  
  
      
    ) dbt_internal_test
[0m14:33:11.615467 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:33:11.647138 [debug] [Thread-3 (]: On test.data_pipeline_project.not_null_fct_shipment_order_id.8b34a6bb78: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_fct_shipment_order_id.8b34a6bb78"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select order_id
from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_shipment`
where order_id is null



  
  
      
    ) dbt_internal_test
[0m14:33:11.647801 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:33:11.830307 [info ] [Thread-4 (]: 17 of 71 PASS not_null_fct_order_net_revenue ................................... [[32mPASS[0m in 2.33s]
[0m14:33:11.831399 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e
[0m14:33:11.831854 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d
[0m14:33:11.832289 [info ] [Thread-4 (]: 20 of 71 START test not_null_fct_shipment_shipment_id .......................... [RUN]
[0m14:33:11.833575 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e, now test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d)
[0m14:33:11.834093 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d
[0m14:33:11.843866 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d"
[0m14:33:11.850981 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d
[0m14:33:11.874634 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d"
[0m14:33:11.872296 [info ] [Thread-2 (]: 16 of 71 PASS not_null_fct_order_discount_total ................................ [[32mPASS[0m in 2.39s]
[0m14:33:11.881472 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea
[0m14:33:11.880560 [debug] [Thread-4 (]: On test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select shipment_id
from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_shipment`
where shipment_id is null



  
  
      
    ) dbt_internal_test
[0m14:33:11.882922 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b
[0m14:33:11.887118 [info ] [Thread-2 (]: 21 of 71 START test not_null_int_shipments_decomposed_delivered_at ............. [RUN]
[0m14:33:11.885650 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:33:11.889330 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea, now test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b)
[0m14:33:11.964155 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b
[0m14:33:11.975412 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b"
[0m14:33:11.977512 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b
[0m14:33:11.982347 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b"
[0m14:33:11.983388 [debug] [Thread-2 (]: On test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select delivered_at
from `data-pipeline-project-474812`.`analytics_staging_staging`.`int_shipments_decomposed`
where delivered_at is null



  
  
      
    ) dbt_internal_test
[0m14:33:11.983965 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:33:12.930171 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:691a2c61-56ce-4638-a189-28ceb06bc6d3&page=queryresults
[0m14:33:13.061413 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:c127eeec-6c9f-429f-a4e3-7becda35dfd5&page=queryresults
[0m14:33:13.129531 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:4a6894df-5f9b-4b77-ba85-fe3cc896ce6b&page=queryresults
[0m14:33:13.673795 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:9912724f-3a7c-4f93-87b4-d5824f6a6923&page=queryresults
[0m14:33:13.849477 [info ] [Thread-3 (]: 19 of 71 PASS not_null_fct_shipment_order_id ................................... [[32mPASS[0m in 2.28s]
[0m14:33:13.850954 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.not_null_fct_shipment_order_id.8b34a6bb78
[0m14:33:13.852119 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31
[0m14:33:13.853136 [info ] [Thread-3 (]: 22 of 71 START test not_null_int_shipments_decomposed_latest_status ............ [RUN]
[0m14:33:13.854747 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_fct_shipment_order_id.8b34a6bb78, now test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31)
[0m14:33:13.855472 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31
[0m14:33:13.861317 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31"
[0m14:33:13.866584 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31
[0m14:33:13.877768 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31"
[0m14:33:13.879163 [debug] [Thread-3 (]: On test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select latest_status
from `data-pipeline-project-474812`.`analytics_staging_staging`.`int_shipments_decomposed`
where latest_status is null



  
  
      
    ) dbt_internal_test
[0m14:33:13.880012 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:33:14.024906 [info ] [Thread-1 (]: 18 of 71 PASS not_null_fct_order_order_id ...................................... [[32mPASS[0m in 2.47s]
[0m14:33:14.026625 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def
[0m14:33:14.028229 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b
[0m14:33:14.029148 [info ] [Thread-1 (]: 23 of 71 START test not_null_mart_acquisition_efficiency_date .................. [RUN]
[0m14:33:14.030340 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def, now test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b)
[0m14:33:14.031332 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b
[0m14:33:14.037217 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b"
[0m14:33:14.039849 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b
[0m14:33:14.050249 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b"
[0m14:33:14.052426 [debug] [Thread-1 (]: On test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select date
from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_acquisition_efficiency`
where date is null



  
  
      
    ) dbt_internal_test
[0m14:33:14.053442 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:33:14.167217 [info ] [Thread-4 (]: 20 of 71 PASS not_null_fct_shipment_shipment_id ................................ [[32mPASS[0m in 2.33s]
[0m14:33:14.169832 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d
[0m14:33:14.170448 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23
[0m14:33:14.171464 [info ] [Thread-4 (]: 24 of 71 START test not_null_mart_revenue_daily_date ........................... [RUN]
[0m14:33:14.172276 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d, now test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23)
[0m14:33:14.174179 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23
[0m14:33:14.181421 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23"
[0m14:33:14.184301 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23
[0m14:33:14.193299 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23"
[0m14:33:14.197144 [debug] [Thread-4 (]: On test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select date
from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_revenue_daily`
where date is null



  
  
      
    ) dbt_internal_test
[0m14:33:14.197945 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:33:14.548030 [warn ] [Thread-2 (]: 21 of 71 WARN 303 not_null_int_shipments_decomposed_delivered_at ............... [[33mWARN 303[0m in 2.66s]
[0m14:33:14.550014 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b
[0m14:33:14.550627 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322
[0m14:33:14.552082 [info ] [Thread-2 (]: 25 of 71 START test not_null_mart_subscription_daily_date ...................... [RUN]
[0m14:33:14.553188 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b, now test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322)
[0m14:33:14.553862 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322
[0m14:33:14.560748 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322"
[0m14:33:14.567717 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322
[0m14:33:14.571006 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322"
[0m14:33:14.572950 [debug] [Thread-2 (]: On test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select date
from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily`
where date is null



  
  
      
    ) dbt_internal_test
[0m14:33:14.573522 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:33:15.187388 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:ab33a828-824b-406a-a89e-fae95ea5f58b&page=queryresults
[0m14:33:15.487164 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:e6a5dc01-d2d4-44aa-915e-95eda80bae81&page=queryresults
[0m14:33:15.502248 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:a6967ec4-d222-4efa-b268-ade4e0fcb41c&page=queryresults
[0m14:33:15.813660 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:a163a2b4-5577-4216-bc2d-3fb3fe2cf271&page=queryresults
[0m14:33:16.113304 [info ] [Thread-3 (]: 22 of 71 PASS not_null_int_shipments_decomposed_latest_status .................. [[32mPASS[0m in 2.26s]
[0m14:33:16.115968 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31
[0m14:33:16.117888 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501
[0m14:33:16.119062 [info ] [Thread-3 (]: 26 of 71 START test not_null_stg_addresses_address_id .......................... [RUN]
[0m14:33:16.120751 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31, now test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501)
[0m14:33:16.122468 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501
[0m14:33:16.130744 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501"
[0m14:33:16.133472 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501
[0m14:33:16.141267 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501"
[0m14:33:16.143244 [debug] [Thread-3 (]: On test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select address_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_addresses`
where address_id is null



  
  
      
    ) dbt_internal_test
[0m14:33:16.144166 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:33:16.496977 [info ] [Thread-1 (]: 23 of 71 PASS not_null_mart_acquisition_efficiency_date ........................ [[32mPASS[0m in 2.47s]
[0m14:33:16.501873 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b
[0m14:33:16.503302 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3
[0m14:33:16.505173 [info ] [Thread-1 (]: 27 of 71 START test not_null_stg_addresses_user_id ............................. [RUN]
[0m14:33:16.507627 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b, now test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3)
[0m14:33:16.509377 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3
[0m14:33:16.518704 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3"
[0m14:33:16.527221 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3
[0m14:33:16.534502 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3"
[0m14:33:16.535894 [debug] [Thread-1 (]: On test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_addresses`
where user_id is null



  
  
      
    ) dbt_internal_test
[0m14:33:16.537014 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:33:16.627262 [info ] [Thread-4 (]: 24 of 71 PASS not_null_mart_revenue_daily_date ................................. [[32mPASS[0m in 2.45s]
[0m14:33:16.629936 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23
[0m14:33:16.630965 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2
[0m14:33:16.631555 [info ] [Thread-4 (]: 28 of 71 START test not_null_stg_countries_country_id .......................... [RUN]
[0m14:33:16.632639 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23, now test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2)
[0m14:33:16.633460 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2
[0m14:33:16.641880 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2"
[0m14:33:16.643229 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2
[0m14:33:16.655621 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2"
[0m14:33:16.656689 [debug] [Thread-4 (]: On test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select country_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_countries`
where country_id is null



  
  
      
    ) dbt_internal_test
[0m14:33:16.660729 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:33:16.834210 [info ] [Thread-2 (]: 25 of 71 PASS not_null_mart_subscription_daily_date ............................ [[32mPASS[0m in 2.28s]
[0m14:33:16.844309 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322
[0m14:33:16.853104 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64
[0m14:33:16.858456 [info ] [Thread-2 (]: 29 of 71 START test not_null_stg_orders_order_id ............................... [RUN]
[0m14:33:16.863135 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322, now test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64)
[0m14:33:16.868597 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64
[0m14:33:16.890097 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64"
[0m14:33:16.895349 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64
[0m14:33:16.899181 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64"
[0m14:33:16.906189 [debug] [Thread-2 (]: On test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select order_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_orders`
where order_id is null



  
  
      
    ) dbt_internal_test
[0m14:33:16.908432 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:33:17.429786 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:e7972791-9771-48e1-92dd-859a24bb3a23&page=queryresults
[0m14:33:17.933273 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:9db27fdd-38f6-4aae-be4a-ac176e0ec2ea&page=queryresults
[0m14:33:17.956223 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:95e0a30e-b568-44c2-afb1-a7ee12ba9a66&page=queryresults
[0m14:33:18.249711 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:fb619c37-674d-4ba1-9d4f-f68a6774e5b8&page=queryresults
[0m14:33:18.321379 [info ] [Thread-3 (]: 26 of 71 PASS not_null_stg_addresses_address_id ................................ [[32mPASS[0m in 2.20s]
[0m14:33:18.322989 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501
[0m14:33:18.323519 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209
[0m14:33:18.324192 [info ] [Thread-3 (]: 30 of 71 START test not_null_stg_orders_user_id ................................ [RUN]
[0m14:33:18.325271 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501, now test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209)
[0m14:33:18.325756 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209
[0m14:33:18.331916 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209"
[0m14:33:18.333618 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209
[0m14:33:18.337225 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209"
[0m14:33:18.338673 [debug] [Thread-3 (]: On test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_orders`
where user_id is null



  
  
      
    ) dbt_internal_test
[0m14:33:18.339530 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:33:18.853199 [info ] [Thread-1 (]: 27 of 71 PASS not_null_stg_addresses_user_id ................................... [[32mPASS[0m in 2.35s]
[0m14:33:18.854434 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3
[0m14:33:18.855347 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380
[0m14:33:18.856897 [info ] [Thread-1 (]: 31 of 71 START test not_null_stg_shipments_shipment_id ......................... [RUN]
[0m14:33:18.857534 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3, now test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380)
[0m14:33:18.858227 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380
[0m14:33:18.864325 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380"
[0m14:33:18.865490 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380
[0m14:33:18.871248 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380"
[0m14:33:18.872429 [debug] [Thread-1 (]: On test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select shipment_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_shipments`
where shipment_id is null



  
  
      
    ) dbt_internal_test
[0m14:33:18.873298 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:33:18.936556 [info ] [Thread-4 (]: 28 of 71 PASS not_null_stg_countries_country_id ................................ [[32mPASS[0m in 2.30s]
[0m14:33:18.940504 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2
[0m14:33:18.942378 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852
[0m14:33:18.944674 [info ] [Thread-4 (]: 32 of 71 START test not_null_stg_shipments_user_id ............................. [RUN]
[0m14:33:18.946604 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2, now test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852)
[0m14:33:18.947473 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852
[0m14:33:18.956793 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852"
[0m14:33:18.958765 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852
[0m14:33:18.971867 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852"
[0m14:33:18.974147 [debug] [Thread-4 (]: On test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_shipments`
where user_id is null



  
  
      
    ) dbt_internal_test
[0m14:33:18.974784 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:33:19.197683 [info ] [Thread-2 (]: 29 of 71 PASS not_null_stg_orders_order_id ..................................... [[32mPASS[0m in 2.33s]
[0m14:33:19.199547 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64
[0m14:33:19.200340 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8
[0m14:33:19.201137 [info ] [Thread-2 (]: 33 of 71 START test not_null_stg_subscriptions_start_date ...................... [RUN]
[0m14:33:19.201759 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64, now test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8)
[0m14:33:19.202401 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8
[0m14:33:19.207658 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8"
[0m14:33:19.209058 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8
[0m14:33:19.212711 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8"
[0m14:33:19.219061 [debug] [Thread-2 (]: On test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select start_date
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_subscriptions`
where start_date is null



  
  
      
    ) dbt_internal_test
[0m14:33:19.220189 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:33:19.701476 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:ec75d468-877d-42e2-84be-492edfc860d9&page=queryresults
[0m14:33:20.193206 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:a90046f7-bb13-4012-80b3-6e866f236216&page=queryresults
[0m14:33:20.569248 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:889fdde8-4613-4490-964e-96dab8546c24&page=queryresults
[0m14:33:20.903066 [info ] [Thread-3 (]: 30 of 71 PASS not_null_stg_orders_user_id ...................................... [[32mPASS[0m in 2.58s]
[0m14:33:20.904341 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209
[0m14:33:20.905121 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m14:33:20.905979 [info ] [Thread-3 (]: 34 of 71 START test not_null_stg_subscriptions_subscription_id ................. [RUN]
[0m14:33:20.907518 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209, now test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91)
[0m14:33:20.908225 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m14:33:20.914661 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91"
[0m14:33:20.916048 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m14:33:20.924775 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91"
[0m14:33:20.926191 [debug] [Thread-3 (]: On test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select subscription_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_subscriptions`
where subscription_id is null



  
  
      
    ) dbt_internal_test
[0m14:33:20.927076 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:33:21.184507 [info ] [Thread-1 (]: 31 of 71 PASS not_null_stg_shipments_shipment_id ............................... [[32mPASS[0m in 2.32s]
[0m14:33:21.188620 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380
[0m14:33:21.191694 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2
[0m14:33:21.193998 [info ] [Thread-1 (]: 35 of 71 START test not_null_stg_subscriptions_user_id ......................... [RUN]
[0m14:33:21.199667 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380, now test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2)
[0m14:33:21.201303 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2
[0m14:33:21.216250 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2"
[0m14:33:21.218110 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2
[0m14:33:21.223350 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2"
[0m14:33:21.224972 [debug] [Thread-1 (]: On test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_subscriptions`
where user_id is null



  
  
      
    ) dbt_internal_test
[0m14:33:21.225622 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:33:21.370069 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:41afcdee-61e1-4986-97be-71d2a1a618d4&page=queryresults
[0m14:33:21.645897 [info ] [Thread-4 (]: 32 of 71 PASS not_null_stg_shipments_user_id ................................... [[32mPASS[0m in 2.70s]
[0m14:33:21.648095 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852
[0m14:33:21.649032 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25
[0m14:33:21.650505 [info ] [Thread-4 (]: 36 of 71 START test not_null_stg_users_created_at .............................. [RUN]
[0m14:33:21.651952 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852, now test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25)
[0m14:33:21.652909 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25
[0m14:33:21.937272 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25"
[0m14:33:21.939031 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25
[0m14:33:21.942530 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25"
[0m14:33:21.944169 [debug] [Thread-4 (]: On test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select created_at
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
where created_at is null



  
  
      
    ) dbt_internal_test
[0m14:33:21.944996 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:33:22.314058 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:5c771813-baf6-4de1-a1cc-08052b319269&page=queryresults
[0m14:33:22.518159 [info ] [Thread-2 (]: 33 of 71 PASS not_null_stg_subscriptions_start_date ............................ [[32mPASS[0m in 3.31s]
[0m14:33:22.520744 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8
[0m14:33:22.522992 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77
[0m14:33:22.524262 [info ] [Thread-2 (]: 37 of 71 START test not_null_stg_users_user_id ................................. [RUN]
[0m14:33:22.526129 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8, now test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77)
[0m14:33:22.527555 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77
[0m14:33:22.534761 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77"
[0m14:33:22.536985 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77
[0m14:33:22.546094 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77"
[0m14:33:22.548218 [debug] [Thread-2 (]: On test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
where user_id is null



  
  
      
    ) dbt_internal_test
[0m14:33:22.549030 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:33:22.826679 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:8945c7c7-f733-463a-b3a9-d4c2cd2f7442&page=queryresults
[0m14:33:23.256674 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:dbfaa726-4208-4f40-be6a-99f96441b057&page=queryresults
[0m14:33:23.415850 [info ] [Thread-3 (]: 34 of 71 PASS not_null_stg_subscriptions_subscription_id ....................... [[32mPASS[0m in 2.51s]
[0m14:33:23.418436 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m14:33:23.419240 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29
[0m14:33:23.419991 [info ] [Thread-3 (]: 38 of 71 START test relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_  [RUN]
[0m14:33:23.421201 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91, now test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29)
[0m14:33:23.421998 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29
[0m14:33:23.428725 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29"
[0m14:33:23.430146 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29
[0m14:33:23.441043 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29"
[0m14:33:23.442410 [debug] [Thread-3 (]: On test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select customer_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_subscription`
    where customer_id is not null
),

parent as (
    select customer_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_customer`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m14:33:23.443274 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:33:23.824900 [info ] [Thread-1 (]: 35 of 71 PASS not_null_stg_subscriptions_user_id ............................... [[32mPASS[0m in 2.63s]
[0m14:33:23.827074 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2
[0m14:33:23.827931 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85
[0m14:33:23.828497 [info ] [Thread-1 (]: 39 of 71 START test relationships_fct_order_customer_id__customer_id__ref_dim_customer_  [RUN]
[0m14:33:23.831903 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2, now test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85)
[0m14:33:23.832647 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85
[0m14:33:23.848192 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85"
[0m14:33:23.850852 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85
[0m14:33:23.855282 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85"
[0m14:33:23.858043 [debug] [Thread-1 (]: On test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select customer_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_order`
    where customer_id is not null
),

parent as (
    select customer_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_customer`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m14:33:23.858923 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:33:24.044483 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:1cdaf836-c65c-4963-b88b-63781b953d89&page=queryresults
[0m14:33:24.260976 [info ] [Thread-4 (]: 36 of 71 PASS not_null_stg_users_created_at .................................... [[32mPASS[0m in 2.59s]
[0m14:33:24.292117 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25
[0m14:33:24.298879 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.relationships_fct_shipment_order_id__order_id__ref_fct_order_.6096f95e39
[0m14:33:24.307343 [info ] [Thread-4 (]: 40 of 71 START test relationships_fct_shipment_order_id__order_id__ref_fct_order_  [RUN]
[0m14:33:24.312756 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25, now test.data_pipeline_project.relationships_fct_shipment_order_id__order_id__ref_fct_order_.6096f95e39)
[0m14:33:24.315410 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.relationships_fct_shipment_order_id__order_id__ref_fct_order_.6096f95e39
[0m14:33:24.331602 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_fct_shipment_order_id__order_id__ref_fct_order_.6096f95e39"
[0m14:33:24.333065 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.relationships_fct_shipment_order_id__order_id__ref_fct_order_.6096f95e39
[0m14:33:24.337268 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_fct_shipment_order_id__order_id__ref_fct_order_.6096f95e39"
[0m14:33:24.344469 [debug] [Thread-4 (]: On test.data_pipeline_project.relationships_fct_shipment_order_id__order_id__ref_fct_order_.6096f95e39: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_fct_shipment_order_id__order_id__ref_fct_order_.6096f95e39"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select order_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_shipment`
    where order_id is not null
),

parent as (
    select order_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_order`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m14:33:24.346550 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:33:24.761402 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:4a88ef70-20ca-4048-a5b1-d05ea142a3ba&page=queryresults
[0m14:33:24.925803 [info ] [Thread-2 (]: 37 of 71 PASS not_null_stg_users_user_id ....................................... [[32mPASS[0m in 2.40s]
[0m14:33:24.927241 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77
[0m14:33:24.927814 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781
[0m14:33:24.928505 [info ] [Thread-2 (]: 41 of 71 START test relationships_stg_addresses_user_id__user_id__ref_stg_users_  [RUN]
[0m14:33:24.929600 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77, now test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781)
[0m14:33:24.930303 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781
[0m14:33:24.937127 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781"
[0m14:33:24.938781 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781
[0m14:33:24.942547 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781"
[0m14:33:24.949934 [debug] [Thread-2 (]: On test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select user_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_addresses`
    where user_id is not null
),

parent as (
    select user_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m14:33:24.950570 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:33:25.177247 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:67fe034f-7ac7-4d4f-ad2c-75bac1424eef&page=queryresults
[0m14:33:25.485749 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:9d36b06a-0aff-49da-82d4-a2ac9b7fbe31&page=queryresults
[0m14:33:25.679235 [info ] [Thread-3 (]: 38 of 71 PASS relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_  [[32mPASS[0m in 2.26s]
[0m14:33:25.682751 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29
[0m14:33:25.684510 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737
[0m14:33:25.686074 [info ] [Thread-3 (]: 42 of 71 START test relationships_stg_orders_user_id__user_id__ref_stg_users_ .. [RUN]
[0m14:33:25.688410 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29, now test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737)
[0m14:33:25.689507 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737
[0m14:33:25.707990 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737"
[0m14:33:25.708948 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737
[0m14:33:25.712283 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737"
[0m14:33:25.713080 [debug] [Thread-3 (]: On test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select user_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_orders`
    where user_id is not null
),

parent as (
    select user_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m14:33:25.713740 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:33:26.264620 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:385e41e0-40a4-4703-9b70-6e8b764db2b5&page=queryresults
[0m14:33:26.326158 [info ] [Thread-1 (]: 39 of 71 PASS relationships_fct_order_customer_id__customer_id__ref_dim_customer_  [[32mPASS[0m in 2.50s]
[0m14:33:26.329675 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85
[0m14:33:26.330901 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8
[0m14:33:26.332444 [info ] [Thread-1 (]: 43 of 71 START test relationships_stg_shipments_user_id__user_id__ref_stg_users_  [RUN]
[0m14:33:26.335543 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85, now test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8)
[0m14:33:26.337036 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8
[0m14:33:26.352650 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8"
[0m14:33:26.354151 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8
[0m14:33:26.358655 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8"
[0m14:33:26.359860 [debug] [Thread-1 (]: On test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select user_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_shipments`
    where user_id is not null
),

parent as (
    select user_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m14:33:26.361327 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:33:26.451485 [error] [Thread-4 (]: 40 of 71 FAIL 476 relationships_fct_shipment_order_id__order_id__ref_fct_order_  [[31mFAIL 476[0m in 2.14s]
[0m14:33:26.454435 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.relationships_fct_shipment_order_id__order_id__ref_fct_order_.6096f95e39
[0m14:33:26.456525 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m14:33:26.458273 [info ] [Thread-4 (]: 44 of 71 START test relationships_stg_subscriptions_user_id__user_id__ref_stg_users_  [RUN]
[0m14:33:26.461132 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_fct_shipment_order_id__order_id__ref_fct_order_.6096f95e39, now test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e)
[0m14:33:26.463337 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m14:33:26.490181 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e"
[0m14:33:26.491120 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m14:33:26.497687 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e"
[0m14:33:26.500485 [debug] [Thread-4 (]: On test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select user_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_subscriptions`
    where user_id is not null
),

parent as (
    select user_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m14:33:26.501615 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:33:27.153185 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:208f5cc0-9cc3-4dce-b909-2e05f9b6b71d&page=queryresults
[0m14:33:27.160180 [info ] [Thread-2 (]: 41 of 71 PASS relationships_stg_addresses_user_id__user_id__ref_stg_users_ ..... [[32mPASS[0m in 2.23s]
[0m14:33:27.163426 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781
[0m14:33:27.164289 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6
[0m14:33:27.165396 [info ] [Thread-2 (]: 45 of 71 START test source_not_null_raw_data_addresses__id ..................... [RUN]
[0m14:33:27.166961 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781, now test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6)
[0m14:33:27.167541 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6
[0m14:33:27.175856 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6"
[0m14:33:27.177188 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6
[0m14:33:27.185086 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6"
[0m14:33:27.189909 [debug] [Thread-2 (]: On test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`addresses`
where _id is null



  
  
      
    ) dbt_internal_test
[0m14:33:27.190717 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:33:27.817192 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:0c52430a-73da-4f91-866d-2d778a3f4898&page=queryresults
[0m14:33:27.938550 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:4b6a033a-8519-4c97-94f4-c42d89064151&page=queryresults
[0m14:33:28.017720 [info ] [Thread-3 (]: 42 of 71 PASS relationships_stg_orders_user_id__user_id__ref_stg_users_ ........ [[32mPASS[0m in 2.33s]
[0m14:33:28.023734 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737
[0m14:33:28.025636 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc
[0m14:33:28.027052 [info ] [Thread-3 (]: 46 of 71 START test source_not_null_raw_data_cities__id ........................ [RUN]
[0m14:33:28.029264 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737, now test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc)
[0m14:33:28.029934 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc
[0m14:33:28.036746 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc"
[0m14:33:28.038022 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc
[0m14:33:28.048371 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc"
[0m14:33:28.049795 [debug] [Thread-3 (]: On test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`cities`
where _id is null



  
  
      
    ) dbt_internal_test
[0m14:33:28.050626 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:33:28.429934 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:01eeb68c-8703-42d1-818b-9abc97879d39&page=queryresults
[0m14:33:28.758770 [info ] [Thread-1 (]: 43 of 71 PASS relationships_stg_shipments_user_id__user_id__ref_stg_users_ ..... [[32mPASS[0m in 2.42s]
[0m14:33:28.760044 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8
[0m14:33:28.760658 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd
[0m14:33:28.761368 [info ] [Thread-1 (]: 47 of 71 START test source_not_null_raw_data_countries__id ..................... [RUN]
[0m14:33:28.762135 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8, now test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd)
[0m14:33:28.762689 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd
[0m14:33:28.768827 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd"
[0m14:33:28.770184 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd
[0m14:33:28.779230 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd"
[0m14:33:28.780430 [debug] [Thread-1 (]: On test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`countries`
where _id is null



  
  
      
    ) dbt_internal_test
[0m14:33:28.781095 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:33:28.862578 [info ] [Thread-4 (]: 44 of 71 PASS relationships_stg_subscriptions_user_id__user_id__ref_stg_users_ . [[32mPASS[0m in 2.40s]
[0m14:33:28.864544 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m14:33:28.866208 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be
[0m14:33:28.866966 [info ] [Thread-4 (]: 48 of 71 START test source_not_null_raw_data_neighborhoods__id ................. [RUN]
[0m14:33:28.868696 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e, now test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be)
[0m14:33:28.869496 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be
[0m14:33:28.877259 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be"
[0m14:33:28.878390 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be
[0m14:33:28.893967 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be"
[0m14:33:28.902655 [debug] [Thread-4 (]: On test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`neighborhoods`
where _id is null



  
  
      
    ) dbt_internal_test
[0m14:33:28.904319 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:33:29.291089 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:2825bcc9-e804-423b-a820-ac0808482faf&page=queryresults
[0m14:33:29.691249 [info ] [Thread-2 (]: 45 of 71 PASS source_not_null_raw_data_addresses__id ........................... [[32mPASS[0m in 2.52s]
[0m14:33:29.692703 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6
[0m14:33:29.693602 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878
[0m14:33:29.694217 [info ] [Thread-2 (]: 49 of 71 START test source_not_null_raw_data_orders__id ........................ [RUN]
[0m14:33:29.695199 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6, now test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878)
[0m14:33:29.695946 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878
[0m14:33:29.703702 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878"
[0m14:33:29.705489 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878
[0m14:33:29.708860 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878"
[0m14:33:29.710363 [debug] [Thread-2 (]: On test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`orders`
where _id is null



  
  
      
    ) dbt_internal_test
[0m14:33:29.711161 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:33:30.037863 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:cd675232-7eda-44d0-9717-7b04acb0ec97&page=queryresults
[0m14:33:30.044811 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:acf481c6-995a-4eb4-9d0b-61735f79a40e&page=queryresults
[0m14:33:30.305751 [info ] [Thread-3 (]: 46 of 71 PASS source_not_null_raw_data_cities__id .............................. [[32mPASS[0m in 2.28s]
[0m14:33:30.307429 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc
[0m14:33:30.309660 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f
[0m14:33:30.310336 [info ] [Thread-3 (]: 50 of 71 START test source_not_null_raw_data_orders__user ...................... [RUN]
[0m14:33:30.311532 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc, now test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f)
[0m14:33:30.312192 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f
[0m14:33:30.324356 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f"
[0m14:33:30.331282 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f
[0m14:33:30.335859 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f"
[0m14:33:30.342335 [debug] [Thread-3 (]: On test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _user
from `data-pipeline-project-474812`.`raw_data`.`orders`
where _user is null



  
  
      
    ) dbt_internal_test
[0m14:33:30.343142 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:33:30.920912 [info ] [Thread-4 (]: 48 of 71 PASS source_not_null_raw_data_neighborhoods__id ....................... [[32mPASS[0m in 2.05s]
[0m14:33:30.921746 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be
[0m14:33:30.922579 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2
[0m14:33:30.923391 [info ] [Thread-4 (]: 51 of 71 START test source_not_null_raw_data_shipments__id ..................... [RUN]
[0m14:33:30.924193 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be, now test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2)
[0m14:33:30.924885 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2
[0m14:33:30.930597 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2"
[0m14:33:30.933055 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2
[0m14:33:30.942497 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2"
[0m14:33:30.943474 [debug] [Thread-4 (]: On test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`shipments`
where _id is null



  
  
      
    ) dbt_internal_test
[0m14:33:30.944116 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:33:31.075104 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:a0c32921-b2fa-4277-84fa-628b469fb907&page=queryresults
[0m14:33:31.104005 [info ] [Thread-1 (]: 47 of 71 PASS source_not_null_raw_data_countries__id ........................... [[32mPASS[0m in 2.34s]
[0m14:33:31.107310 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd
[0m14:33:31.110199 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1
[0m14:33:31.112937 [info ] [Thread-1 (]: 52 of 71 START test source_not_null_raw_data_shipments__order .................. [RUN]
[0m14:33:31.113824 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd, now test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1)
[0m14:33:31.115561 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1
[0m14:33:31.123687 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1"
[0m14:33:31.128626 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1
[0m14:33:31.133616 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1"
[0m14:33:31.135632 [debug] [Thread-1 (]: On test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _order
from `data-pipeline-project-474812`.`raw_data`.`shipments`
where _order is null



  
  
      
    ) dbt_internal_test
[0m14:33:31.139045 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:33:31.679233 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:1a30886c-4c3d-4eae-88be-14bd42bcb71f&page=queryresults
[0m14:33:32.051643 [info ] [Thread-2 (]: 49 of 71 PASS source_not_null_raw_data_orders__id .............................. [[32mPASS[0m in 2.36s]
[0m14:33:32.053434 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878
[0m14:33:32.055128 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98
[0m14:33:32.056224 [info ] [Thread-2 (]: 53 of 71 START test source_not_null_raw_data_shipments__user ................... [RUN]
[0m14:33:32.057293 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878, now test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98)
[0m14:33:32.058283 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98
[0m14:33:32.065674 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98"
[0m14:33:32.067544 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98
[0m14:33:32.076372 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98"
[0m14:33:32.078335 [debug] [Thread-2 (]: On test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _user
from `data-pipeline-project-474812`.`raw_data`.`shipments`
where _user is null



  
  
      
    ) dbt_internal_test
[0m14:33:32.079039 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:33:32.469903 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:c5523fa7-7d9f-4822-8811-3113ec98ad60&page=queryresults
[0m14:33:32.757860 [info ] [Thread-3 (]: 50 of 71 PASS source_not_null_raw_data_orders__user ............................ [[32mPASS[0m in 2.45s]
[0m14:33:32.760633 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f
[0m14:33:32.762648 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86
[0m14:33:32.764483 [info ] [Thread-3 (]: 54 of 71 START test source_not_null_raw_data_states__id ........................ [RUN]
[0m14:33:32.771136 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f, now test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86)
[0m14:33:32.773113 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86
[0m14:33:32.781520 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86"
[0m14:33:32.783157 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86
[0m14:33:32.786019 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86"
[0m14:33:32.786826 [debug] [Thread-3 (]: On test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`states`
where _id is null



  
  
      
    ) dbt_internal_test
[0m14:33:32.787974 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:33:33.007743 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:ae3ed028-a231-4e40-8f33-aeb5798d3446&page=queryresults
[0m14:33:33.364198 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:96d51813-e5ae-4550-a143-3562d90d8454&page=queryresults
[0m14:33:33.578627 [info ] [Thread-4 (]: 51 of 71 PASS source_not_null_raw_data_shipments__id ........................... [[32mPASS[0m in 2.65s]
[0m14:33:33.587353 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2
[0m14:33:33.589348 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a
[0m14:33:33.590799 [info ] [Thread-4 (]: 55 of 71 START test source_not_null_raw_data_subscriptions__id ................. [RUN]
[0m14:33:33.592728 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2, now test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a)
[0m14:33:33.594334 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a
[0m14:33:33.605554 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a"
[0m14:33:33.611394 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a
[0m14:33:33.619113 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a"
[0m14:33:33.626267 [debug] [Thread-4 (]: On test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`subscriptions`
where _id is null



  
  
      
    ) dbt_internal_test
[0m14:33:33.631263 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:33:33.908901 [info ] [Thread-1 (]: 52 of 71 PASS source_not_null_raw_data_shipments__order ........................ [[32mPASS[0m in 2.79s]
[0m14:33:33.912802 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1
[0m14:33:33.915354 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6
[0m14:33:33.916972 [info ] [Thread-1 (]: 56 of 71 START test source_not_null_raw_data_subscriptions__user ............... [RUN]
[0m14:33:33.919401 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1, now test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6)
[0m14:33:33.924431 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6
[0m14:33:33.936400 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6"
[0m14:33:33.940155 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6
[0m14:33:33.943708 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6"
[0m14:33:33.946160 [debug] [Thread-1 (]: On test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _user
from `data-pipeline-project-474812`.`raw_data`.`subscriptions`
where _user is null



  
  
      
    ) dbt_internal_test
[0m14:33:33.947219 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:33:33.982703 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:2c06080e-9260-4c01-b1f3-0ca37ebe130c&page=queryresults
[0m14:33:34.369859 [info ] [Thread-2 (]: 53 of 71 PASS source_not_null_raw_data_shipments__user ......................... [[32mPASS[0m in 2.31s]
[0m14:33:34.372267 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98
[0m14:33:34.374220 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b
[0m14:33:34.376805 [info ] [Thread-2 (]: 57 of 71 START test source_not_null_raw_data_users__id ......................... [RUN]
[0m14:33:34.378663 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98, now test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b)
[0m14:33:34.379398 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b
[0m14:33:34.384727 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b"
[0m14:33:34.387211 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b
[0m14:33:34.391702 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b"
[0m14:33:34.394154 [debug] [Thread-2 (]: On test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`users`
where _id is null



  
  
      
    ) dbt_internal_test
[0m14:33:34.394959 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:33:34.835885 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:2c25ed0f-1341-41e3-abe8-db16365aa9f5&page=queryresults
[0m14:33:35.042366 [info ] [Thread-3 (]: 54 of 71 PASS source_not_null_raw_data_states__id .............................. [[32mPASS[0m in 2.27s]
[0m14:33:35.050668 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86
[0m14:33:35.052561 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1
[0m14:33:35.056712 [info ] [Thread-3 (]: 58 of 71 START test unique_dim_customer_customer_id ............................ [RUN]
[0m14:33:35.058184 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86, now test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1)
[0m14:33:35.061890 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1
[0m14:33:35.076746 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1"
[0m14:33:35.080352 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1
[0m14:33:35.093093 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1"
[0m14:33:35.095668 [debug] [Thread-3 (]: On test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select customer_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_customer`
  where customer_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:33:35.096145 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:33:35.282161 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:491a159f-0091-4f0a-93ef-74f7d365dee3&page=queryresults
[0m14:33:35.718981 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:fa575a9b-0c88-4fe4-8f4a-1687ee2832c9&page=queryresults
[0m14:33:35.773965 [info ] [Thread-4 (]: 55 of 71 PASS source_not_null_raw_data_subscriptions__id ....................... [[32mPASS[0m in 2.18s]
[0m14:33:35.776370 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a
[0m14:33:35.778613 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469
[0m14:33:35.779924 [info ] [Thread-4 (]: 59 of 71 START test unique_dim_subscription_subscription_id .................... [RUN]
[0m14:33:35.781154 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a, now test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469)
[0m14:33:35.783436 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469
[0m14:33:35.792320 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469"
[0m14:33:35.798042 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469
[0m14:33:35.804240 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469"
[0m14:33:35.805914 [debug] [Thread-4 (]: On test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select subscription_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_subscription`
  where subscription_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:33:35.807320 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:33:36.156185 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:2d268240-600d-4bfc-8537-5fbb4af0bce0&page=queryresults
[0m14:33:36.220980 [info ] [Thread-1 (]: 56 of 71 PASS source_not_null_raw_data_subscriptions__user ..................... [[32mPASS[0m in 2.30s]
[0m14:33:36.222677 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6
[0m14:33:36.223749 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.unique_fct_marketing_spend_date.e93d255723
[0m14:33:36.224333 [info ] [Thread-1 (]: 60 of 71 START test unique_fct_marketing_spend_date ............................ [RUN]
[0m14:33:36.225084 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6, now test.data_pipeline_project.unique_fct_marketing_spend_date.e93d255723)
[0m14:33:36.226218 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.unique_fct_marketing_spend_date.e93d255723
[0m14:33:36.234466 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.unique_fct_marketing_spend_date.e93d255723"
[0m14:33:36.238407 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.unique_fct_marketing_spend_date.e93d255723
[0m14:33:36.241829 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.unique_fct_marketing_spend_date.e93d255723"
[0m14:33:36.242574 [debug] [Thread-1 (]: On test.data_pipeline_project.unique_fct_marketing_spend_date.e93d255723: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_fct_marketing_spend_date.e93d255723"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select date as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_marketing_spend`
  where date is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:33:36.246208 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:33:36.696688 [info ] [Thread-2 (]: 57 of 71 PASS source_not_null_raw_data_users__id ............................... [[32mPASS[0m in 2.32s]
[0m14:33:36.698350 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b
[0m14:33:36.699286 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.unique_fct_order_order_id.653705d6a5
[0m14:33:36.700051 [info ] [Thread-2 (]: 61 of 71 START test unique_fct_order_order_id .................................. [RUN]
[0m14:33:36.700998 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b, now test.data_pipeline_project.unique_fct_order_order_id.653705d6a5)
[0m14:33:36.701673 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.unique_fct_order_order_id.653705d6a5
[0m14:33:36.706966 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.unique_fct_order_order_id.653705d6a5"
[0m14:33:36.714483 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.unique_fct_order_order_id.653705d6a5
[0m14:33:36.731690 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.unique_fct_order_order_id.653705d6a5"
[0m14:33:36.733170 [debug] [Thread-2 (]: On test.data_pipeline_project.unique_fct_order_order_id.653705d6a5: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_fct_order_order_id.653705d6a5"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select order_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_order`
  where order_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:33:36.734306 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:33:37.130474 [error] [Thread-3 (]: 58 of 71 FAIL 16 unique_dim_customer_customer_id ............................... [[31mFAIL 16[0m in 2.07s]
[0m14:33:37.135483 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1
[0m14:33:37.139384 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:93df51de-8979-4421-8c30-a077f2937bfd&page=queryresults
[0m14:33:37.140811 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3
[0m14:33:37.145066 [info ] [Thread-3 (]: 62 of 71 START test unique_fct_shipment_shipment_id ............................ [RUN]
[0m14:33:37.146062 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1, now test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3)
[0m14:33:37.147480 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3
[0m14:33:37.153842 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3"
[0m14:33:37.156181 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3
[0m14:33:37.159084 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3"
[0m14:33:37.163602 [debug] [Thread-3 (]: On test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select shipment_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_shipment`
  where shipment_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:33:37.168801 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:33:37.591386 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:55db116d-9ca7-4688-9bb5-afc18e0eb74c&page=queryresults
[0m14:33:37.992561 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:35514c4e-c317-4417-bae4-b381215c9844&page=queryresults
[0m14:33:38.065343 [info ] [Thread-4 (]: 59 of 71 PASS unique_dim_subscription_subscription_id .......................... [[32mPASS[0m in 2.28s]
[0m14:33:38.068919 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469
[0m14:33:38.070468 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd
[0m14:33:38.072376 [info ] [Thread-4 (]: 63 of 71 START test unique_mart_acquisition_efficiency_date .................... [RUN]
[0m14:33:38.075471 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469, now test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd)
[0m14:33:38.077267 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd
[0m14:33:38.093356 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd"
[0m14:33:38.095515 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd
[0m14:33:38.099318 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd"
[0m14:33:38.100982 [debug] [Thread-4 (]: On test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select date as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_acquisition_efficiency`
  where date is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:33:38.101642 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:33:38.293618 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:15d85511-5b5e-4946-8e9c-e03e7968ba48&page=queryresults
[0m14:33:38.546501 [error] [Thread-1 (]: 60 of 71 FAIL 8 unique_fct_marketing_spend_date ................................ [[31mFAIL 8[0m in 2.32s]
[0m14:33:38.557463 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.unique_fct_marketing_spend_date.e93d255723
[0m14:33:38.563230 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8
[0m14:33:38.569219 [info ] [Thread-1 (]: 64 of 71 START test unique_mart_revenue_daily_date ............................. [RUN]
[0m14:33:38.571632 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_fct_marketing_spend_date.e93d255723, now test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8)
[0m14:33:38.574190 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8
[0m14:33:38.589023 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8"
[0m14:33:38.595145 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8
[0m14:33:38.600699 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8"
[0m14:33:38.607318 [debug] [Thread-1 (]: On test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select date as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_revenue_daily`
  where date is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:33:38.609033 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:33:38.968896 [info ] [Thread-2 (]: 61 of 71 PASS unique_fct_order_order_id ........................................ [[32mPASS[0m in 2.27s]
[0m14:33:38.970347 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.unique_fct_order_order_id.653705d6a5
[0m14:33:38.971206 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c
[0m14:33:38.972150 [info ] [Thread-2 (]: 65 of 71 START test unique_mart_subscription_daily_date ........................ [RUN]
[0m14:33:38.973133 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_fct_order_order_id.653705d6a5, now test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c)
[0m14:33:38.973701 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c
[0m14:33:38.979511 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c"
[0m14:33:38.981654 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c
[0m14:33:38.986524 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c"
[0m14:33:38.992224 [debug] [Thread-2 (]: On test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select date as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily`
  where date is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:33:38.992928 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:33:39.271694 [info ] [Thread-3 (]: 62 of 71 PASS unique_fct_shipment_shipment_id .................................. [[32mPASS[0m in 2.12s]
[0m14:33:39.288202 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3
[0m14:33:39.291572 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00
[0m14:33:39.293109 [info ] [Thread-3 (]: 66 of 71 START test unique_stg_addresses_address_id ............................ [RUN]
[0m14:33:39.296476 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3, now test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00)
[0m14:33:39.298199 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00
[0m14:33:39.314741 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00"
[0m14:33:39.317867 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00
[0m14:33:39.323546 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00"
[0m14:33:39.327828 [debug] [Thread-3 (]: On test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select address_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_addresses`
  where address_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:33:39.328400 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:33:39.457624 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:1733b11d-111f-4233-91f8-a450bfdab576&page=queryresults
[0m14:33:39.789176 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:435b72c3-e3af-4942-b77a-0d7a17e553c7&page=queryresults
[0m14:33:40.161626 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:a94dcd60-dc3e-49e8-9110-a38887359f45&page=queryresults
[0m14:33:40.356152 [info ] [Thread-4 (]: 63 of 71 PASS unique_mart_acquisition_efficiency_date .......................... [[32mPASS[0m in 2.28s]
[0m14:33:40.360288 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd
[0m14:33:40.362406 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.unique_stg_countries_country_id.8679936442
[0m14:33:40.363979 [info ] [Thread-4 (]: 67 of 71 START test unique_stg_countries_country_id ............................ [RUN]
[0m14:33:40.367004 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd, now test.data_pipeline_project.unique_stg_countries_country_id.8679936442)
[0m14:33:40.368289 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.unique_stg_countries_country_id.8679936442
[0m14:33:40.377222 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.unique_stg_countries_country_id.8679936442"
[0m14:33:40.378268 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.unique_stg_countries_country_id.8679936442
[0m14:33:40.381284 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.unique_stg_countries_country_id.8679936442"
[0m14:33:40.382054 [debug] [Thread-4 (]: On test.data_pipeline_project.unique_stg_countries_country_id.8679936442: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_stg_countries_country_id.8679936442"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select country_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_countries`
  where country_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:33:40.383264 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:33:40.718964 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:b8768707-c7a0-4cda-9306-03b6955ce80f&page=queryresults
[0m14:33:40.825927 [info ] [Thread-1 (]: 64 of 71 PASS unique_mart_revenue_daily_date ................................... [[32mPASS[0m in 2.24s]
[0m14:33:40.834964 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8
[0m14:33:40.838232 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a
[0m14:33:40.846531 [info ] [Thread-1 (]: 68 of 71 START test unique_stg_orders_order_id ................................. [RUN]
[0m14:33:40.849573 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8, now test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a)
[0m14:33:40.851551 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a
[0m14:33:40.872306 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a"
[0m14:33:40.875133 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a
[0m14:33:40.879402 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a"
[0m14:33:40.883094 [debug] [Thread-1 (]: On test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select order_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_orders`
  where order_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:33:40.884169 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:33:41.005547 [info ] [Thread-2 (]: 65 of 71 PASS unique_mart_subscription_daily_date .............................. [[32mPASS[0m in 2.03s]
[0m14:33:41.023987 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c
[0m14:33:41.025511 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c
[0m14:33:41.031508 [info ] [Thread-2 (]: 69 of 71 START test unique_stg_shipments_shipment_id ........................... [RUN]
[0m14:33:41.034160 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c, now test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c)
[0m14:33:41.034959 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c
[0m14:33:41.053234 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c"
[0m14:33:41.055800 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c
[0m14:33:41.063948 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c"
[0m14:33:41.067332 [debug] [Thread-2 (]: On test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select shipment_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_shipments`
  where shipment_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:33:41.068023 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:33:41.617307 [info ] [Thread-3 (]: 66 of 71 PASS unique_stg_addresses_address_id .................................. [[32mPASS[0m in 2.32s]
[0m14:33:41.624009 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00
[0m14:33:41.625310 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m14:33:41.627304 [info ] [Thread-3 (]: 70 of 71 START test unique_stg_subscriptions_subscription_id ................... [RUN]
[0m14:33:41.629208 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00, now test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e)
[0m14:33:41.630364 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m14:33:41.637872 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e"
[0m14:33:41.649910 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m14:33:41.660213 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e"
[0m14:33:41.664747 [debug] [Thread-3 (]: On test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select subscription_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_subscriptions`
  where subscription_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:33:41.663243 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:b6126f49-6bfe-4348-a3ff-e8c3cf054e7b&page=queryresults
[0m14:33:41.667995 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:33:42.119245 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:bcdcae4e-e746-49b8-9565-3286e1b1e3fc&page=queryresults
[0m14:33:42.449367 [info ] [Thread-4 (]: 67 of 71 PASS unique_stg_countries_country_id .................................. [[32mPASS[0m in 2.08s]
[0m14:33:42.450890 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.unique_stg_countries_country_id.8679936442
[0m14:33:42.451829 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b
[0m14:33:42.452339 [info ] [Thread-4 (]: 71 of 71 START test unique_stg_users_user_id ................................... [RUN]
[0m14:33:42.453247 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_stg_countries_country_id.8679936442, now test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b)
[0m14:33:42.453929 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b
[0m14:33:42.462204 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b"
[0m14:33:42.463572 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:38386faa-0c4d-40bb-9b54-27796db67dc7&page=queryresults
[0m14:33:42.465963 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b
[0m14:33:42.477467 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b"
[0m14:33:42.479078 [debug] [Thread-4 (]: On test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select user_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
  where user_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:33:42.480196 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:33:42.906437 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:d483a00b-bf32-45cd-a42b-c7d235f568d4&page=queryresults
[0m14:33:42.989623 [info ] [Thread-1 (]: 68 of 71 PASS unique_stg_orders_order_id ....................................... [[32mPASS[0m in 2.14s]
[0m14:33:42.997545 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a
[0m14:33:43.663144 [info ] [Thread-2 (]: 69 of 71 PASS unique_stg_shipments_shipment_id ................................. [[32mPASS[0m in 2.63s]
[0m14:33:43.665144 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c
[0m14:33:43.675117 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:3c7ef556-f7b5-4d15-8d3e-f3cb8875a075&page=queryresults
[0m14:33:43.942404 [info ] [Thread-3 (]: 70 of 71 PASS unique_stg_subscriptions_subscription_id ......................... [[32mPASS[0m in 2.31s]
[0m14:33:43.950811 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m14:33:45.100089 [info ] [Thread-4 (]: 71 of 71 PASS unique_stg_users_user_id ......................................... [[32mPASS[0m in 2.65s]
[0m14:33:45.105425 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b
[0m14:33:45.112717 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:33:45.116317 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:33:45.117221 [debug] [MainThread]: Connection 'test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c' was properly closed.
[0m14:33:45.118105 [debug] [MainThread]: Connection 'test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a' was properly closed.
[0m14:33:45.118847 [debug] [MainThread]: Connection 'test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e' was properly closed.
[0m14:33:45.120100 [debug] [MainThread]: Connection 'test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b' was properly closed.
[0m14:33:45.129972 [info ] [MainThread]: 
[0m14:33:45.132303 [info ] [MainThread]: Finished running 71 data tests in 0 hours 0 minutes and 39.78 seconds (39.78s).
[0m14:33:45.143235 [debug] [MainThread]: Command end result
[0m14:33:45.203752 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m14:33:45.206945 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m14:33:45.223994 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m14:33:45.225270 [info ] [MainThread]: 
[0m14:33:45.226115 [info ] [MainThread]: [31mCompleted with 12 errors, 0 partial successes, and 1 warning:[0m
[0m14:33:45.226907 [info ] [MainThread]: 
[0m14:33:45.227424 [error] [MainThread]: [31mFailure in test accepted_range_fct_marketing_spend_spend_try__0 (models/intermediate/int_facts_marts_schema.yml)[0m
[0m14:33:45.227876 [error] [MainThread]:   Compilation Error in test accepted_range_fct_marketing_spend_spend_try__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:33:45.228272 [info ] [MainThread]: 
[0m14:33:45.228729 [error] [MainThread]: [31mFailure in test accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0 (models/intermediate/int_facts_marts_schema.yml)[0m
[0m14:33:45.229502 [error] [MainThread]:   Compilation Error in test accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:33:45.230336 [info ] [MainThread]: 
[0m14:33:45.231273 [error] [MainThread]: [31mFailure in test accepted_range_fct_order_net_revenue__0 (models/intermediate/int_facts_marts_schema.yml)[0m
[0m14:33:45.232728 [error] [MainThread]:   Compilation Error in test accepted_range_fct_order_net_revenue__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:33:45.233988 [info ] [MainThread]: 
[0m14:33:45.235206 [error] [MainThread]: [31mFailure in test accepted_range_fct_order_discount_total__0 (models/intermediate/int_facts_marts_schema.yml)[0m
[0m14:33:45.236105 [error] [MainThread]:   Compilation Error in test accepted_range_fct_order_discount_total__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:33:45.237238 [info ] [MainThread]: 
[0m14:33:45.238170 [error] [MainThread]: [31mFailure in test accepted_range_mart_kpis_latest_cac_payback_months__0 (models/intermediate/int_facts_marts_schema.yml)[0m
[0m14:33:45.239305 [error] [MainThread]:   Compilation Error in test accepted_range_mart_kpis_latest_cac_payback_months__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:33:45.240091 [info ] [MainThread]: 
[0m14:33:45.240994 [error] [MainThread]: [31mFailure in test accepted_range_mart_revenue_daily_daily_revenue__0 (models/intermediate/int_facts_marts_schema.yml)[0m
[0m14:33:45.242107 [error] [MainThread]:   Compilation Error in test accepted_range_mart_revenue_daily_daily_revenue__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:33:45.243240 [info ] [MainThread]: 
[0m14:33:45.244296 [error] [MainThread]: [31mFailure in test accepted_range_mart_kpis_latest_monthly_recurring_revenue__0 (models/intermediate/int_facts_marts_schema.yml)[0m
[0m14:33:45.245377 [error] [MainThread]:   Compilation Error in test accepted_range_mart_kpis_latest_monthly_recurring_revenue__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:33:45.246328 [info ] [MainThread]: 
[0m14:33:45.247899 [error] [MainThread]: [31mFailure in test accepted_range_mart_subscription_daily_active_subscribers__0 (models/intermediate/int_facts_marts_schema.yml)[0m
[0m14:33:45.249585 [error] [MainThread]:   Compilation Error in test accepted_range_mart_subscription_daily_active_subscribers__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:33:45.251715 [info ] [MainThread]: 
[0m14:33:45.254270 [error] [MainThread]: [31mFailure in test accepted_range_mart_subscription_daily_new_subscribers__0 (models/intermediate/int_facts_marts_schema.yml)[0m
[0m14:33:45.257524 [error] [MainThread]:   Compilation Error in test accepted_range_mart_subscription_daily_new_subscribers__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:33:45.259383 [info ] [MainThread]: 
[0m14:33:45.262957 [error] [MainThread]: [31mFailure in test relationships_fct_shipment_order_id__order_id__ref_fct_order_ (models/intermediate/int_facts_marts_schema.yml)[0m
[0m14:33:45.266341 [error] [MainThread]:   Got 476 results, configured to fail if != 0
[0m14:33:45.271537 [info ] [MainThread]: 
[0m14:33:45.276353 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_project/models/intermediate/int_facts_marts_schema.yml/relationships_fct_shipment_order_id__order_id__ref_fct_order_.sql
[0m14:33:45.279098 [info ] [MainThread]: 
[0m14:33:45.281108 [error] [MainThread]: [31mFailure in test unique_dim_customer_customer_id (models/intermediate/int_facts_marts_schema.yml)[0m
[0m14:33:45.282182 [error] [MainThread]:   Got 16 results, configured to fail if != 0
[0m14:33:45.283049 [info ] [MainThread]: 
[0m14:33:45.284088 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_project/models/intermediate/int_facts_marts_schema.yml/unique_dim_customer_customer_id.sql
[0m14:33:45.285290 [info ] [MainThread]: 
[0m14:33:45.286187 [error] [MainThread]: [31mFailure in test unique_fct_marketing_spend_date (models/intermediate/int_facts_marts_schema.yml)[0m
[0m14:33:45.287035 [error] [MainThread]:   Got 8 results, configured to fail if != 0
[0m14:33:45.289260 [info ] [MainThread]: 
[0m14:33:45.290349 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_project/models/intermediate/int_facts_marts_schema.yml/unique_fct_marketing_spend_date.sql
[0m14:33:45.291170 [info ] [MainThread]: 
[0m14:33:45.292066 [warn ] [MainThread]: [33mWarning in test not_null_int_shipments_decomposed_delivered_at (models/staging/staging_schema.yml)[0m
[0m14:33:45.292740 [warn ] [MainThread]: Got 303 results, configured to warn if != 0
[0m14:33:45.294334 [info ] [MainThread]: 
[0m14:33:45.296456 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_project/models/staging/staging_schema.yml/not_null_int_shipments_decomposed_delivered_at.sql
[0m14:33:45.298262 [info ] [MainThread]: 
[0m14:33:45.299033 [info ] [MainThread]: Done. PASS=58 WARN=1 ERROR=12 SKIP=0 NO-OP=0 TOTAL=71
[0m14:33:45.300689 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 27 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m14:33:45.302466 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 45.39996, "process_in_blocks": "0", "process_kernel_time": 5.224983, "process_mem_max_rss": "401288", "process_out_blocks": "8912", "process_user_time": 17.064274}
[0m14:33:45.304421 [debug] [MainThread]: Command `dbt test` failed at 14:33:45.304178 after 45.40 seconds
[0m14:33:45.305585 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c5c55b7e2a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c5c304ade20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c5c33baca70>]}
[0m14:33:45.306800 [debug] [MainThread]: Flushing usage events
[0m14:33:46.750068 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:38:14.140584 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c8730c63ef0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c8730809e20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c8730c639b0>]}


============================== 14:38:14.144631 | 7a8af05d-9d43-4879-bd08-0e287bd84a4d ==============================
[0m14:38:14.144631 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m14:38:14.145418 [debug] [MainThread]: running dbt with arguments {'target_path': 'None', 'warn_error': 'None', 'log_format': 'default', 'debug': 'False', 'use_colors': 'True', 'cache_selected_only': 'False', 'printer_width': '80', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'quiet': 'False', 'profiles_dir': '/home/ecem/.dbt', 'invocation_command': 'dbt run-operation clean', 'fail_fast': 'False', 'indirect_selection': 'eager', 'empty': 'None', 'write_json': 'True', 'log_cache_events': 'False', 'static_parser': 'True', 'use_experimental_parser': 'False', 'version_check': 'True', 'send_anonymous_usage_stats': 'True', 'no_print': 'None', 'partial_parse': 'True', 'introspect': 'True'}
[0m14:38:16.419047 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7a8af05d-9d43-4879-bd08-0e287bd84a4d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c8710d47560>]}
[0m14:38:16.483302 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7a8af05d-9d43-4879-bd08-0e287bd84a4d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c8710b61670>]}
[0m14:38:16.485528 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m14:38:16.791611 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m14:38:17.084111 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:38:17.087398 [debug] [MainThread]: Partial parsing: updated file: data_pipeline_project://models/facts/fct_shipment.sql
[0m14:38:17.090169 [debug] [MainThread]: Partial parsing: updated file: data_pipeline_project://models/intermediate/int_facts_marts_schema.yml
[0m14:38:17.649394 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7a8af05d-9d43-4879-bd08-0e287bd84a4d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c870f6068d0>]}
[0m14:38:17.805344 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m14:38:17.808782 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m14:38:17.829609 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7a8af05d-9d43-4879-bd08-0e287bd84a4d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c870f687230>]}
[0m14:38:17.830405 [info ] [MainThread]: Found 21 models, 71 data tests, 10 sources, 891 macros
[0m14:38:17.831105 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7a8af05d-9d43-4879-bd08-0e287bd84a4d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c870f59a2d0>]}
[0m14:38:17.832247 [debug] [MainThread]: Acquiring new bigquery connection 'macro_clean'
[0m14:38:17.833443 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:38:17.836048 [error] [MainThread]: Encountered an error while running operation: Runtime Error
  dbt could not find a macro with the name "clean" in any package
[0m14:38:17.838319 [debug] [MainThread]: Traceback (most recent call last):
  File "/home/ecem/Desktop/data-pipeline-project/venv/lib/python3.12/site-packages/dbt/task/run_operation.py", line 64, in run
    self._run_unsafe(package_name, macro_name)
  File "/home/ecem/Desktop/data-pipeline-project/venv/lib/python3.12/site-packages/dbt/task/run_operation.py", line 45, in _run_unsafe
    res = adapter.execute_macro(
          ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ecem/Desktop/data-pipeline-project/venv/lib/python3.12/site-packages/dbt/adapters/base/impl.py", line 1278, in execute_macro
    raise DbtRuntimeError(
dbt_common.exceptions.base.DbtRuntimeError: Runtime Error
  dbt could not find a macro with the name "clean" in any package

[0m14:38:17.839968 [error] [MainThread]: Encountered an error:
Compilation Error
  dbt could not find a macro with the name 'clean' in any package. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:38:17.841481 [debug] [MainThread]: Resource report: {"command_name": "run-operation", "command_success": false, "command_wall_clock_time": 3.7627962, "process_in_blocks": "32", "process_kernel_time": 1.046549, "process_mem_max_rss": "378688", "process_out_blocks": "5032", "process_user_time": 4.531561}
[0m14:38:17.842632 [debug] [MainThread]: Command `dbt run-operation` failed at 14:38:17.842348 after 3.76 seconds
[0m14:38:17.843338 [debug] [MainThread]: Connection 'macro_clean' was properly closed.
[0m14:38:17.843832 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c8732411640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c871087ab10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c871035b620>]}
[0m14:38:17.844452 [debug] [MainThread]: Flushing usage events
[0m14:38:18.767696 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:38:21.141552 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7669214c4260>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x766921654620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x766923483440>]}


============================== 14:38:21.147132 | 6da3faf1-3294-4b76-80d2-ef037e69ae7c ==============================
[0m14:38:21.147132 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m14:38:21.148000 [debug] [MainThread]: running dbt with arguments {'use_colors': 'True', 'write_json': 'True', 'static_parser': 'True', 'invocation_command': 'dbt deps', 'use_experimental_parser': 'False', 'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'debug': 'False', 'partial_parse': 'True', 'version_check': 'True', 'profiles_dir': '/home/ecem/.dbt', 'indirect_selection': 'eager', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'send_anonymous_usage_stats': 'True', 'empty': 'None', 'quiet': 'False', 'target_path': 'None', 'no_print': 'None', 'log_cache_events': 'False', 'printer_width': '80', 'cache_selected_only': 'False', 'warn_error': 'None', 'log_format': 'default', 'introspect': 'True'}
[0m14:38:21.263504 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6da3faf1-3294-4b76-80d2-ef037e69ae7c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7669208cdac0>]}
[0m14:38:21.321356 [debug] [MainThread]: Set downloads directory='/tmp/dbt-downloads-g3bfa_7d'
[0m14:38:21.322260 [debug] [MainThread]: Making package index registry request: GET https://hub.getdbt.com/api/v1/index.json
[0m14:38:21.880589 [debug] [MainThread]: Response from registry index: GET https://hub.getdbt.com/api/v1/index.json 200
[0m14:38:21.889372 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
[0m14:38:22.120543 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
[0m14:38:22.146840 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/calogica/dbt_expectations.json
[0m14:38:22.364133 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/calogica/dbt_expectations.json 200
[0m14:38:22.373005 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality
The `calogica/dbt_expectations` package is deprecated in favor of
`metaplane/dbt_expectations`. Please update your `packages.yml` configuration to
use `metaplane/dbt_expectations` instead.
[0m14:38:22.375257 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '6da3faf1-3294-4b76-80d2-ef037e69ae7c', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7669209d9100>]}
[0m14:38:22.382911 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/calogica/dbt_date.json
[0m14:38:22.560105 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/calogica/dbt_date.json 200
[0m14:38:22.564199 [info ] [MainThread]: Installing dbt-labs/dbt_utils
[0m14:38:23.206808 [info ] [MainThread]: Installed from version 1.3.1
[0m14:38:23.207794 [info ] [MainThread]: Up to date!
[0m14:38:23.208692 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': '6da3faf1-3294-4b76-80d2-ef037e69ae7c', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7669216e9a30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76692098a780>]}
[0m14:38:23.209389 [info ] [MainThread]: Installing calogica/dbt_expectations
[0m14:38:25.054778 [info ] [MainThread]: Installed from version 0.10.4
[0m14:38:25.055591 [info ] [MainThread]: Up to date!
[0m14:38:25.056471 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': '6da3faf1-3294-4b76-80d2-ef037e69ae7c', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76692087d640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7669207ed700>]}
[0m14:38:25.057366 [info ] [MainThread]: Installing calogica/dbt_date
[0m14:38:25.538385 [info ] [MainThread]: Installed from version 0.10.1
[0m14:38:25.539211 [info ] [MainThread]: Up to date!
[0m14:38:25.540171 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': '6da3faf1-3294-4b76-80d2-ef037e69ae7c', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7669208ae180>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7669208ac980>]}
[0m14:38:25.542032 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- PackageRedirectDeprecation: 2 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m14:38:25.543449 [debug] [MainThread]: Resource report: {"command_name": "deps", "command_success": true, "command_wall_clock_time": 4.4579067, "process_in_blocks": "0", "process_kernel_time": 0.913132, "process_mem_max_rss": "110784", "process_out_blocks": "23104", "process_user_time": 1.706189}
[0m14:38:25.544019 [debug] [MainThread]: Command `dbt deps` succeeded at 14:38:25.543921 after 4.46 seconds
[0m14:38:25.544680 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7669234e6e70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76692086bd10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76692087d640>]}
[0m14:38:25.545237 [debug] [MainThread]: Flushing usage events
[0m14:38:26.293309 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:38:28.500265 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7033fa93da30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7033fa277470>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7033fc2aa3c0>]}


============================== 14:38:28.506269 | b788e2b7-570b-4350-9fae-5ea44a13870e ==============================
[0m14:38:28.506269 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m14:38:28.507460 [debug] [MainThread]: running dbt with arguments {'use_experimental_parser': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'debug': 'False', 'static_parser': 'True', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'warn_error': 'None', 'partial_parse': 'True', 'no_print': 'None', 'empty': 'None', 'write_json': 'True', 'quiet': 'False', 'version_check': 'True', 'profiles_dir': '/home/ecem/.dbt', 'printer_width': '80', 'invocation_command': 'dbt test', 'indirect_selection': 'eager', 'use_colors': 'True', 'fail_fast': 'False', 'log_format': 'default', 'send_anonymous_usage_stats': 'True', 'cache_selected_only': 'False', 'introspect': 'True', 'target_path': 'None', 'log_cache_events': 'False'}
[0m14:38:30.833093 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b788e2b7-570b-4350-9fae-5ea44a13870e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7033f98726f0>]}
[0m14:38:30.902720 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b788e2b7-570b-4350-9fae-5ea44a13870e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7033dd0ffdd0>]}
[0m14:38:30.903803 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m14:38:31.177698 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m14:38:31.469419 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:38:31.470013 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:38:31.529106 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b788e2b7-570b-4350-9fae-5ea44a13870e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7033da1cfe00>]}
[0m14:38:31.665713 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m14:38:31.669028 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m14:38:31.702963 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b788e2b7-570b-4350-9fae-5ea44a13870e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7033d9eb6630>]}
[0m14:38:31.703902 [info ] [MainThread]: Found 21 models, 71 data tests, 10 sources, 891 macros
[0m14:38:31.704802 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b788e2b7-570b-4350-9fae-5ea44a13870e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7033d9d6aba0>]}
[0m14:38:31.712664 [info ] [MainThread]: 
[0m14:38:31.713778 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:38:31.714428 [info ] [MainThread]: 
[0m14:38:31.715527 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m14:38:31.723412 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_staging'
[0m14:38:31.723959 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:38:31.762808 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_marts'
[0m14:38:31.766103 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:38:33.226474 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b788e2b7-570b-4350-9fae-5ea44a13870e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7033da9c6960>]}
[0m14:38:33.227389 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:38:33.239291 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.accepted_range_fct_marketing_spend_spend_try__0.29ff739b28
[0m14:38:33.240201 [info ] [Thread-1 (]: 1 of 71 START test accepted_range_fct_marketing_spend_spend_try__0 ............. [RUN]
[0m14:38:33.241656 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_marts, now test.data_pipeline_project.accepted_range_fct_marketing_spend_spend_try__0.29ff739b28)
[0m14:38:33.243530 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.accepted_range_fct_order_discount_total__0.5e17f8aa8a
[0m14:38:33.244289 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.accepted_range_fct_marketing_spend_spend_try__0.29ff739b28
[0m14:38:33.245501 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.accepted_range_fct_order_net_revenue__0.49d8f40cb7
[0m14:38:33.246431 [info ] [Thread-2 (]: 2 of 71 START test accepted_range_fct_order_discount_total__0 .................. [RUN]
[0m14:38:33.247224 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0.494a0c0ebc
[0m14:38:33.260374 [info ] [Thread-3 (]: 3 of 71 START test accepted_range_fct_order_net_revenue__0 ..................... [RUN]
[0m14:38:33.268014 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now test.data_pipeline_project.accepted_range_fct_order_discount_total__0.5e17f8aa8a)
[0m14:38:33.269136 [info ] [Thread-4 (]: 4 of 71 START test accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0  [RUN]
[0m14:38:33.285584 [debug] [Thread-3 (]: Acquiring new bigquery connection 'test.data_pipeline_project.accepted_range_fct_order_net_revenue__0.49d8f40cb7'
[0m14:38:33.289249 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.accepted_range_fct_order_discount_total__0.5e17f8aa8a
[0m14:38:33.291037 [debug] [Thread-4 (]: Acquiring new bigquery connection 'test.data_pipeline_project.accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0.494a0c0ebc'
[0m14:38:33.292592 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:38:33.293440 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.accepted_range_fct_order_net_revenue__0.49d8f40cb7
[0m14:38:33.300423 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0.494a0c0ebc
[0m14:38:33.301450 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:38:33.329041 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m14:38:33.330003 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m14:38:33.345094 [debug] [Thread-1 (]: Compilation Error in test accepted_range_fct_marketing_spend_spend_try__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:38:33.346550 [debug] [Thread-3 (]: Compilation Error in test accepted_range_fct_order_net_revenue__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:38:33.350145 [debug] [Thread-4 (]: Compilation Error in test accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:38:33.351639 [error] [Thread-1 (]: 1 of 71 ERROR accepted_range_fct_marketing_spend_spend_try__0 .................. [[31mERROR[0m in 0.11s]
[0m14:38:33.353128 [debug] [Thread-2 (]: Compilation Error in test accepted_range_fct_order_discount_total__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:38:33.354327 [error] [Thread-3 (]: 3 of 71 ERROR accepted_range_fct_order_net_revenue__0 .......................... [[31mERROR[0m in 0.07s]
[0m14:38:33.361368 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.accepted_range_fct_order_net_revenue__0.49d8f40cb7
[0m14:38:33.362119 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.accepted_range_mart_kpis_latest_cac_payback_months__0.b5860838e1
[0m14:38:33.360143 [error] [Thread-2 (]: 2 of 71 ERROR accepted_range_fct_order_discount_total__0 ....................... [[31mERROR[0m in 0.09s]
[0m14:38:33.356818 [error] [Thread-4 (]: 4 of 71 ERROR accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0  [[31mERROR[0m in 0.07s]
[0m14:38:33.358477 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.accepted_range_fct_marketing_spend_spend_try__0.29ff739b28
[0m14:38:33.367426 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.accepted_range_mart_kpis_latest_monthly_recurring_revenue__0.b1fd398ff0
[0m14:38:33.364260 [debug] [Thread-7 (]: Marking all children of 'test.data_pipeline_project.accepted_range_fct_order_net_revenue__0.49d8f40cb7' to be skipped because of status 'error'.  Reason: Compilation Error in test accepted_range_fct_order_net_revenue__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps"..
[0m14:38:33.370465 [debug] [Thread-7 (]: Marking all children of 'test.data_pipeline_project.accepted_range_fct_marketing_spend_spend_try__0.29ff739b28' to be skipped because of status 'error'.  Reason: Compilation Error in test accepted_range_fct_marketing_spend_spend_try__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps"..
[0m14:38:33.365318 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.accepted_range_fct_order_discount_total__0.5e17f8aa8a
[0m14:38:33.368168 [info ] [Thread-1 (]: 6 of 71 START test accepted_range_mart_kpis_latest_monthly_recurring_revenue__0  [RUN]
[0m14:38:33.374159 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.accepted_range_fct_marketing_spend_spend_try__0.29ff739b28, now test.data_pipeline_project.accepted_range_mart_kpis_latest_monthly_recurring_revenue__0.b1fd398ff0)
[0m14:38:33.362688 [info ] [Thread-3 (]: 5 of 71 START test accepted_range_mart_kpis_latest_cac_payback_months__0 ....... [RUN]
[0m14:38:33.372251 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.accepted_range_mart_revenue_daily_daily_revenue__0.12a5734f28
[0m14:38:33.372872 [debug] [Thread-7 (]: Marking all children of 'test.data_pipeline_project.accepted_range_fct_order_discount_total__0.5e17f8aa8a' to be skipped because of status 'error'.  Reason: Compilation Error in test accepted_range_fct_order_discount_total__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps"..
[0m14:38:33.366257 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0.494a0c0ebc
[0m14:38:33.384831 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.accepted_range_mart_subscription_daily_active_subscribers__0.5d06041ada
[0m14:38:33.380499 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.accepted_range_fct_order_net_revenue__0.49d8f40cb7, now test.data_pipeline_project.accepted_range_mart_kpis_latest_cac_payback_months__0.b5860838e1)
[0m14:38:33.386997 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.accepted_range_mart_kpis_latest_cac_payback_months__0.b5860838e1
[0m14:38:33.379032 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.accepted_range_mart_kpis_latest_monthly_recurring_revenue__0.b1fd398ff0
[0m14:38:33.395180 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:38:33.409858 [debug] [Thread-3 (]: Compilation Error in test accepted_range_mart_kpis_latest_cac_payback_months__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:38:33.381862 [info ] [Thread-2 (]: 7 of 71 START test accepted_range_mart_revenue_daily_daily_revenue__0 .......... [RUN]
[0m14:38:33.396581 [debug] [Thread-7 (]: Marking all children of 'test.data_pipeline_project.accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0.494a0c0ebc' to be skipped because of status 'error'.  Reason: Compilation Error in test accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps"..
[0m14:38:33.412014 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:38:33.386303 [info ] [Thread-4 (]: 8 of 71 START test accepted_range_mart_subscription_daily_active_subscribers__0  [RUN]
[0m14:38:33.414432 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.accepted_range_fct_order_discount_total__0.5e17f8aa8a, now test.data_pipeline_project.accepted_range_mart_revenue_daily_daily_revenue__0.12a5734f28)
[0m14:38:33.413506 [error] [Thread-3 (]: 5 of 71 ERROR accepted_range_mart_kpis_latest_cac_payback_months__0 ............ [[31mERROR[0m in 0.03s]
[0m14:38:33.419042 [debug] [Thread-1 (]: Compilation Error in test accepted_range_mart_kpis_latest_monthly_recurring_revenue__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:38:33.424181 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0.494a0c0ebc, now test.data_pipeline_project.accepted_range_mart_subscription_daily_active_subscribers__0.5d06041ada)
[0m14:38:33.425157 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.accepted_range_mart_revenue_daily_daily_revenue__0.12a5734f28
[0m14:38:33.426114 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.accepted_range_mart_kpis_latest_cac_payback_months__0.b5860838e1
[0m14:38:33.427696 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.accepted_range_mart_subscription_daily_active_subscribers__0.5d06041ada
[0m14:38:33.427006 [error] [Thread-1 (]: 6 of 71 ERROR accepted_range_mart_kpis_latest_monthly_recurring_revenue__0 ..... [[31mERROR[0m in 0.05s]
[0m14:38:33.462104 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.accepted_range_mart_kpis_latest_monthly_recurring_revenue__0.b1fd398ff0
[0m14:38:33.447538 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.accepted_range_mart_subscription_daily_new_subscribers__0.f410e5c26f
[0m14:38:33.460819 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:38:33.443314 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:38:33.462930 [debug] [Thread-7 (]: Marking all children of 'test.data_pipeline_project.accepted_range_mart_kpis_latest_cac_payback_months__0.b5860838e1' to be skipped because of status 'error'.  Reason: Compilation Error in test accepted_range_mart_kpis_latest_cac_payback_months__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps"..
[0m14:38:33.463546 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc
[0m14:38:33.470698 [debug] [Thread-4 (]: Compilation Error in test accepted_range_mart_subscription_daily_active_subscribers__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:38:33.464995 [info ] [Thread-3 (]: 9 of 71 START test accepted_range_mart_subscription_daily_new_subscribers__0 ... [RUN]
[0m14:38:33.482576 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.accepted_range_mart_kpis_latest_cac_payback_months__0.b5860838e1, now test.data_pipeline_project.accepted_range_mart_subscription_daily_new_subscribers__0.f410e5c26f)
[0m14:38:33.480042 [debug] [Thread-7 (]: Marking all children of 'test.data_pipeline_project.accepted_range_mart_kpis_latest_monthly_recurring_revenue__0.b1fd398ff0' to be skipped because of status 'error'.  Reason: Compilation Error in test accepted_range_mart_kpis_latest_monthly_recurring_revenue__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps"..
[0m14:38:33.478166 [debug] [Thread-2 (]: Compilation Error in test accepted_range_mart_revenue_daily_daily_revenue__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:38:33.483198 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.accepted_range_mart_subscription_daily_new_subscribers__0.f410e5c26f
[0m14:38:33.480990 [info ] [Thread-1 (]: 10 of 71 START test not_null_dim_customer_customer_id .......................... [RUN]
[0m14:38:33.481918 [error] [Thread-4 (]: 8 of 71 ERROR accepted_range_mart_subscription_daily_active_subscribers__0 ..... [[31mERROR[0m in 0.06s]
[0m14:38:33.484859 [error] [Thread-2 (]: 7 of 71 ERROR accepted_range_mart_revenue_daily_daily_revenue__0 ............... [[31mERROR[0m in 0.07s]
[0m14:38:33.491456 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.accepted_range_mart_kpis_latest_monthly_recurring_revenue__0.b1fd398ff0, now test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc)
[0m14:38:33.494411 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc
[0m14:38:33.492937 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.accepted_range_mart_subscription_daily_active_subscribers__0.5d06041ada
[0m14:38:33.493754 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.accepted_range_mart_revenue_daily_daily_revenue__0.12a5734f28
[0m14:38:33.490445 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:38:33.503643 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc"
[0m14:38:33.505359 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723
[0m14:38:33.507893 [debug] [Thread-7 (]: Marking all children of 'test.data_pipeline_project.accepted_range_mart_subscription_daily_active_subscribers__0.5d06041ada' to be skipped because of status 'error'.  Reason: Compilation Error in test accepted_range_mart_subscription_daily_active_subscribers__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps"..
[0m14:38:33.518130 [debug] [Thread-7 (]: Marking all children of 'test.data_pipeline_project.accepted_range_mart_revenue_daily_daily_revenue__0.12a5734f28' to be skipped because of status 'error'.  Reason: Compilation Error in test accepted_range_mart_revenue_daily_daily_revenue__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps"..
[0m14:38:33.508605 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6
[0m14:38:33.516471 [info ] [Thread-4 (]: 11 of 71 START test not_null_dim_subscription_customer_id ...................... [RUN]
[0m14:38:33.513348 [debug] [Thread-3 (]: Compilation Error in test accepted_range_mart_subscription_daily_new_subscribers__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:38:33.520175 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.accepted_range_mart_subscription_daily_active_subscribers__0.5d06041ada, now test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723)
[0m14:38:33.519390 [info ] [Thread-2 (]: 12 of 71 START test not_null_dim_subscription_subscription_id .................. [RUN]
[0m14:38:33.521924 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc
[0m14:38:33.524008 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.accepted_range_mart_revenue_daily_daily_revenue__0.12a5734f28, now test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6)
[0m14:38:33.523181 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723
[0m14:38:33.521315 [error] [Thread-3 (]: 9 of 71 ERROR accepted_range_mart_subscription_daily_new_subscribers__0 ........ [[31mERROR[0m in 0.04s]
[0m14:38:33.546033 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6
[0m14:38:33.562449 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc"
[0m14:38:33.565997 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723"
[0m14:38:33.567193 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.accepted_range_mart_subscription_daily_new_subscribers__0.f410e5c26f
[0m14:38:33.583290 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719
[0m14:38:33.582394 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6"
[0m14:38:33.578594 [debug] [Thread-1 (]: On test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select customer_id
from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_customer`
where customer_id is null



  
  
      
    ) dbt_internal_test
[0m14:38:33.584574 [debug] [Thread-7 (]: Marking all children of 'test.data_pipeline_project.accepted_range_mart_subscription_daily_new_subscribers__0.f410e5c26f' to be skipped because of status 'error'.  Reason: Compilation Error in test accepted_range_mart_subscription_daily_new_subscribers__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps"..
[0m14:38:33.584076 [info ] [Thread-3 (]: 13 of 71 START test not_null_fct_marketing_spend_date .......................... [RUN]
[0m14:38:33.586249 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723
[0m14:38:33.587500 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:38:33.588453 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6
[0m14:38:33.590085 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.accepted_range_mart_subscription_daily_new_subscribers__0.f410e5c26f, now test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719)
[0m14:38:33.594476 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723"
[0m14:38:33.644529 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719
[0m14:38:33.643694 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6"
[0m14:38:33.657094 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719"
[0m14:38:33.660606 [debug] [Thread-2 (]: On test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select subscription_id
from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_subscription`
where subscription_id is null



  
  
      
    ) dbt_internal_test
[0m14:38:33.658282 [debug] [Thread-4 (]: On test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select customer_id
from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_subscription`
where customer_id is null



  
  
      
    ) dbt_internal_test
[0m14:38:33.661694 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719
[0m14:38:33.663104 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:38:33.664126 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:38:33.668879 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719"
[0m14:38:33.748543 [debug] [Thread-3 (]: On test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select date
from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_marketing_spend`
where date is null



  
  
      
    ) dbt_internal_test
[0m14:38:33.749427 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:38:34.819744 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:49fadb4e-9afc-478c-b064-f7698fe04efa&page=queryresults
[0m14:38:34.879585 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:36371e8a-c410-43e3-bb7f-b2a83fd2b34f&page=queryresults
[0m14:38:34.976610 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:850ff223-9a5e-450e-99e5-51cba5d0e32c&page=queryresults
[0m14:38:35.017731 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:1eea66e8-f80a-46b0-aeeb-a05046ccb96f&page=queryresults
[0m14:38:35.791904 [info ] [Thread-1 (]: 10 of 71 PASS not_null_dim_customer_customer_id ................................ [[32mPASS[0m in 2.30s]
[0m14:38:35.795010 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc
[0m14:38:35.796462 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d
[0m14:38:35.797801 [info ] [Thread-1 (]: 14 of 71 START test not_null_fct_marketing_spend_spend_try ..................... [RUN]
[0m14:38:35.802317 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc, now test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d)
[0m14:38:35.803932 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d
[0m14:38:35.810427 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d"
[0m14:38:35.814870 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d
[0m14:38:35.818082 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d"
[0m14:38:35.820171 [debug] [Thread-1 (]: On test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select spend_try
from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_marketing_spend`
where spend_try is null



  
  
      
    ) dbt_internal_test
[0m14:38:35.821288 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:38:35.865587 [info ] [Thread-4 (]: 11 of 71 PASS not_null_dim_subscription_customer_id ............................ [[32mPASS[0m in 2.34s]
[0m14:38:35.867529 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723
[0m14:38:35.868234 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff
[0m14:38:35.869077 [info ] [Thread-4 (]: 15 of 71 START test not_null_fct_order_customer_id ............................. [RUN]
[0m14:38:35.870512 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723, now test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff)
[0m14:38:35.871255 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff
[0m14:38:35.879677 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff"
[0m14:38:35.881551 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff
[0m14:38:35.887524 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff"
[0m14:38:35.889444 [debug] [Thread-4 (]: On test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select customer_id
from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_order`
where customer_id is null



  
  
      
    ) dbt_internal_test
[0m14:38:35.890431 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:38:35.944014 [info ] [Thread-3 (]: 13 of 71 PASS not_null_fct_marketing_spend_date ................................ [[32mPASS[0m in 2.35s]
[0m14:38:35.951503 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719
[0m14:38:35.953864 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea
[0m14:38:35.956467 [info ] [Thread-3 (]: 16 of 71 START test not_null_fct_order_discount_total .......................... [RUN]
[0m14:38:35.957741 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719, now test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea)
[0m14:38:35.958403 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea
[0m14:38:35.965978 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea"
[0m14:38:35.968127 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea
[0m14:38:35.972774 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea"
[0m14:38:35.975458 [debug] [Thread-3 (]: On test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select discount_total
from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_order`
where discount_total is null



  
  
      
    ) dbt_internal_test
[0m14:38:35.976591 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:38:36.065117 [info ] [Thread-2 (]: 12 of 71 PASS not_null_dim_subscription_subscription_id ........................ [[32mPASS[0m in 2.54s]
[0m14:38:36.069384 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6
[0m14:38:36.071684 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e
[0m14:38:36.072222 [info ] [Thread-2 (]: 17 of 71 START test not_null_fct_order_net_revenue ............................. [RUN]
[0m14:38:36.074634 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6, now test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e)
[0m14:38:36.077612 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e
[0m14:38:36.092930 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e"
[0m14:38:36.095171 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e
[0m14:38:36.106147 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e"
[0m14:38:36.108512 [debug] [Thread-2 (]: On test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select net_revenue
from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_order`
where net_revenue is null



  
  
      
    ) dbt_internal_test
[0m14:38:36.110566 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:38:37.093283 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:0e67a1a0-0b0a-4449-af28-f413b34adb0f&page=queryresults
[0m14:38:37.128945 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:ca50d722-0edd-4659-82ef-495548efa7af&page=queryresults
[0m14:38:37.199092 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:ba88e838-69c5-4475-abf7-2e39e8c56f15&page=queryresults
[0m14:38:37.281199 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:3eac043d-2199-4066-a056-646cfb7071f0&page=queryresults
[0m14:38:37.957116 [info ] [Thread-1 (]: 14 of 71 PASS not_null_fct_marketing_spend_spend_try ........................... [[32mPASS[0m in 2.15s]
[0m14:38:37.961105 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d
[0m14:38:37.962947 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def
[0m14:38:37.963894 [info ] [Thread-1 (]: 18 of 71 START test not_null_fct_order_order_id ................................ [RUN]
[0m14:38:37.968898 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d, now test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def)
[0m14:38:37.970461 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def
[0m14:38:37.979354 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def"
[0m14:38:37.980731 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def
[0m14:38:37.984425 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def"
[0m14:38:37.986080 [debug] [Thread-1 (]: On test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select order_id
from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_order`
where order_id is null



  
  
      
    ) dbt_internal_test
[0m14:38:37.986733 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:38:38.135011 [info ] [Thread-3 (]: 16 of 71 PASS not_null_fct_order_discount_total ................................ [[32mPASS[0m in 2.18s]
[0m14:38:38.137713 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea
[0m14:38:38.139492 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.not_null_fct_shipment_order_id.8b34a6bb78
[0m14:38:38.141728 [info ] [Thread-3 (]: 19 of 71 START test not_null_fct_shipment_order_id ............................. [RUN]
[0m14:38:38.143957 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea, now test.data_pipeline_project.not_null_fct_shipment_order_id.8b34a6bb78)
[0m14:38:38.145495 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.not_null_fct_shipment_order_id.8b34a6bb78
[0m14:38:38.159088 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_fct_shipment_order_id.8b34a6bb78"
[0m14:38:38.161214 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.not_null_fct_shipment_order_id.8b34a6bb78
[0m14:38:38.172989 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_fct_shipment_order_id.8b34a6bb78"
[0m14:38:38.180136 [debug] [Thread-3 (]: On test.data_pipeline_project.not_null_fct_shipment_order_id.8b34a6bb78: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_fct_shipment_order_id.8b34a6bb78"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select order_id
from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_shipment`
where order_id is null



  
  
      
    ) dbt_internal_test
[0m14:38:38.181739 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:38:38.226773 [info ] [Thread-2 (]: 17 of 71 PASS not_null_fct_order_net_revenue ................................... [[32mPASS[0m in 2.15s]
[0m14:38:38.229135 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e
[0m14:38:38.230945 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d
[0m14:38:38.231647 [info ] [Thread-2 (]: 20 of 71 START test not_null_fct_shipment_shipment_id .......................... [RUN]
[0m14:38:38.233032 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e, now test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d)
[0m14:38:38.233586 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d
[0m14:38:38.241345 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d"
[0m14:38:38.243463 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d
[0m14:38:38.252117 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d"
[0m14:38:38.277266 [debug] [Thread-2 (]: On test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select shipment_id
from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_shipment`
where shipment_id is null



  
  
      
    ) dbt_internal_test
[0m14:38:38.284447 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:38:38.280903 [info ] [Thread-4 (]: 15 of 71 PASS not_null_fct_order_customer_id ................................... [[32mPASS[0m in 2.41s]
[0m14:38:38.301081 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff
[0m14:38:38.334402 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b
[0m14:38:38.337247 [info ] [Thread-4 (]: 21 of 71 START test not_null_int_shipments_decomposed_delivered_at ............. [RUN]
[0m14:38:38.339191 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff, now test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b)
[0m14:38:38.340390 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b
[0m14:38:38.349558 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b"
[0m14:38:38.353749 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b
[0m14:38:38.359033 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b"
[0m14:38:38.380823 [debug] [Thread-4 (]: On test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select delivered_at
from `data-pipeline-project-474812`.`analytics_staging_staging`.`int_shipments_decomposed`
where delivered_at is null



  
  
      
    ) dbt_internal_test
[0m14:38:38.385866 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:38:39.260368 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:4893f89e-e040-477f-8665-514747c0abe6&page=queryresults
[0m14:38:39.439656 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:e6b6af21-698a-4079-9c2e-1ca4f94ded1e&page=queryresults
[0m14:38:39.507278 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:0798b78c-251a-48bd-88c5-2c108ad910ea&page=queryresults
[0m14:38:39.663047 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:4eb30be6-aa03-4f5a-ba36-9542232049ba&page=queryresults
[0m14:38:40.207837 [info ] [Thread-1 (]: 18 of 71 PASS not_null_fct_order_order_id ...................................... [[32mPASS[0m in 2.24s]
[0m14:38:40.209069 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def
[0m14:38:40.210154 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31
[0m14:38:40.210750 [info ] [Thread-1 (]: 22 of 71 START test not_null_int_shipments_decomposed_latest_status ............ [RUN]
[0m14:38:40.211936 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def, now test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31)
[0m14:38:40.212471 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31
[0m14:38:40.217780 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31"
[0m14:38:40.218922 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31
[0m14:38:40.223197 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31"
[0m14:38:40.224964 [debug] [Thread-1 (]: On test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select latest_status
from `data-pipeline-project-474812`.`analytics_staging_staging`.`int_shipments_decomposed`
where latest_status is null



  
  
      
    ) dbt_internal_test
[0m14:38:40.225795 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:38:40.489035 [info ] [Thread-2 (]: 20 of 71 PASS not_null_fct_shipment_shipment_id ................................ [[32mPASS[0m in 2.25s]
[0m14:38:40.491231 [info ] [Thread-3 (]: 19 of 71 PASS not_null_fct_shipment_order_id ................................... [[32mPASS[0m in 2.33s]
[0m14:38:40.494603 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d
[0m14:38:40.496412 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.not_null_fct_shipment_order_id.8b34a6bb78
[0m14:38:40.500528 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b
[0m14:38:40.501446 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23
[0m14:38:40.502270 [info ] [Thread-2 (]: 23 of 71 START test not_null_mart_acquisition_efficiency_date .................. [RUN]
[0m14:38:40.504875 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d, now test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b)
[0m14:38:40.506890 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b
[0m14:38:40.502926 [info ] [Thread-3 (]: 24 of 71 START test not_null_mart_revenue_daily_date ........................... [RUN]
[0m14:38:40.515822 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_fct_shipment_order_id.8b34a6bb78, now test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23)
[0m14:38:40.518179 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23
[0m14:38:40.523971 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23"
[0m14:38:40.528018 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b"
[0m14:38:40.530361 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23
[0m14:38:40.531629 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b
[0m14:38:40.541362 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b"
[0m14:38:40.542230 [debug] [Thread-2 (]: On test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select date
from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_acquisition_efficiency`
where date is null



  
  
      
    ) dbt_internal_test
[0m14:38:40.544007 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:38:40.538198 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23"
[0m14:38:40.578614 [debug] [Thread-3 (]: On test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select date
from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_revenue_daily`
where date is null



  
  
      
    ) dbt_internal_test
[0m14:38:40.582405 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:38:40.633056 [warn ] [Thread-4 (]: 21 of 71 WARN 303 not_null_int_shipments_decomposed_delivered_at ............... [[33mWARN 303[0m in 2.29s]
[0m14:38:40.640193 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b
[0m14:38:40.641320 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322
[0m14:38:40.647120 [info ] [Thread-4 (]: 25 of 71 START test not_null_mart_subscription_daily_date ...................... [RUN]
[0m14:38:40.648653 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b, now test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322)
[0m14:38:40.649200 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322
[0m14:38:40.656727 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322"
[0m14:38:40.658024 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322
[0m14:38:40.662715 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322"
[0m14:38:40.664375 [debug] [Thread-4 (]: On test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select date
from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily`
where date is null



  
  
      
    ) dbt_internal_test
[0m14:38:40.666128 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:38:41.584342 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:6e80fb23-fd5a-40ff-80fc-adefee6191b2&page=queryresults
[0m14:38:41.747221 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:e5925d3c-a827-4c05-b982-2f5cd3fb7b15&page=queryresults
[0m14:38:41.783589 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:e842be77-3bef-45dc-88c4-7848ed8a23a3&page=queryresults
[0m14:38:41.915761 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:ea3e1043-577f-4002-acfc-811794df0dcc&page=queryresults
[0m14:38:42.715638 [info ] [Thread-1 (]: 22 of 71 PASS not_null_int_shipments_decomposed_latest_status .................. [[32mPASS[0m in 2.50s]
[0m14:38:42.721426 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31
[0m14:38:42.723753 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501
[0m14:38:42.725660 [info ] [Thread-1 (]: 26 of 71 START test not_null_stg_addresses_address_id .......................... [RUN]
[0m14:38:42.728006 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31, now test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501)
[0m14:38:42.731091 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501
[0m14:38:42.742336 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501"
[0m14:38:42.753088 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501
[0m14:38:42.759413 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501"
[0m14:38:42.761185 [debug] [Thread-1 (]: On test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select address_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_addresses`
where address_id is null



  
  
      
    ) dbt_internal_test
[0m14:38:42.762469 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:38:42.811509 [info ] [Thread-2 (]: 23 of 71 PASS not_null_mart_acquisition_efficiency_date ........................ [[32mPASS[0m in 2.31s]
[0m14:38:42.815952 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b
[0m14:38:42.816914 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3
[0m14:38:42.814278 [info ] [Thread-3 (]: 24 of 71 PASS not_null_mart_revenue_daily_date ................................. [[32mPASS[0m in 2.29s]
[0m14:38:42.818461 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23
[0m14:38:42.820640 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2
[0m14:38:42.817834 [info ] [Thread-2 (]: 27 of 71 START test not_null_stg_addresses_user_id ............................. [RUN]
[0m14:38:42.824327 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b, now test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3)
[0m14:38:42.821756 [info ] [Thread-3 (]: 28 of 71 START test not_null_stg_countries_country_id .......................... [RUN]
[0m14:38:42.825021 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3
[0m14:38:42.833148 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23, now test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2)
[0m14:38:42.854877 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2
[0m14:38:42.859522 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3"
[0m14:38:42.869074 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3
[0m14:38:42.873290 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3"
[0m14:38:42.874869 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2"
[0m14:38:42.877677 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2
[0m14:38:42.876171 [debug] [Thread-2 (]: On test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_addresses`
where user_id is null



  
  
      
    ) dbt_internal_test
[0m14:38:42.884724 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:38:42.883143 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2"
[0m14:38:42.890134 [debug] [Thread-3 (]: On test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select country_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_countries`
where country_id is null



  
  
      
    ) dbt_internal_test
[0m14:38:42.891104 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:38:43.082480 [info ] [Thread-4 (]: 25 of 71 PASS not_null_mart_subscription_daily_date ............................ [[32mPASS[0m in 2.43s]
[0m14:38:43.091559 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322
[0m14:38:43.093351 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64
[0m14:38:43.094827 [info ] [Thread-4 (]: 29 of 71 START test not_null_stg_orders_order_id ............................... [RUN]
[0m14:38:43.096922 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322, now test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64)
[0m14:38:43.098055 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64
[0m14:38:43.114934 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64"
[0m14:38:43.119609 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64
[0m14:38:43.129167 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64"
[0m14:38:43.136317 [debug] [Thread-4 (]: On test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select order_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_orders`
where order_id is null



  
  
      
    ) dbt_internal_test
[0m14:38:43.137530 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:38:44.120840 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:845f6c1f-c8e0-49c0-a79f-916ce757fa11&page=queryresults
[0m14:38:44.491409 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:0c4a1c8e-fd79-4da1-acbb-a40104f5dcc7&page=queryresults
[0m14:38:44.499428 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:fa684af2-7c35-4fa2-a3ef-d3abea366db0&page=queryresults
[0m14:38:44.567046 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:661f29d8-ba03-4ee6-b11d-f1c4e9cecdc5&page=queryresults
[0m14:38:44.975239 [info ] [Thread-1 (]: 26 of 71 PASS not_null_stg_addresses_address_id ................................ [[32mPASS[0m in 2.25s]
[0m14:38:44.976660 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501
[0m14:38:44.977348 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209
[0m14:38:44.978042 [info ] [Thread-1 (]: 30 of 71 START test not_null_stg_orders_user_id ................................ [RUN]
[0m14:38:44.979013 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501, now test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209)
[0m14:38:44.979459 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209
[0m14:38:44.985201 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209"
[0m14:38:44.986948 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209
[0m14:38:44.991456 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209"
[0m14:38:44.996015 [debug] [Thread-1 (]: On test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_orders`
where user_id is null



  
  
      
    ) dbt_internal_test
[0m14:38:44.997010 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:38:45.379735 [info ] [Thread-3 (]: 28 of 71 PASS not_null_stg_countries_country_id ................................ [[32mPASS[0m in 2.54s]
[0m14:38:45.387553 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2
[0m14:38:45.420665 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380
[0m14:38:45.437042 [info ] [Thread-3 (]: 31 of 71 START test not_null_stg_shipments_shipment_id ......................... [RUN]
[0m14:38:45.443139 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2, now test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380)
[0m14:38:45.441857 [info ] [Thread-2 (]: 27 of 71 PASS not_null_stg_addresses_user_id ................................... [[32mPASS[0m in 2.62s]
[0m14:38:45.447006 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380
[0m14:38:45.450303 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3
[0m14:38:45.449345 [info ] [Thread-4 (]: 29 of 71 PASS not_null_stg_orders_order_id ..................................... [[32mPASS[0m in 2.35s]
[0m14:38:45.455088 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380"
[0m14:38:45.457446 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852
[0m14:38:45.458559 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64
[0m14:38:45.461176 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380
[0m14:38:45.459989 [info ] [Thread-2 (]: 32 of 71 START test not_null_stg_shipments_user_id ............................. [RUN]
[0m14:38:45.462503 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8
[0m14:38:45.466094 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380"
[0m14:38:45.467185 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3, now test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852)
[0m14:38:45.470655 [info ] [Thread-4 (]: 33 of 71 START test not_null_stg_subscriptions_start_date ...................... [RUN]
[0m14:38:45.473471 [debug] [Thread-3 (]: On test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select shipment_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_shipments`
where shipment_id is null



  
  
      
    ) dbt_internal_test
[0m14:38:45.474613 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852
[0m14:38:45.475976 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64, now test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8)
[0m14:38:45.477376 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:38:45.489921 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852"
[0m14:38:45.490853 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8
[0m14:38:45.526676 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852
[0m14:38:45.540857 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852"
[0m14:38:45.543868 [debug] [Thread-2 (]: On test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_shipments`
where user_id is null



  
  
      
    ) dbt_internal_test
[0m14:38:45.545205 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:38:45.590158 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8"
[0m14:38:45.592949 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8
[0m14:38:45.597307 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8"
[0m14:38:45.602152 [debug] [Thread-4 (]: On test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select start_date
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_subscriptions`
where start_date is null



  
  
      
    ) dbt_internal_test
[0m14:38:45.603438 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:38:46.405632 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:4f5da696-7e2e-41a7-840d-766d03926a8f&page=queryresults
[0m14:38:46.870977 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:d34c85e9-8a9c-4f57-9ff4-64e84edb5faa&page=queryresults
[0m14:38:46.935234 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:b97c83b4-d338-4ce6-a325-c490ef64b3ca&page=queryresults
[0m14:38:47.048787 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:b015c8cc-077e-47cd-bfe9-1eb9f88ed610&page=queryresults
[0m14:38:47.357265 [info ] [Thread-1 (]: 30 of 71 PASS not_null_stg_orders_user_id ...................................... [[32mPASS[0m in 2.38s]
[0m14:38:47.359289 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209
[0m14:38:47.360482 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m14:38:47.361446 [info ] [Thread-1 (]: 34 of 71 START test not_null_stg_subscriptions_subscription_id ................. [RUN]
[0m14:38:47.362892 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209, now test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91)
[0m14:38:47.364483 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m14:38:47.374962 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91"
[0m14:38:47.379921 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m14:38:47.387109 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91"
[0m14:38:47.389398 [debug] [Thread-1 (]: On test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select subscription_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_subscriptions`
where subscription_id is null



  
  
      
    ) dbt_internal_test
[0m14:38:47.392385 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:38:47.855603 [info ] [Thread-4 (]: 33 of 71 PASS not_null_stg_subscriptions_start_date ............................ [[32mPASS[0m in 2.38s]
[0m14:38:47.856859 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8
[0m14:38:47.858007 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2
[0m14:38:47.859034 [info ] [Thread-4 (]: 35 of 71 START test not_null_stg_subscriptions_user_id ......................... [RUN]
[0m14:38:47.859881 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8, now test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2)
[0m14:38:47.860380 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2
[0m14:38:47.866786 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2"
[0m14:38:47.867846 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2
[0m14:38:47.875496 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2"
[0m14:38:47.876693 [debug] [Thread-4 (]: On test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_subscriptions`
where user_id is null



  
  
      
    ) dbt_internal_test
[0m14:38:47.877298 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:38:48.041038 [info ] [Thread-2 (]: 32 of 71 PASS not_null_stg_shipments_user_id ................................... [[32mPASS[0m in 2.57s]
[0m14:38:48.044438 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852
[0m14:38:48.046459 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25
[0m14:38:48.048037 [info ] [Thread-2 (]: 36 of 71 START test not_null_stg_users_created_at .............................. [RUN]
[0m14:38:48.052422 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852, now test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25)
[0m14:38:48.059758 [info ] [Thread-3 (]: 31 of 71 PASS not_null_stg_shipments_shipment_id ............................... [[32mPASS[0m in 2.62s]
[0m14:38:48.061015 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25
[0m14:38:48.062413 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380
[0m14:38:48.070128 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25"
[0m14:38:48.073193 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77
[0m14:38:48.083681 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25
[0m14:38:48.082583 [info ] [Thread-3 (]: 37 of 71 START test not_null_stg_users_user_id ................................. [RUN]
[0m14:38:48.090764 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380, now test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77)
[0m14:38:48.092965 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77
[0m14:38:48.099227 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77"
[0m14:38:48.103866 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25"
[0m14:38:48.106448 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77
[0m14:38:48.114157 [debug] [Thread-2 (]: On test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select created_at
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
where created_at is null



  
  
      
    ) dbt_internal_test
[0m14:38:48.116009 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77"
[0m14:38:48.118188 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:38:48.187419 [debug] [Thread-3 (]: On test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
where user_id is null



  
  
      
    ) dbt_internal_test
[0m14:38:48.189410 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:38:48.771643 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:e4f6cf85-190c-4abb-ab44-c36ff06d649b&page=queryresults
[0m14:38:49.285938 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:12d17f1d-7d6d-4348-b85c-812c183238e1&page=queryresults
[0m14:38:49.408467 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:134df8fc-4e18-4298-8007-f0060bdae6a7&page=queryresults
[0m14:38:49.488671 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:a157d2de-9a3f-4cd1-ab47-f7f4705b2ef8&page=queryresults
[0m14:38:49.608093 [info ] [Thread-1 (]: 34 of 71 PASS not_null_stg_subscriptions_subscription_id ....................... [[32mPASS[0m in 2.24s]
[0m14:38:49.613092 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m14:38:49.615292 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29
[0m14:38:49.617253 [info ] [Thread-1 (]: 38 of 71 START test relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_  [RUN]
[0m14:38:49.619634 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91, now test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29)
[0m14:38:49.620855 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29
[0m14:38:49.639385 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29"
[0m14:38:49.640577 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29
[0m14:38:49.646478 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29"
[0m14:38:49.647420 [debug] [Thread-1 (]: On test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select customer_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_subscription`
    where customer_id is not null
),

parent as (
    select customer_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_customer`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m14:38:49.648479 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:38:50.125017 [info ] [Thread-4 (]: 35 of 71 PASS not_null_stg_subscriptions_user_id ............................... [[32mPASS[0m in 2.26s]
[0m14:38:50.126084 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2
[0m14:38:50.126991 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85
[0m14:38:50.127542 [info ] [Thread-4 (]: 39 of 71 START test relationships_fct_order_customer_id__customer_id__ref_dim_customer_  [RUN]
[0m14:38:50.129026 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2, now test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85)
[0m14:38:50.129523 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85
[0m14:38:50.137734 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85"
[0m14:38:50.139600 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85
[0m14:38:50.148012 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85"
[0m14:38:50.151075 [debug] [Thread-4 (]: On test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select customer_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_order`
    where customer_id is not null
),

parent as (
    select customer_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_customer`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m14:38:50.152638 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:38:50.326983 [info ] [Thread-2 (]: 36 of 71 PASS not_null_stg_users_created_at .................................... [[32mPASS[0m in 2.27s]
[0m14:38:50.338372 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25
[0m14:38:50.346501 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.relationships_fct_shipment_order_id__order_id__ref_fct_order_.6096f95e39
[0m14:38:50.355707 [info ] [Thread-2 (]: 40 of 71 START test relationships_fct_shipment_order_id__order_id__ref_fct_order_  [RUN]
[0m14:38:50.360544 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25, now test.data_pipeline_project.relationships_fct_shipment_order_id__order_id__ref_fct_order_.6096f95e39)
[0m14:38:50.361976 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.relationships_fct_shipment_order_id__order_id__ref_fct_order_.6096f95e39
[0m14:38:50.359220 [info ] [Thread-3 (]: 37 of 71 PASS not_null_stg_users_user_id ....................................... [[32mPASS[0m in 2.26s]
[0m14:38:50.379269 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77
[0m14:38:50.380667 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781
[0m14:38:50.383870 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_fct_shipment_order_id__order_id__ref_fct_order_.6096f95e39"
[0m14:38:50.386056 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.relationships_fct_shipment_order_id__order_id__ref_fct_order_.6096f95e39
[0m14:38:50.384919 [info ] [Thread-3 (]: 41 of 71 START test relationships_stg_addresses_user_id__user_id__ref_stg_users_  [RUN]
[0m14:38:50.390527 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_fct_shipment_order_id__order_id__ref_fct_order_.6096f95e39"
[0m14:38:50.391418 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77, now test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781)
[0m14:38:50.394769 [debug] [Thread-2 (]: On test.data_pipeline_project.relationships_fct_shipment_order_id__order_id__ref_fct_order_.6096f95e39: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_fct_shipment_order_id__order_id__ref_fct_order_.6096f95e39"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select order_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_shipment`
    where order_id is not null
),

parent as (
    select order_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_order`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m14:38:50.397174 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:38:50.396113 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781
[0m14:38:50.449619 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781"
[0m14:38:50.452278 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781
[0m14:38:50.460537 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781"
[0m14:38:50.463334 [debug] [Thread-3 (]: On test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select user_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_addresses`
    where user_id is not null
),

parent as (
    select user_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m14:38:50.464270 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:38:50.795753 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:150fed91-e099-42e4-84b8-84a5730817eb&page=queryresults
[0m14:38:51.407718 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:90692fcc-c09b-4362-8ab6-e56a8861ec6e&page=queryresults
[0m14:38:51.484515 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:0aad4c82-099d-4a74-ac7b-9420c7c0aae9&page=queryresults
[0m14:38:51.683031 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:78efe06d-b807-403a-9d0a-9a415c90a0b6&page=queryresults
[0m14:38:51.723606 [info ] [Thread-1 (]: 38 of 71 PASS relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_  [[32mPASS[0m in 2.10s]
[0m14:38:51.725080 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29
[0m14:38:51.726061 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737
[0m14:38:51.726575 [info ] [Thread-1 (]: 42 of 71 START test relationships_stg_orders_user_id__user_id__ref_stg_users_ .. [RUN]
[0m14:38:51.728180 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29, now test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737)
[0m14:38:51.729302 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737
[0m14:38:51.736787 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737"
[0m14:38:51.738206 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737
[0m14:38:51.746711 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737"
[0m14:38:51.748182 [debug] [Thread-1 (]: On test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select user_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_orders`
    where user_id is not null
),

parent as (
    select user_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m14:38:51.752671 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:38:52.459460 [error] [Thread-2 (]: 40 of 71 FAIL 476 relationships_fct_shipment_order_id__order_id__ref_fct_order_  [[31mFAIL 476[0m in 2.10s]
[0m14:38:52.461115 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.relationships_fct_shipment_order_id__order_id__ref_fct_order_.6096f95e39
[0m14:38:52.461911 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8
[0m14:38:52.462765 [info ] [Thread-2 (]: 43 of 71 START test relationships_stg_shipments_user_id__user_id__ref_stg_users_  [RUN]
[0m14:38:52.463802 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_fct_shipment_order_id__order_id__ref_fct_order_.6096f95e39, now test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8)
[0m14:38:52.464542 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8
[0m14:38:52.472742 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8"
[0m14:38:52.478522 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8
[0m14:38:52.484741 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8"
[0m14:38:52.477740 [info ] [Thread-4 (]: 39 of 71 PASS relationships_fct_order_customer_id__customer_id__ref_dim_customer_  [[32mPASS[0m in 2.35s]
[0m14:38:52.490492 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85
[0m14:38:52.491120 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m14:38:52.493482 [debug] [Thread-2 (]: On test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select user_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_shipments`
    where user_id is not null
),

parent as (
    select user_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m14:38:52.492381 [info ] [Thread-4 (]: 44 of 71 START test relationships_stg_subscriptions_user_id__user_id__ref_stg_users_  [RUN]
[0m14:38:52.495041 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:38:52.496124 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85, now test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e)
[0m14:38:52.554590 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m14:38:52.572576 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e"
[0m14:38:52.578712 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m14:38:52.587395 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e"
[0m14:38:52.589553 [debug] [Thread-4 (]: On test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select user_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_subscriptions`
    where user_id is not null
),

parent as (
    select user_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m14:38:52.592156 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:38:52.709445 [info ] [Thread-3 (]: 41 of 71 PASS relationships_stg_addresses_user_id__user_id__ref_stg_users_ ..... [[32mPASS[0m in 2.32s]
[0m14:38:52.711191 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781
[0m14:38:52.717209 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6
[0m14:38:52.718558 [info ] [Thread-3 (]: 45 of 71 START test source_not_null_raw_data_addresses__id ..................... [RUN]
[0m14:38:52.720112 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781, now test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6)
[0m14:38:52.720595 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6
[0m14:38:52.729077 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6"
[0m14:38:52.730958 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6
[0m14:38:52.736386 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6"
[0m14:38:52.737656 [debug] [Thread-3 (]: On test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`addresses`
where _id is null



  
  
      
    ) dbt_internal_test
[0m14:38:52.738333 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:38:53.112173 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:bbe729c3-683f-4165-88bd-1e2f78e9924d&page=queryresults
[0m14:38:53.830769 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:88118c39-567f-4474-a196-f0de3fdbd6fd&page=queryresults
[0m14:38:53.890920 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:c3075fed-6919-4fe9-b3b0-ebabebcf5a6a&page=queryresults
[0m14:38:53.979895 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:02cdc4b8-3091-454d-9300-9adf5c1904bf&page=queryresults
[0m14:38:54.017212 [info ] [Thread-1 (]: 42 of 71 PASS relationships_stg_orders_user_id__user_id__ref_stg_users_ ........ [[32mPASS[0m in 2.29s]
[0m14:38:54.021236 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737
[0m14:38:54.023329 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc
[0m14:38:54.024800 [info ] [Thread-1 (]: 46 of 71 START test source_not_null_raw_data_cities__id ........................ [RUN]
[0m14:38:54.026332 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737, now test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc)
[0m14:38:54.027380 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc
[0m14:38:54.040175 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc"
[0m14:38:54.041473 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc
[0m14:38:54.046569 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc"
[0m14:38:54.049023 [debug] [Thread-1 (]: On test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`cities`
where _id is null



  
  
      
    ) dbt_internal_test
[0m14:38:54.049902 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:38:54.716948 [info ] [Thread-3 (]: 45 of 71 PASS source_not_null_raw_data_addresses__id ........................... [[32mPASS[0m in 2.00s]
[0m14:38:54.722098 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6
[0m14:38:54.724179 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd
[0m14:38:54.726699 [info ] [Thread-3 (]: 47 of 71 START test source_not_null_raw_data_countries__id ..................... [RUN]
[0m14:38:54.728528 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6, now test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd)
[0m14:38:54.729828 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd
[0m14:38:54.737722 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd"
[0m14:38:54.739998 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd
[0m14:38:54.743013 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd"
[0m14:38:54.745291 [debug] [Thread-3 (]: On test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`countries`
where _id is null



  
  
      
    ) dbt_internal_test
[0m14:38:54.746090 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:38:54.792303 [info ] [Thread-2 (]: 43 of 71 PASS relationships_stg_shipments_user_id__user_id__ref_stg_users_ ..... [[32mPASS[0m in 2.33s]
[0m14:38:54.801327 [info ] [Thread-4 (]: 44 of 71 PASS relationships_stg_subscriptions_user_id__user_id__ref_stg_users_ . [[32mPASS[0m in 2.30s]
[0m14:38:54.802632 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8
[0m14:38:54.804100 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m14:38:54.806549 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be
[0m14:38:54.808558 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878
[0m14:38:54.810758 [info ] [Thread-2 (]: 48 of 71 START test source_not_null_raw_data_neighborhoods__id ................. [RUN]
[0m14:38:54.811445 [info ] [Thread-4 (]: 49 of 71 START test source_not_null_raw_data_orders__id ........................ [RUN]
[0m14:38:54.813622 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8, now test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be)
[0m14:38:54.816386 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e, now test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878)
[0m14:38:54.818444 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be
[0m14:38:54.821107 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878
[0m14:38:54.842385 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be"
[0m14:38:54.843597 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878"
[0m14:38:54.848759 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be
[0m14:38:54.849971 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878
[0m14:38:54.871037 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be"
[0m14:38:54.879402 [debug] [Thread-2 (]: On test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`neighborhoods`
where _id is null



  
  
      
    ) dbt_internal_test
[0m14:38:54.880371 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:38:54.940989 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878"
[0m14:38:54.948473 [debug] [Thread-4 (]: On test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`orders`
where _id is null



  
  
      
    ) dbt_internal_test
[0m14:38:54.951116 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:38:55.238053 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:897b3dc6-a626-440c-b08a-6ab31c27bdb3&page=queryresults
[0m14:38:55.923855 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:ff2ee52b-b412-4cc2-87a2-9304b0b57c1b&page=queryresults
[0m14:38:55.959231 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:46e8f7ab-1dee-4c5a-b01a-4cda642a9e39&page=queryresults
[0m14:38:56.064671 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:f66f0d61-06a4-4af4-9dc5-7b95b0b46772&page=queryresults
[0m14:38:56.157498 [info ] [Thread-1 (]: 46 of 71 PASS source_not_null_raw_data_cities__id .............................. [[32mPASS[0m in 2.13s]
[0m14:38:56.161301 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc
[0m14:38:56.164011 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f
[0m14:38:56.165213 [info ] [Thread-1 (]: 50 of 71 START test source_not_null_raw_data_orders__user ...................... [RUN]
[0m14:38:56.168301 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc, now test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f)
[0m14:38:56.169355 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f
[0m14:38:56.179390 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f"
[0m14:38:56.180865 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f
[0m14:38:56.185476 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f"
[0m14:38:56.186758 [debug] [Thread-1 (]: On test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _user
from `data-pipeline-project-474812`.`raw_data`.`orders`
where _user is null



  
  
      
    ) dbt_internal_test
[0m14:38:56.187324 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:38:56.814449 [info ] [Thread-3 (]: 47 of 71 PASS source_not_null_raw_data_countries__id ........................... [[32mPASS[0m in 2.09s]
[0m14:38:56.821705 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd
[0m14:38:56.822524 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2
[0m14:38:56.820291 [info ] [Thread-2 (]: 48 of 71 PASS source_not_null_raw_data_neighborhoods__id ....................... [[32mPASS[0m in 2.01s]
[0m14:38:56.824401 [info ] [Thread-3 (]: 51 of 71 START test source_not_null_raw_data_shipments__id ..................... [RUN]
[0m14:38:56.825657 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be
[0m14:38:56.826644 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd, now test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2)
[0m14:38:56.832302 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2
[0m14:38:56.827635 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1
[0m14:38:56.837978 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2"
[0m14:38:56.839375 [info ] [Thread-2 (]: 52 of 71 START test source_not_null_raw_data_shipments__order .................. [RUN]
[0m14:38:56.840597 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be, now test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1)
[0m14:38:56.842045 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1
[0m14:38:56.850279 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1"
[0m14:38:56.852173 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2
[0m14:38:56.856280 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2"
[0m14:38:56.856997 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1
[0m14:38:56.865018 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1"
[0m14:38:56.866279 [debug] [Thread-3 (]: On test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`shipments`
where _id is null



  
  
      
    ) dbt_internal_test
[0m14:38:56.867162 [debug] [Thread-2 (]: On test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _order
from `data-pipeline-project-474812`.`raw_data`.`shipments`
where _order is null



  
  
      
    ) dbt_internal_test
[0m14:38:56.867797 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:38:56.869457 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:38:56.969333 [info ] [Thread-4 (]: 49 of 71 PASS source_not_null_raw_data_orders__id .............................. [[32mPASS[0m in 2.15s]
[0m14:38:56.970900 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878
[0m14:38:56.972054 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98
[0m14:38:56.972683 [info ] [Thread-4 (]: 53 of 71 START test source_not_null_raw_data_shipments__user ................... [RUN]
[0m14:38:56.973847 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878, now test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98)
[0m14:38:56.974348 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98
[0m14:38:56.983768 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98"
[0m14:38:56.986122 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98
[0m14:38:56.989370 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98"
[0m14:38:56.991065 [debug] [Thread-4 (]: On test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _user
from `data-pipeline-project-474812`.`raw_data`.`shipments`
where _user is null



  
  
      
    ) dbt_internal_test
[0m14:38:56.992349 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:38:57.364199 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:54303cf5-6b94-474a-9179-7cbe889ce583&page=queryresults
[0m14:38:58.144617 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:ab44597c-7af0-4256-a25f-4fe2e900c853&page=queryresults
[0m14:38:58.196950 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:05e367fa-a8fb-41f1-8c07-f4875efea22e&page=queryresults
[0m14:38:58.211798 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:5603d29b-936e-476b-8d99-18de495f7e69&page=queryresults
[0m14:38:58.320092 [info ] [Thread-1 (]: 50 of 71 PASS source_not_null_raw_data_orders__user ............................ [[32mPASS[0m in 2.15s]
[0m14:38:58.325343 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f
[0m14:38:58.327974 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86
[0m14:38:58.330196 [info ] [Thread-1 (]: 54 of 71 START test source_not_null_raw_data_states__id ........................ [RUN]
[0m14:38:58.333634 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f, now test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86)
[0m14:38:58.334386 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86
[0m14:38:58.344034 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86"
[0m14:38:58.347645 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86
[0m14:38:58.351369 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86"
[0m14:38:58.353191 [debug] [Thread-1 (]: On test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`states`
where _id is null



  
  
      
    ) dbt_internal_test
[0m14:38:58.357483 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:38:58.999706 [info ] [Thread-3 (]: 51 of 71 PASS source_not_null_raw_data_shipments__id ........................... [[32mPASS[0m in 2.17s]
[0m14:38:59.001098 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2
[0m14:38:59.002339 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a
[0m14:38:59.003310 [info ] [Thread-3 (]: 55 of 71 START test source_not_null_raw_data_subscriptions__id ................. [RUN]
[0m14:38:59.004268 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2, now test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a)
[0m14:38:59.005016 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a
[0m14:38:59.012248 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a"
[0m14:38:59.014005 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a
[0m14:38:59.019098 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a"
[0m14:38:59.020508 [debug] [Thread-3 (]: On test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`subscriptions`
where _id is null



  
  
      
    ) dbt_internal_test
[0m14:38:59.021137 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:38:59.064759 [info ] [Thread-4 (]: 53 of 71 PASS source_not_null_raw_data_shipments__user ......................... [[32mPASS[0m in 2.09s]
[0m14:38:59.068892 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98
[0m14:38:59.070648 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6
[0m14:38:59.071944 [info ] [Thread-4 (]: 56 of 71 START test source_not_null_raw_data_subscriptions__user ............... [RUN]
[0m14:38:59.073566 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98, now test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6)
[0m14:38:59.075442 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6
[0m14:38:59.082909 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6"
[0m14:38:59.084407 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6
[0m14:38:59.115335 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6"
[0m14:38:59.126672 [debug] [Thread-4 (]: On test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _user
from `data-pipeline-project-474812`.`raw_data`.`subscriptions`
where _user is null



  
  
      
    ) dbt_internal_test
[0m14:38:59.129274 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:38:59.124599 [info ] [Thread-2 (]: 52 of 71 PASS source_not_null_raw_data_shipments__order ........................ [[32mPASS[0m in 2.28s]
[0m14:38:59.136493 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1
[0m14:38:59.138586 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b
[0m14:38:59.140966 [info ] [Thread-2 (]: 57 of 71 START test source_not_null_raw_data_users__id ......................... [RUN]
[0m14:38:59.142366 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1, now test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b)
[0m14:38:59.189089 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b
[0m14:38:59.224116 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b"
[0m14:38:59.227920 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b
[0m14:38:59.240055 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b"
[0m14:38:59.241373 [debug] [Thread-2 (]: On test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`users`
where _id is null



  
  
      
    ) dbt_internal_test
[0m14:38:59.245351 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:38:59.501296 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:3adda47e-3c3b-4c43-9c3d-75d71181f2bb&page=queryresults
[0m14:39:00.262893 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:dcc32cb6-5f5b-4f22-986b-960ee1ded2df&page=queryresults
[0m14:39:00.471860 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:cf69a1b5-e8f7-419e-b401-a7c6c51e2e85&page=queryresults
[0m14:39:00.568413 [info ] [Thread-1 (]: 54 of 71 PASS source_not_null_raw_data_states__id .............................. [[32mPASS[0m in 2.23s]
[0m14:39:00.569841 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86
[0m14:39:00.570896 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1
[0m14:39:00.571395 [info ] [Thread-1 (]: 58 of 71 START test unique_dim_customer_customer_id ............................ [RUN]
[0m14:39:00.572565 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86, now test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1)
[0m14:39:00.573389 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1
[0m14:39:00.583378 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1"
[0m14:39:00.590925 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1
[0m14:39:00.596111 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1"
[0m14:39:00.597906 [debug] [Thread-1 (]: On test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select customer_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_customer`
  where customer_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:39:00.598858 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:39:00.694126 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:e85d44ba-0e42-403a-8da8-9037a542f303&page=queryresults
[0m14:39:01.183377 [info ] [Thread-3 (]: 55 of 71 PASS source_not_null_raw_data_subscriptions__id ....................... [[32mPASS[0m in 2.18s]
[0m14:39:01.184913 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a
[0m14:39:01.185707 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469
[0m14:39:01.186326 [info ] [Thread-3 (]: 59 of 71 START test unique_dim_subscription_subscription_id .................... [RUN]
[0m14:39:01.187465 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a, now test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469)
[0m14:39:01.188225 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469
[0m14:39:01.195975 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469"
[0m14:39:01.200461 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469
[0m14:39:01.208466 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469"
[0m14:39:01.210133 [debug] [Thread-3 (]: On test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select subscription_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_subscription`
  where subscription_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:39:01.211110 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:39:01.474644 [info ] [Thread-4 (]: 56 of 71 PASS source_not_null_raw_data_subscriptions__user ..................... [[32mPASS[0m in 2.40s]
[0m14:39:01.479338 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6
[0m14:39:01.480771 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.unique_fct_marketing_spend_date.e93d255723
[0m14:39:01.482293 [info ] [Thread-4 (]: 60 of 71 START test unique_fct_marketing_spend_date ............................ [RUN]
[0m14:39:01.483781 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6, now test.data_pipeline_project.unique_fct_marketing_spend_date.e93d255723)
[0m14:39:01.484759 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.unique_fct_marketing_spend_date.e93d255723
[0m14:39:01.493415 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.unique_fct_marketing_spend_date.e93d255723"
[0m14:39:01.497518 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.unique_fct_marketing_spend_date.e93d255723
[0m14:39:01.501724 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.unique_fct_marketing_spend_date.e93d255723"
[0m14:39:01.507020 [debug] [Thread-4 (]: On test.data_pipeline_project.unique_fct_marketing_spend_date.e93d255723: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_fct_marketing_spend_date.e93d255723"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select date as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_marketing_spend`
  where date is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:39:01.508292 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:39:01.682552 [info ] [Thread-2 (]: 57 of 71 PASS source_not_null_raw_data_users__id ............................... [[32mPASS[0m in 2.54s]
[0m14:39:01.683750 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b
[0m14:39:01.684604 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.unique_fct_order_order_id.653705d6a5
[0m14:39:01.685143 [info ] [Thread-2 (]: 61 of 71 START test unique_fct_order_order_id .................................. [RUN]
[0m14:39:01.686029 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b, now test.data_pipeline_project.unique_fct_order_order_id.653705d6a5)
[0m14:39:01.686495 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.unique_fct_order_order_id.653705d6a5
[0m14:39:01.697070 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.unique_fct_order_order_id.653705d6a5"
[0m14:39:01.700751 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.unique_fct_order_order_id.653705d6a5
[0m14:39:01.705239 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.unique_fct_order_order_id.653705d6a5"
[0m14:39:01.707117 [debug] [Thread-2 (]: On test.data_pipeline_project.unique_fct_order_order_id.653705d6a5: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_fct_order_order_id.653705d6a5"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select order_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_order`
  where order_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:39:01.707992 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:39:01.864303 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:b508972c-11ea-40ab-a3d6-cf90a7209b63&page=queryresults
[0m14:39:02.554561 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:a7f4fc7d-c87c-41ac-86a6-753f9529a0cc&page=queryresults
[0m14:39:02.935296 [error] [Thread-1 (]: 58 of 71 FAIL 16 unique_dim_customer_customer_id ............................... [[31mFAIL 16[0m in 2.36s]
[0m14:39:02.939950 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1
[0m14:39:02.941594 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3
[0m14:39:02.943234 [info ] [Thread-1 (]: 62 of 71 START test unique_fct_shipment_shipment_id ............................ [RUN]
[0m14:39:02.945329 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1, now test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3)
[0m14:39:02.947731 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3
[0m14:39:02.958172 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3"
[0m14:39:02.961976 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3
[0m14:39:02.966938 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3"
[0m14:39:02.968946 [debug] [Thread-1 (]: On test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select shipment_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_shipment`
  where shipment_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:39:02.973459 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:39:03.171108 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:0f26a8c0-66f4-493c-b642-fa464ad0d5cc&page=queryresults
[0m14:39:03.231763 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:4b393596-6173-4f4c-89af-ec5b6cc8f4f0&page=queryresults
[0m14:39:03.553586 [info ] [Thread-3 (]: 59 of 71 PASS unique_dim_subscription_subscription_id .......................... [[32mPASS[0m in 2.36s]
[0m14:39:03.557246 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469
[0m14:39:03.563000 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd
[0m14:39:03.565966 [info ] [Thread-3 (]: 63 of 71 START test unique_mart_acquisition_efficiency_date .................... [RUN]
[0m14:39:03.568184 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469, now test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd)
[0m14:39:03.568898 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd
[0m14:39:04.175188 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd"
[0m14:39:04.182282 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd
[0m14:39:04.196211 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd"
[0m14:39:04.200402 [debug] [Thread-3 (]: On test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select date as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_acquisition_efficiency`
  where date is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:39:04.203120 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:39:04.436674 [info ] [Thread-2 (]: 61 of 71 PASS unique_fct_order_order_id ........................................ [[32mPASS[0m in 2.75s]
[0m14:39:04.446439 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.unique_fct_order_order_id.653705d6a5
[0m14:39:04.451632 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8
[0m14:39:04.457429 [info ] [Thread-2 (]: 64 of 71 START test unique_mart_revenue_daily_date ............................. [RUN]
[0m14:39:04.466498 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_fct_order_order_id.653705d6a5, now test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8)
[0m14:39:04.469379 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8
[0m14:39:04.497747 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8"
[0m14:39:04.501390 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8
[0m14:39:04.505175 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8"
[0m14:39:04.506061 [debug] [Thread-2 (]: On test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select date as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_revenue_daily`
  where date is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:39:04.508079 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:39:04.665711 [error] [Thread-4 (]: 60 of 71 FAIL 8 unique_fct_marketing_spend_date ................................ [[31mFAIL 8[0m in 3.18s]
[0m14:39:04.671413 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.unique_fct_marketing_spend_date.e93d255723
[0m14:39:04.674439 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c
[0m14:39:04.680492 [info ] [Thread-4 (]: 65 of 71 START test unique_mart_subscription_daily_date ........................ [RUN]
[0m14:39:04.686699 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_fct_marketing_spend_date.e93d255723, now test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c)
[0m14:39:04.687962 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c
[0m14:39:04.708406 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c"
[0m14:39:04.709298 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c
[0m14:39:04.711958 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c"
[0m14:39:04.713079 [debug] [Thread-4 (]: On test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select date as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily`
  where date is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:39:04.713880 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:39:04.858330 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:b2e37d5c-55a4-4357-91a1-9a5f16ebbaf0&page=queryresults
[0m14:39:05.524729 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:0b286dd0-8dd1-405f-842f-dda3018f1716&page=queryresults
[0m14:39:05.910652 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:9eb54e27-ed0b-4e45-9676-bbb16a8e9df2&page=queryresults
[0m14:39:05.918800 [info ] [Thread-1 (]: 62 of 71 PASS unique_fct_shipment_shipment_id .................................. [[32mPASS[0m in 2.97s]
[0m14:39:05.920086 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3
[0m14:39:05.920952 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00
[0m14:39:05.921984 [info ] [Thread-1 (]: 66 of 71 START test unique_stg_addresses_address_id ............................ [RUN]
[0m14:39:05.922855 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3, now test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00)
[0m14:39:05.923343 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00
[0m14:39:05.929926 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00"
[0m14:39:05.934121 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00
[0m14:39:05.937169 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00"
[0m14:39:05.938565 [debug] [Thread-1 (]: On test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select address_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_addresses`
  where address_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:39:05.939947 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:39:06.108013 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:3931bc9f-75e3-4e8a-9e0c-363bc3bee677&page=queryresults
[0m14:39:06.486850 [info ] [Thread-3 (]: 63 of 71 PASS unique_mart_acquisition_efficiency_date .......................... [[32mPASS[0m in 2.92s]
[0m14:39:06.491318 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd
[0m14:39:06.493633 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.unique_stg_countries_country_id.8679936442
[0m14:39:06.494564 [info ] [Thread-3 (]: 67 of 71 START test unique_stg_countries_country_id ............................ [RUN]
[0m14:39:06.495947 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd, now test.data_pipeline_project.unique_stg_countries_country_id.8679936442)
[0m14:39:06.496522 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.unique_stg_countries_country_id.8679936442
[0m14:39:06.502276 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.unique_stg_countries_country_id.8679936442"
[0m14:39:06.503812 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.unique_stg_countries_country_id.8679936442
[0m14:39:06.507441 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.unique_stg_countries_country_id.8679936442"
[0m14:39:06.509451 [debug] [Thread-3 (]: On test.data_pipeline_project.unique_stg_countries_country_id.8679936442: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_stg_countries_country_id.8679936442"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select country_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_countries`
  where country_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:39:06.510949 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:39:06.940858 [info ] [Thread-4 (]: 65 of 71 PASS unique_mart_subscription_daily_date .............................. [[32mPASS[0m in 2.25s]
[0m14:39:06.943388 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c
[0m14:39:06.944419 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a
[0m14:39:06.945515 [info ] [Thread-4 (]: 68 of 71 START test unique_stg_orders_order_id ................................. [RUN]
[0m14:39:06.947352 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c, now test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a)
[0m14:39:06.947961 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a
[0m14:39:06.954167 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a"
[0m14:39:06.955614 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a
[0m14:39:06.960119 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a"
[0m14:39:06.963680 [debug] [Thread-4 (]: On test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select order_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_orders`
  where order_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:39:06.966604 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:39:07.002570 [info ] [Thread-2 (]: 64 of 71 PASS unique_mart_revenue_daily_date ................................... [[32mPASS[0m in 2.54s]
[0m14:39:07.006571 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8
[0m14:39:07.007410 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c
[0m14:39:07.008327 [info ] [Thread-2 (]: 69 of 71 START test unique_stg_shipments_shipment_id ........................... [RUN]
[0m14:39:07.009977 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8, now test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c)
[0m14:39:07.011101 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c
[0m14:39:07.017382 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c"
[0m14:39:07.018584 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c
[0m14:39:07.022402 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c"
[0m14:39:07.024590 [debug] [Thread-2 (]: On test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select shipment_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_shipments`
  where shipment_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:39:07.025308 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:39:07.295058 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:0c15ed3b-6042-4822-9464-a209b6ce9d9b&page=queryresults
[0m14:39:07.796796 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:cfb37300-4b27-4fda-ab39-d036c9b7295c&page=queryresults
[0m14:39:08.142358 [info ] [Thread-1 (]: 66 of 71 PASS unique_stg_addresses_address_id .................................. [[32mPASS[0m in 2.22s]
[0m14:39:08.144773 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00
[0m14:39:08.147006 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m14:39:08.148582 [info ] [Thread-1 (]: 70 of 71 START test unique_stg_subscriptions_subscription_id ................... [RUN]
[0m14:39:08.151457 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00, now test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e)
[0m14:39:08.152299 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m14:39:08.160355 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e"
[0m14:39:08.164609 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m14:39:08.168574 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e"
[0m14:39:08.169924 [debug] [Thread-1 (]: On test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select subscription_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_subscriptions`
  where subscription_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:39:08.171013 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:39:08.310430 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:09a99d46-c37a-4efb-a176-47acecd5ad32&page=queryresults
[0m14:39:08.315405 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:a8489cf7-f498-41ef-a456-7d9f183eda11&page=queryresults
[0m14:39:08.722207 [info ] [Thread-3 (]: 67 of 71 PASS unique_stg_countries_country_id .................................. [[32mPASS[0m in 2.23s]
[0m14:39:08.724050 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.unique_stg_countries_country_id.8679936442
[0m14:39:08.726032 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b
[0m14:39:08.726883 [info ] [Thread-3 (]: 71 of 71 START test unique_stg_users_user_id ................................... [RUN]
[0m14:39:08.728186 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_stg_countries_country_id.8679936442, now test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b)
[0m14:39:08.729589 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b
[0m14:39:08.735847 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b"
[0m14:39:08.737398 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b
[0m14:39:08.740249 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b"
[0m14:39:08.741057 [debug] [Thread-3 (]: On test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select user_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
  where user_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:39:08.741871 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:39:09.227330 [info ] [Thread-4 (]: 68 of 71 PASS unique_stg_orders_order_id ....................................... [[32mPASS[0m in 2.28s]
[0m14:39:09.228900 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a
[0m14:39:09.232248 [info ] [Thread-2 (]: 69 of 71 PASS unique_stg_shipments_shipment_id ................................. [[32mPASS[0m in 2.22s]
[0m14:39:09.234379 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c
[0m14:39:09.736805 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:1df9c98f-a2d6-4c59-9b73-377f71ba4feb&page=queryresults
[0m14:39:10.073977 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:4f8e308a-c992-47fd-9ef4-e84690685d65&page=queryresults
[0m14:39:10.630115 [info ] [Thread-1 (]: 70 of 71 PASS unique_stg_subscriptions_subscription_id ......................... [[32mPASS[0m in 2.48s]
[0m14:39:10.631994 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m14:39:11.034021 [info ] [Thread-3 (]: 71 of 71 PASS unique_stg_users_user_id ......................................... [[32mPASS[0m in 2.30s]
[0m14:39:11.037993 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b
[0m14:39:11.048545 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:39:11.052156 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:39:11.052789 [debug] [MainThread]: Connection 'test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c' was properly closed.
[0m14:39:11.053545 [debug] [MainThread]: Connection 'test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e' was properly closed.
[0m14:39:11.054518 [debug] [MainThread]: Connection 'test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b' was properly closed.
[0m14:39:11.055146 [debug] [MainThread]: Connection 'test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a' was properly closed.
[0m14:39:11.055757 [info ] [MainThread]: 
[0m14:39:11.059287 [info ] [MainThread]: Finished running 71 data tests in 0 hours 0 minutes and 39.34 seconds (39.34s).
[0m14:39:11.071506 [debug] [MainThread]: Command end result
[0m14:39:11.178246 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m14:39:11.183532 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m14:39:11.199001 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m14:39:11.199878 [info ] [MainThread]: 
[0m14:39:11.200510 [info ] [MainThread]: [31mCompleted with 12 errors, 0 partial successes, and 1 warning:[0m
[0m14:39:11.201329 [info ] [MainThread]: 
[0m14:39:11.202077 [error] [MainThread]: [31mFailure in test accepted_range_fct_order_net_revenue__0 (models/intermediate/int_facts_marts_schema.yml)[0m
[0m14:39:11.202951 [error] [MainThread]:   Compilation Error in test accepted_range_fct_order_net_revenue__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:39:11.203498 [info ] [MainThread]: 
[0m14:39:11.204198 [error] [MainThread]: [31mFailure in test accepted_range_fct_marketing_spend_spend_try__0 (models/intermediate/int_facts_marts_schema.yml)[0m
[0m14:39:11.204937 [error] [MainThread]:   Compilation Error in test accepted_range_fct_marketing_spend_spend_try__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:39:11.205468 [info ] [MainThread]: 
[0m14:39:11.206245 [error] [MainThread]: [31mFailure in test accepted_range_fct_order_discount_total__0 (models/intermediate/int_facts_marts_schema.yml)[0m
[0m14:39:11.206979 [error] [MainThread]:   Compilation Error in test accepted_range_fct_order_discount_total__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:39:11.207689 [info ] [MainThread]: 
[0m14:39:11.208426 [error] [MainThread]: [31mFailure in test accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0 (models/intermediate/int_facts_marts_schema.yml)[0m
[0m14:39:11.209276 [error] [MainThread]:   Compilation Error in test accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:39:11.209971 [info ] [MainThread]: 
[0m14:39:11.210597 [error] [MainThread]: [31mFailure in test accepted_range_mart_kpis_latest_cac_payback_months__0 (models/intermediate/int_facts_marts_schema.yml)[0m
[0m14:39:11.211342 [error] [MainThread]:   Compilation Error in test accepted_range_mart_kpis_latest_cac_payback_months__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:39:11.212148 [info ] [MainThread]: 
[0m14:39:11.213072 [error] [MainThread]: [31mFailure in test accepted_range_mart_kpis_latest_monthly_recurring_revenue__0 (models/intermediate/int_facts_marts_schema.yml)[0m
[0m14:39:11.213806 [error] [MainThread]:   Compilation Error in test accepted_range_mart_kpis_latest_monthly_recurring_revenue__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:39:11.214759 [info ] [MainThread]: 
[0m14:39:11.216225 [error] [MainThread]: [31mFailure in test accepted_range_mart_subscription_daily_active_subscribers__0 (models/intermediate/int_facts_marts_schema.yml)[0m
[0m14:39:11.217591 [error] [MainThread]:   Compilation Error in test accepted_range_mart_subscription_daily_active_subscribers__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:39:11.218635 [info ] [MainThread]: 
[0m14:39:11.219992 [error] [MainThread]: [31mFailure in test accepted_range_mart_revenue_daily_daily_revenue__0 (models/intermediate/int_facts_marts_schema.yml)[0m
[0m14:39:11.221035 [error] [MainThread]:   Compilation Error in test accepted_range_mart_revenue_daily_daily_revenue__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:39:11.222130 [info ] [MainThread]: 
[0m14:39:11.223072 [error] [MainThread]: [31mFailure in test accepted_range_mart_subscription_daily_new_subscribers__0 (models/intermediate/int_facts_marts_schema.yml)[0m
[0m14:39:11.223723 [error] [MainThread]:   Compilation Error in test accepted_range_mart_subscription_daily_new_subscribers__0 (models/intermediate/int_facts_marts_schema.yml)
  'test_accepted_range' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:39:11.224335 [info ] [MainThread]: 
[0m14:39:11.225019 [error] [MainThread]: [31mFailure in test relationships_fct_shipment_order_id__order_id__ref_fct_order_ (models/intermediate/int_facts_marts_schema.yml)[0m
[0m14:39:11.225539 [error] [MainThread]:   Got 476 results, configured to fail if != 0
[0m14:39:11.226178 [info ] [MainThread]: 
[0m14:39:11.226907 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_project/models/intermediate/int_facts_marts_schema.yml/relationships_fct_shipment_order_id__order_id__ref_fct_order_.sql
[0m14:39:11.227597 [info ] [MainThread]: 
[0m14:39:11.228643 [error] [MainThread]: [31mFailure in test unique_dim_customer_customer_id (models/intermediate/int_facts_marts_schema.yml)[0m
[0m14:39:11.229489 [error] [MainThread]:   Got 16 results, configured to fail if != 0
[0m14:39:11.230216 [info ] [MainThread]: 
[0m14:39:11.230861 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_project/models/intermediate/int_facts_marts_schema.yml/unique_dim_customer_customer_id.sql
[0m14:39:11.231566 [info ] [MainThread]: 
[0m14:39:11.232412 [error] [MainThread]: [31mFailure in test unique_fct_marketing_spend_date (models/intermediate/int_facts_marts_schema.yml)[0m
[0m14:39:11.233628 [error] [MainThread]:   Got 8 results, configured to fail if != 0
[0m14:39:11.234534 [info ] [MainThread]: 
[0m14:39:11.235879 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_project/models/intermediate/int_facts_marts_schema.yml/unique_fct_marketing_spend_date.sql
[0m14:39:11.236815 [info ] [MainThread]: 
[0m14:39:11.240777 [warn ] [MainThread]: [33mWarning in test not_null_int_shipments_decomposed_delivered_at (models/staging/staging_schema.yml)[0m
[0m14:39:11.242589 [warn ] [MainThread]: Got 303 results, configured to warn if != 0
[0m14:39:11.243315 [info ] [MainThread]: 
[0m14:39:11.244184 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_project/models/staging/staging_schema.yml/not_null_int_shipments_decomposed_delivered_at.sql
[0m14:39:11.244904 [info ] [MainThread]: 
[0m14:39:11.245827 [info ] [MainThread]: Done. PASS=58 WARN=1 ERROR=12 SKIP=0 NO-OP=0 TOTAL=71
[0m14:39:11.248657 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 42.805176, "process_in_blocks": "0", "process_kernel_time": 4.632518, "process_mem_max_rss": "397964", "process_out_blocks": "6600", "process_user_time": 15.23132}
[0m14:39:11.249485 [debug] [MainThread]: Command `dbt test` failed at 14:39:11.249375 after 42.81 seconds
[0m14:39:11.249942 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7033fa385190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7033c93045f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7033c961a9c0>]}
[0m14:39:11.250349 [debug] [MainThread]: Flushing usage events
[0m14:39:12.002980 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:48:05.055803 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74892888b3e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x748927ab0800>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x748927efe780>]}


============================== 14:48:05.068406 | 9028fd75-68e1-4e6d-ad28-34f09024acef ==============================
[0m14:48:05.068406 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m14:48:05.080778 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'warn_error': 'None', 'invocation_command': 'dbt run-operation clean', 'send_anonymous_usage_stats': 'True', 'partial_parse': 'True', 'debug': 'False', 'target_path': 'None', 'log_format': 'default', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'introspect': 'True', 'fail_fast': 'False', 'use_experimental_parser': 'False', 'log_cache_events': 'False', 'profiles_dir': '/home/ecem/.dbt', 'cache_selected_only': 'False', 'quiet': 'False', 'indirect_selection': 'eager', 'no_print': 'None', 'empty': 'None', 'use_colors': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True'}
[0m14:48:08.081868 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9028fd75-68e1-4e6d-ad28-34f09024acef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7489087bf140>]}
[0m14:48:08.166812 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9028fd75-68e1-4e6d-ad28-34f09024acef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74890800af60>]}
[0m14:48:08.168456 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m14:48:08.424663 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m14:48:08.716468 [debug] [MainThread]: Partial parsing enabled: 78 files deleted, 0 files added, 1 files changed.
[0m14:48:08.718163 [debug] [MainThread]: Partial parsing: updated file: data_pipeline_project://models/facts/fct_shipment.sql
[0m14:48:08.719120 [debug] [MainThread]: Partial parsing: updated file: data_pipeline_project://models/intermediate/int_facts_marts_schema.yml
[0m14:48:08.719553 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/schema_tests/table_shape/expect_table_columns_to_not_contain_set.sql
[0m14:48:08.720319 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/math/percentile_cont.sql
[0m14:48:08.721305 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/schema_tests/aggregate_functions/expect_column_max_to_be_between.sql
[0m14:48:08.722215 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/schema_tests/_generalized/expression_is_true.sql
[0m14:48:08.722634 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/schema_tests/table_shape/expect_table_column_count_to_be_between.sql
[0m14:48:08.723048 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/schema_tests/multi-column/expect_column_pair_values_to_be_in_set.sql
[0m14:48:08.723676 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/schema_tests/aggregate_functions/expect_column_stdev_to_be_between.sql
[0m14:48:08.724246 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/schema_tests/column_values_basic/expect_column_values_to_be_in_set.sql
[0m14:48:08.724666 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/schema_tests/aggregate_functions/expect_column_distinct_count_to_be_greater_than.sql
[0m14:48:08.725213 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/schema_tests/multi-column/expect_select_column_values_to_be_unique_within_record.sql
[0m14:48:08.725628 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/schema_tests/table_shape/expect_table_row_count_to_equal_other_table_times_factor.sql
[0m14:48:08.726187 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/schema_tests/table_shape/expect_column_to_exist.sql
[0m14:48:08.726604 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/schema_tests/string_matching/expect_column_values_to_not_match_regex.sql
[0m14:48:08.727153 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/schema_tests/column_values_basic/expect_column_values_to_not_be_in_set.sql
[0m14:48:08.727562 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/schema_tests/aggregate_functions/expect_column_quantile_values_to_be_between.sql
[0m14:48:08.727996 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/schema_tests/distributional/expect_column_values_to_be_within_n_moving_stdevs.sql
[0m14:48:08.728542 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/schema_tests/aggregate_functions/expect_column_distinct_count_to_equal_other_table.sql
[0m14:48:08.729087 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/schema_tests/column_values_basic/expect_column_values_to_be_decreasing.sql
[0m14:48:08.729504 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/schema_tests/table_shape/expect_grouped_row_values_to_have_recent_data.sql
[0m14:48:08.730063 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/schema_tests/multi-column/expect_column_pair_values_to_be_equal.sql
[0m14:48:08.730475 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/schema_tests/aggregate_functions/expect_column_proportion_of_unique_values_to_be_between.sql
[0m14:48:08.731036 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/schema_tests/_generalized/_ignore_row_if_expression.sql
[0m14:48:08.731449 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/schema_tests/table_shape/expect_table_column_count_to_equal.sql
[0m14:48:08.731859 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/math/median.sql
[0m14:48:08.732402 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/schema_tests/aggregate_functions/expect_column_distinct_values_to_be_in_set.sql
[0m14:48:08.732817 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/utils/md5.sql
[0m14:48:08.733354 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/schema_tests/aggregate_functions/expect_column_distinct_count_to_be_less_than.sql
[0m14:48:08.733763 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/schema_tests/string_matching/expect_column_values_to_not_match_regex_list.sql
[0m14:48:08.734375 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/schema_tests/distributional/expect_column_values_to_be_within_n_stdevs.sql
[0m14:48:08.734818 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/regex/regexp_instr.sql
[0m14:48:08.735363 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/schema_tests/aggregate_functions/expect_column_unique_value_count_to_be_between.sql
[0m14:48:08.735769 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/schema_tests/column_values_basic/expect_column_values_to_be_in_type_list.sql
[0m14:48:08.736327 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/utils/groupby.sql
[0m14:48:08.736729 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/schema_tests/column_values_basic/expect_column_values_to_be_null.sql
[0m14:48:08.737295 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/schema_tests/_generalized/equal_expression.sql
[0m14:48:08.737699 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/schema_tests/string_matching/expect_column_value_lengths_to_be_between.sql
[0m14:48:08.738259 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/schema_tests/table_shape/_get_column_list.sql
[0m14:48:08.738671 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/schema_tests/table_shape/expect_table_columns_to_match_set.sql
[0m14:48:08.739290 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/schema_tests/_generalized/_truth_expression.sql
[0m14:48:08.739698 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/schema_tests/column_values_basic/expect_column_values_to_be_between.sql
[0m14:48:08.740266 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/schema_tests/multi-column/expect_multicolumn_sum_to_equal.sql
[0m14:48:08.740677 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/schema_tests/table_shape/expect_row_values_to_have_recent_data.sql
[0m14:48:08.741220 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/schema_tests/string_matching/expect_column_values_to_not_match_like_pattern_list.sql
[0m14:48:08.741630 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/schema_tests/column_values_basic/expect_column_values_to_not_be_null.sql
[0m14:48:08.742197 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/schema_tests/table_shape/expect_table_row_count_to_be_between.sql
[0m14:48:08.742606 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/schema_tests/column_values_basic/expect_column_values_to_be_increasing.sql
[0m14:48:08.743193 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/schema_tests/multi-column/expect_compound_columns_to_be_unique.sql
[0m14:48:08.743601 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/schema_tests/aggregate_functions/expect_column_distinct_count_to_equal.sql
[0m14:48:08.744166 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/schema_tests/string_matching/_get_like_pattern_expression.sql
[0m14:48:08.744574 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/schema_tests/table_shape/expect_table_columns_to_contain_set.sql
[0m14:48:08.745167 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/schema_tests/column_values_basic/expect_column_values_to_be_of_type.sql
[0m14:48:08.745579 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/schema_tests/aggregate_functions/expect_column_most_common_value_to_be_in_set.sql
[0m14:48:08.746124 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/schema_tests/string_matching/expect_column_value_lengths_to_equal.sql
[0m14:48:08.746546 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/schema_tests/table_shape/expect_table_row_count_to_equal_other_table.sql
[0m14:48:08.746982 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/utils/datatypes.sql
[0m14:48:08.747562 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/schema_tests/string_matching/expect_column_values_to_not_match_like_pattern.sql
[0m14:48:08.748113 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/schema_tests/table_shape/expect_table_columns_to_match_ordered_list.sql
[0m14:48:08.748517 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/schema_tests/aggregate_functions/expect_column_sum_to_be_between.sql
[0m14:48:08.749049 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/schema_tests/table_shape/_list_intersect.sql
[0m14:48:08.749629 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/schema_tests/column_values_basic/expect_column_values_to_have_consistent_casing.sql
[0m14:48:08.750193 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/schema_tests/aggregate_functions/expect_column_mean_to_be_between.sql
[0m14:48:08.750608 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/schema_tests/string_matching/expect_column_values_to_match_like_pattern_list.sql
[0m14:48:08.751192 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/schema_tests/table_shape/expect_table_row_count_to_equal.sql
[0m14:48:08.751607 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/math/log_natural.sql
[0m14:48:08.752205 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/schema_tests/string_matching/expect_column_values_to_match_like_pattern.sql
[0m14:48:08.752606 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/schema_tests/string_matching/expect_column_values_to_match_regex_list.sql
[0m14:48:08.753179 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/schema_tests/aggregate_functions/expect_column_distinct_values_to_equal_set.sql
[0m14:48:08.753601 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/schema_tests/distributional/expect_row_values_to_have_data_for_every_n_datepart.sql
[0m14:48:08.754194 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/schema_tests/multi-column/expect_column_pair_values_A_to_be_greater_than_B.sql
[0m14:48:08.754602 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/schema_tests/aggregate_functions/expect_column_min_to_be_between.sql
[0m14:48:08.755141 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/schema_tests/string_matching/expect_column_values_to_match_regex.sql
[0m14:48:08.755543 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/schema_tests/table_shape/expect_table_aggregation_to_equal_other_table.sql
[0m14:48:08.755961 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/schema_tests/_generalized/expression_between.sql
[0m14:48:08.756549 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/schema_tests/aggregate_functions/expect_column_distinct_values_to_contain_set.sql
[0m14:48:08.756962 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/math/rand.sql
[0m14:48:08.757499 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/schema_tests/table_shape/expect_table_column_count_to_equal_other_table.sql
[0m14:48:08.758106 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/schema_tests/aggregate_functions/expect_column_median_to_be_between.sql
[0m14:48:08.758511 [debug] [MainThread]: Partial parsing: deleted file: dbt_expectations://macros/schema_tests/column_values_basic/expect_column_values_to_be_unique.sql
[0m14:48:09.324713 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9028fd75-68e1-4e6d-ad28-34f09024acef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x748906eb1be0>]}
[0m14:48:09.454041 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m14:48:09.457076 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m14:48:09.475906 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9028fd75-68e1-4e6d-ad28-34f09024acef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x748906db3290>]}
[0m14:48:09.478290 [info ] [MainThread]: Found 21 models, 71 data tests, 10 sources, 756 macros
[0m14:48:09.478920 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9028fd75-68e1-4e6d-ad28-34f09024acef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7489077c29c0>]}
[0m14:48:09.479776 [debug] [MainThread]: Acquiring new bigquery connection 'macro_clean'
[0m14:48:09.480407 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:48:09.482325 [error] [MainThread]: Encountered an error while running operation: Runtime Error
  dbt could not find a macro with the name "clean" in any package
[0m14:48:09.483493 [debug] [MainThread]: Traceback (most recent call last):
  File "/home/ecem/Desktop/data-pipeline-project/venv/lib/python3.12/site-packages/dbt/task/run_operation.py", line 64, in run
    self._run_unsafe(package_name, macro_name)
  File "/home/ecem/Desktop/data-pipeline-project/venv/lib/python3.12/site-packages/dbt/task/run_operation.py", line 45, in _run_unsafe
    res = adapter.execute_macro(
          ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ecem/Desktop/data-pipeline-project/venv/lib/python3.12/site-packages/dbt/adapters/base/impl.py", line 1278, in execute_macro
    raise DbtRuntimeError(
dbt_common.exceptions.base.DbtRuntimeError: Runtime Error
  dbt could not find a macro with the name "clean" in any package

[0m14:48:09.484063 [error] [MainThread]: Encountered an error:
Compilation Error
  dbt could not find a macro with the name 'clean' in any package. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m14:48:09.486048 [debug] [MainThread]: Resource report: {"command_name": "run-operation", "command_success": false, "command_wall_clock_time": 4.607763, "process_in_blocks": "0", "process_kernel_time": 1.311273, "process_mem_max_rss": "378572", "process_out_blocks": "4272", "process_user_time": 5.526257}
[0m14:48:09.487274 [debug] [MainThread]: Command `dbt run-operation` failed at 14:48:09.487055 after 4.61 seconds
[0m14:48:09.487995 [debug] [MainThread]: Connection 'macro_clean' was properly closed.
[0m14:48:09.488489 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x748928426090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x748929c8aba0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7489077f14c0>]}
[0m14:48:09.489357 [debug] [MainThread]: Flushing usage events
[0m14:48:10.471597 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:48:35.048886 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e899ccb9a30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e899d4a9220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e899d60b5f0>]}


============================== 14:48:35.053280 | 5319d31e-e92b-45c1-9372-4d4c9f8a1bfb ==============================
[0m14:48:35.053280 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m14:48:35.056605 [debug] [MainThread]: running dbt with arguments {'log_cache_events': 'False', 'log_format': 'default', 'partial_parse': 'True', 'printer_width': '80', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'use_colors': 'True', 'cache_selected_only': 'False', 'version_check': 'True', 'debug': 'False', 'send_anonymous_usage_stats': 'True', 'fail_fast': 'False', 'static_parser': 'True', 'warn_error': 'None', 'no_print': 'None', 'quiet': 'False', 'write_json': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'profiles_dir': '/home/ecem/.dbt', 'invocation_command': 'dbt clean', 'introspect': 'True', 'indirect_selection': 'eager'}
[0m14:48:35.207850 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5319d31e-e92b-45c1-9372-4d4c9f8a1bfb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e899ca764e0>]}
[0m14:48:35.253614 [debug] [MainThread]: Resource report: {"command_name": "clean", "command_success": true, "command_wall_clock_time": 0.26382864, "process_in_blocks": "0", "process_kernel_time": 0.579398, "process_mem_max_rss": "101832", "process_out_blocks": "0", "process_user_time": 1.06499}
[0m14:48:35.255132 [debug] [MainThread]: Command `dbt clean` succeeded at 14:48:35.255016 after 0.27 seconds
[0m14:48:35.255571 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e899d5c4e90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e899cfea300>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e899cfc75c0>]}
[0m14:48:35.256393 [debug] [MainThread]: Flushing usage events
[0m14:48:36.173951 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:48:47.453104 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c452cf1da00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c452cda3890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c452c72bbc0>]}


============================== 14:48:47.456690 | c42411b2-2b7e-462f-80c0-a5d1413b7163 ==============================
[0m14:48:47.456690 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m14:48:47.457666 [debug] [MainThread]: running dbt with arguments {'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'no_print': 'None', 'fail_fast': 'False', 'empty': 'None', 'partial_parse': 'True', 'static_parser': 'True', 'warn_error': 'None', 'log_format': 'default', 'indirect_selection': 'eager', 'invocation_command': 'dbt test', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'use_colors': 'True', 'send_anonymous_usage_stats': 'True', 'profiles_dir': '/home/ecem/.dbt', 'printer_width': '80', 'target_path': 'None', 'introspect': 'True', 'write_json': 'True', 'debug': 'False', 'quiet': 'False', 'use_experimental_parser': 'False', 'version_check': 'True', 'log_cache_events': 'False', 'cache_selected_only': 'False'}
[0m14:48:49.794961 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c42411b2-2b7e-462f-80c0-a5d1413b7163', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c450cb31bb0>]}
[0m14:48:49.866012 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c42411b2-2b7e-462f-80c0-a5d1413b7163', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c452e7edf40>]}
[0m14:48:49.868474 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m14:48:50.113749 [error] [MainThread]: Encountered an error:
Compilation Error
  dbt found 1 package(s) specified in packages.yml, but only 0 package(s) installed in dbt_packages. Run "dbt deps" to install package dependencies.
[0m14:48:50.118784 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 2.7263513, "process_in_blocks": "0", "process_kernel_time": 0.836294, "process_mem_max_rss": "364592", "process_out_blocks": "8", "process_user_time": 3.6432}
[0m14:48:50.119882 [debug] [MainThread]: Command `dbt test` failed at 14:48:50.119768 after 2.73 seconds
[0m14:48:50.120569 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c452c683320>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c452e68e870>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c450c9777a0>]}
[0m14:48:50.121134 [debug] [MainThread]: Flushing usage events
[0m14:48:51.315697 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:49:07.862232 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d505cad36e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d505e4ebd40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d505e4e8950>]}


============================== 14:49:07.866274 | 37df48f2-115f-4dd3-ab67-57863b5be691 ==============================
[0m14:49:07.866274 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m14:49:07.867531 [debug] [MainThread]: running dbt with arguments {'indirect_selection': 'eager', 'use_colors': 'True', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'debug': 'False', 'log_format': 'default', 'static_parser': 'True', 'printer_width': '80', 'log_cache_events': 'False', 'version_check': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'profiles_dir': '/home/ecem/.dbt', 'cache_selected_only': 'False', 'invocation_command': 'dbt deps', 'quiet': 'False', 'write_json': 'True', 'warn_error': 'None', 'empty': 'None', 'fail_fast': 'False', 'introspect': 'True', 'target_path': 'None'}
[0m14:49:08.028635 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '37df48f2-115f-4dd3-ab67-57863b5be691', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d505e97fcb0>]}
[0m14:49:08.042240 [debug] [MainThread]: Set downloads directory='/tmp/dbt-downloads-erb9zzuq'
[0m14:49:08.044093 [debug] [MainThread]: Making package index registry request: GET https://hub.getdbt.com/api/v1/index.json
[0m14:49:08.458867 [debug] [MainThread]: Response from registry index: GET https://hub.getdbt.com/api/v1/index.json 200
[0m14:49:08.461724 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
[0m14:49:08.599804 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
[0m14:49:08.625991 [info ] [MainThread]: Updating lock file in file path: /home/ecem/Desktop/data-pipeline-project/dbt/package-lock.yml
[0m14:49:08.629570 [debug] [MainThread]: Set downloads directory='/tmp/dbt-downloads-bpnll6yu'
[0m14:49:08.633598 [info ] [MainThread]: Installing dbt-labs/dbt_utils
[0m14:49:09.359805 [info ] [MainThread]: Installed from version 1.3.1
[0m14:49:09.360875 [info ] [MainThread]: Up to date!
[0m14:49:09.361939 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': '37df48f2-115f-4dd3-ab67-57863b5be691', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d505c778800>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d505c0561b0>]}
[0m14:49:09.363496 [debug] [MainThread]: Resource report: {"command_name": "deps", "command_success": true, "command_wall_clock_time": 1.5620188, "process_in_blocks": "0", "process_kernel_time": 0.236308, "process_mem_max_rss": "103688", "process_out_blocks": "2288", "process_user_time": 1.752836}
[0m14:49:09.364263 [debug] [MainThread]: Command `dbt deps` succeeded at 14:49:09.364153 after 1.56 seconds
[0m14:49:09.367312 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d505c0e9f40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d505bf27e30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d505c89bb60>]}
[0m14:49:09.367895 [debug] [MainThread]: Flushing usage events
[0m14:49:10.121435 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:49:42.393114 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7239cc215c40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7239cc0fb080>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7239cdd84c80>]}


============================== 14:49:42.396565 | 2a0c1f65-f248-4146-b61e-c6d68e2d6f63 ==============================
[0m14:49:42.396565 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m14:49:42.397986 [debug] [MainThread]: running dbt with arguments {'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'printer_width': '80', 'invocation_command': 'dbt test', 'warn_error': 'None', 'use_colors': 'True', 'version_check': 'True', 'indirect_selection': 'eager', 'cache_selected_only': 'False', 'send_anonymous_usage_stats': 'True', 'partial_parse': 'True', 'profiles_dir': '/home/ecem/.dbt', 'write_json': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'empty': 'None', 'target_path': 'None', 'log_cache_events': 'False', 'static_parser': 'True', 'no_print': 'None', 'quiet': 'False', 'use_experimental_parser': 'False', 'debug': 'False', 'log_format': 'default', 'introspect': 'True'}
[0m14:49:44.772812 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2a0c1f65-f248-4146-b61e-c6d68e2d6f63', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7239abf68e60>]}
[0m14:49:44.857312 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2a0c1f65-f248-4146-b61e-c6d68e2d6f63', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7239aea17f50>]}
[0m14:49:44.859068 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m14:49:45.133506 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m14:49:45.137000 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m14:49:45.137898 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '2a0c1f65-f248-4146-b61e-c6d68e2d6f63', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7239abb24710>]}
[0m14:49:46.711936 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `relationships` defined on 'stg_orders' in
package 'data_pipeline_project' (models/staging/staging_schema.yml). Arguments
to generic tests should be nested under the `arguments` property.
[0m14:49:46.713103 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '2a0c1f65-f248-4146-b61e-c6d68e2d6f63', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7239ab819280>]}
[0m14:49:47.207301 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2a0c1f65-f248-4146-b61e-c6d68e2d6f63', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7239aacacad0>]}
[0m14:49:47.346344 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m14:49:47.348734 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m14:49:47.382671 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2a0c1f65-f248-4146-b61e-c6d68e2d6f63', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7239aad7bf20>]}
[0m14:49:47.383807 [info ] [MainThread]: Found 21 models, 71 data tests, 10 sources, 624 macros
[0m14:49:47.384811 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2a0c1f65-f248-4146-b61e-c6d68e2d6f63', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7239aaccf1a0>]}
[0m14:49:47.389758 [info ] [MainThread]: 
[0m14:49:47.390815 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:49:47.391430 [info ] [MainThread]: 
[0m14:49:47.392662 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m14:49:47.399345 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_marts'
[0m14:49:47.400325 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:49:47.401908 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_staging'
[0m14:49:47.447127 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:49:48.523386 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2a0c1f65-f248-4146-b61e-c6d68e2d6f63', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7239aabcf9b0>]}
[0m14:49:48.524170 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:49:48.533094 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.dbt_utils_accepted_range_fct_marketing_spend_spend_try__0.bfe523cb61
[0m14:49:48.535253 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.dbt_utils_accepted_range_fct_order_discount_total__0.5f31954cd6
[0m14:49:48.536977 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.dbt_utils_accepted_range_fct_order_net_revenue__0.cb846760ce
[0m14:49:48.538616 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.dbt_utils_accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0.be1f75eba9
[0m14:49:48.534114 [info ] [Thread-1 (]: 1 of 71 START test dbt_utils_accepted_range_fct_marketing_spend_spend_try__0 ... [RUN]
[0m14:49:48.536064 [info ] [Thread-2 (]: 2 of 71 START test dbt_utils_accepted_range_fct_order_discount_total__0 ........ [RUN]
[0m14:49:48.539714 [info ] [Thread-4 (]: 4 of 71 START test dbt_utils_accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0  [RUN]
[0m14:49:48.541161 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_marts, now test.data_pipeline_project.dbt_utils_accepted_range_fct_marketing_spend_spend_try__0.bfe523cb61)
[0m14:49:48.542209 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now test.data_pipeline_project.dbt_utils_accepted_range_fct_order_discount_total__0.5f31954cd6)
[0m14:49:48.538064 [info ] [Thread-3 (]: 3 of 71 START test dbt_utils_accepted_range_fct_order_net_revenue__0 ........... [RUN]
[0m14:49:48.546694 [debug] [Thread-3 (]: Acquiring new bigquery connection 'test.data_pipeline_project.dbt_utils_accepted_range_fct_order_net_revenue__0.cb846760ce'
[0m14:49:48.544223 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.dbt_utils_accepted_range_fct_marketing_spend_spend_try__0.bfe523cb61
[0m14:49:48.547550 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.dbt_utils_accepted_range_fct_order_net_revenue__0.cb846760ce
[0m14:49:48.545111 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.dbt_utils_accepted_range_fct_order_discount_total__0.5f31954cd6
[0m14:49:48.543429 [debug] [Thread-4 (]: Acquiring new bigquery connection 'test.data_pipeline_project.dbt_utils_accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0.be1f75eba9'
[0m14:49:48.568571 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.dbt_utils_accepted_range_fct_marketing_spend_spend_try__0.bfe523cb61"
[0m14:49:48.565019 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.dbt_utils_accepted_range_fct_order_net_revenue__0.cb846760ce"
[0m14:49:48.574015 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.dbt_utils_accepted_range_fct_order_discount_total__0.5f31954cd6"
[0m14:49:48.574693 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.dbt_utils_accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0.be1f75eba9
[0m14:49:48.582579 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.dbt_utils_accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0.be1f75eba9"
[0m14:49:48.577127 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.dbt_utils_accepted_range_fct_marketing_spend_spend_try__0.bfe523cb61
[0m14:49:48.577870 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.dbt_utils_accepted_range_fct_order_discount_total__0.5f31954cd6
[0m14:49:48.576444 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.dbt_utils_accepted_range_fct_order_net_revenue__0.cb846760ce
[0m14:49:48.584234 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.dbt_utils_accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0.be1f75eba9
[0m14:49:48.640430 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.dbt_utils_accepted_range_fct_order_discount_total__0.5f31954cd6"
[0m14:49:48.650510 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.dbt_utils_accepted_range_fct_order_net_revenue__0.cb846760ce"
[0m14:49:48.651448 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.dbt_utils_accepted_range_fct_marketing_spend_spend_try__0.bfe523cb61"
[0m14:49:48.656216 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.dbt_utils_accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0.be1f75eba9"
[0m14:49:48.659221 [debug] [Thread-2 (]: On test.data_pipeline_project.dbt_utils_accepted_range_fct_order_discount_total__0.5f31954cd6: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.dbt_utils_accepted_range_fct_order_discount_total__0.5f31954cd6"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  

with meet_condition as(
  select *
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_order`
),

validation_errors as (
  select *
  from meet_condition
  where
    -- never true, defaults to an empty result set. Exists to ensure any combo of the `or` clauses below succeeds
    1 = 2
    -- records with a value >= min_value are permitted. The `not` flips this to find records that don't meet the rule.
    or not discount_total >= 0
)

select *
from validation_errors


  
  
      
    ) dbt_internal_test
[0m14:49:48.660525 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:49:48.669291 [debug] [Thread-1 (]: On test.data_pipeline_project.dbt_utils_accepted_range_fct_marketing_spend_spend_try__0.bfe523cb61: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.dbt_utils_accepted_range_fct_marketing_spend_spend_try__0.bfe523cb61"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  

with meet_condition as(
  select *
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_marketing_spend`
),

validation_errors as (
  select *
  from meet_condition
  where
    -- never true, defaults to an empty result set. Exists to ensure any combo of the `or` clauses below succeeds
    1 = 2
    -- records with a value >= min_value are permitted. The `not` flips this to find records that don't meet the rule.
    or not spend_try >= 0
)

select *
from validation_errors


  
  
      
    ) dbt_internal_test
[0m14:49:48.703731 [debug] [Thread-3 (]: On test.data_pipeline_project.dbt_utils_accepted_range_fct_order_net_revenue__0.cb846760ce: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.dbt_utils_accepted_range_fct_order_net_revenue__0.cb846760ce"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  

with meet_condition as(
  select *
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_order`
),

validation_errors as (
  select *
  from meet_condition
  where
    -- never true, defaults to an empty result set. Exists to ensure any combo of the `or` clauses below succeeds
    1 = 2
    -- records with a value >= min_value are permitted. The `not` flips this to find records that don't meet the rule.
    or not net_revenue >= 0
)

select *
from validation_errors


  
  
      
    ) dbt_internal_test
[0m14:49:48.706089 [debug] [Thread-4 (]: On test.data_pipeline_project.dbt_utils_accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0.be1f75eba9: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.dbt_utils_accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0.be1f75eba9"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  

with meet_condition as(
  select *
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_acquisition_efficiency`
),

validation_errors as (
  select *
  from meet_condition
  where
    -- never true, defaults to an empty result set. Exists to ensure any combo of the `or` clauses below succeeds
    1 = 2
    -- records with a value >= min_value are permitted. The `not` flips this to find records that don't meet the rule.
    or not daily_customer_acquisition_cost >= 0
)

select *
from validation_errors


  
  
      
    ) dbt_internal_test
[0m14:49:48.709477 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m14:49:48.710155 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m14:49:48.708548 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:49:50.006227 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:080915d3-b09f-456f-bae2-b8e6bd86ea50&page=queryresults
[0m14:49:50.018428 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:692ed4a5-6253-4850-a2ee-b7b1390ea3f4&page=queryresults
[0m14:49:50.101101 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:6ee059ae-75e5-46a6-bb00-e5f57dc3cdff&page=queryresults
[0m14:49:50.219921 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:fd800d57-d155-4a35-9b3e-1abaae4af585&page=queryresults
[0m14:49:50.964040 [info ] [Thread-1 (]: 1 of 71 PASS dbt_utils_accepted_range_fct_marketing_spend_spend_try__0 ......... [[32mPASS[0m in 2.42s]
[0m14:49:50.967380 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.dbt_utils_accepted_range_fct_marketing_spend_spend_try__0.bfe523cb61
[0m14:49:50.969526 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_cac_payback_months__0.440e9fbb47
[0m14:49:50.970772 [info ] [Thread-1 (]: 5 of 71 START test dbt_utils_accepted_range_mart_kpis_latest_cac_payback_months__0  [RUN]
[0m14:49:50.976458 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.dbt_utils_accepted_range_fct_marketing_spend_spend_try__0.bfe523cb61, now test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_cac_payback_months__0.440e9fbb47)
[0m14:49:50.977479 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_cac_payback_months__0.440e9fbb47
[0m14:49:50.994709 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_cac_payback_months__0.440e9fbb47"
[0m14:49:51.000036 [info ] [Thread-2 (]: 2 of 71 PASS dbt_utils_accepted_range_fct_order_discount_total__0 .............. [[32mPASS[0m in 2.46s]
[0m14:49:51.000955 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_cac_payback_months__0.440e9fbb47
[0m14:49:51.001803 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.dbt_utils_accepted_range_fct_order_discount_total__0.5f31954cd6
[0m14:49:51.005652 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_monthly_recurring_revenue__0.e7fa0e4f46
[0m14:49:51.004484 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_cac_payback_months__0.440e9fbb47"
[0m14:49:51.006307 [info ] [Thread-2 (]: 6 of 71 START test dbt_utils_accepted_range_mart_kpis_latest_monthly_recurring_revenue__0  [RUN]
[0m14:49:51.007515 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.dbt_utils_accepted_range_fct_order_discount_total__0.5f31954cd6, now test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_monthly_recurring_revenue__0.e7fa0e4f46)
[0m14:49:51.008239 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_monthly_recurring_revenue__0.e7fa0e4f46
[0m14:49:51.012960 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_monthly_recurring_revenue__0.e7fa0e4f46"
[0m14:49:51.013565 [debug] [Thread-1 (]: On test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_cac_payback_months__0.440e9fbb47: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_cac_payback_months__0.440e9fbb47"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  

with meet_condition as(
  select *
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_kpis_latest`
),

validation_errors as (
  select *
  from meet_condition
  where
    -- never true, defaults to an empty result set. Exists to ensure any combo of the `or` clauses below succeeds
    1 = 2
    -- records with a value >= min_value are permitted. The `not` flips this to find records that don't meet the rule.
    or not cac_payback_months >= 0
)

select *
from validation_errors


  
  
      
    ) dbt_internal_test
[0m14:49:51.014225 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:49:51.046599 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_monthly_recurring_revenue__0.e7fa0e4f46
[0m14:49:51.052210 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_monthly_recurring_revenue__0.e7fa0e4f46"
[0m14:49:51.056223 [debug] [Thread-2 (]: On test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_monthly_recurring_revenue__0.e7fa0e4f46: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_monthly_recurring_revenue__0.e7fa0e4f46"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  

with meet_condition as(
  select *
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_kpis_latest`
),

validation_errors as (
  select *
  from meet_condition
  where
    -- never true, defaults to an empty result set. Exists to ensure any combo of the `or` clauses below succeeds
    1 = 2
    -- records with a value >= min_value are permitted. The `not` flips this to find records that don't meet the rule.
    or not monthly_recurring_revenue >= 0
)

select *
from validation_errors


  
  
      
    ) dbt_internal_test
[0m14:49:51.061289 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:49:51.238694 [info ] [Thread-4 (]: 4 of 71 PASS dbt_utils_accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0  [[32mPASS[0m in 2.69s]
[0m14:49:51.243004 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.dbt_utils_accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0.be1f75eba9
[0m14:49:51.245087 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.dbt_utils_accepted_range_mart_revenue_daily_daily_revenue__0.611ebe5f4e
[0m14:49:51.247329 [info ] [Thread-4 (]: 7 of 71 START test dbt_utils_accepted_range_mart_revenue_daily_daily_revenue__0  [RUN]
[0m14:49:51.255052 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.dbt_utils_accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0.be1f75eba9, now test.data_pipeline_project.dbt_utils_accepted_range_mart_revenue_daily_daily_revenue__0.611ebe5f4e)
[0m14:49:51.255643 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.dbt_utils_accepted_range_mart_revenue_daily_daily_revenue__0.611ebe5f4e
[0m14:49:51.263422 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.dbt_utils_accepted_range_mart_revenue_daily_daily_revenue__0.611ebe5f4e"
[0m14:49:51.251593 [info ] [Thread-3 (]: 3 of 71 PASS dbt_utils_accepted_range_fct_order_net_revenue__0 ................. [[32mPASS[0m in 2.70s]
[0m14:49:51.267793 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.dbt_utils_accepted_range_mart_revenue_daily_daily_revenue__0.611ebe5f4e
[0m14:49:51.270942 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.dbt_utils_accepted_range_fct_order_net_revenue__0.cb846760ce
[0m14:49:51.285069 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_active_subscribers__0.74fd82653d
[0m14:49:51.287425 [info ] [Thread-3 (]: 8 of 71 START test dbt_utils_accepted_range_mart_subscription_daily_active_subscribers__0  [RUN]
[0m14:49:51.288065 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.dbt_utils_accepted_range_fct_order_net_revenue__0.cb846760ce, now test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_active_subscribers__0.74fd82653d)
[0m14:49:51.288552 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_active_subscribers__0.74fd82653d
[0m14:49:51.293326 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_active_subscribers__0.74fd82653d"
[0m14:49:51.295241 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.dbt_utils_accepted_range_mart_revenue_daily_daily_revenue__0.611ebe5f4e"
[0m14:49:51.297404 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_active_subscribers__0.74fd82653d
[0m14:49:51.299150 [debug] [Thread-4 (]: On test.data_pipeline_project.dbt_utils_accepted_range_mart_revenue_daily_daily_revenue__0.611ebe5f4e: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.dbt_utils_accepted_range_mart_revenue_daily_daily_revenue__0.611ebe5f4e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  

with meet_condition as(
  select *
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_revenue_daily`
),

validation_errors as (
  select *
  from meet_condition
  where
    -- never true, defaults to an empty result set. Exists to ensure any combo of the `or` clauses below succeeds
    1 = 2
    -- records with a value >= min_value are permitted. The `not` flips this to find records that don't meet the rule.
    or not daily_revenue >= 0
)

select *
from validation_errors


  
  
      
    ) dbt_internal_test
[0m14:49:51.307181 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_active_subscribers__0.74fd82653d"
[0m14:49:51.308248 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:49:51.311427 [debug] [Thread-3 (]: On test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_active_subscribers__0.74fd82653d: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_active_subscribers__0.74fd82653d"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  

with meet_condition as(
  select *
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily`
),

validation_errors as (
  select *
  from meet_condition
  where
    -- never true, defaults to an empty result set. Exists to ensure any combo of the `or` clauses below succeeds
    1 = 2
    -- records with a value >= min_value are permitted. The `not` flips this to find records that don't meet the rule.
    or not active_subscribers >= 0
)

select *
from validation_errors


  
  
      
    ) dbt_internal_test
[0m14:49:51.313467 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:49:52.136551 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:1b7f1c76-92e4-4009-a470-a85c1319c3a0&page=queryresults
[0m14:49:52.142425 [error] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:1b7f1c76-92e4-4009-a470-a85c1319c3a0&page=queryresults
[0m14:49:52.208287 [debug] [Thread-1 (]: Database Error in test dbt_utils_accepted_range_mart_kpis_latest_cac_payback_months__0 (models/intermediate/int_facts_marts_schema.yml)
  Not found: Table data-pipeline-project-474812:analytics_staging_marts.mart_kpis_latest was not found in location US
  compiled code at target/run/data_pipeline_project/models/intermediate/int_facts_marts_schema.yml/dbt_utils_accepted_range_mart_kpis_latest_cac_payback_months__0.sql
[0m14:49:52.211510 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:23f7bb60-4add-48fe-b8a9-679639769099&page=queryresults
[0m14:49:52.212363 [error] [Thread-1 (]: 5 of 71 ERROR dbt_utils_accepted_range_mart_kpis_latest_cac_payback_months__0 .. [[31mERROR[0m in 1.24s]
[0m14:49:52.215472 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_cac_payback_months__0.440e9fbb47
[0m14:49:52.213768 [error] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:23f7bb60-4add-48fe-b8a9-679639769099&page=queryresults
[0m14:49:52.216450 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_new_subscribers__0.654a8ca7d1
[0m14:49:52.223147 [debug] [Thread-7 (]: Marking all children of 'test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_cac_payback_months__0.440e9fbb47' to be skipped because of status 'error'.  Reason: Database Error in test dbt_utils_accepted_range_mart_kpis_latest_cac_payback_months__0 (models/intermediate/int_facts_marts_schema.yml)
  Not found: Table data-pipeline-project-474812:analytics_staging_marts.mart_kpis_latest was not found in location US
  compiled code at target/run/data_pipeline_project/models/intermediate/int_facts_marts_schema.yml/dbt_utils_accepted_range_mart_kpis_latest_cac_payback_months__0.sql.
[0m14:49:52.227134 [info ] [Thread-1 (]: 9 of 71 START test dbt_utils_accepted_range_mart_subscription_daily_new_subscribers__0  [RUN]
[0m14:49:52.237466 [debug] [Thread-2 (]: Database Error in test dbt_utils_accepted_range_mart_kpis_latest_monthly_recurring_revenue__0 (models/intermediate/int_facts_marts_schema.yml)
  Not found: Table data-pipeline-project-474812:analytics_staging_marts.mart_kpis_latest was not found in location US
  compiled code at target/run/data_pipeline_project/models/intermediate/int_facts_marts_schema.yml/dbt_utils_accepted_range_mart__e59ea97756b83d6bad13e90b7eb4c03a.sql
[0m14:49:52.239944 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_cac_payback_months__0.440e9fbb47, now test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_new_subscribers__0.654a8ca7d1)
[0m14:49:52.241272 [error] [Thread-2 (]: 6 of 71 ERROR dbt_utils_accepted_range_mart_kpis_latest_monthly_recurring_revenue__0  [[31mERROR[0m in 1.23s]
[0m14:49:52.248195 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_new_subscribers__0.654a8ca7d1
[0m14:49:52.252268 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_monthly_recurring_revenue__0.e7fa0e4f46
[0m14:49:52.259795 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_new_subscribers__0.654a8ca7d1"
[0m14:49:52.261173 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc
[0m14:49:52.262627 [debug] [Thread-7 (]: Marking all children of 'test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_monthly_recurring_revenue__0.e7fa0e4f46' to be skipped because of status 'error'.  Reason: Database Error in test dbt_utils_accepted_range_mart_kpis_latest_monthly_recurring_revenue__0 (models/intermediate/int_facts_marts_schema.yml)
  Not found: Table data-pipeline-project-474812:analytics_staging_marts.mart_kpis_latest was not found in location US
  compiled code at target/run/data_pipeline_project/models/intermediate/int_facts_marts_schema.yml/dbt_utils_accepted_range_mart__e59ea97756b83d6bad13e90b7eb4c03a.sql.
[0m14:49:52.266440 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_new_subscribers__0.654a8ca7d1
[0m14:49:52.273244 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_new_subscribers__0.654a8ca7d1"
[0m14:49:52.267939 [info ] [Thread-2 (]: 10 of 71 START test not_null_dim_customer_customer_id .......................... [RUN]
[0m14:49:52.276043 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_monthly_recurring_revenue__0.e7fa0e4f46, now test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc)
[0m14:49:52.278004 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc
[0m14:49:52.286278 [debug] [Thread-1 (]: On test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_new_subscribers__0.654a8ca7d1: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_new_subscribers__0.654a8ca7d1"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  

with meet_condition as(
  select *
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily`
),

validation_errors as (
  select *
  from meet_condition
  where
    -- never true, defaults to an empty result set. Exists to ensure any combo of the `or` clauses below succeeds
    1 = 2
    -- records with a value >= min_value are permitted. The `not` flips this to find records that don't meet the rule.
    or not new_subscribers >= 0
)

select *
from validation_errors


  
  
      
    ) dbt_internal_test
[0m14:49:52.294786 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:49:52.335871 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc"
[0m14:49:52.342929 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc
[0m14:49:52.347049 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc"
[0m14:49:52.349419 [debug] [Thread-2 (]: On test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select customer_id
from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_customer`
where customer_id is null



  
  
      
    ) dbt_internal_test
[0m14:49:52.350154 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:49:52.503692 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:9635c98c-dcae-4f7e-9cc3-9029b32536cf&page=queryresults
[0m14:49:52.625811 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:c69ff2fe-4e8c-41d6-b992-3ba55395df65&page=queryresults
[0m14:49:53.368213 [info ] [Thread-4 (]: 7 of 71 PASS dbt_utils_accepted_range_mart_revenue_daily_daily_revenue__0 ...... [[32mPASS[0m in 2.11s]
[0m14:49:53.372483 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.dbt_utils_accepted_range_mart_revenue_daily_daily_revenue__0.611ebe5f4e
[0m14:49:53.374344 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723
[0m14:49:53.376622 [info ] [Thread-4 (]: 11 of 71 START test not_null_dim_subscription_customer_id ...................... [RUN]
[0m14:49:53.378213 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.dbt_utils_accepted_range_mart_revenue_daily_daily_revenue__0.611ebe5f4e, now test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723)
[0m14:49:53.379423 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723
[0m14:49:53.392816 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723"
[0m14:49:53.394158 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723
[0m14:49:53.397854 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723"
[0m14:49:53.400731 [debug] [Thread-4 (]: On test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select customer_id
from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_subscription`
where customer_id is null



  
  
      
    ) dbt_internal_test
[0m14:49:53.403381 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:3093d6bc-63f4-4474-93f6-733997756b8d&page=queryresults
[0m14:49:53.404070 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:49:53.529186 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:d6765085-2ff6-4028-aaa6-24dec80d4490&page=queryresults
[0m14:49:53.568176 [info ] [Thread-3 (]: 8 of 71 PASS dbt_utils_accepted_range_mart_subscription_daily_active_subscribers__0  [[32mPASS[0m in 2.28s]
[0m14:49:53.569627 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_active_subscribers__0.74fd82653d
[0m14:49:53.570510 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6
[0m14:49:53.571586 [info ] [Thread-3 (]: 12 of 71 START test not_null_dim_subscription_subscription_id .................. [RUN]
[0m14:49:53.573113 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_active_subscribers__0.74fd82653d, now test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6)
[0m14:49:53.573661 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6
[0m14:49:53.578419 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6"
[0m14:49:53.579378 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6
[0m14:49:53.584198 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6"
[0m14:49:53.586023 [debug] [Thread-3 (]: On test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select subscription_id
from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_subscription`
where subscription_id is null



  
  
      
    ) dbt_internal_test
[0m14:49:53.590777 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:49:54.346889 [info ] [Thread-1 (]: 9 of 71 PASS dbt_utils_accepted_range_mart_subscription_daily_new_subscribers__0  [[32mPASS[0m in 2.11s]
[0m14:49:54.348273 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_new_subscribers__0.654a8ca7d1
[0m14:49:54.349238 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719
[0m14:49:54.350066 [info ] [Thread-1 (]: 13 of 71 START test not_null_fct_marketing_spend_date .......................... [RUN]
[0m14:49:54.351068 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_new_subscribers__0.654a8ca7d1, now test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719)
[0m14:49:54.351585 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719
[0m14:49:54.360173 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719"
[0m14:49:54.362346 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719
[0m14:49:54.364933 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719"
[0m14:49:54.365687 [debug] [Thread-1 (]: On test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select date
from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_marketing_spend`
where date is null



  
  
      
    ) dbt_internal_test
[0m14:49:54.366238 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:49:54.412124 [info ] [Thread-2 (]: 10 of 71 PASS not_null_dim_customer_customer_id ................................ [[32mPASS[0m in 2.13s]
[0m14:49:54.415079 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc
[0m14:49:54.415680 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d
[0m14:49:54.423200 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:a6dca85b-58fb-4f79-9f05-817fd44eb86a&page=queryresults
[0m14:49:54.421487 [info ] [Thread-2 (]: 14 of 71 START test not_null_fct_marketing_spend_spend_try ..................... [RUN]
[0m14:49:54.432391 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc, now test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d)
[0m14:49:54.433229 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d
[0m14:49:54.442000 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d"
[0m14:49:54.443119 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d
[0m14:49:54.446029 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d"
[0m14:49:54.448064 [debug] [Thread-2 (]: On test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select spend_try
from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_marketing_spend`
where spend_try is null



  
  
      
    ) dbt_internal_test
[0m14:49:54.450083 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:49:54.674717 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:b9afdcc0-0cb4-4201-9646-84e8accdf913&page=queryresults
[0m14:49:55.325303 [info ] [Thread-4 (]: 11 of 71 PASS not_null_dim_subscription_customer_id ............................ [[32mPASS[0m in 1.95s]
[0m14:49:55.328525 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723
[0m14:49:55.331186 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff
[0m14:49:55.334767 [info ] [Thread-4 (]: 15 of 71 START test not_null_fct_order_customer_id ............................. [RUN]
[0m14:49:55.337063 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723, now test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff)
[0m14:49:55.339015 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff
[0m14:49:55.348049 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff"
[0m14:49:55.352058 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff
[0m14:49:55.357144 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff"
[0m14:49:55.360144 [debug] [Thread-4 (]: On test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select customer_id
from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_order`
where customer_id is null



  
  
      
    ) dbt_internal_test
[0m14:49:55.361998 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:49:55.458850 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:b7b013fb-da48-4ce9-ac0e-761af286a97e&page=queryresults
[0m14:49:55.491674 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:5727ed7b-c3c7-45df-a187-36fce82214d6&page=queryresults
[0m14:49:55.714006 [info ] [Thread-3 (]: 12 of 71 PASS not_null_dim_subscription_subscription_id ........................ [[32mPASS[0m in 2.14s]
[0m14:49:55.724330 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6
[0m14:49:55.729502 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea
[0m14:49:55.741326 [info ] [Thread-3 (]: 16 of 71 START test not_null_fct_order_discount_total .......................... [RUN]
[0m14:49:55.763243 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6, now test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea)
[0m14:49:55.773306 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea
[0m14:49:55.788223 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea"
[0m14:49:55.793113 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea
[0m14:49:55.801344 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea"
[0m14:49:55.802619 [debug] [Thread-3 (]: On test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select discount_total
from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_order`
where discount_total is null



  
  
      
    ) dbt_internal_test
[0m14:49:55.803619 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:49:56.329252 [info ] [Thread-1 (]: 13 of 71 PASS not_null_fct_marketing_spend_date ................................ [[32mPASS[0m in 1.98s]
[0m14:49:56.336404 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719
[0m14:49:56.340583 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e
[0m14:49:56.343689 [info ] [Thread-1 (]: 17 of 71 START test not_null_fct_order_net_revenue ............................. [RUN]
[0m14:49:56.349212 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719, now test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e)
[0m14:49:56.351842 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e
[0m14:49:56.385602 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e"
[0m14:49:56.388664 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e
[0m14:49:56.381733 [info ] [Thread-2 (]: 14 of 71 PASS not_null_fct_marketing_spend_spend_try ........................... [[32mPASS[0m in 1.95s]
[0m14:49:56.402550 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d
[0m14:49:56.401308 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e"
[0m14:49:56.403859 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def
[0m14:49:56.405043 [debug] [Thread-1 (]: On test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select net_revenue
from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_order`
where net_revenue is null



  
  
      
    ) dbt_internal_test
[0m14:49:56.407437 [info ] [Thread-2 (]: 18 of 71 START test not_null_fct_order_order_id ................................ [RUN]
[0m14:49:56.409424 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:49:56.410269 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d, now test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def)
[0m14:49:56.467756 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def
[0m14:49:56.480574 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def"
[0m14:49:56.487886 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def
[0m14:49:56.492028 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def"
[0m14:49:56.494154 [debug] [Thread-2 (]: On test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select order_id
from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_order`
where order_id is null



  
  
      
    ) dbt_internal_test
[0m14:49:56.495534 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:49:56.541174 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:4c9753b7-769f-4c19-98dd-bec52ddb7fbc&page=queryresults
[0m14:49:57.103169 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:77bbc6cc-eaea-45d6-b289-c7abfa0003da&page=queryresults
[0m14:49:57.512299 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:bdb36aea-90eb-4f8a-afd1-39f9fe52bf0c&page=queryresults
[0m14:49:57.516454 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:162af25e-2d51-46f9-8e3a-9e67f0c79d7c&page=queryresults
[0m14:49:57.586879 [info ] [Thread-4 (]: 15 of 71 PASS not_null_fct_order_customer_id ................................... [[32mPASS[0m in 2.25s]
[0m14:49:57.590437 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff
[0m14:49:57.592020 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.not_null_fct_shipment_order_id.8b34a6bb78
[0m14:49:57.593169 [info ] [Thread-4 (]: 19 of 71 START test not_null_fct_shipment_order_id ............................. [RUN]
[0m14:49:57.595420 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff, now test.data_pipeline_project.not_null_fct_shipment_order_id.8b34a6bb78)
[0m14:49:57.596473 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.not_null_fct_shipment_order_id.8b34a6bb78
[0m14:49:57.603557 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_fct_shipment_order_id.8b34a6bb78"
[0m14:49:57.608941 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.not_null_fct_shipment_order_id.8b34a6bb78
[0m14:49:57.614029 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_fct_shipment_order_id.8b34a6bb78"
[0m14:49:57.615110 [debug] [Thread-4 (]: On test.data_pipeline_project.not_null_fct_shipment_order_id.8b34a6bb78: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_fct_shipment_order_id.8b34a6bb78"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select order_id
from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_shipment`
where order_id is null



  
  
      
    ) dbt_internal_test
[0m14:49:57.615721 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:49:58.102285 [info ] [Thread-3 (]: 16 of 71 PASS not_null_fct_order_discount_total ................................ [[32mPASS[0m in 2.34s]
[0m14:49:58.106156 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea
[0m14:49:58.108383 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d
[0m14:49:58.110125 [info ] [Thread-3 (]: 20 of 71 START test not_null_fct_shipment_shipment_id .......................... [RUN]
[0m14:49:58.112127 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea, now test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d)
[0m14:49:58.113465 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d
[0m14:49:58.122453 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d"
[0m14:49:58.124625 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d
[0m14:49:58.127803 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d"
[0m14:49:58.129190 [debug] [Thread-3 (]: On test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select shipment_id
from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_shipment`
where shipment_id is null



  
  
      
    ) dbt_internal_test
[0m14:49:58.131378 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:49:58.477505 [info ] [Thread-1 (]: 17 of 71 PASS not_null_fct_order_net_revenue ................................... [[32mPASS[0m in 2.13s]
[0m14:49:58.495336 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e
[0m14:49:58.499752 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b
[0m14:49:58.507199 [info ] [Thread-1 (]: 21 of 71 START test not_null_int_shipments_decomposed_delivered_at ............. [RUN]
[0m14:49:58.508445 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e, now test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b)
[0m14:49:58.509180 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b
[0m14:49:58.506632 [info ] [Thread-2 (]: 18 of 71 PASS not_null_fct_order_order_id ...................................... [[32mPASS[0m in 2.10s]
[0m14:49:58.517649 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def
[0m14:49:58.519733 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b"
[0m14:49:58.520677 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31
[0m14:49:58.524388 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b
[0m14:49:58.530299 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b"
[0m14:49:58.525334 [info ] [Thread-2 (]: 22 of 71 START test not_null_int_shipments_decomposed_latest_status ............ [RUN]
[0m14:49:58.533155 [debug] [Thread-1 (]: On test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select delivered_at
from `data-pipeline-project-474812`.`analytics_staging_staging`.`int_shipments_decomposed`
where delivered_at is null



  
  
      
    ) dbt_internal_test
[0m14:49:58.535005 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:49:58.534105 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def, now test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31)
[0m14:49:58.569036 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31
[0m14:49:58.575533 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31"
[0m14:49:58.581977 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31
[0m14:49:58.584826 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31"
[0m14:49:58.586231 [debug] [Thread-2 (]: On test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select latest_status
from `data-pipeline-project-474812`.`analytics_staging_staging`.`int_shipments_decomposed`
where latest_status is null



  
  
      
    ) dbt_internal_test
[0m14:49:58.587263 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:49:59.041761 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:dcac94eb-d1b7-4223-b2fb-ed1d4bdb8cfe&page=queryresults
[0m14:49:59.310197 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:5dc2991b-cdcb-435c-bb72-77837ca861dd&page=queryresults
[0m14:50:00.097527 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:b9dcefca-999a-4903-b915-8dd78e759d7d&page=queryresults
[0m14:50:00.101630 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:108891bf-068c-454d-af57-df06c909f423&page=queryresults
[0m14:50:00.334356 [info ] [Thread-4 (]: 19 of 71 PASS not_null_fct_shipment_order_id ................................... [[32mPASS[0m in 2.74s]
[0m14:50:00.335984 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.not_null_fct_shipment_order_id.8b34a6bb78
[0m14:50:00.336528 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b
[0m14:50:00.337220 [info ] [Thread-4 (]: 23 of 71 START test not_null_mart_acquisition_efficiency_date .................. [RUN]
[0m14:50:00.338040 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_fct_shipment_order_id.8b34a6bb78, now test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b)
[0m14:50:00.338548 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b
[0m14:50:00.343646 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b"
[0m14:50:00.344671 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b
[0m14:50:00.348032 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b"
[0m14:50:00.348891 [debug] [Thread-4 (]: On test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select date
from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_acquisition_efficiency`
where date is null



  
  
      
    ) dbt_internal_test
[0m14:50:00.349554 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:50:00.753052 [info ] [Thread-3 (]: 20 of 71 PASS not_null_fct_shipment_shipment_id ................................ [[32mPASS[0m in 2.64s]
[0m14:50:00.755864 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d
[0m14:50:00.757806 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23
[0m14:50:00.759343 [info ] [Thread-3 (]: 24 of 71 START test not_null_mart_revenue_daily_date ........................... [RUN]
[0m14:50:00.761419 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d, now test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23)
[0m14:50:00.762232 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23
[0m14:50:00.778169 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23"
[0m14:50:00.779588 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23
[0m14:50:00.782540 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23"
[0m14:50:00.784631 [debug] [Thread-3 (]: On test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select date
from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_revenue_daily`
where date is null



  
  
      
    ) dbt_internal_test
[0m14:50:00.785382 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:50:01.080064 [info ] [Thread-2 (]: 22 of 71 PASS not_null_int_shipments_decomposed_latest_status .................. [[32mPASS[0m in 2.54s]
[0m14:50:01.097670 [warn ] [Thread-1 (]: 21 of 71 WARN 303 not_null_int_shipments_decomposed_delivered_at ............... [[33mWARN 303[0m in 2.58s]
[0m14:50:01.104992 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31
[0m14:50:01.124189 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322
[0m14:50:01.113718 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b
[0m14:50:01.129122 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501
[0m14:50:01.127175 [info ] [Thread-2 (]: 25 of 71 START test not_null_mart_subscription_daily_date ...................... [RUN]
[0m14:50:01.132008 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31, now test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322)
[0m14:50:01.131007 [info ] [Thread-1 (]: 26 of 71 START test not_null_stg_addresses_address_id .......................... [RUN]
[0m14:50:01.134411 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322
[0m14:50:01.135570 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b, now test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501)
[0m14:50:01.143888 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501
[0m14:50:01.148413 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322"
[0m14:50:01.155111 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322
[0m14:50:01.153657 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501"
[0m14:50:01.161812 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501
[0m14:50:01.160953 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322"
[0m14:50:01.164137 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501"
[0m14:50:01.166088 [debug] [Thread-2 (]: On test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select date
from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily`
where date is null



  
  
      
    ) dbt_internal_test
[0m14:50:01.166997 [debug] [Thread-1 (]: On test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select address_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_addresses`
where address_id is null



  
  
      
    ) dbt_internal_test
[0m14:50:01.169804 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:50:01.169032 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:50:01.541304 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:80ddbd9e-4400-46dd-ae70-c0198ec54f02&page=queryresults
[0m14:50:02.034923 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:d1fdfba0-31f1-4e8a-aa17-fb04d6e09049&page=queryresults
[0m14:50:02.446540 [info ] [Thread-4 (]: 23 of 71 PASS not_null_mart_acquisition_efficiency_date ........................ [[32mPASS[0m in 2.11s]
[0m14:50:02.447990 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b
[0m14:50:02.449024 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3
[0m14:50:02.449725 [info ] [Thread-4 (]: 27 of 71 START test not_null_stg_addresses_user_id ............................. [RUN]
[0m14:50:02.450792 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b, now test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3)
[0m14:50:02.451288 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3
[0m14:50:02.456955 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3"
[0m14:50:02.458382 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3
[0m14:50:02.469905 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3"
[0m14:50:02.471235 [debug] [Thread-4 (]: On test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_addresses`
where user_id is null



  
  
      
    ) dbt_internal_test
[0m14:50:02.471718 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:50:02.510096 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:7a6dc77d-988d-4f50-a8c6-08620260a36d&page=queryresults
[0m14:50:02.825137 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:6ddd55dd-c64e-4a12-a7a3-d8495b98f19e&page=queryresults
[0m14:50:02.967014 [info ] [Thread-3 (]: 24 of 71 PASS not_null_mart_revenue_daily_date ................................. [[32mPASS[0m in 2.21s]
[0m14:50:02.968939 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23
[0m14:50:02.969511 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2
[0m14:50:02.970232 [info ] [Thread-3 (]: 28 of 71 START test not_null_stg_countries_country_id .......................... [RUN]
[0m14:50:02.971379 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23, now test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2)
[0m14:50:02.971816 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2
[0m14:50:02.975830 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2"
[0m14:50:02.977485 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2
[0m14:50:02.981107 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2"
[0m14:50:02.982918 [debug] [Thread-3 (]: On test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select country_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_countries`
where country_id is null



  
  
      
    ) dbt_internal_test
[0m14:50:02.987544 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:50:03.390994 [info ] [Thread-2 (]: 25 of 71 PASS not_null_mart_subscription_daily_date ............................ [[32mPASS[0m in 2.26s]
[0m14:50:03.391928 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322
[0m14:50:03.392454 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64
[0m14:50:03.393833 [info ] [Thread-2 (]: 29 of 71 START test not_null_stg_orders_order_id ............................... [RUN]
[0m14:50:03.394872 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322, now test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64)
[0m14:50:03.395438 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64
[0m14:50:03.404090 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64"
[0m14:50:03.413119 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64
[0m14:50:03.416463 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64"
[0m14:50:03.419143 [debug] [Thread-2 (]: On test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select order_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_orders`
where order_id is null



  
  
      
    ) dbt_internal_test
[0m14:50:03.419995 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:50:03.659400 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:1057035b-1a1d-4f54-ad16-cdd42479ee65&page=queryresults
[0m14:50:03.725687 [info ] [Thread-1 (]: 26 of 71 PASS not_null_stg_addresses_address_id ................................ [[32mPASS[0m in 2.59s]
[0m14:50:03.729173 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501
[0m14:50:03.730759 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209
[0m14:50:03.732506 [info ] [Thread-1 (]: 30 of 71 START test not_null_stg_orders_user_id ................................ [RUN]
[0m14:50:03.735039 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501, now test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209)
[0m14:50:03.737691 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209
[0m14:50:03.749619 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209"
[0m14:50:03.750451 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209
[0m14:50:03.755977 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209"
[0m14:50:03.756794 [debug] [Thread-1 (]: On test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_orders`
where user_id is null



  
  
      
    ) dbt_internal_test
[0m14:50:03.757581 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:50:04.265754 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:0067890c-be60-452a-a805-c53f88b2d376&page=queryresults
[0m14:50:04.671337 [info ] [Thread-4 (]: 27 of 71 PASS not_null_stg_addresses_user_id ................................... [[32mPASS[0m in 2.22s]
[0m14:50:04.675354 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3
[0m14:50:04.678020 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380
[0m14:50:04.679358 [info ] [Thread-4 (]: 31 of 71 START test not_null_stg_shipments_shipment_id ......................... [RUN]
[0m14:50:04.681036 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3, now test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380)
[0m14:50:04.682398 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380
[0m14:50:04.692975 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380"
[0m14:50:04.693949 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380
[0m14:50:04.697485 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380"
[0m14:50:04.699960 [debug] [Thread-4 (]: On test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select shipment_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_shipments`
where shipment_id is null



  
  
      
    ) dbt_internal_test
[0m14:50:04.700497 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:50:04.699147 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:601c32a1-e94c-4521-b235-77736d6202b9&page=queryresults
[0m14:50:05.076324 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:f40485f9-bd8d-4e87-be81-d80c6f29f1e5&page=queryresults
[0m14:50:05.117585 [info ] [Thread-3 (]: 28 of 71 PASS not_null_stg_countries_country_id ................................ [[32mPASS[0m in 2.14s]
[0m14:50:05.123591 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2
[0m14:50:05.125457 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852
[0m14:50:05.127251 [info ] [Thread-3 (]: 32 of 71 START test not_null_stg_shipments_user_id ............................. [RUN]
[0m14:50:05.129431 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2, now test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852)
[0m14:50:05.130490 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852
[0m14:50:05.144390 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852"
[0m14:50:05.145532 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852
[0m14:50:05.150443 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852"
[0m14:50:05.151964 [debug] [Thread-3 (]: On test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_shipments`
where user_id is null



  
  
      
    ) dbt_internal_test
[0m14:50:05.153116 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:50:05.606289 [info ] [Thread-2 (]: 29 of 71 PASS not_null_stg_orders_order_id ..................................... [[32mPASS[0m in 2.21s]
[0m14:50:05.607987 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64
[0m14:50:05.609367 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8
[0m14:50:05.610205 [info ] [Thread-2 (]: 33 of 71 START test not_null_stg_subscriptions_start_date ...................... [RUN]
[0m14:50:05.611312 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64, now test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8)
[0m14:50:05.612093 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8
[0m14:50:05.620607 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8"
[0m14:50:05.626384 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8
[0m14:50:05.629577 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8"
[0m14:50:05.630638 [debug] [Thread-2 (]: On test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select start_date
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_subscriptions`
where start_date is null



  
  
      
    ) dbt_internal_test
[0m14:50:05.631496 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:50:05.978504 [info ] [Thread-1 (]: 30 of 71 PASS not_null_stg_orders_user_id ...................................... [[32mPASS[0m in 2.24s]
[0m14:50:05.981500 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209
[0m14:50:05.983237 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m14:50:05.985280 [info ] [Thread-1 (]: 34 of 71 START test not_null_stg_subscriptions_subscription_id ................. [RUN]
[0m14:50:05.987346 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209, now test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91)
[0m14:50:05.990032 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m14:50:06.002776 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91"
[0m14:50:06.006407 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m14:50:06.010137 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91"
[0m14:50:06.011108 [debug] [Thread-1 (]: On test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select subscription_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_subscriptions`
where subscription_id is null



  
  
      
    ) dbt_internal_test
[0m14:50:06.011992 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:50:06.123395 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:2f68e5a5-ddef-431d-a368-a587aee441c7&page=queryresults
[0m14:50:06.812320 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:592aaa1f-a6b9-493d-88c0-fc3734428ed2&page=queryresults
[0m14:50:07.030490 [info ] [Thread-4 (]: 31 of 71 PASS not_null_stg_shipments_shipment_id ............................... [[32mPASS[0m in 2.35s]
[0m14:50:07.031942 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380
[0m14:50:07.032643 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2
[0m14:50:07.033425 [info ] [Thread-4 (]: 35 of 71 START test not_null_stg_subscriptions_user_id ......................... [RUN]
[0m14:50:07.034686 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380, now test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2)
[0m14:50:07.035241 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2
[0m14:50:07.040671 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2"
[0m14:50:07.042333 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2
[0m14:50:07.047836 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2"
[0m14:50:07.049098 [debug] [Thread-4 (]: On test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_subscriptions`
where user_id is null



  
  
      
    ) dbt_internal_test
[0m14:50:07.049962 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:50:07.086005 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:5c8a1962-c0f4-4eb8-964c-555f1977cb7e&page=queryresults
[0m14:50:07.319190 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:3d52ec0f-721c-4f89-a3e8-2463e1b19b1e&page=queryresults
[0m14:50:07.702666 [info ] [Thread-3 (]: 32 of 71 PASS not_null_stg_shipments_user_id ................................... [[32mPASS[0m in 2.57s]
[0m14:50:07.703628 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852
[0m14:50:07.704533 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25
[0m14:50:07.705202 [info ] [Thread-3 (]: 36 of 71 START test not_null_stg_users_created_at .............................. [RUN]
[0m14:50:07.705999 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852, now test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25)
[0m14:50:07.708301 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25
[0m14:50:07.714239 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25"
[0m14:50:07.715276 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25
[0m14:50:07.719762 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25"
[0m14:50:07.720705 [debug] [Thread-3 (]: On test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select created_at
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
where created_at is null



  
  
      
    ) dbt_internal_test
[0m14:50:07.721234 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:50:07.971609 [info ] [Thread-2 (]: 33 of 71 PASS not_null_stg_subscriptions_start_date ............................ [[32mPASS[0m in 2.36s]
[0m14:50:07.973641 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8
[0m14:50:07.974811 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77
[0m14:50:07.976325 [info ] [Thread-2 (]: 37 of 71 START test not_null_stg_users_user_id ................................. [RUN]
[0m14:50:07.977940 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8, now test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77)
[0m14:50:07.978678 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77
[0m14:50:07.988593 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77"
[0m14:50:07.993480 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77
[0m14:50:07.996431 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77"
[0m14:50:07.997158 [debug] [Thread-2 (]: On test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
where user_id is null



  
  
      
    ) dbt_internal_test
[0m14:50:07.997696 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:50:08.278546 [info ] [Thread-1 (]: 34 of 71 PASS not_null_stg_subscriptions_subscription_id ....................... [[32mPASS[0m in 2.29s]
[0m14:50:08.286784 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m14:50:08.292104 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29
[0m14:50:08.299597 [info ] [Thread-1 (]: 38 of 71 START test relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_  [RUN]
[0m14:50:08.305392 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91, now test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29)
[0m14:50:08.308253 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29
[0m14:50:08.329135 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29"
[0m14:50:08.330154 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29
[0m14:50:08.335208 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29"
[0m14:50:08.337835 [debug] [Thread-1 (]: On test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select customer_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_subscription`
    where customer_id is not null
),

parent as (
    select customer_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_customer`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m14:50:08.338520 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:50:08.336905 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:0063a667-47b1-4a0b-a177-3f16b2798c51&page=queryresults
[0m14:50:09.238416 [info ] [Thread-4 (]: 35 of 71 PASS not_null_stg_subscriptions_user_id ............................... [[32mPASS[0m in 2.20s]
[0m14:50:09.241537 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2
[0m14:50:09.242513 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85
[0m14:50:09.243516 [info ] [Thread-4 (]: 39 of 71 START test relationships_fct_order_customer_id__customer_id__ref_dim_customer_  [RUN]
[0m14:50:09.244280 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2, now test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85)
[0m14:50:09.244796 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85
[0m14:50:09.252355 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85"
[0m14:50:09.253164 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85
[0m14:50:09.256315 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85"
[0m14:50:09.257542 [debug] [Thread-4 (]: On test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select customer_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_order`
    where customer_id is not null
),

parent as (
    select customer_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_customer`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m14:50:09.258425 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:50:09.289244 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:4cf263ae-5ad6-4992-9191-e8c2a3f03436&page=queryresults
[0m14:50:09.324437 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:26fc3f7e-133c-4978-9c22-ae295c00dcaf&page=queryresults
[0m14:50:09.503105 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:cd185b32-c45c-4faf-8de2-5a52e0d199e8&page=queryresults
[0m14:50:10.159579 [info ] [Thread-2 (]: 37 of 71 PASS not_null_stg_users_user_id ....................................... [[32mPASS[0m in 2.18s]
[0m14:50:10.160788 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77
[0m14:50:10.161945 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.relationships_fct_shipment_order_id__order_id__ref_fct_order_.6096f95e39
[0m14:50:10.163425 [info ] [Thread-2 (]: 40 of 71 START test relationships_fct_shipment_order_id__order_id__ref_fct_order_  [RUN]
[0m14:50:10.164415 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77, now test.data_pipeline_project.relationships_fct_shipment_order_id__order_id__ref_fct_order_.6096f95e39)
[0m14:50:10.164949 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.relationships_fct_shipment_order_id__order_id__ref_fct_order_.6096f95e39
[0m14:50:10.175353 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_fct_shipment_order_id__order_id__ref_fct_order_.6096f95e39"
[0m14:50:10.176492 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.relationships_fct_shipment_order_id__order_id__ref_fct_order_.6096f95e39
[0m14:50:10.182174 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_fct_shipment_order_id__order_id__ref_fct_order_.6096f95e39"
[0m14:50:10.182949 [debug] [Thread-2 (]: On test.data_pipeline_project.relationships_fct_shipment_order_id__order_id__ref_fct_order_.6096f95e39: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_fct_shipment_order_id__order_id__ref_fct_order_.6096f95e39"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select order_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_shipment`
    where order_id is not null
),

parent as (
    select order_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_order`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m14:50:10.183510 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:50:10.241605 [info ] [Thread-3 (]: 36 of 71 PASS not_null_stg_users_created_at .................................... [[32mPASS[0m in 2.53s]
[0m14:50:10.244919 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25
[0m14:50:10.245470 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781
[0m14:50:10.246941 [info ] [Thread-3 (]: 41 of 71 START test relationships_stg_addresses_user_id__user_id__ref_stg_users_  [RUN]
[0m14:50:10.248239 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25, now test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781)
[0m14:50:10.249358 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781
[0m14:50:10.272779 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781"
[0m14:50:10.279217 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781
[0m14:50:10.287424 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781"
[0m14:50:10.288449 [debug] [Thread-3 (]: On test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select user_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_addresses`
    where user_id is not null
),

parent as (
    select user_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m14:50:10.289294 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:50:10.360587 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:efb3aaeb-38b1-4a2b-9bcd-e82dec32a830&page=queryresults
[0m14:50:10.453499 [info ] [Thread-1 (]: 38 of 71 PASS relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_  [[32mPASS[0m in 2.15s]
[0m14:50:10.454591 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29
[0m14:50:10.455362 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737
[0m14:50:10.456989 [info ] [Thread-1 (]: 42 of 71 START test relationships_stg_orders_user_id__user_id__ref_stg_users_ .. [RUN]
[0m14:50:10.458039 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29, now test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737)
[0m14:50:10.458575 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737
[0m14:50:10.466146 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737"
[0m14:50:10.467018 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737
[0m14:50:10.471726 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737"
[0m14:50:10.478718 [debug] [Thread-1 (]: On test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select user_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_orders`
    where user_id is not null
),

parent as (
    select user_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m14:50:10.482406 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:50:11.258132 [info ] [Thread-4 (]: 39 of 71 PASS relationships_fct_order_customer_id__customer_id__ref_dim_customer_  [[32mPASS[0m in 2.01s]
[0m14:50:11.263292 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85
[0m14:50:11.265157 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8
[0m14:50:11.266521 [info ] [Thread-4 (]: 43 of 71 START test relationships_stg_shipments_user_id__user_id__ref_stg_users_  [RUN]
[0m14:50:11.270046 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85, now test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8)
[0m14:50:11.271137 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8
[0m14:50:11.285382 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8"
[0m14:50:11.286764 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8
[0m14:50:11.290497 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8"
[0m14:50:11.291395 [debug] [Thread-4 (]: On test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select user_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_shipments`
    where user_id is not null
),

parent as (
    select user_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m14:50:11.292611 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:50:11.464167 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:68c387bd-4044-40cd-9c72-2ebb49335479&page=queryresults
[0m14:50:11.749800 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:9dbc4ad3-6f2d-4fef-a2ad-9e9706a50e1d&page=queryresults
[0m14:50:11.870952 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:e8c933fa-7b0f-47c9-bd7a-e80a152091a5&page=queryresults
[0m14:50:12.497711 [error] [Thread-2 (]: 40 of 71 FAIL 476 relationships_fct_shipment_order_id__order_id__ref_fct_order_  [[31mFAIL 476[0m in 2.33s]
[0m14:50:12.501385 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.relationships_fct_shipment_order_id__order_id__ref_fct_order_.6096f95e39
[0m14:50:12.503163 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m14:50:12.504534 [info ] [Thread-2 (]: 44 of 71 START test relationships_stg_subscriptions_user_id__user_id__ref_stg_users_  [RUN]
[0m14:50:12.506531 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_fct_shipment_order_id__order_id__ref_fct_order_.6096f95e39, now test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e)
[0m14:50:12.509423 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m14:50:12.518430 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e"
[0m14:50:12.519335 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m14:50:12.523673 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e"
[0m14:50:12.527706 [debug] [Thread-2 (]: On test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select user_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_subscriptions`
    where user_id is not null
),

parent as (
    select user_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m14:50:12.528587 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:50:12.561237 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:13bfad55-5772-4e55-887d-bead8d3ede72&page=queryresults
[0m14:50:12.661683 [info ] [Thread-3 (]: 41 of 71 PASS relationships_stg_addresses_user_id__user_id__ref_stg_users_ ..... [[32mPASS[0m in 2.41s]
[0m14:50:12.663033 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781
[0m14:50:12.663901 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6
[0m14:50:12.664464 [info ] [Thread-3 (]: 45 of 71 START test source_not_null_raw_data_addresses__id ..................... [RUN]
[0m14:50:12.665286 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781, now test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6)
[0m14:50:12.665952 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6
[0m14:50:12.671184 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6"
[0m14:50:12.673716 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6
[0m14:50:12.688731 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6"
[0m14:50:12.690044 [debug] [Thread-3 (]: On test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`addresses`
where _id is null



  
  
      
    ) dbt_internal_test
[0m14:50:12.691392 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:50:12.743650 [info ] [Thread-1 (]: 42 of 71 PASS relationships_stg_orders_user_id__user_id__ref_stg_users_ ........ [[32mPASS[0m in 2.29s]
[0m14:50:12.746067 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737
[0m14:50:12.749714 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc
[0m14:50:12.750544 [info ] [Thread-1 (]: 46 of 71 START test source_not_null_raw_data_cities__id ........................ [RUN]
[0m14:50:12.751694 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737, now test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc)
[0m14:50:12.754104 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc
[0m14:50:12.771286 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc"
[0m14:50:12.772929 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc
[0m14:50:12.778323 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc"
[0m14:50:12.779823 [debug] [Thread-1 (]: On test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`cities`
where _id is null



  
  
      
    ) dbt_internal_test
[0m14:50:12.781572 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:50:13.432896 [info ] [Thread-4 (]: 43 of 71 PASS relationships_stg_shipments_user_id__user_id__ref_stg_users_ ..... [[32mPASS[0m in 2.16s]
[0m14:50:13.434709 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8
[0m14:50:13.437215 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd
[0m14:50:13.438663 [info ] [Thread-4 (]: 47 of 71 START test source_not_null_raw_data_countries__id ..................... [RUN]
[0m14:50:13.439767 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8, now test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd)
[0m14:50:13.441222 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd
[0m14:50:13.448467 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd"
[0m14:50:13.453047 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd
[0m14:50:13.456094 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd"
[0m14:50:13.457003 [debug] [Thread-4 (]: On test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`countries`
where _id is null



  
  
      
    ) dbt_internal_test
[0m14:50:13.457810 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:50:13.836159 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:66b606c0-27c9-458e-b2ab-fe92ead22cf5&page=queryresults
[0m14:50:13.844499 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:2f825ccd-882b-47f5-b15b-b26b9dbbc539&page=queryresults
[0m14:50:14.101132 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:64946919-25d6-485f-8d34-7b3219000786&page=queryresults
[0m14:50:14.529715 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:171b8dd1-9697-4c89-8f55-06bba485e7a3&page=queryresults
[0m14:50:14.710465 [info ] [Thread-3 (]: 45 of 71 PASS source_not_null_raw_data_addresses__id ........................... [[32mPASS[0m in 2.04s]
[0m14:50:14.713022 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6
[0m14:50:14.714471 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be
[0m14:50:14.716003 [info ] [Thread-3 (]: 48 of 71 START test source_not_null_raw_data_neighborhoods__id ................. [RUN]
[0m14:50:14.718050 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6, now test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be)
[0m14:50:14.719305 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be
[0m14:50:14.727184 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be"
[0m14:50:14.729094 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be
[0m14:50:14.732596 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be"
[0m14:50:14.744641 [debug] [Thread-3 (]: On test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`neighborhoods`
where _id is null



  
  
      
    ) dbt_internal_test
[0m14:50:14.742624 [info ] [Thread-2 (]: 44 of 71 PASS relationships_stg_subscriptions_user_id__user_id__ref_stg_users_ . [[32mPASS[0m in 2.24s]
[0m14:50:14.747576 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:50:14.749180 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m14:50:14.805248 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878
[0m14:50:14.807510 [info ] [Thread-2 (]: 49 of 71 START test source_not_null_raw_data_orders__id ........................ [RUN]
[0m14:50:14.817217 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e, now test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878)
[0m14:50:14.822232 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878
[0m14:50:14.829113 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878"
[0m14:50:14.830022 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878
[0m14:50:14.838882 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878"
[0m14:50:14.839939 [debug] [Thread-2 (]: On test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`orders`
where _id is null



  
  
      
    ) dbt_internal_test
[0m14:50:14.840403 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:50:15.061417 [info ] [Thread-1 (]: 46 of 71 PASS source_not_null_raw_data_cities__id .............................. [[32mPASS[0m in 2.31s]
[0m14:50:15.063340 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc
[0m14:50:15.064653 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f
[0m14:50:15.066055 [info ] [Thread-1 (]: 50 of 71 START test source_not_null_raw_data_orders__user ...................... [RUN]
[0m14:50:15.067370 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc, now test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f)
[0m14:50:15.068657 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f
[0m14:50:15.081194 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f"
[0m14:50:15.082305 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f
[0m14:50:15.086370 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f"
[0m14:50:15.088011 [debug] [Thread-1 (]: On test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _user
from `data-pipeline-project-474812`.`raw_data`.`orders`
where _user is null



  
  
      
    ) dbt_internal_test
[0m14:50:15.088981 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:50:15.747564 [info ] [Thread-4 (]: 47 of 71 PASS source_not_null_raw_data_countries__id ........................... [[32mPASS[0m in 2.31s]
[0m14:50:15.749427 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd
[0m14:50:15.750332 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2
[0m14:50:15.751182 [info ] [Thread-4 (]: 51 of 71 START test source_not_null_raw_data_shipments__id ..................... [RUN]
[0m14:50:15.751940 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd, now test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2)
[0m14:50:15.753028 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2
[0m14:50:15.761452 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2"
[0m14:50:15.765041 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2
[0m14:50:15.768073 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2"
[0m14:50:15.768931 [debug] [Thread-4 (]: On test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`shipments`
where _id is null



  
  
      
    ) dbt_internal_test
[0m14:50:15.769745 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:50:15.958864 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:8648d58b-2827-4cfa-b498-112a4472ca1e&page=queryresults
[0m14:50:16.574501 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:1fb89160-6703-4477-b4eb-1cd1b9c43d57&page=queryresults
[0m14:50:16.647199 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:6ab31fe7-b62f-4754-8c7b-0af5215f6eef&page=queryresults
[0m14:50:16.863444 [info ] [Thread-3 (]: 48 of 71 PASS source_not_null_raw_data_neighborhoods__id ....................... [[32mPASS[0m in 2.14s]
[0m14:50:16.867294 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be
[0m14:50:16.868563 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1
[0m14:50:16.869907 [info ] [Thread-3 (]: 52 of 71 START test source_not_null_raw_data_shipments__order .................. [RUN]
[0m14:50:16.871382 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be, now test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1)
[0m14:50:16.872264 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1
[0m14:50:16.883039 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1"
[0m14:50:16.885055 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1
[0m14:50:16.895952 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1"
[0m14:50:16.896832 [debug] [Thread-3 (]: On test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _order
from `data-pipeline-project-474812`.`raw_data`.`shipments`
where _order is null



  
  
      
    ) dbt_internal_test
[0m14:50:16.897392 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:50:16.934967 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:56711d04-c0bb-4515-821a-dfceae98360a&page=queryresults
[0m14:50:17.431058 [info ] [Thread-1 (]: 50 of 71 PASS source_not_null_raw_data_orders__user ............................ [[32mPASS[0m in 2.36s]
[0m14:50:17.432242 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f
[0m14:50:17.432940 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98
[0m14:50:17.433543 [info ] [Thread-1 (]: 53 of 71 START test source_not_null_raw_data_shipments__user ................... [RUN]
[0m14:50:17.434379 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f, now test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98)
[0m14:50:17.434986 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98
[0m14:50:17.440517 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98"
[0m14:50:17.441649 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98
[0m14:50:17.450231 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98"
[0m14:50:17.451376 [debug] [Thread-1 (]: On test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _user
from `data-pipeline-project-474812`.`raw_data`.`shipments`
where _user is null



  
  
      
    ) dbt_internal_test
[0m14:50:17.452433 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:50:17.507112 [info ] [Thread-2 (]: 49 of 71 PASS source_not_null_raw_data_orders__id .............................. [[32mPASS[0m in 2.69s]
[0m14:50:17.508557 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878
[0m14:50:17.509146 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86
[0m14:50:17.509698 [info ] [Thread-2 (]: 54 of 71 START test source_not_null_raw_data_states__id ........................ [RUN]
[0m14:50:17.511037 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878, now test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86)
[0m14:50:17.512038 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86
[0m14:50:17.803026 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86"
[0m14:50:17.808228 [info ] [Thread-4 (]: 51 of 71 PASS source_not_null_raw_data_shipments__id ........................... [[32mPASS[0m in 2.06s]
[0m14:50:17.810967 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86
[0m14:50:17.812281 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2
[0m14:50:17.815845 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86"
[0m14:50:17.817286 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a
[0m14:50:17.819430 [debug] [Thread-2 (]: On test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`states`
where _id is null



  
  
      
    ) dbt_internal_test
[0m14:50:17.820275 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:50:17.818370 [info ] [Thread-4 (]: 55 of 71 START test source_not_null_raw_data_subscriptions__id ................. [RUN]
[0m14:50:17.875019 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2, now test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a)
[0m14:50:17.875648 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a
[0m14:50:17.886233 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a"
[0m14:50:17.887618 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a
[0m14:50:17.890818 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a"
[0m14:50:17.891982 [debug] [Thread-4 (]: On test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`subscriptions`
where _id is null



  
  
      
    ) dbt_internal_test
[0m14:50:17.892470 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:50:18.124352 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:fb736e7f-77a9-426c-98cf-67121b99cc5e&page=queryresults
[0m14:50:18.849668 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:e426509d-eac2-4e0a-9958-da544bcaa3e3&page=queryresults
[0m14:50:18.852627 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:2ab974d4-f3c3-4a77-99fd-609892ae2780&page=queryresults
[0m14:50:18.934886 [info ] [Thread-3 (]: 52 of 71 PASS source_not_null_raw_data_shipments__order ........................ [[32mPASS[0m in 2.06s]
[0m14:50:18.936679 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1
[0m14:50:18.937903 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6
[0m14:50:18.938593 [info ] [Thread-3 (]: 56 of 71 START test source_not_null_raw_data_subscriptions__user ............... [RUN]
[0m14:50:18.939646 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1, now test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6)
[0m14:50:18.940500 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6
[0m14:50:18.947540 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6"
[0m14:50:18.952095 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6
[0m14:50:18.957924 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6"
[0m14:50:18.959126 [debug] [Thread-3 (]: On test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _user
from `data-pipeline-project-474812`.`raw_data`.`subscriptions`
where _user is null



  
  
      
    ) dbt_internal_test
[0m14:50:18.959706 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:50:19.027553 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:c15e6363-ac5b-4e06-9c88-49a9254f6262&page=queryresults
[0m14:50:19.691913 [info ] [Thread-1 (]: 53 of 71 PASS source_not_null_raw_data_shipments__user ......................... [[32mPASS[0m in 2.26s]
[0m14:50:19.693194 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98
[0m14:50:19.693870 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b
[0m14:50:19.694553 [info ] [Thread-1 (]: 57 of 71 START test source_not_null_raw_data_users__id ......................... [RUN]
[0m14:50:19.695647 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98, now test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b)
[0m14:50:19.696271 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b
[0m14:50:19.702180 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b"
[0m14:50:19.703141 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b
[0m14:50:19.710226 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b"
[0m14:50:19.711238 [debug] [Thread-1 (]: On test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`users`
where _id is null



  
  
      
    ) dbt_internal_test
[0m14:50:19.712178 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:50:19.751366 [info ] [Thread-2 (]: 54 of 71 PASS source_not_null_raw_data_states__id .............................. [[32mPASS[0m in 2.24s]
[0m14:50:19.752227 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86
[0m14:50:19.752831 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1
[0m14:50:19.754574 [info ] [Thread-2 (]: 58 of 71 START test unique_dim_customer_customer_id ............................ [RUN]
[0m14:50:19.755086 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86, now test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1)
[0m14:50:19.757540 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1
[0m14:50:19.768758 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1"
[0m14:50:19.769526 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1
[0m14:50:19.772487 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1"
[0m14:50:19.773212 [debug] [Thread-2 (]: On test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select customer_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_customer`
  where customer_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:50:19.773739 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:50:19.876942 [info ] [Thread-4 (]: 55 of 71 PASS source_not_null_raw_data_subscriptions__id ....................... [[32mPASS[0m in 2.00s]
[0m14:50:19.879738 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a
[0m14:50:19.880906 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469
[0m14:50:19.882042 [info ] [Thread-4 (]: 59 of 71 START test unique_dim_subscription_subscription_id .................... [RUN]
[0m14:50:19.883229 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a, now test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469)
[0m14:50:19.885159 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469
[0m14:50:19.891057 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469"
[0m14:50:19.892335 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469
[0m14:50:19.896301 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469"
[0m14:50:19.897380 [debug] [Thread-4 (]: On test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select subscription_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_subscription`
  where subscription_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:50:19.898107 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:50:19.966748 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:0a80d36f-340b-4535-b024-e7c769a65a6d&page=queryresults
[0m14:50:20.801860 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:45bb7bb6-939a-4cd5-8959-f64a43eb3548&page=queryresults
[0m14:50:20.814668 [info ] [Thread-3 (]: 56 of 71 PASS source_not_null_raw_data_subscriptions__user ..................... [[32mPASS[0m in 1.87s]
[0m14:50:20.817108 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6
[0m14:50:20.818575 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.unique_fct_marketing_spend_date.e93d255723
[0m14:50:20.820107 [info ] [Thread-3 (]: 60 of 71 START test unique_fct_marketing_spend_date ............................ [RUN]
[0m14:50:20.822325 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6, now test.data_pipeline_project.unique_fct_marketing_spend_date.e93d255723)
[0m14:50:20.823526 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.unique_fct_marketing_spend_date.e93d255723
[0m14:50:20.832471 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.unique_fct_marketing_spend_date.e93d255723"
[0m14:50:20.833232 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.unique_fct_marketing_spend_date.e93d255723
[0m14:50:20.836385 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.unique_fct_marketing_spend_date.e93d255723"
[0m14:50:20.837410 [debug] [Thread-3 (]: On test.data_pipeline_project.unique_fct_marketing_spend_date.e93d255723: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_fct_marketing_spend_date.e93d255723"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select date as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_marketing_spend`
  where date is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:50:20.838195 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:50:20.875741 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:e094c2f2-6443-48dc-88cb-5d4fa973ad5e&page=queryresults
[0m14:50:20.919365 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:1f3d89fe-8571-44cb-a357-8a54f667cda5&page=queryresults
[0m14:50:21.651110 [info ] [Thread-1 (]: 57 of 71 PASS source_not_null_raw_data_users__id ............................... [[32mPASS[0m in 1.96s]
[0m14:50:21.652682 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b
[0m14:50:21.653989 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.unique_fct_order_order_id.653705d6a5
[0m14:50:21.654661 [info ] [Thread-1 (]: 61 of 71 START test unique_fct_order_order_id .................................. [RUN]
[0m14:50:21.655489 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b, now test.data_pipeline_project.unique_fct_order_order_id.653705d6a5)
[0m14:50:21.656186 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.unique_fct_order_order_id.653705d6a5
[0m14:50:21.662102 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.unique_fct_order_order_id.653705d6a5"
[0m14:50:21.664155 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.unique_fct_order_order_id.653705d6a5
[0m14:50:21.667190 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.unique_fct_order_order_id.653705d6a5"
[0m14:50:21.670123 [debug] [Thread-1 (]: On test.data_pipeline_project.unique_fct_order_order_id.653705d6a5: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_fct_order_order_id.653705d6a5"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select order_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_order`
  where order_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:50:21.672458 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:50:21.735550 [error] [Thread-2 (]: 58 of 71 FAIL 16 unique_dim_customer_customer_id ............................... [[31mFAIL 16[0m in 1.98s]
[0m14:50:21.739354 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1
[0m14:50:21.741126 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3
[0m14:50:21.742191 [info ] [Thread-2 (]: 62 of 71 START test unique_fct_shipment_shipment_id ............................ [RUN]
[0m14:50:21.743913 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1, now test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3)
[0m14:50:21.744562 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3
[0m14:50:21.770673 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3"
[0m14:50:21.774995 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3
[0m14:50:21.783563 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3"
[0m14:50:21.787125 [debug] [Thread-2 (]: On test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select shipment_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_shipment`
  where shipment_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:50:21.788030 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:50:21.856003 [info ] [Thread-4 (]: 59 of 71 PASS unique_dim_subscription_subscription_id .......................... [[32mPASS[0m in 1.97s]
[0m14:50:21.857630 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469
[0m14:50:21.859359 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd
[0m14:50:21.863233 [info ] [Thread-4 (]: 63 of 71 START test unique_mart_acquisition_efficiency_date .................... [RUN]
[0m14:50:21.865738 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469, now test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd)
[0m14:50:21.867345 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd
[0m14:50:21.869941 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:ca379e77-dfbc-490e-9a94-477064b28dc7&page=queryresults
[0m14:50:21.895411 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd"
[0m14:50:21.900760 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd
[0m14:50:21.921446 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd"
[0m14:50:21.925923 [debug] [Thread-4 (]: On test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select date as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_acquisition_efficiency`
  where date is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:50:21.929937 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:50:22.790100 [error] [Thread-3 (]: 60 of 71 FAIL 8 unique_fct_marketing_spend_date ................................ [[31mFAIL 8[0m in 1.97s]
[0m14:50:22.792064 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.unique_fct_marketing_spend_date.e93d255723
[0m14:50:22.793051 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8
[0m14:50:22.794048 [info ] [Thread-3 (]: 64 of 71 START test unique_mart_revenue_daily_date ............................. [RUN]
[0m14:50:22.794736 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_fct_marketing_spend_date.e93d255723, now test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8)
[0m14:50:22.795440 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8
[0m14:50:22.800947 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8"
[0m14:50:22.802134 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8
[0m14:50:22.805144 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8"
[0m14:50:22.805896 [debug] [Thread-3 (]: On test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select date as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_revenue_daily`
  where date is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:50:22.806406 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:50:23.001385 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:40804f59-e02f-4749-ad0e-6d5ecb0b4cb8&page=queryresults
[0m14:50:23.083928 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:b0f6d022-7579-4e4e-88e4-b465bc4ee362&page=queryresults
[0m14:50:23.125120 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:57892bc7-bdb8-4df3-85f9-46ca18ebdb18&page=queryresults
[0m14:50:23.864301 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:de43890c-0564-4de7-8c22-8b5886d4aaf9&page=queryresults
[0m14:50:23.932757 [info ] [Thread-1 (]: 61 of 71 PASS unique_fct_order_order_id ........................................ [[32mPASS[0m in 2.28s]
[0m14:50:23.939037 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.unique_fct_order_order_id.653705d6a5
[0m14:50:23.942782 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c
[0m14:50:23.944721 [info ] [Thread-1 (]: 65 of 71 START test unique_mart_subscription_daily_date ........................ [RUN]
[0m14:50:23.948772 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_fct_order_order_id.653705d6a5, now test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c)
[0m14:50:23.950211 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c
[0m14:50:23.960500 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c"
[0m14:50:23.979593 [info ] [Thread-4 (]: 63 of 71 PASS unique_mart_acquisition_efficiency_date .......................... [[32mPASS[0m in 2.11s]
[0m14:50:23.982259 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd
[0m14:50:23.982974 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00
[0m14:50:23.984034 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c
[0m14:50:23.988208 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c"
[0m14:50:23.981319 [info ] [Thread-2 (]: 62 of 71 PASS unique_fct_shipment_shipment_id .................................. [[32mPASS[0m in 2.24s]
[0m14:50:23.989989 [debug] [Thread-1 (]: On test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select date as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily`
  where date is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:50:23.985264 [info ] [Thread-4 (]: 66 of 71 START test unique_stg_addresses_address_id ............................ [RUN]
[0m14:50:23.991237 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3
[0m14:50:23.992185 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd, now test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00)
[0m14:50:23.992995 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.unique_stg_countries_country_id.8679936442
[0m14:50:23.993639 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:50:23.994317 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00
[0m14:50:24.031026 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00"
[0m14:50:23.995431 [info ] [Thread-2 (]: 67 of 71 START test unique_stg_countries_country_id ............................ [RUN]
[0m14:50:24.033128 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3, now test.data_pipeline_project.unique_stg_countries_country_id.8679936442)
[0m14:50:24.036314 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.unique_stg_countries_country_id.8679936442
[0m14:50:24.050440 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.unique_stg_countries_country_id.8679936442"
[0m14:50:24.051999 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.unique_stg_countries_country_id.8679936442
[0m14:50:24.057206 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.unique_stg_countries_country_id.8679936442"
[0m14:50:24.058247 [debug] [Thread-2 (]: On test.data_pipeline_project.unique_stg_countries_country_id.8679936442: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_stg_countries_country_id.8679936442"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select country_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_countries`
  where country_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:50:24.060459 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00
[0m14:50:24.061464 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:50:24.067514 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00"
[0m14:50:24.150485 [debug] [Thread-4 (]: On test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select address_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_addresses`
  where address_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:50:24.152215 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:50:24.697917 [info ] [Thread-3 (]: 64 of 71 PASS unique_mart_revenue_daily_date ................................... [[32mPASS[0m in 1.90s]
[0m14:50:24.702394 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8
[0m14:50:24.704281 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a
[0m14:50:24.705860 [info ] [Thread-3 (]: 68 of 71 START test unique_stg_orders_order_id ................................. [RUN]
[0m14:50:24.707525 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8, now test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a)
[0m14:50:24.708918 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a
[0m14:50:24.715652 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a"
[0m14:50:24.726419 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a
[0m14:50:24.729963 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a"
[0m14:50:24.731029 [debug] [Thread-3 (]: On test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select order_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_orders`
  where order_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:50:24.731850 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:50:25.102000 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:39dd4b2f-21fb-4cdf-9a21-652a57a71c83&page=queryresults
[0m14:50:25.291818 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:5c3bef86-feab-4166-b4ae-d28b819a4643&page=queryresults
[0m14:50:25.314622 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:da454080-9520-40fc-8ab4-bb7792278c0a&page=queryresults
[0m14:50:25.915771 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:b3e044a6-bd7a-426c-8ad1-7cab9d2abe5c&page=queryresults
[0m14:50:25.978158 [info ] [Thread-1 (]: 65 of 71 PASS unique_mart_subscription_daily_date .............................. [[32mPASS[0m in 2.03s]
[0m14:50:25.980291 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c
[0m14:50:25.981621 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c
[0m14:50:25.982835 [info ] [Thread-1 (]: 69 of 71 START test unique_stg_shipments_shipment_id ........................... [RUN]
[0m14:50:25.983990 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c, now test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c)
[0m14:50:25.984974 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c
[0m14:50:25.995606 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c"
[0m14:50:25.997247 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c
[0m14:50:26.008558 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c"
[0m14:50:26.012374 [debug] [Thread-1 (]: On test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select shipment_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_shipments`
  where shipment_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:50:26.013002 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:50:26.098165 [info ] [Thread-4 (]: 66 of 71 PASS unique_stg_addresses_address_id .................................. [[32mPASS[0m in 2.11s]
[0m14:50:26.099574 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00
[0m14:50:26.100472 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m14:50:26.101429 [info ] [Thread-4 (]: 70 of 71 START test unique_stg_subscriptions_subscription_id ................... [RUN]
[0m14:50:26.102352 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00, now test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e)
[0m14:50:26.103255 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m14:50:26.108820 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e"
[0m14:50:26.109995 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m14:50:26.114243 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e"
[0m14:50:26.115037 [debug] [Thread-4 (]: On test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select subscription_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_subscriptions`
  where subscription_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:50:26.115654 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:50:26.247828 [info ] [Thread-2 (]: 67 of 71 PASS unique_stg_countries_country_id .................................. [[32mPASS[0m in 2.21s]
[0m14:50:26.250909 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.unique_stg_countries_country_id.8679936442
[0m14:50:26.251594 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b
[0m14:50:26.255221 [info ] [Thread-2 (]: 71 of 71 START test unique_stg_users_user_id ................................... [RUN]
[0m14:50:26.256880 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_stg_countries_country_id.8679936442, now test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b)
[0m14:50:26.257500 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b
[0m14:50:26.267088 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b"
[0m14:50:26.269401 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b
[0m14:50:26.283265 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b"
[0m14:50:26.285221 [debug] [Thread-2 (]: On test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select user_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
  where user_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:50:26.288685 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:50:26.732209 [info ] [Thread-3 (]: 68 of 71 PASS unique_stg_orders_order_id ....................................... [[32mPASS[0m in 2.02s]
[0m14:50:26.734052 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a
[0m14:50:27.303112 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:de75b5c0-8593-4091-b0a0-7430f6697db4&page=queryresults
[0m14:50:27.342051 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:1f61d961-ce2a-4fcd-a5be-7d5e66fb1286&page=queryresults
[0m14:50:27.518725 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:d1adbdda-e1c7-4a57-925c-7002e3432308&page=queryresults
[0m14:50:28.168750 [info ] [Thread-1 (]: 69 of 71 PASS unique_stg_shipments_shipment_id ................................. [[32mPASS[0m in 2.18s]
[0m14:50:28.169912 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c
[0m14:50:28.184610 [info ] [Thread-4 (]: 70 of 71 PASS unique_stg_subscriptions_subscription_id ......................... [[32mPASS[0m in 2.08s]
[0m14:50:28.186065 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m14:50:28.330240 [info ] [Thread-2 (]: 71 of 71 PASS unique_stg_users_user_id ......................................... [[32mPASS[0m in 2.07s]
[0m14:50:28.331449 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b
[0m14:50:28.338460 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:50:28.344597 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:50:28.347068 [debug] [MainThread]: Connection 'test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c' was properly closed.
[0m14:50:28.348206 [debug] [MainThread]: Connection 'test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b' was properly closed.
[0m14:50:28.349239 [debug] [MainThread]: Connection 'test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e' was properly closed.
[0m14:50:28.350100 [debug] [MainThread]: Connection 'test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a' was properly closed.
[0m14:50:28.351083 [info ] [MainThread]: 
[0m14:50:28.352013 [info ] [MainThread]: Finished running 71 data tests in 0 hours 0 minutes and 40.96 seconds (40.96s).
[0m14:50:28.385084 [debug] [MainThread]: Command end result
[0m14:50:28.524773 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m14:50:28.529671 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m14:50:28.551485 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m14:50:28.552187 [info ] [MainThread]: 
[0m14:50:28.552955 [info ] [MainThread]: [31mCompleted with 5 errors, 0 partial successes, and 1 warning:[0m
[0m14:50:28.554079 [info ] [MainThread]: 
[0m14:50:28.555010 [error] [MainThread]: [31mFailure in test dbt_utils_accepted_range_mart_kpis_latest_cac_payback_months__0 (models/intermediate/int_facts_marts_schema.yml)[0m
[0m14:50:28.555675 [error] [MainThread]:   Database Error in test dbt_utils_accepted_range_mart_kpis_latest_cac_payback_months__0 (models/intermediate/int_facts_marts_schema.yml)
  Not found: Table data-pipeline-project-474812:analytics_staging_marts.mart_kpis_latest was not found in location US
  compiled code at target/run/data_pipeline_project/models/intermediate/int_facts_marts_schema.yml/dbt_utils_accepted_range_mart_kpis_latest_cac_payback_months__0.sql
[0m14:50:28.556357 [info ] [MainThread]: 
[0m14:50:28.557105 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_project/models/intermediate/int_facts_marts_schema.yml/dbt_utils_accepted_range_mart_kpis_latest_cac_payback_months__0.sql
[0m14:50:28.557657 [info ] [MainThread]: 
[0m14:50:28.558478 [error] [MainThread]: [31mFailure in test dbt_utils_accepted_range_mart_kpis_latest_monthly_recurring_revenue__0 (models/intermediate/int_facts_marts_schema.yml)[0m
[0m14:50:28.559197 [error] [MainThread]:   Database Error in test dbt_utils_accepted_range_mart_kpis_latest_monthly_recurring_revenue__0 (models/intermediate/int_facts_marts_schema.yml)
  Not found: Table data-pipeline-project-474812:analytics_staging_marts.mart_kpis_latest was not found in location US
  compiled code at target/run/data_pipeline_project/models/intermediate/int_facts_marts_schema.yml/dbt_utils_accepted_range_mart__e59ea97756b83d6bad13e90b7eb4c03a.sql
[0m14:50:28.560122 [info ] [MainThread]: 
[0m14:50:28.561723 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_project/models/intermediate/int_facts_marts_schema.yml/dbt_utils_accepted_range_mart__e59ea97756b83d6bad13e90b7eb4c03a.sql
[0m14:50:28.563769 [info ] [MainThread]: 
[0m14:50:28.565698 [error] [MainThread]: [31mFailure in test relationships_fct_shipment_order_id__order_id__ref_fct_order_ (models/intermediate/int_facts_marts_schema.yml)[0m
[0m14:50:28.566596 [error] [MainThread]:   Got 476 results, configured to fail if != 0
[0m14:50:28.567431 [info ] [MainThread]: 
[0m14:50:28.568228 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_project/models/intermediate/int_facts_marts_schema.yml/relationships_fct_shipment_order_id__order_id__ref_fct_order_.sql
[0m14:50:28.568745 [info ] [MainThread]: 
[0m14:50:28.569729 [error] [MainThread]: [31mFailure in test unique_dim_customer_customer_id (models/intermediate/int_facts_marts_schema.yml)[0m
[0m14:50:28.570649 [error] [MainThread]:   Got 16 results, configured to fail if != 0
[0m14:50:28.571447 [info ] [MainThread]: 
[0m14:50:28.572283 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_project/models/intermediate/int_facts_marts_schema.yml/unique_dim_customer_customer_id.sql
[0m14:50:28.573186 [info ] [MainThread]: 
[0m14:50:28.573856 [error] [MainThread]: [31mFailure in test unique_fct_marketing_spend_date (models/intermediate/int_facts_marts_schema.yml)[0m
[0m14:50:28.577484 [error] [MainThread]:   Got 8 results, configured to fail if != 0
[0m14:50:28.581195 [info ] [MainThread]: 
[0m14:50:28.582475 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_project/models/intermediate/int_facts_marts_schema.yml/unique_fct_marketing_spend_date.sql
[0m14:50:28.583298 [info ] [MainThread]: 
[0m14:50:28.584275 [warn ] [MainThread]: [33mWarning in test not_null_int_shipments_decomposed_delivered_at (models/staging/staging_schema.yml)[0m
[0m14:50:28.585451 [warn ] [MainThread]: Got 303 results, configured to warn if != 0
[0m14:50:28.586657 [info ] [MainThread]: 
[0m14:50:28.588553 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_project/models/staging/staging_schema.yml/not_null_int_shipments_decomposed_delivered_at.sql
[0m14:50:28.590383 [info ] [MainThread]: 
[0m14:50:28.591825 [info ] [MainThread]: Done. PASS=65 WARN=1 ERROR=5 SKIP=0 NO-OP=0 TOTAL=71
[0m14:50:28.600649 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 18 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m14:50:28.604345 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 46.27581, "process_in_blocks": "40", "process_kernel_time": 3.797072, "process_mem_max_rss": "402312", "process_out_blocks": "7304", "process_user_time": 19.267973}
[0m14:50:28.606147 [debug] [MainThread]: Command `dbt test` failed at 14:50:28.605423 after 46.28 seconds
[0m14:50:28.606695 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7239cbe4d190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7239aac93cb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7239a2cd5460>]}
[0m14:50:28.607316 [debug] [MainThread]: Flushing usage events
[0m14:50:34.397790 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:52:34.614700 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x753be2c61940>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x753be27c1340>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x753be26e74a0>]}


============================== 14:52:34.617824 | a02db158-5f7f-486f-83e3-70aa2225cac5 ==============================
[0m14:52:34.617824 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m14:52:34.620375 [debug] [MainThread]: running dbt with arguments {'no_print': 'None', 'empty': 'False', 'invocation_command': 'dbt run --select mart_kpis_latest', 'debug': 'False', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'target_path': 'None', 'fail_fast': 'False', 'introspect': 'True', 'quiet': 'False', 'profiles_dir': '/home/ecem/.dbt', 'version_check': 'True', 'warn_error': 'None', 'send_anonymous_usage_stats': 'True', 'cache_selected_only': 'False', 'log_format': 'default', 'partial_parse': 'True', 'write_json': 'True', 'printer_width': '80', 'use_experimental_parser': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_colors': 'True', 'log_cache_events': 'False', 'indirect_selection': 'eager', 'static_parser': 'True'}
[0m14:52:36.987197 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a02db158-5f7f-486f-83e3-70aa2225cac5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x753bc25afd10>]}
[0m14:52:37.052608 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a02db158-5f7f-486f-83e3-70aa2225cac5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x753bc2c76180>]}
[0m14:52:37.053689 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m14:52:37.327195 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m14:52:37.598706 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:52:37.599912 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:52:37.666838 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a02db158-5f7f-486f-83e3-70aa2225cac5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x753bc246fd40>]}
[0m14:52:37.780083 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m14:52:37.783510 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m14:52:37.805005 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a02db158-5f7f-486f-83e3-70aa2225cac5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x753bc21612b0>]}
[0m14:52:37.805948 [info ] [MainThread]: Found 21 models, 71 data tests, 10 sources, 624 macros
[0m14:52:37.806530 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a02db158-5f7f-486f-83e3-70aa2225cac5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x753bc240b6b0>]}
[0m14:52:37.808970 [info ] [MainThread]: 
[0m14:52:37.809794 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:52:37.810310 [info ] [MainThread]: 
[0m14:52:37.811315 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m14:52:37.813250 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812'
[0m14:52:37.814264 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:52:38.820631 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812, now list_data-pipeline-project-474812_analytics_staging_staging)
[0m14:52:38.822297 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_marts'
[0m14:52:38.823052 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:52:38.824333 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:52:39.783546 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a02db158-5f7f-486f-83e3-70aa2225cac5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x753be436e600>]}
[0m14:52:39.784563 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:52:39.798388 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.mart_kpis_latest
[0m14:52:39.799507 [info ] [Thread-1 (]: 1 of 1 START sql table model analytics_staging_marts.mart_kpis_latest .......... [RUN]
[0m14:52:39.800692 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now model.data_pipeline_project.mart_kpis_latest)
[0m14:52:39.801937 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.mart_kpis_latest
[0m14:52:39.816239 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.mart_kpis_latest"
[0m14:52:39.817642 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.mart_kpis_latest
[0m14:52:39.865231 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.mart_kpis_latest"
[0m14:52:39.866174 [debug] [Thread-1 (]: On model.data_pipeline_project.mart_kpis_latest: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.mart_kpis_latest"} */

  
    

    create or replace table `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_kpis_latest`
      
    
    

    
    OPTIONS()
    as (
      with latest_subscription_data as (
    -- Get the latest date and active subscribers in separate steps
    select 
        max(date) as latest_date
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily`
),

snapshot_kpis as (
    select
        lsd.latest_date,
        -- Get active subscribers for the latest date
        (select active_subscribers 
         from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily` 
         where date = lsd.latest_date) as active_subscribers_latest
    from latest_subscription_data lsd
),

-- CTE: Calculate the Average Revenue Per Active Subscriber (ARPAS) over the latest 30 days.
monthly_revenue_metrics as (
    select
        -- Total gross revenue from delivered orders over the last 30 days
        sum(mr.daily_revenue) as total_gross_revenue_30d,
        -- Total sum of active subscribers across those 30 days
        sum(msd.active_subscribers) as total_active_subs_30d
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_revenue_daily` mr
    inner join `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily` msd
        on mr.date = msd.date
    where mr.date >= date_sub(
        (select max(date) from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily`), 
        interval 30 day
    )
)

-- Final assembly with lookback calculations
select
    t1.latest_date,
    t1.active_subscribers_latest,

    -- 2. AVERAGE REVENUE PER ACTIVE SUB (ARPAS)
    round(
        (t2.total_gross_revenue_30d / nullif(t2.total_active_subs_30d, 0)) * 30
    , 2) as monthly_revenue_per_active_sub,

    -- 3. MONTHLY RECURRING REVENUE (MRR)
    round(
        t1.active_subscribers_latest * (t2.total_gross_revenue_30d / nullif(t2.total_active_subs_30d, 0)) * 30
    , 2) as monthly_recurring_revenue,

    -- 4. MONTHLY CHURN RATE %
    round(
        (
            -- Total cancellations in the last 30 days
            (select sum(cancellations) from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily` where date >= date_sub(t1.latest_date, interval 30 day))
            / 
            -- Active subscribers 30 days ago
            nullif((select active_subscribers from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily` where date = date_sub(t1.latest_date, interval 30 day)), 0)
        ) * 100
        , 2
    ) as monthly_churn_rate_pct,

    -- 5. CAC PAYBACK PERIOD
    round(
        (
            -- Average CAC over the last 30 days
            (select avg(daily_customer_acquisition_cost) from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_acquisition_efficiency` where date >= date_sub(t1.latest_date, interval 30 day))
            / 
            -- Monthly Revenue Per Active Sub (ARPAS)
            nullif(
                (t2.total_gross_revenue_30d / nullif(t2.total_active_subs_30d, 0)) * 30
                , 0
            )
        )
        , 2
    ) as cac_payback_months

from snapshot_kpis t1
cross join monthly_revenue_metrics t2
    );
  
[0m14:52:39.866850 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:52:41.667437 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:a14db547-f047-40fd-b57d-71f8d88e07de&page=queryresults
[0m14:52:45.024801 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a02db158-5f7f-486f-83e3-70aa2225cac5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x753bc2927b60>]}
[0m14:52:45.026299 [info ] [Thread-1 (]: 1 of 1 OK created sql table model analytics_staging_marts.mart_kpis_latest ..... [[32mCREATE TABLE (1.0 rows, 1.1 KiB processed)[0m in 5.22s]
[0m14:52:45.027511 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.mart_kpis_latest
[0m14:52:45.030930 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:52:45.033450 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:52:45.034015 [debug] [MainThread]: Connection 'model.data_pipeline_project.mart_kpis_latest' was properly closed.
[0m14:52:45.034539 [debug] [MainThread]: Connection 'list_data-pipeline-project-474812_analytics_staging_marts' was properly closed.
[0m14:52:45.035219 [info ] [MainThread]: 
[0m14:52:45.035904 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 7.22 seconds (7.22s).
[0m14:52:45.037016 [debug] [MainThread]: Command end result
[0m14:52:45.100802 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m14:52:45.106521 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m14:52:45.116410 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m14:52:45.117162 [info ] [MainThread]: 
[0m14:52:45.117914 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:52:45.118520 [info ] [MainThread]: 
[0m14:52:45.119375 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m14:52:45.122256 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 10.559973, "process_in_blocks": "0", "process_kernel_time": 1.030544, "process_mem_max_rss": "381952", "process_out_blocks": "3936", "process_user_time": 4.850431}
[0m14:52:45.123554 [debug] [MainThread]: Command `dbt run` succeeded at 14:52:45.123429 after 10.56 seconds
[0m14:52:45.124185 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x753be2343b00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x753bc25ae2a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x753bc2420650>]}
[0m14:52:45.124903 [debug] [MainThread]: Flushing usage events
[0m14:52:46.169390 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:53:31.606874 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x733306ee5550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x733306ee5430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x733306a3d8e0>]}


============================== 14:53:31.612037 | 87d80925-cdbb-46aa-a359-dc39227a3ec7 ==============================
[0m14:53:31.612037 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m14:53:31.613722 [debug] [MainThread]: running dbt with arguments {'indirect_selection': 'eager', 'static_parser': 'True', 'log_cache_events': 'False', 'fail_fast': 'False', 'empty': 'False', 'introspect': 'True', 'partial_parse': 'True', 'profiles_dir': '/home/ecem/.dbt', 'log_format': 'default', 'write_json': 'True', 'use_colors': 'True', 'debug': 'False', 'quiet': 'False', 'version_check': 'True', 'cache_selected_only': 'False', 'invocation_command': 'dbt run --select mart_kpis_latest --full-refresh', 'target_path': 'None', 'send_anonymous_usage_stats': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'use_experimental_parser': 'False', 'no_print': 'None', 'warn_error': 'None', 'printer_width': '80'}
[0m14:53:33.954905 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '87d80925-cdbb-46aa-a359-dc39227a3ec7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7332e73198e0>]}
[0m14:53:34.024371 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '87d80925-cdbb-46aa-a359-dc39227a3ec7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x733306a9d820>]}
[0m14:53:34.026412 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m14:53:34.277597 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m14:53:34.561636 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:53:34.566619 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:53:34.612608 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '87d80925-cdbb-46aa-a359-dc39227a3ec7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7332e6f15f10>]}
[0m14:53:34.726523 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m14:53:34.729248 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m14:53:34.749587 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '87d80925-cdbb-46aa-a359-dc39227a3ec7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7332e6cbf9e0>]}
[0m14:53:34.751139 [info ] [MainThread]: Found 21 models, 71 data tests, 10 sources, 624 macros
[0m14:53:34.752145 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '87d80925-cdbb-46aa-a359-dc39227a3ec7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7332e6c07260>]}
[0m14:53:34.755015 [info ] [MainThread]: 
[0m14:53:34.755924 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:53:34.756447 [info ] [MainThread]: 
[0m14:53:34.757466 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m14:53:34.759446 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812'
[0m14:53:34.760279 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:53:35.606327 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_staging'
[0m14:53:35.607131 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:53:35.608267 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812, now list_data-pipeline-project-474812_analytics_staging_marts)
[0m14:53:35.641852 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:53:36.577355 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '87d80925-cdbb-46aa-a359-dc39227a3ec7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7332e6a35b50>]}
[0m14:53:36.579094 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:53:36.588767 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.mart_kpis_latest
[0m14:53:36.589727 [info ] [Thread-1 (]: 1 of 1 START sql table model analytics_staging_marts.mart_kpis_latest .......... [RUN]
[0m14:53:36.590626 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_marts, now model.data_pipeline_project.mart_kpis_latest)
[0m14:53:36.591402 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.mart_kpis_latest
[0m14:53:36.601505 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.mart_kpis_latest"
[0m14:53:36.603925 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.mart_kpis_latest
[0m14:53:36.624513 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:53:37.663942 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.mart_kpis_latest"
[0m14:53:37.665848 [debug] [Thread-1 (]: On model.data_pipeline_project.mart_kpis_latest: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.mart_kpis_latest"} */

  
    

    create or replace table `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_kpis_latest`
      
    
    

    
    OPTIONS()
    as (
      with latest_subscription_data as (
    -- Get the latest date and active subscribers in separate steps
    select 
        max(date) as latest_date
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily`
),

snapshot_kpis as (
    select
        lsd.latest_date,
        -- Get active subscribers for the latest date
        (select active_subscribers 
         from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily` 
         where date = lsd.latest_date) as active_subscribers_latest
    from latest_subscription_data lsd
),

-- CTE: Calculate the Average Revenue Per Active Subscriber (ARPAS) over the latest 30 days.
monthly_revenue_metrics as (
    select
        -- Total gross revenue from delivered orders over the last 30 days
        sum(mr.daily_revenue) as total_gross_revenue_30d,
        -- Total sum of active subscribers across those 30 days
        sum(msd.active_subscribers) as total_active_subs_30d
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_revenue_daily` mr
    inner join `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily` msd
        on mr.date = msd.date
    where mr.date >= date_sub(
        (select max(date) from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily`), 
        interval 30 day
    )
)

-- Final assembly with lookback calculations
select
    t1.latest_date,
    t1.active_subscribers_latest,

    -- 2. AVERAGE REVENUE PER ACTIVE SUB (ARPAS)
    round(
        (t2.total_gross_revenue_30d / nullif(t2.total_active_subs_30d, 0)) * 30
    , 2) as monthly_revenue_per_active_sub,

    -- 3. MONTHLY RECURRING REVENUE (MRR)
    round(
        t1.active_subscribers_latest * (t2.total_gross_revenue_30d / nullif(t2.total_active_subs_30d, 0)) * 30
    , 2) as monthly_recurring_revenue,

    -- 4. MONTHLY CHURN RATE %
    round(
        (
            -- Total cancellations in the last 30 days
            (select sum(cancellations) from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily` where date >= date_sub(t1.latest_date, interval 30 day))
            / 
            -- Active subscribers 30 days ago
            nullif((select active_subscribers from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily` where date = date_sub(t1.latest_date, interval 30 day)), 0)
        ) * 100
        , 2
    ) as monthly_churn_rate_pct,

    -- 5. CAC PAYBACK PERIOD
    round(
        (
            -- Average CAC over the last 30 days
            (select avg(daily_customer_acquisition_cost) from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_acquisition_efficiency` where date >= date_sub(t1.latest_date, interval 30 day))
            / 
            -- Monthly Revenue Per Active Sub (ARPAS)
            nullif(
                (t2.total_gross_revenue_30d / nullif(t2.total_active_subs_30d, 0)) * 30
                , 0
            )
        )
        , 2
    ) as cac_payback_months

from snapshot_kpis t1
cross join monthly_revenue_metrics t2
    );
  
[0m14:53:38.266208 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:dd35ad3f-08ae-4d85-88c5-f6302b230b2e&page=queryresults
[0m14:53:41.145252 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '87d80925-cdbb-46aa-a359-dc39227a3ec7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7332e6a94410>]}
[0m14:53:41.146276 [info ] [Thread-1 (]: 1 of 1 OK created sql table model analytics_staging_marts.mart_kpis_latest ..... [[32mCREATE TABLE (1.0 rows, 1.1 KiB processed)[0m in 4.55s]
[0m14:53:41.148064 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.mart_kpis_latest
[0m14:53:41.152676 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:53:41.155776 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:53:41.156244 [debug] [MainThread]: Connection 'model.data_pipeline_project.mart_kpis_latest' was properly closed.
[0m14:53:41.157326 [debug] [MainThread]: Connection 'list_data-pipeline-project-474812_analytics_staging_staging' was properly closed.
[0m14:53:41.157818 [info ] [MainThread]: 
[0m14:53:41.158996 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 6.40 seconds (6.40s).
[0m14:53:41.160172 [debug] [MainThread]: Command end result
[0m14:53:41.222138 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m14:53:41.227322 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m14:53:41.239131 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m14:53:41.240276 [info ] [MainThread]: 
[0m14:53:41.241206 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:53:41.242046 [info ] [MainThread]: 
[0m14:53:41.243163 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m14:53:41.244725 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 9.691974, "process_in_blocks": "0", "process_kernel_time": 1.075996, "process_mem_max_rss": "381996", "process_out_blocks": "3936", "process_user_time": 4.760002}
[0m14:53:41.245790 [debug] [MainThread]: Command `dbt run` succeeded at 14:53:41.245552 after 9.69 seconds
[0m14:53:41.246937 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x733306f74530>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7333075cd9a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7332e6cbfb90>]}
[0m14:53:41.247637 [debug] [MainThread]: Flushing usage events
[0m14:53:41.981794 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:40:32.608733 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbefd6037a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbefd773440>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbefe2f8bc0>]}


============================== 15:40:32.614085 | c9a217ae-b324-409c-af40-2f4fb0367f04 ==============================
[0m15:40:32.614085 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m15:40:32.615434 [debug] [MainThread]: running dbt with arguments {'empty': 'None', 'cache_selected_only': 'False', 'fail_fast': 'False', 'target_path': 'None', 'quiet': 'False', 'indirect_selection': 'eager', 'partial_parse': 'True', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'write_json': 'True', 'send_anonymous_usage_stats': 'True', 'introspect': 'True', 'use_experimental_parser': 'False', 'printer_width': '80', 'no_print': 'None', 'invocation_command': 'dbt clean', 'warn_error': 'None', 'log_format': 'default', 'profiles_dir': '/home/ecem/.dbt', 'static_parser': 'True', 'use_colors': 'True', 'log_cache_events': 'False', 'version_check': 'True', 'debug': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])'}
[0m15:40:32.780138 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c9a217ae-b324-409c-af40-2f4fb0367f04', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbefd731880>]}
[0m15:40:32.828451 [debug] [MainThread]: Resource report: {"command_name": "clean", "command_success": true, "command_wall_clock_time": 0.2941652, "process_in_blocks": "0", "process_kernel_time": 0.294071, "process_mem_max_rss": "101804", "process_out_blocks": "8", "process_user_time": 1.569445}
[0m15:40:32.829053 [debug] [MainThread]: Command `dbt clean` succeeded at 15:40:32.828934 after 0.29 seconds
[0m15:40:32.829453 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbefd2137a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbefdd0a240>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbefe2f8bc0>]}
[0m15:40:32.829858 [debug] [MainThread]: Flushing usage events
[0m15:40:33.856623 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:41:08.643762 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75c39e615e20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75c39e1a6e40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75c39eaf3830>]}


============================== 15:41:08.651196 | 58b8c915-9040-40a0-8340-6b91862c29db ==============================
[0m15:41:08.651196 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m15:41:08.656766 [debug] [MainThread]: running dbt with arguments {'warn_error': 'None', 'send_anonymous_usage_stats': 'True', 'write_json': 'True', 'target_path': 'None', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'introspect': 'True', 'static_parser': 'True', 'empty': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'debug': 'False', 'version_check': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'profiles_dir': '/home/ecem/.dbt', 'indirect_selection': 'eager', 'no_print': 'None', 'partial_parse': 'True', 'use_experimental_parser': 'False', 'use_colors': 'True', 'printer_width': '80', 'invocation_command': 'dbt run --select mart_kpis_latest --full-refresh', 'log_format': 'default', 'quiet': 'False', 'log_cache_events': 'False'}
[0m15:41:12.454382 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '58b8c915-9040-40a0-8340-6b91862c29db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75c37e8c20c0>]}
[0m15:41:12.561472 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '58b8c915-9040-40a0-8340-6b91862c29db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75c380d481a0>]}
[0m15:41:12.563037 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m15:41:13.203368 [error] [MainThread]: Encountered an error:
Compilation Error
  dbt found 1 package(s) specified in packages.yml, but only 0 package(s) installed in dbt_packages. Run "dbt deps" to install package dependencies.
[0m15:41:13.209600 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 4.681297, "process_in_blocks": "0", "process_kernel_time": 1.633381, "process_mem_max_rss": "364420", "process_out_blocks": "16", "process_user_time": 5.755931}
[0m15:41:13.213214 [debug] [MainThread]: Command `dbt run` failed at 15:41:13.213007 after 4.69 seconds
[0m15:41:13.214850 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75c39ddb5550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75c37df3a150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75c37df39790>]}
[0m15:41:13.216014 [debug] [MainThread]: Flushing usage events
[0m15:41:14.965945 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:41:57.858588 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x789d02045760>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x789d01ef62d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x789d013b8770>]}


============================== 15:41:57.863380 | 9bdf32b5-7a86-438c-a754-5332c8044b93 ==============================
[0m15:41:57.863380 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m15:41:57.864446 [debug] [MainThread]: running dbt with arguments {'use_experimental_parser': 'False', 'empty': 'None', 'indirect_selection': 'eager', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'send_anonymous_usage_stats': 'True', 'log_cache_events': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'profiles_dir': '/home/ecem/.dbt', 'partial_parse': 'True', 'introspect': 'True', 'quiet': 'False', 'static_parser': 'True', 'write_json': 'True', 'debug': 'False', 'no_print': 'None', 'target_path': 'None', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'fail_fast': 'False', 'invocation_command': 'dbt deps', 'cache_selected_only': 'False', 'log_format': 'default', 'printer_width': '80'}
[0m15:41:58.015848 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9bdf32b5-7a86-438c-a754-5332c8044b93', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x789d00e5ba70>]}
[0m15:41:58.043697 [debug] [MainThread]: Set downloads directory='/tmp/dbt-downloads-1wzloubd'
[0m15:41:58.044278 [debug] [MainThread]: Making package index registry request: GET https://hub.getdbt.com/api/v1/index.json
[0m15:41:58.936394 [debug] [MainThread]: Response from registry index: GET https://hub.getdbt.com/api/v1/index.json 200
[0m15:41:58.947383 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
[0m15:41:59.205864 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
[0m15:41:59.217551 [info ] [MainThread]: Installing dbt-labs/dbt_utils
[0m15:42:00.494842 [info ] [MainThread]: Installed from version 1.3.1
[0m15:42:00.495640 [info ] [MainThread]: Up to date!
[0m15:42:00.496485 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': '9bdf32b5-7a86-438c-a754-5332c8044b93', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x789d00e6b770>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x789d00e68dd0>]}
[0m15:42:00.498658 [debug] [MainThread]: Resource report: {"command_name": "deps", "command_success": true, "command_wall_clock_time": 2.696939, "process_in_blocks": "0", "process_kernel_time": 0.28007, "process_mem_max_rss": "103420", "process_out_blocks": "2288", "process_user_time": 1.730887}
[0m15:42:00.499520 [debug] [MainThread]: Command `dbt deps` succeeded at 15:42:00.499307 after 2.70 seconds
[0m15:42:00.499964 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x789d035845f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x789d00e9b110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x789d01ed1670>]}
[0m15:42:00.500570 [debug] [MainThread]: Flushing usage events
[0m15:42:01.393006 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:42:07.297988 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b405c0d86e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b405cada810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b405ae0f470>]}


============================== 15:42:07.302048 | 902070b2-99bb-4a7f-b3bd-7bf1d423e0d3 ==============================
[0m15:42:07.302048 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m15:42:07.303407 [debug] [MainThread]: running dbt with arguments {'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'use_colors': 'True', 'version_check': 'True', 'empty': 'False', 'send_anonymous_usage_stats': 'True', 'warn_error': 'None', 'log_cache_events': 'False', 'write_json': 'True', 'profiles_dir': '/home/ecem/.dbt', 'target_path': 'None', 'use_experimental_parser': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'quiet': 'False', 'invocation_command': 'dbt run --select mart_kpis_latest --full-refresh', 'log_format': 'default', 'introspect': 'True', 'fail_fast': 'False', 'static_parser': 'True', 'no_print': 'None', 'indirect_selection': 'eager', 'printer_width': '80', 'partial_parse': 'True', 'debug': 'False', 'cache_selected_only': 'False'}
[0m15:42:09.628739 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '902070b2-99bb-4a7f-b3bd-7bf1d423e0d3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b405c9abd70>]}
[0m15:42:09.695438 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '902070b2-99bb-4a7f-b3bd-7bf1d423e0d3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b403ab7c230>]}
[0m15:42:09.699542 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m15:42:09.964733 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m15:42:09.966476 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m15:42:09.967521 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '902070b2-99bb-4a7f-b3bd-7bf1d423e0d3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b403ab7e150>]}
[0m15:42:11.562044 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `relationships` defined on 'stg_orders' in
package 'data_pipeline_project' (models/staging/staging_schema.yml). Arguments
to generic tests should be nested under the `arguments` property.
[0m15:42:11.563594 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '902070b2-99bb-4a7f-b3bd-7bf1d423e0d3', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b403ab7e810>]}
[0m15:42:12.022410 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '902070b2-99bb-4a7f-b3bd-7bf1d423e0d3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b4039c24aa0>]}
[0m15:42:12.151331 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m15:42:12.154286 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m15:42:12.177531 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '902070b2-99bb-4a7f-b3bd-7bf1d423e0d3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b403980ee10>]}
[0m15:42:12.178491 [info ] [MainThread]: Found 21 models, 71 data tests, 10 sources, 624 macros
[0m15:42:12.179035 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '902070b2-99bb-4a7f-b3bd-7bf1d423e0d3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b4039c27c50>]}
[0m15:42:12.181633 [info ] [MainThread]: 
[0m15:42:12.182524 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:42:12.183594 [info ] [MainThread]: 
[0m15:42:12.184669 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m15:42:12.186527 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812'
[0m15:42:12.187177 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:42:13.263731 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812, now list_data-pipeline-project-474812_analytics_staging_staging)
[0m15:42:13.265803 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:42:13.267935 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_marts'
[0m15:42:13.315792 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:42:14.320552 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '902070b2-99bb-4a7f-b3bd-7bf1d423e0d3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b40399ad9d0>]}
[0m15:42:14.321264 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:42:14.330239 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.mart_kpis_latest
[0m15:42:14.331563 [info ] [Thread-1 (]: 1 of 1 START sql table model analytics_staging_marts.mart_kpis_latest .......... [RUN]
[0m15:42:14.332543 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now model.data_pipeline_project.mart_kpis_latest)
[0m15:42:14.333065 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.mart_kpis_latest
[0m15:42:14.342181 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.mart_kpis_latest"
[0m15:42:14.348974 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.mart_kpis_latest
[0m15:42:14.376975 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:42:15.337986 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.mart_kpis_latest"
[0m15:42:15.340504 [debug] [Thread-1 (]: On model.data_pipeline_project.mart_kpis_latest: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.mart_kpis_latest"} */

  
    

    create or replace table `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_kpis_latest`
      
    
    

    
    OPTIONS()
    as (
      

with latest_subscription_data as (
    -- Get the latest date and active subscribers in separate steps
    select 
        max(date) as latest_date
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily`
),

snapshot_kpis as (
    select
        lsd.latest_date,
        -- Get active subscribers for the latest date
        (select active_subscribers 
         from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily` 
         where date = lsd.latest_date) as active_subscribers_latest
    from latest_subscription_data lsd
),

-- CTE: Calculate the Average Revenue Per Active Subscriber (ARPAS) over the latest 30 days.
monthly_revenue_metrics as (
    select
        -- Total gross revenue from delivered orders over the last 30 days
        sum(mr.daily_revenue) as total_gross_revenue_30d,
        -- Total sum of active subscribers across those 30 days
        sum(msd.active_subscribers) as total_active_subs_30d
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_revenue_daily` mr
    inner join `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily` msd
        on mr.date = msd.date
    where mr.date >= date_sub(
        (select max(date) from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily`), 
        interval 30 day
    )
)

-- Final assembly with lookback calculations
select
    t1.latest_date,
    t1.active_subscribers_latest,

    -- 2. AVERAGE REVENUE PER ACTIVE SUB (ARPAS)
    round(
        (t2.total_gross_revenue_30d / nullif(t2.total_active_subs_30d, 0)) * 30
    , 2) as monthly_revenue_per_active_sub,

    -- 3. MONTHLY RECURRING REVENUE (MRR)
    round(
        t1.active_subscribers_latest * (t2.total_gross_revenue_30d / nullif(t2.total_active_subs_30d, 0)) * 30
    , 2) as monthly_recurring_revenue,

    -- 4. MONTHLY CHURN RATE %
    round(
        (
            -- Total cancellations in the last 30 days
            (select sum(cancellations) from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily` where date >= date_sub(t1.latest_date, interval 30 day))
            / 
            -- Active subscribers 30 days ago
            nullif((select active_subscribers from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily` where date = date_sub(t1.latest_date, interval 30 day)), 0)
        ) * 100
        , 2
    ) as monthly_churn_rate_pct,

    -- 5. CAC PAYBACK PERIOD
    round(
        (
            -- Average CAC over the last 30 days
            (select avg(daily_customer_acquisition_cost) from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_acquisition_efficiency` where date >= date_sub(t1.latest_date, interval 30 day))
            / 
            -- Monthly Revenue Per Active Sub (ARPAS)
            nullif(
                (t2.total_gross_revenue_30d / nullif(t2.total_active_subs_30d, 0)) * 30
                , 0
            )
        )
        , 2
    ) as cac_payback_months

from snapshot_kpis t1
cross join monthly_revenue_metrics t2
    );
  
[0m15:42:16.303626 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:29f30618-5b37-4455-a679-3c1cfce3f5d3&page=queryresults
[0m15:42:19.741832 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '902070b2-99bb-4a7f-b3bd-7bf1d423e0d3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b405bd8f410>]}
[0m15:42:19.744003 [info ] [Thread-1 (]: 1 of 1 OK created sql table model analytics_staging_marts.mart_kpis_latest ..... [[32mCREATE TABLE (1.0 rows, 1.1 KiB processed)[0m in 5.40s]
[0m15:42:19.756233 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.mart_kpis_latest
[0m15:42:19.783224 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:42:19.787058 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:42:19.788670 [debug] [MainThread]: Connection 'model.data_pipeline_project.mart_kpis_latest' was properly closed.
[0m15:42:19.799462 [debug] [MainThread]: Connection 'list_data-pipeline-project-474812_analytics_staging_marts' was properly closed.
[0m15:42:19.809122 [info ] [MainThread]: 
[0m15:42:19.813370 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 7.62 seconds (7.62s).
[0m15:42:19.818148 [debug] [MainThread]: Command end result
[0m15:42:20.004993 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m15:42:20.015799 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m15:42:20.026398 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m15:42:20.026947 [info ] [MainThread]: 
[0m15:42:20.028256 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:42:20.028821 [info ] [MainThread]: 
[0m15:42:20.029803 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m15:42:20.030609 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 18 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m15:42:20.031832 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 12.791169, "process_in_blocks": "0", "process_kernel_time": 1.315235, "process_mem_max_rss": "389912", "process_out_blocks": "5584", "process_user_time": 6.636319}
[0m15:42:20.032663 [debug] [MainThread]: Command `dbt run` succeeded at 15:42:20.032557 after 12.79 seconds
[0m15:42:20.033166 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b405a8000e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b4039b7c3b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b405afa0950>]}
[0m15:42:20.033725 [debug] [MainThread]: Flushing usage events
[0m15:42:20.824301 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:42:51.797384 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79b8707068d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79b870c778f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79b8728f65d0>]}


============================== 15:42:51.814495 | 980a5e3f-a165-44bb-9a78-b99189950432 ==============================
[0m15:42:51.814495 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m15:42:51.816903 [debug] [MainThread]: running dbt with arguments {'fail_fast': 'False', 'warn_error': 'None', 'debug': 'False', 'no_print': 'None', 'log_cache_events': 'False', 'version_check': 'True', 'indirect_selection': 'eager', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'empty': 'None', 'cache_selected_only': 'False', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt test', 'use_colors': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'profiles_dir': '/home/ecem/.dbt', 'quiet': 'False', 'partial_parse': 'True', 'use_experimental_parser': 'False', 'static_parser': 'True', 'write_json': 'True', 'printer_width': '80', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m15:42:54.172668 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '980a5e3f-a165-44bb-9a78-b99189950432', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79b851f42c90>]}
[0m15:42:54.241041 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '980a5e3f-a165-44bb-9a78-b99189950432', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79b8725221b0>]}
[0m15:42:54.242792 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m15:42:54.503474 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m15:42:54.787458 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:42:54.788366 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:42:54.840796 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '980a5e3f-a165-44bb-9a78-b99189950432', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79b850437650>]}
[0m15:42:54.963531 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m15:42:54.966298 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m15:42:55.000565 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '980a5e3f-a165-44bb-9a78-b99189950432', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79b850239fa0>]}
[0m15:42:55.001489 [info ] [MainThread]: Found 21 models, 71 data tests, 10 sources, 624 macros
[0m15:42:55.002447 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '980a5e3f-a165-44bb-9a78-b99189950432', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79b85059f3e0>]}
[0m15:42:55.007670 [info ] [MainThread]: 
[0m15:42:55.008821 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:42:55.010291 [info ] [MainThread]: 
[0m15:42:55.011271 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m15:42:55.018941 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_staging'
[0m15:42:55.020605 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_marts'
[0m15:42:55.021443 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:42:55.023363 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:42:56.065315 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '980a5e3f-a165-44bb-9a78-b99189950432', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79b850938770>]}
[0m15:42:56.066574 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:42:56.076205 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.dbt_utils_accepted_range_fct_marketing_spend_spend_try__0.bfe523cb61
[0m15:42:56.077944 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.dbt_utils_accepted_range_fct_order_discount_total__0.5f31954cd6
[0m15:42:56.078799 [info ] [Thread-1 (]: 1 of 71 START test dbt_utils_accepted_range_fct_marketing_spend_spend_try__0 ... [RUN]
[0m15:42:56.080677 [info ] [Thread-2 (]: 2 of 71 START test dbt_utils_accepted_range_fct_order_discount_total__0 ........ [RUN]
[0m15:42:56.082463 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_marts, now test.data_pipeline_project.dbt_utils_accepted_range_fct_marketing_spend_spend_try__0.bfe523cb61)
[0m15:42:56.083244 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.dbt_utils_accepted_range_fct_order_net_revenue__0.cb846760ce
[0m15:42:56.084146 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.dbt_utils_accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0.be1f75eba9
[0m15:42:56.085355 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now test.data_pipeline_project.dbt_utils_accepted_range_fct_order_discount_total__0.5f31954cd6)
[0m15:42:56.086559 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.dbt_utils_accepted_range_fct_marketing_spend_spend_try__0.bfe523cb61
[0m15:42:56.088477 [info ] [Thread-3 (]: 3 of 71 START test dbt_utils_accepted_range_fct_order_net_revenue__0 ........... [RUN]
[0m15:42:56.089979 [info ] [Thread-4 (]: 4 of 71 START test dbt_utils_accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0  [RUN]
[0m15:42:56.105835 [debug] [Thread-4 (]: Acquiring new bigquery connection 'test.data_pipeline_project.dbt_utils_accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0.be1f75eba9'
[0m15:42:56.091098 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.dbt_utils_accepted_range_fct_order_discount_total__0.5f31954cd6
[0m15:42:56.106767 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.dbt_utils_accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0.be1f75eba9
[0m15:42:56.099714 [debug] [Thread-3 (]: Acquiring new bigquery connection 'test.data_pipeline_project.dbt_utils_accepted_range_fct_order_net_revenue__0.cb846760ce'
[0m15:42:56.140965 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.dbt_utils_accepted_range_fct_marketing_spend_spend_try__0.bfe523cb61"
[0m15:42:56.153867 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.dbt_utils_accepted_range_fct_order_discount_total__0.5f31954cd6"
[0m15:42:56.158924 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.dbt_utils_accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0.be1f75eba9"
[0m15:42:56.164700 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.dbt_utils_accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0.be1f75eba9
[0m15:42:56.161723 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.dbt_utils_accepted_range_fct_order_discount_total__0.5f31954cd6
[0m15:42:56.160267 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.dbt_utils_accepted_range_fct_order_net_revenue__0.cb846760ce
[0m15:42:56.165784 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.dbt_utils_accepted_range_fct_marketing_spend_spend_try__0.bfe523cb61
[0m15:42:56.217499 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.dbt_utils_accepted_range_fct_order_net_revenue__0.cb846760ce"
[0m15:42:56.297527 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.dbt_utils_accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0.be1f75eba9"
[0m15:42:56.299614 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.dbt_utils_accepted_range_fct_marketing_spend_spend_try__0.bfe523cb61"
[0m15:42:56.301081 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.dbt_utils_accepted_range_fct_order_net_revenue__0.cb846760ce
[0m15:42:56.302941 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.dbt_utils_accepted_range_fct_order_discount_total__0.5f31954cd6"
[0m15:42:56.307389 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.dbt_utils_accepted_range_fct_order_net_revenue__0.cb846760ce"
[0m15:42:56.308812 [debug] [Thread-4 (]: On test.data_pipeline_project.dbt_utils_accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0.be1f75eba9: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.dbt_utils_accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0.be1f75eba9"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  

with meet_condition as(
  select *
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_acquisition_efficiency`
),

validation_errors as (
  select *
  from meet_condition
  where
    -- never true, defaults to an empty result set. Exists to ensure any combo of the `or` clauses below succeeds
    1 = 2
    -- records with a value >= min_value are permitted. The `not` flips this to find records that don't meet the rule.
    or not daily_customer_acquisition_cost >= 0
)

select *
from validation_errors


  
  
      
    ) dbt_internal_test
[0m15:42:56.309630 [debug] [Thread-1 (]: On test.data_pipeline_project.dbt_utils_accepted_range_fct_marketing_spend_spend_try__0.bfe523cb61: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.dbt_utils_accepted_range_fct_marketing_spend_spend_try__0.bfe523cb61"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  

with meet_condition as(
  select *
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_marketing_spend`
),

validation_errors as (
  select *
  from meet_condition
  where
    -- never true, defaults to an empty result set. Exists to ensure any combo of the `or` clauses below succeeds
    1 = 2
    -- records with a value >= min_value are permitted. The `not` flips this to find records that don't meet the rule.
    or not spend_try >= 0
)

select *
from validation_errors


  
  
      
    ) dbt_internal_test
[0m15:42:56.310557 [debug] [Thread-2 (]: On test.data_pipeline_project.dbt_utils_accepted_range_fct_order_discount_total__0.5f31954cd6: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.dbt_utils_accepted_range_fct_order_discount_total__0.5f31954cd6"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  

with meet_condition as(
  select *
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_order`
),

validation_errors as (
  select *
  from meet_condition
  where
    -- never true, defaults to an empty result set. Exists to ensure any combo of the `or` clauses below succeeds
    1 = 2
    -- records with a value >= min_value are permitted. The `not` flips this to find records that don't meet the rule.
    or not discount_total >= 0
)

select *
from validation_errors


  
  
      
    ) dbt_internal_test
[0m15:42:56.311927 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m15:42:56.313119 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:42:56.314100 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m15:42:56.314972 [debug] [Thread-3 (]: On test.data_pipeline_project.dbt_utils_accepted_range_fct_order_net_revenue__0.cb846760ce: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.dbt_utils_accepted_range_fct_order_net_revenue__0.cb846760ce"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  

with meet_condition as(
  select *
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_order`
),

validation_errors as (
  select *
  from meet_condition
  where
    -- never true, defaults to an empty result set. Exists to ensure any combo of the `or` clauses below succeeds
    1 = 2
    -- records with a value >= min_value are permitted. The `not` flips this to find records that don't meet the rule.
    or not net_revenue >= 0
)

select *
from validation_errors


  
  
      
    ) dbt_internal_test
[0m15:42:56.412325 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m15:42:57.761178 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:d2c2d5b6-35d3-46a1-8f4b-e692263c4750&page=queryresults
[0m15:42:57.768279 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:df3592b7-c513-461b-a9b8-296014281f0e&page=queryresults
[0m15:42:57.778416 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:3d27251a-1154-409d-9fca-2056fca538b8&page=queryresults
[0m15:42:57.789121 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:85f90d27-3712-46c8-ac63-93c7ad24a435&page=queryresults
[0m15:42:58.710133 [info ] [Thread-1 (]: 1 of 71 PASS dbt_utils_accepted_range_fct_marketing_spend_spend_try__0 ......... [[32mPASS[0m in 2.63s]
[0m15:42:58.720094 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.dbt_utils_accepted_range_fct_marketing_spend_spend_try__0.bfe523cb61
[0m15:42:58.723533 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_cac_payback_months__0.440e9fbb47
[0m15:42:58.724259 [info ] [Thread-1 (]: 5 of 71 START test dbt_utils_accepted_range_mart_kpis_latest_cac_payback_months__0  [RUN]
[0m15:42:58.725692 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.dbt_utils_accepted_range_fct_marketing_spend_spend_try__0.bfe523cb61, now test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_cac_payback_months__0.440e9fbb47)
[0m15:42:58.728291 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_cac_payback_months__0.440e9fbb47
[0m15:42:58.742584 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_cac_payback_months__0.440e9fbb47"
[0m15:42:58.741406 [info ] [Thread-2 (]: 2 of 71 PASS dbt_utils_accepted_range_fct_order_discount_total__0 .............. [[32mPASS[0m in 2.65s]
[0m15:42:58.744649 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_cac_payback_months__0.440e9fbb47
[0m15:42:58.746455 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.dbt_utils_accepted_range_fct_order_discount_total__0.5f31954cd6
[0m15:42:58.751550 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_cac_payback_months__0.440e9fbb47"
[0m15:42:58.753073 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_monthly_recurring_revenue__0.e7fa0e4f46
[0m15:42:58.754541 [debug] [Thread-1 (]: On test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_cac_payback_months__0.440e9fbb47: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_cac_payback_months__0.440e9fbb47"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  

with meet_condition as(
  select *
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_kpis_latest`
),

validation_errors as (
  select *
  from meet_condition
  where
    -- never true, defaults to an empty result set. Exists to ensure any combo of the `or` clauses below succeeds
    1 = 2
    -- records with a value >= min_value are permitted. The `not` flips this to find records that don't meet the rule.
    or not cac_payback_months >= 0
)

select *
from validation_errors


  
  
      
    ) dbt_internal_test
[0m15:42:58.756281 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:42:58.755357 [info ] [Thread-2 (]: 6 of 71 START test dbt_utils_accepted_range_mart_kpis_latest_monthly_recurring_revenue__0  [RUN]
[0m15:42:58.761385 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.dbt_utils_accepted_range_fct_order_discount_total__0.5f31954cd6, now test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_monthly_recurring_revenue__0.e7fa0e4f46)
[0m15:42:58.762256 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_monthly_recurring_revenue__0.e7fa0e4f46
[0m15:42:58.767873 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_monthly_recurring_revenue__0.e7fa0e4f46"
[0m15:42:58.826809 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_monthly_recurring_revenue__0.e7fa0e4f46
[0m15:42:58.840819 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_monthly_recurring_revenue__0.e7fa0e4f46"
[0m15:42:58.846977 [debug] [Thread-2 (]: On test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_monthly_recurring_revenue__0.e7fa0e4f46: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_monthly_recurring_revenue__0.e7fa0e4f46"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  

with meet_condition as(
  select *
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_kpis_latest`
),

validation_errors as (
  select *
  from meet_condition
  where
    -- never true, defaults to an empty result set. Exists to ensure any combo of the `or` clauses below succeeds
    1 = 2
    -- records with a value >= min_value are permitted. The `not` flips this to find records that don't meet the rule.
    or not monthly_recurring_revenue >= 0
)

select *
from validation_errors


  
  
      
    ) dbt_internal_test
[0m15:42:58.848692 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m15:42:58.926581 [info ] [Thread-3 (]: 3 of 71 PASS dbt_utils_accepted_range_fct_order_net_revenue__0 ................. [[32mPASS[0m in 2.83s]
[0m15:42:58.928751 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.dbt_utils_accepted_range_fct_order_net_revenue__0.cb846760ce
[0m15:42:58.929981 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.dbt_utils_accepted_range_mart_revenue_daily_daily_revenue__0.611ebe5f4e
[0m15:42:58.931368 [info ] [Thread-3 (]: 7 of 71 START test dbt_utils_accepted_range_mart_revenue_daily_daily_revenue__0  [RUN]
[0m15:42:58.932780 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.dbt_utils_accepted_range_fct_order_net_revenue__0.cb846760ce, now test.data_pipeline_project.dbt_utils_accepted_range_mart_revenue_daily_daily_revenue__0.611ebe5f4e)
[0m15:42:58.933669 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.dbt_utils_accepted_range_mart_revenue_daily_daily_revenue__0.611ebe5f4e
[0m15:42:58.953900 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.dbt_utils_accepted_range_mart_revenue_daily_daily_revenue__0.611ebe5f4e"
[0m15:42:58.957339 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.dbt_utils_accepted_range_mart_revenue_daily_daily_revenue__0.611ebe5f4e
[0m15:42:58.966940 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.dbt_utils_accepted_range_mart_revenue_daily_daily_revenue__0.611ebe5f4e"
[0m15:42:58.971391 [debug] [Thread-3 (]: On test.data_pipeline_project.dbt_utils_accepted_range_mart_revenue_daily_daily_revenue__0.611ebe5f4e: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.dbt_utils_accepted_range_mart_revenue_daily_daily_revenue__0.611ebe5f4e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  

with meet_condition as(
  select *
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_revenue_daily`
),

validation_errors as (
  select *
  from meet_condition
  where
    -- never true, defaults to an empty result set. Exists to ensure any combo of the `or` clauses below succeeds
    1 = 2
    -- records with a value >= min_value are permitted. The `not` flips this to find records that don't meet the rule.
    or not daily_revenue >= 0
)

select *
from validation_errors


  
  
      
    ) dbt_internal_test
[0m15:42:58.973041 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m15:42:59.138984 [info ] [Thread-4 (]: 4 of 71 PASS dbt_utils_accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0  [[32mPASS[0m in 3.03s]
[0m15:42:59.141969 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.dbt_utils_accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0.be1f75eba9
[0m15:42:59.143697 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_active_subscribers__0.74fd82653d
[0m15:42:59.145569 [info ] [Thread-4 (]: 8 of 71 START test dbt_utils_accepted_range_mart_subscription_daily_active_subscribers__0  [RUN]
[0m15:42:59.147046 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.dbt_utils_accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0.be1f75eba9, now test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_active_subscribers__0.74fd82653d)
[0m15:42:59.148379 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_active_subscribers__0.74fd82653d
[0m15:42:59.158405 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_active_subscribers__0.74fd82653d"
[0m15:42:59.160195 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_active_subscribers__0.74fd82653d
[0m15:42:59.166229 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_active_subscribers__0.74fd82653d"
[0m15:42:59.167447 [debug] [Thread-4 (]: On test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_active_subscribers__0.74fd82653d: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_active_subscribers__0.74fd82653d"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  

with meet_condition as(
  select *
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily`
),

validation_errors as (
  select *
  from meet_condition
  where
    -- never true, defaults to an empty result set. Exists to ensure any combo of the `or` clauses below succeeds
    1 = 2
    -- records with a value >= min_value are permitted. The `not` flips this to find records that don't meet the rule.
    or not active_subscribers >= 0
)

select *
from validation_errors


  
  
      
    ) dbt_internal_test
[0m15:42:59.168344 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m15:43:00.235537 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:229d6574-a8de-4590-b392-efab0936900a&page=queryresults
[0m15:43:00.266204 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:d36ce928-7a75-4690-8420-41dc59a00e0a&page=queryresults
[0m15:43:00.297114 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:b5a7d22b-5cf7-4cdc-b1a1-31c30391f8a8&page=queryresults
[0m15:43:00.376834 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:986a8a3d-87dc-4e0a-960f-763f866990b2&page=queryresults
[0m15:43:01.262772 [info ] [Thread-3 (]: 7 of 71 PASS dbt_utils_accepted_range_mart_revenue_daily_daily_revenue__0 ...... [[32mPASS[0m in 2.33s]
[0m15:43:01.263812 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.dbt_utils_accepted_range_mart_revenue_daily_daily_revenue__0.611ebe5f4e
[0m15:43:01.264484 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_new_subscribers__0.654a8ca7d1
[0m15:43:01.264926 [info ] [Thread-3 (]: 9 of 71 START test dbt_utils_accepted_range_mart_subscription_daily_new_subscribers__0  [RUN]
[0m15:43:01.266165 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.dbt_utils_accepted_range_mart_revenue_daily_daily_revenue__0.611ebe5f4e, now test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_new_subscribers__0.654a8ca7d1)
[0m15:43:01.266915 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_new_subscribers__0.654a8ca7d1
[0m15:43:01.272643 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_new_subscribers__0.654a8ca7d1"
[0m15:43:01.274229 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_new_subscribers__0.654a8ca7d1
[0m15:43:01.277799 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_new_subscribers__0.654a8ca7d1"
[0m15:43:01.279322 [debug] [Thread-3 (]: On test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_new_subscribers__0.654a8ca7d1: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_new_subscribers__0.654a8ca7d1"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  

with meet_condition as(
  select *
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily`
),

validation_errors as (
  select *
  from meet_condition
  where
    -- never true, defaults to an empty result set. Exists to ensure any combo of the `or` clauses below succeeds
    1 = 2
    -- records with a value >= min_value are permitted. The `not` flips this to find records that don't meet the rule.
    or not new_subscribers >= 0
)

select *
from validation_errors


  
  
      
    ) dbt_internal_test
[0m15:43:01.280257 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m15:43:01.393066 [info ] [Thread-4 (]: 8 of 71 PASS dbt_utils_accepted_range_mart_subscription_daily_active_subscribers__0  [[32mPASS[0m in 2.24s]
[0m15:43:01.402686 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_active_subscribers__0.74fd82653d
[0m15:43:01.408647 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc
[0m15:43:01.407757 [info ] [Thread-2 (]: 6 of 71 PASS dbt_utils_accepted_range_mart_kpis_latest_monthly_recurring_revenue__0  [[32mPASS[0m in 2.65s]
[0m15:43:01.423242 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_monthly_recurring_revenue__0.e7fa0e4f46
[0m15:43:01.414329 [info ] [Thread-4 (]: 10 of 71 START test not_null_dim_customer_customer_id .......................... [RUN]
[0m15:43:01.425017 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723
[0m15:43:01.428914 [info ] [Thread-1 (]: 5 of 71 PASS dbt_utils_accepted_range_mart_kpis_latest_cac_payback_months__0 ... [[32mPASS[0m in 2.70s]
[0m15:43:01.437882 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_cac_payback_months__0.440e9fbb47
[0m15:43:01.432415 [info ] [Thread-2 (]: 11 of 71 START test not_null_dim_subscription_customer_id ...................... [RUN]
[0m15:43:01.438791 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6
[0m15:43:01.430736 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_active_subscribers__0.74fd82653d, now test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc)
[0m15:43:01.439868 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_monthly_recurring_revenue__0.e7fa0e4f46, now test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723)
[0m15:43:01.445682 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc
[0m15:43:01.446582 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723
[0m15:43:01.444629 [info ] [Thread-1 (]: 12 of 71 START test not_null_dim_subscription_subscription_id .................. [RUN]
[0m15:43:01.455729 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc"
[0m15:43:01.483392 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc
[0m15:43:01.482164 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_cac_payback_months__0.440e9fbb47, now test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6)
[0m15:43:01.481509 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723"
[0m15:43:01.492336 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723
[0m15:43:01.490762 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6
[0m15:43:01.498621 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723"
[0m15:43:01.501220 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc"
[0m15:43:01.511228 [debug] [Thread-2 (]: On test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select customer_id
from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_subscription`
where customer_id is null



  
  
      
    ) dbt_internal_test
[0m15:43:01.513460 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m15:43:01.508963 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6"
[0m15:43:01.516777 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6
[0m15:43:01.512544 [debug] [Thread-4 (]: On test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select customer_id
from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_customer`
where customer_id is null



  
  
      
    ) dbt_internal_test
[0m15:43:01.520914 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6"
[0m15:43:01.563797 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m15:43:01.567372 [debug] [Thread-1 (]: On test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select subscription_id
from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_subscription`
where subscription_id is null



  
  
      
    ) dbt_internal_test
[0m15:43:01.570892 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:43:02.581800 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:57728fcd-293c-406e-886c-c1d39750828b&page=queryresults
[0m15:43:02.726802 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:3919e2c7-8590-4ade-bbf6-2aed1412997f&page=queryresults
[0m15:43:02.728650 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:a84d8fa9-6680-41ac-b01c-6e932bef61ba&page=queryresults
[0m15:43:02.737631 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:4036d4b4-3202-4aae-9a07-24f3c6868906&page=queryresults
[0m15:43:03.631653 [info ] [Thread-3 (]: 9 of 71 PASS dbt_utils_accepted_range_mart_subscription_daily_new_subscribers__0  [[32mPASS[0m in 2.36s]
[0m15:43:03.632776 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_new_subscribers__0.654a8ca7d1
[0m15:43:03.634341 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719
[0m15:43:03.634942 [info ] [Thread-3 (]: 13 of 71 START test not_null_fct_marketing_spend_date .......................... [RUN]
[0m15:43:03.636033 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_new_subscribers__0.654a8ca7d1, now test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719)
[0m15:43:03.636647 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719
[0m15:43:03.641399 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719"
[0m15:43:03.643395 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719
[0m15:43:03.646771 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719"
[0m15:43:03.648392 [debug] [Thread-3 (]: On test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select date
from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_marketing_spend`
where date is null



  
  
      
    ) dbt_internal_test
[0m15:43:03.652575 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m15:43:03.698601 [info ] [Thread-4 (]: 10 of 71 PASS not_null_dim_customer_customer_id ................................ [[32mPASS[0m in 2.27s]
[0m15:43:03.699762 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc
[0m15:43:03.700290 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d
[0m15:43:03.701311 [info ] [Thread-4 (]: 14 of 71 START test not_null_fct_marketing_spend_spend_try ..................... [RUN]
[0m15:43:03.702144 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc, now test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d)
[0m15:43:03.702787 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d
[0m15:43:03.709116 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d"
[0m15:43:03.709889 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d
[0m15:43:03.714040 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d"
[0m15:43:03.715456 [debug] [Thread-4 (]: On test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select spend_try
from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_marketing_spend`
where spend_try is null



  
  
      
    ) dbt_internal_test
[0m15:43:03.718107 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m15:43:03.806530 [info ] [Thread-1 (]: 12 of 71 PASS not_null_dim_subscription_subscription_id ........................ [[32mPASS[0m in 2.32s]
[0m15:43:03.810490 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6
[0m15:43:03.814467 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff
[0m15:43:03.815490 [info ] [Thread-1 (]: 15 of 71 START test not_null_fct_order_customer_id ............................. [RUN]
[0m15:43:03.816300 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6, now test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff)
[0m15:43:03.817838 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff
[0m15:43:03.823288 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff"
[0m15:43:03.830507 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff
[0m15:43:03.829461 [info ] [Thread-2 (]: 11 of 71 PASS not_null_dim_subscription_customer_id ............................ [[32mPASS[0m in 2.39s]
[0m15:43:03.840879 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723
[0m15:43:03.842069 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea
[0m15:43:03.845195 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff"
[0m15:43:03.843005 [info ] [Thread-2 (]: 16 of 71 START test not_null_fct_order_discount_total .......................... [RUN]
[0m15:43:03.850826 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723, now test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea)
[0m15:43:03.848994 [debug] [Thread-1 (]: On test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select customer_id
from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_order`
where customer_id is null



  
  
      
    ) dbt_internal_test
[0m15:43:03.852080 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea
[0m15:43:03.855864 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:43:03.874056 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea"
[0m15:43:03.918901 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea
[0m15:43:03.926243 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea"
[0m15:43:03.927814 [debug] [Thread-2 (]: On test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select discount_total
from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_order`
where discount_total is null



  
  
      
    ) dbt_internal_test
[0m15:43:03.928774 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m15:43:04.812484 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:85fe02f7-c42c-4f46-bdb7-fc14f03a4031&page=queryresults
[0m15:43:04.887752 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:fb0774a5-a389-4976-8145-659a8045881e&page=queryresults
[0m15:43:05.060583 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:560e6363-ac2c-4541-9a00-c932640a19a8&page=queryresults
[0m15:43:05.126724 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:93059932-fcbf-4e1d-9404-7ab04fab7d27&page=queryresults
[0m15:43:05.745624 [info ] [Thread-4 (]: 14 of 71 PASS not_null_fct_marketing_spend_spend_try ........................... [[32mPASS[0m in 2.04s]
[0m15:43:05.750936 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d
[0m15:43:05.753628 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e
[0m15:43:05.755023 [info ] [Thread-4 (]: 17 of 71 START test not_null_fct_order_net_revenue ............................. [RUN]
[0m15:43:05.757169 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d, now test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e)
[0m15:43:05.758991 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e
[0m15:43:05.781310 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e"
[0m15:43:05.782659 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e
[0m15:43:05.785659 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e"
[0m15:43:05.787200 [debug] [Thread-4 (]: On test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select net_revenue
from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_order`
where net_revenue is null



  
  
      
    ) dbt_internal_test
[0m15:43:05.787902 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m15:43:05.933538 [info ] [Thread-3 (]: 13 of 71 PASS not_null_fct_marketing_spend_date ................................ [[32mPASS[0m in 2.30s]
[0m15:43:05.938663 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719
[0m15:43:05.939671 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def
[0m15:43:05.940193 [info ] [Thread-3 (]: 18 of 71 START test not_null_fct_order_order_id ................................ [RUN]
[0m15:43:05.941052 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719, now test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def)
[0m15:43:05.941588 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def
[0m15:43:05.957109 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def"
[0m15:43:05.967762 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def
[0m15:43:05.971250 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def"
[0m15:43:05.972164 [debug] [Thread-3 (]: On test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select order_id
from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_order`
where order_id is null



  
  
      
    ) dbt_internal_test
[0m15:43:05.975825 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m15:43:06.055432 [info ] [Thread-1 (]: 15 of 71 PASS not_null_fct_order_customer_id ................................... [[32mPASS[0m in 2.24s]
[0m15:43:06.063165 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff
[0m15:43:06.066893 [info ] [Thread-2 (]: 16 of 71 PASS not_null_fct_order_discount_total ................................ [[32mPASS[0m in 2.22s]
[0m15:43:06.068432 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_fct_shipment_order_id.8b34a6bb78
[0m15:43:06.069771 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea
[0m15:43:06.071645 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d
[0m15:43:06.070736 [info ] [Thread-1 (]: 19 of 71 START test not_null_fct_shipment_order_id ............................. [RUN]
[0m15:43:06.074815 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff, now test.data_pipeline_project.not_null_fct_shipment_order_id.8b34a6bb78)
[0m15:43:06.075923 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_fct_shipment_order_id.8b34a6bb78
[0m15:43:06.072581 [info ] [Thread-2 (]: 20 of 71 START test not_null_fct_shipment_shipment_id .......................... [RUN]
[0m15:43:06.081694 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_fct_shipment_order_id.8b34a6bb78"
[0m15:43:06.083642 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea, now test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d)
[0m15:43:06.085693 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d
[0m15:43:06.091683 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d"
[0m15:43:06.093772 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d
[0m15:43:06.099911 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d"
[0m15:43:06.101063 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_fct_shipment_order_id.8b34a6bb78
[0m15:43:06.105916 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_fct_shipment_order_id.8b34a6bb78"
[0m15:43:06.108312 [debug] [Thread-2 (]: On test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select shipment_id
from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_shipment`
where shipment_id is null



  
  
      
    ) dbt_internal_test
[0m15:43:06.111629 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m15:43:06.110841 [debug] [Thread-1 (]: On test.data_pipeline_project.not_null_fct_shipment_order_id.8b34a6bb78: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_fct_shipment_order_id.8b34a6bb78"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select order_id
from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_shipment`
where order_id is null



  
  
      
    ) dbt_internal_test
[0m15:43:06.166748 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:43:07.037352 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:4f2aaf48-1fcc-4890-8ec0-906758d4d59b&page=queryresults
[0m15:43:07.135291 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:4313927d-a877-485e-8dcd-6d5bf79ab6b8&page=queryresults
[0m15:43:07.203839 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:7a5a4210-65f3-4b92-a72e-76cf56452fe5&page=queryresults
[0m15:43:07.297568 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:4093b441-87f3-4c80-8ead-3f09ba01f7ea&page=queryresults
[0m15:43:07.956899 [info ] [Thread-4 (]: 17 of 71 PASS not_null_fct_order_net_revenue ................................... [[32mPASS[0m in 2.20s]
[0m15:43:07.961990 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e
[0m15:43:07.965456 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b
[0m15:43:07.968797 [info ] [Thread-4 (]: 21 of 71 START test not_null_int_shipments_decomposed_delivered_at ............. [RUN]
[0m15:43:07.970731 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e, now test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b)
[0m15:43:07.971849 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b
[0m15:43:07.988400 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b"
[0m15:43:07.992800 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b
[0m15:43:08.000054 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b"
[0m15:43:08.000889 [debug] [Thread-4 (]: On test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select delivered_at
from `data-pipeline-project-474812`.`analytics_staging_staging`.`int_shipments_decomposed`
where delivered_at is null



  
  
      
    ) dbt_internal_test
[0m15:43:08.001449 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m15:43:08.046540 [info ] [Thread-3 (]: 18 of 71 PASS not_null_fct_order_order_id ...................................... [[32mPASS[0m in 2.10s]
[0m15:43:08.047936 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def
[0m15:43:08.051466 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31
[0m15:43:08.053942 [info ] [Thread-3 (]: 22 of 71 START test not_null_int_shipments_decomposed_latest_status ............ [RUN]
[0m15:43:08.056132 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def, now test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31)
[0m15:43:08.057457 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31
[0m15:43:08.067015 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31"
[0m15:43:08.068538 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31
[0m15:43:08.073103 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31"
[0m15:43:08.074542 [debug] [Thread-3 (]: On test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select latest_status
from `data-pipeline-project-474812`.`analytics_staging_staging`.`int_shipments_decomposed`
where latest_status is null



  
  
      
    ) dbt_internal_test
[0m15:43:08.075175 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m15:43:08.162804 [info ] [Thread-2 (]: 20 of 71 PASS not_null_fct_shipment_shipment_id ................................ [[32mPASS[0m in 2.07s]
[0m15:43:08.167588 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d
[0m15:43:08.170360 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b
[0m15:43:08.173048 [info ] [Thread-2 (]: 23 of 71 START test not_null_mart_acquisition_efficiency_date .................. [RUN]
[0m15:43:08.173905 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d, now test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b)
[0m15:43:08.174593 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b
[0m15:43:08.182288 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b"
[0m15:43:08.183588 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b
[0m15:43:08.187053 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b"
[0m15:43:08.187820 [debug] [Thread-2 (]: On test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select date
from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_acquisition_efficiency`
where date is null



  
  
      
    ) dbt_internal_test
[0m15:43:08.188336 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m15:43:08.438136 [info ] [Thread-1 (]: 19 of 71 PASS not_null_fct_shipment_order_id ................................... [[32mPASS[0m in 2.36s]
[0m15:43:08.445159 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_fct_shipment_order_id.8b34a6bb78
[0m15:43:08.448974 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23
[0m15:43:08.451533 [info ] [Thread-1 (]: 24 of 71 START test not_null_mart_revenue_daily_date ........................... [RUN]
[0m15:43:08.453153 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_fct_shipment_order_id.8b34a6bb78, now test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23)
[0m15:43:08.454206 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23
[0m15:43:08.472091 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23"
[0m15:43:08.474477 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23
[0m15:43:08.482434 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23"
[0m15:43:08.484157 [debug] [Thread-1 (]: On test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select date
from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_revenue_daily`
where date is null



  
  
      
    ) dbt_internal_test
[0m15:43:08.485754 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:43:09.317031 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:bface242-5db2-426f-a17e-adb451a71eb6&page=queryresults
[0m15:43:09.410390 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:ca90fcdf-bcd2-4d01-8c68-67d1af967e50&page=queryresults
[0m15:43:09.452614 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:cf155267-d72d-4aa2-8043-dd013af59e5d&page=queryresults
[0m15:43:09.609632 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:b950656c-a42b-4406-8db8-007f1e6557d6&page=queryresults
[0m15:43:10.402415 [info ] [Thread-3 (]: 22 of 71 PASS not_null_int_shipments_decomposed_latest_status .................. [[32mPASS[0m in 2.35s]
[0m15:43:10.404704 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31
[0m15:43:10.406948 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322
[0m15:43:10.408422 [info ] [Thread-3 (]: 25 of 71 START test not_null_mart_subscription_daily_date ...................... [RUN]
[0m15:43:10.409643 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31, now test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322)
[0m15:43:10.410833 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322
[0m15:43:10.417766 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322"
[0m15:43:10.419357 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322
[0m15:43:10.422206 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322"
[0m15:43:10.423888 [debug] [Thread-3 (]: On test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select date
from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily`
where date is null



  
  
      
    ) dbt_internal_test
[0m15:43:10.424553 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m15:43:10.494519 [info ] [Thread-2 (]: 23 of 71 PASS not_null_mart_acquisition_efficiency_date ........................ [[32mPASS[0m in 2.32s]
[0m15:43:10.496684 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b
[0m15:43:10.497898 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501
[0m15:43:10.498827 [info ] [Thread-2 (]: 26 of 71 START test not_null_stg_addresses_address_id .......................... [RUN]
[0m15:43:10.500364 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b, now test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501)
[0m15:43:10.501932 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501
[0m15:43:10.512716 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501"
[0m15:43:10.515249 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501
[0m15:43:10.525970 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501"
[0m15:43:10.527484 [debug] [Thread-2 (]: On test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select address_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_addresses`
where address_id is null



  
  
      
    ) dbt_internal_test
[0m15:43:10.530154 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m15:43:10.618840 [warn ] [Thread-4 (]: 21 of 71 WARN 303 not_null_int_shipments_decomposed_delivered_at ............... [[33mWARN 303[0m in 2.65s]
[0m15:43:10.623504 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b
[0m15:43:10.624363 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3
[0m15:43:10.626767 [info ] [Thread-4 (]: 27 of 71 START test not_null_stg_addresses_user_id ............................. [RUN]
[0m15:43:10.628919 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b, now test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3)
[0m15:43:10.632517 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3
[0m15:43:10.641564 [info ] [Thread-1 (]: 24 of 71 PASS not_null_mart_revenue_daily_date ................................. [[32mPASS[0m in 2.19s]
[0m15:43:10.643907 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3"
[0m15:43:10.645473 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23
[0m15:43:10.650416 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3
[0m15:43:10.656189 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3"
[0m15:43:10.652674 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2
[0m15:43:10.659785 [debug] [Thread-4 (]: On test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_addresses`
where user_id is null



  
  
      
    ) dbt_internal_test
[0m15:43:10.664386 [info ] [Thread-1 (]: 28 of 71 START test not_null_stg_countries_country_id .......................... [RUN]
[0m15:43:10.665617 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m15:43:10.669917 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23, now test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2)
[0m15:43:10.677200 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2
[0m15:43:10.722092 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2"
[0m15:43:10.724536 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2
[0m15:43:10.735020 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2"
[0m15:43:10.758835 [debug] [Thread-1 (]: On test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select country_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_countries`
where country_id is null



  
  
      
    ) dbt_internal_test
[0m15:43:10.759854 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:43:11.769030 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:8f1d2f53-65bf-4f4f-8479-6ee98aa8d448&page=queryresults
[0m15:43:11.872118 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:65ad599d-a18b-40bc-a9a9-75cfa2670435&page=queryresults
[0m15:43:11.880822 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:a07548b0-f567-42e0-a295-2b7085c8cd00&page=queryresults
[0m15:43:12.082492 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:700de20f-48b5-414d-b62a-05049d40f315&page=queryresults
[0m15:43:12.786387 [info ] [Thread-3 (]: 25 of 71 PASS not_null_mart_subscription_daily_date ............................ [[32mPASS[0m in 2.38s]
[0m15:43:12.788863 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322
[0m15:43:12.792016 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64
[0m15:43:12.793005 [info ] [Thread-3 (]: 29 of 71 START test not_null_stg_orders_order_id ............................... [RUN]
[0m15:43:12.794329 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322, now test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64)
[0m15:43:12.794915 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64
[0m15:43:12.801431 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64"
[0m15:43:12.805474 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64
[0m15:43:12.809634 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64"
[0m15:43:12.810608 [debug] [Thread-3 (]: On test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select order_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_orders`
where order_id is null



  
  
      
    ) dbt_internal_test
[0m15:43:12.814280 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m15:43:12.880135 [info ] [Thread-4 (]: 27 of 71 PASS not_null_stg_addresses_user_id ................................... [[32mPASS[0m in 2.25s]
[0m15:43:12.881852 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3
[0m15:43:12.882904 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209
[0m15:43:12.883858 [info ] [Thread-4 (]: 30 of 71 START test not_null_stg_orders_user_id ................................ [RUN]
[0m15:43:12.886287 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3, now test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209)
[0m15:43:12.886954 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209
[0m15:43:12.909601 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209"
[0m15:43:12.919925 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209
[0m15:43:12.925559 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209"
[0m15:43:12.931001 [debug] [Thread-4 (]: On test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_orders`
where user_id is null



  
  
      
    ) dbt_internal_test
[0m15:43:12.931533 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m15:43:12.929951 [info ] [Thread-2 (]: 26 of 71 PASS not_null_stg_addresses_address_id ................................ [[32mPASS[0m in 2.43s]
[0m15:43:12.973897 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501
[0m15:43:12.974641 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380
[0m15:43:12.979800 [info ] [Thread-2 (]: 31 of 71 START test not_null_stg_shipments_shipment_id ......................... [RUN]
[0m15:43:12.982270 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501, now test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380)
[0m15:43:12.984288 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380
[0m15:43:12.989172 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380"
[0m15:43:12.991629 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380
[0m15:43:12.996130 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380"
[0m15:43:12.997035 [debug] [Thread-2 (]: On test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select shipment_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_shipments`
where shipment_id is null



  
  
      
    ) dbt_internal_test
[0m15:43:12.997597 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m15:43:13.071373 [info ] [Thread-1 (]: 28 of 71 PASS not_null_stg_countries_country_id ................................ [[32mPASS[0m in 2.40s]
[0m15:43:13.078193 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2
[0m15:43:13.086517 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852
[0m15:43:13.089281 [info ] [Thread-1 (]: 32 of 71 START test not_null_stg_shipments_user_id ............................. [RUN]
[0m15:43:13.092587 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2, now test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852)
[0m15:43:13.094980 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852
[0m15:43:13.115536 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852"
[0m15:43:13.116333 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852
[0m15:43:13.133386 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852"
[0m15:43:13.142585 [debug] [Thread-1 (]: On test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_shipments`
where user_id is null



  
  
      
    ) dbt_internal_test
[0m15:43:13.143655 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:43:14.260083 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:87ad4ee2-edb1-4b0b-8132-e0be476247bf&page=queryresults
[0m15:43:14.273530 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:31ffb320-263b-40f2-93bc-d4c66ff4a49c&page=queryresults
[0m15:43:14.344045 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:74424ef1-cd88-407f-b6a3-d963535ceec0&page=queryresults
[0m15:43:14.572037 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:8b605d73-a23a-481e-959d-5efb1a68984f&page=queryresults
[0m15:43:15.190715 [info ] [Thread-4 (]: 30 of 71 PASS not_null_stg_orders_user_id ...................................... [[32mPASS[0m in 2.30s]
[0m15:43:15.193152 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209
[0m15:43:15.194061 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8
[0m15:43:15.195079 [info ] [Thread-4 (]: 33 of 71 START test not_null_stg_subscriptions_start_date ...................... [RUN]
[0m15:43:15.197393 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209, now test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8)
[0m15:43:15.198251 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8
[0m15:43:15.204789 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8"
[0m15:43:15.205749 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8
[0m15:43:15.209603 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8"
[0m15:43:15.210721 [debug] [Thread-4 (]: On test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select start_date
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_subscriptions`
where start_date is null



  
  
      
    ) dbt_internal_test
[0m15:43:15.211632 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m15:43:15.309088 [info ] [Thread-3 (]: 29 of 71 PASS not_null_stg_orders_order_id ..................................... [[32mPASS[0m in 2.50s]
[0m15:43:15.313781 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64
[0m15:43:15.314853 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m15:43:15.327293 [info ] [Thread-3 (]: 34 of 71 START test not_null_stg_subscriptions_subscription_id ................. [RUN]
[0m15:43:15.332102 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64, now test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91)
[0m15:43:15.321231 [info ] [Thread-2 (]: 31 of 71 PASS not_null_stg_shipments_shipment_id ............................... [[32mPASS[0m in 2.34s]
[0m15:43:15.333479 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m15:43:15.340646 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380
[0m15:43:15.355282 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2
[0m15:43:15.364609 [info ] [Thread-2 (]: 35 of 71 START test not_null_stg_subscriptions_user_id ......................... [RUN]
[0m15:43:15.365940 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380, now test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2)
[0m15:43:15.367256 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2
[0m15:43:15.384864 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2"
[0m15:43:15.395600 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2
[0m15:43:15.399733 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91"
[0m15:43:15.416121 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m15:43:15.421082 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2"
[0m15:43:15.433227 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91"
[0m15:43:15.437018 [debug] [Thread-2 (]: On test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_subscriptions`
where user_id is null



  
  
      
    ) dbt_internal_test
[0m15:43:15.440356 [debug] [Thread-3 (]: On test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select subscription_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_subscriptions`
where subscription_id is null



  
  
      
    ) dbt_internal_test
[0m15:43:15.441450 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m15:43:15.443105 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m15:43:15.601170 [info ] [Thread-1 (]: 32 of 71 PASS not_null_stg_shipments_user_id ................................... [[32mPASS[0m in 2.51s]
[0m15:43:15.607196 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852
[0m15:43:15.608385 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25
[0m15:43:15.609497 [info ] [Thread-1 (]: 36 of 71 START test not_null_stg_users_created_at .............................. [RUN]
[0m15:43:15.610758 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852, now test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25)
[0m15:43:15.611348 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25
[0m15:43:15.618221 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25"
[0m15:43:15.619015 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25
[0m15:43:15.626466 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25"
[0m15:43:15.648721 [debug] [Thread-1 (]: On test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select created_at
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
where created_at is null



  
  
      
    ) dbt_internal_test
[0m15:43:15.653765 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:43:16.531836 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:d8626a94-3ed6-4308-8a07-a83058c9858d&page=queryresults
[0m15:43:16.831743 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:bcf1278f-7040-4518-a0e3-26c1e0bf208a&page=queryresults
[0m15:43:16.844322 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:527a4bde-c234-4e6b-a85d-ba8ef37771b0&page=queryresults
[0m15:43:16.934396 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:1839e950-4a51-4597-8395-233b45f98a91&page=queryresults
[0m15:43:17.558352 [info ] [Thread-4 (]: 33 of 71 PASS not_null_stg_subscriptions_start_date ............................ [[32mPASS[0m in 2.36s]
[0m15:43:17.562951 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8
[0m15:43:17.565400 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77
[0m15:43:17.566809 [info ] [Thread-4 (]: 37 of 71 START test not_null_stg_users_user_id ................................. [RUN]
[0m15:43:17.568748 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8, now test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77)
[0m15:43:17.569712 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77
[0m15:43:17.586519 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77"
[0m15:43:17.588360 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77
[0m15:43:17.598837 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77"
[0m15:43:17.600856 [debug] [Thread-4 (]: On test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
where user_id is null



  
  
      
    ) dbt_internal_test
[0m15:43:17.601960 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m15:43:17.811442 [info ] [Thread-3 (]: 34 of 71 PASS not_null_stg_subscriptions_subscription_id ....................... [[32mPASS[0m in 2.48s]
[0m15:43:17.826877 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m15:43:17.831865 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29
[0m15:43:17.836453 [info ] [Thread-3 (]: 38 of 71 START test relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_  [RUN]
[0m15:43:17.845109 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91, now test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29)
[0m15:43:17.846817 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29
[0m15:43:17.848575 [info ] [Thread-2 (]: 35 of 71 PASS not_null_stg_subscriptions_user_id ............................... [[32mPASS[0m in 2.48s]
[0m15:43:17.882739 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2
[0m15:43:17.903344 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85
[0m15:43:17.892139 [info ] [Thread-1 (]: 36 of 71 PASS not_null_stg_users_created_at .................................... [[32mPASS[0m in 2.28s]
[0m15:43:17.929623 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25
[0m15:43:17.917229 [info ] [Thread-2 (]: 39 of 71 START test relationships_fct_order_customer_id__customer_id__ref_dim_customer_  [RUN]
[0m15:43:17.935210 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29"
[0m15:43:17.943673 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29
[0m15:43:17.942238 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2, now test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85)
[0m15:43:17.949435 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85
[0m15:43:17.940448 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.relationships_fct_shipment_order_id__order_id__ref_fct_order_.6096f95e39
[0m15:43:17.947764 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29"
[0m15:43:17.969737 [debug] [Thread-3 (]: On test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select customer_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_subscription`
    where customer_id is not null
),

parent as (
    select customer_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_customer`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m15:43:17.968406 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85"
[0m15:43:17.970898 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m15:43:17.958485 [info ] [Thread-1 (]: 40 of 71 START test relationships_fct_shipment_order_id__order_id__ref_fct_order_  [RUN]
[0m15:43:17.973853 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25, now test.data_pipeline_project.relationships_fct_shipment_order_id__order_id__ref_fct_order_.6096f95e39)
[0m15:43:17.974725 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.relationships_fct_shipment_order_id__order_id__ref_fct_order_.6096f95e39
[0m15:43:17.972411 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85
[0m15:43:17.991697 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85"
[0m15:43:17.999347 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_fct_shipment_order_id__order_id__ref_fct_order_.6096f95e39"
[0m15:43:18.015145 [debug] [Thread-2 (]: On test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select customer_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_order`
    where customer_id is not null
),

parent as (
    select customer_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_customer`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m15:43:18.021661 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m15:43:18.025936 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.relationships_fct_shipment_order_id__order_id__ref_fct_order_.6096f95e39
[0m15:43:18.164827 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_fct_shipment_order_id__order_id__ref_fct_order_.6096f95e39"
[0m15:43:18.167611 [debug] [Thread-1 (]: On test.data_pipeline_project.relationships_fct_shipment_order_id__order_id__ref_fct_order_.6096f95e39: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_fct_shipment_order_id__order_id__ref_fct_order_.6096f95e39"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select order_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_shipment`
    where order_id is not null
),

parent as (
    select order_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_order`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m15:43:18.168603 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:43:19.006778 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:5ed327fc-3bfe-4a38-b094-780eae12c315&page=queryresults
[0m15:43:19.286009 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:78edcfb8-fce2-453b-800a-6390d057cbeb&page=queryresults
[0m15:43:19.293242 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:516cf4df-c094-44ae-a98d-84e909f124b2&page=queryresults
[0m15:43:19.318524 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:d9abf30e-33b2-4c47-98d7-7c775b05156b&page=queryresults
[0m15:43:19.917884 [info ] [Thread-4 (]: 37 of 71 PASS not_null_stg_users_user_id ....................................... [[32mPASS[0m in 2.35s]
[0m15:43:19.919686 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77
[0m15:43:19.920628 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781
[0m15:43:19.921484 [info ] [Thread-4 (]: 41 of 71 START test relationships_stg_addresses_user_id__user_id__ref_stg_users_  [RUN]
[0m15:43:19.922691 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77, now test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781)
[0m15:43:19.923448 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781
[0m15:43:19.941235 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781"
[0m15:43:19.942963 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781
[0m15:43:19.950746 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781"
[0m15:43:19.956921 [debug] [Thread-4 (]: On test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select user_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_addresses`
    where user_id is not null
),

parent as (
    select user_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m15:43:19.960427 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m15:43:20.189922 [info ] [Thread-2 (]: 39 of 71 PASS relationships_fct_order_customer_id__customer_id__ref_dim_customer_  [[32mPASS[0m in 2.25s]
[0m15:43:20.191586 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85
[0m15:43:20.192282 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737
[0m15:43:20.193259 [info ] [Thread-2 (]: 42 of 71 START test relationships_stg_orders_user_id__user_id__ref_stg_users_ .. [RUN]
[0m15:43:20.194524 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85, now test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737)
[0m15:43:20.195120 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737
[0m15:43:20.200804 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737"
[0m15:43:20.203443 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737
[0m15:43:20.207950 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737"
[0m15:43:20.213803 [debug] [Thread-2 (]: On test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select user_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_orders`
    where user_id is not null
),

parent as (
    select user_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m15:43:20.214750 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m15:43:20.276828 [error] [Thread-1 (]: 40 of 71 FAIL 476 relationships_fct_shipment_order_id__order_id__ref_fct_order_  [[31mFAIL 476[0m in 2.30s]
[0m15:43:20.283729 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.relationships_fct_shipment_order_id__order_id__ref_fct_order_.6096f95e39
[0m15:43:20.284561 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8
[0m15:43:20.288072 [info ] [Thread-1 (]: 43 of 71 START test relationships_stg_shipments_user_id__user_id__ref_stg_users_  [RUN]
[0m15:43:20.290537 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_fct_shipment_order_id__order_id__ref_fct_order_.6096f95e39, now test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8)
[0m15:43:20.291444 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8
[0m15:43:20.316962 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8"
[0m15:43:20.320695 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8
[0m15:43:20.346048 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8"
[0m15:43:20.358368 [debug] [Thread-1 (]: On test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select user_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_shipments`
    where user_id is not null
),

parent as (
    select user_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m15:43:20.359146 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:43:20.356683 [info ] [Thread-3 (]: 38 of 71 PASS relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_  [[32mPASS[0m in 2.51s]
[0m15:43:20.372442 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29
[0m15:43:20.436825 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m15:43:20.442701 [info ] [Thread-3 (]: 44 of 71 START test relationships_stg_subscriptions_user_id__user_id__ref_stg_users_  [RUN]
[0m15:43:20.445899 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29, now test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e)
[0m15:43:20.448645 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m15:43:20.468055 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e"
[0m15:43:20.469793 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m15:43:20.473124 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e"
[0m15:43:20.474231 [debug] [Thread-3 (]: On test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select user_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_subscriptions`
    where user_id is not null
),

parent as (
    select user_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m15:43:20.475015 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m15:43:21.445323 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:94184813-6931-462d-a8ac-d3f6482459bd&page=queryresults
[0m15:43:21.745960 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:be96a6cc-228d-4d6c-88a6-9a8d419d56e4&page=queryresults
[0m15:43:21.767648 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:c79c7183-017d-44a8-b8e3-dc6bf489ef83&page=queryresults
[0m15:43:21.845293 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:e379fc82-a248-43fc-ac83-ce2718fe2c0a&page=queryresults
[0m15:43:22.411329 [info ] [Thread-4 (]: 41 of 71 PASS relationships_stg_addresses_user_id__user_id__ref_stg_users_ ..... [[32mPASS[0m in 2.49s]
[0m15:43:22.413412 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781
[0m15:43:22.415370 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6
[0m15:43:22.416341 [info ] [Thread-4 (]: 45 of 71 START test source_not_null_raw_data_addresses__id ..................... [RUN]
[0m15:43:22.418762 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781, now test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6)
[0m15:43:22.419982 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6
[0m15:43:22.429962 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6"
[0m15:43:22.431355 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6
[0m15:43:22.435250 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6"
[0m15:43:22.436120 [debug] [Thread-4 (]: On test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`addresses`
where _id is null



  
  
      
    ) dbt_internal_test
[0m15:43:22.437052 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m15:43:22.680932 [info ] [Thread-3 (]: 44 of 71 PASS relationships_stg_subscriptions_user_id__user_id__ref_stg_users_ . [[32mPASS[0m in 2.23s]
[0m15:43:22.683047 [info ] [Thread-2 (]: 42 of 71 PASS relationships_stg_orders_user_id__user_id__ref_stg_users_ ........ [[32mPASS[0m in 2.49s]
[0m15:43:22.685498 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m15:43:22.686653 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737
[0m15:43:22.687688 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc
[0m15:43:22.688902 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd
[0m15:43:22.690398 [info ] [Thread-3 (]: 46 of 71 START test source_not_null_raw_data_cities__id ........................ [RUN]
[0m15:43:22.691670 [info ] [Thread-2 (]: 47 of 71 START test source_not_null_raw_data_countries__id ..................... [RUN]
[0m15:43:22.699793 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737, now test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd)
[0m15:43:22.700657 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd
[0m15:43:22.693849 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e, now test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc)
[0m15:43:22.708800 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc
[0m15:43:22.722581 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd"
[0m15:43:22.729314 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc"
[0m15:43:22.733875 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd
[0m15:43:22.735041 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc
[0m15:43:22.742350 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd"
[0m15:43:22.749109 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc"
[0m15:43:22.751003 [debug] [Thread-2 (]: On test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`countries`
where _id is null



  
  
      
    ) dbt_internal_test
[0m15:43:22.755326 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m15:43:22.756550 [debug] [Thread-3 (]: On test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`cities`
where _id is null



  
  
      
    ) dbt_internal_test
[0m15:43:22.853530 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m15:43:23.006505 [info ] [Thread-1 (]: 43 of 71 PASS relationships_stg_shipments_user_id__user_id__ref_stg_users_ ..... [[32mPASS[0m in 2.72s]
[0m15:43:23.008624 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8
[0m15:43:23.009191 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be
[0m15:43:23.009621 [info ] [Thread-1 (]: 48 of 71 START test source_not_null_raw_data_neighborhoods__id ................. [RUN]
[0m15:43:23.011167 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8, now test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be)
[0m15:43:23.012013 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be
[0m15:43:23.020546 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be"
[0m15:43:23.026969 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be
[0m15:43:23.037075 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be"
[0m15:43:23.041293 [debug] [Thread-1 (]: On test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`neighborhoods`
where _id is null



  
  
      
    ) dbt_internal_test
[0m15:43:23.043315 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:43:23.734597 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:589602cf-1eb6-4fd0-9e26-2035408294b5&page=queryresults
[0m15:43:23.839979 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:1d601efc-66e7-452d-a877-6e42a4f53ba6&page=queryresults
[0m15:43:24.133611 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:39181055-b3c8-4d05-8b61-39bd957f8ba8&page=queryresults
[0m15:43:24.254919 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:83a7610c-c3ef-4752-8203-a389baeaf46b&page=queryresults
[0m15:43:24.692563 [info ] [Thread-2 (]: 47 of 71 PASS source_not_null_raw_data_countries__id ........................... [[32mPASS[0m in 1.99s]
[0m15:43:24.693960 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd
[0m15:43:24.694915 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878
[0m15:43:24.695755 [info ] [Thread-2 (]: 49 of 71 START test source_not_null_raw_data_orders__id ........................ [RUN]
[0m15:43:24.696791 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd, now test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878)
[0m15:43:24.697747 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878
[0m15:43:24.706729 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878"
[0m15:43:24.708181 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878
[0m15:43:24.713595 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878"
[0m15:43:24.714705 [debug] [Thread-2 (]: On test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`orders`
where _id is null



  
  
      
    ) dbt_internal_test
[0m15:43:24.715385 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m15:43:24.772943 [info ] [Thread-4 (]: 45 of 71 PASS source_not_null_raw_data_addresses__id ........................... [[32mPASS[0m in 2.35s]
[0m15:43:24.780533 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6
[0m15:43:24.783694 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f
[0m15:43:24.785492 [info ] [Thread-4 (]: 50 of 71 START test source_not_null_raw_data_orders__user ...................... [RUN]
[0m15:43:24.787237 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6, now test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f)
[0m15:43:24.788703 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f
[0m15:43:24.815449 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f"
[0m15:43:24.816968 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f
[0m15:43:24.826855 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f"
[0m15:43:24.829574 [debug] [Thread-4 (]: On test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _user
from `data-pipeline-project-474812`.`raw_data`.`orders`
where _user is null



  
  
      
    ) dbt_internal_test
[0m15:43:24.830577 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m15:43:25.187362 [info ] [Thread-3 (]: 46 of 71 PASS source_not_null_raw_data_cities__id .............................. [[32mPASS[0m in 2.49s]
[0m15:43:25.191309 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc
[0m15:43:25.192872 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2
[0m15:43:25.194760 [info ] [Thread-3 (]: 51 of 71 START test source_not_null_raw_data_shipments__id ..................... [RUN]
[0m15:43:25.195930 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc, now test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2)
[0m15:43:25.198553 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2
[0m15:43:25.208718 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2"
[0m15:43:25.211891 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2
[0m15:43:25.218852 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2"
[0m15:43:25.228801 [debug] [Thread-3 (]: On test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`shipments`
where _id is null



  
  
      
    ) dbt_internal_test
[0m15:43:25.230932 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m15:43:25.366073 [info ] [Thread-1 (]: 48 of 71 PASS source_not_null_raw_data_neighborhoods__id ....................... [[32mPASS[0m in 2.34s]
[0m15:43:25.371696 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be
[0m15:43:25.372735 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1
[0m15:43:25.378096 [info ] [Thread-1 (]: 52 of 71 START test source_not_null_raw_data_shipments__order .................. [RUN]
[0m15:43:25.386452 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be, now test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1)
[0m15:43:25.390632 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1
[0m15:43:25.405278 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1"
[0m15:43:25.410028 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1
[0m15:43:25.432377 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1"
[0m15:43:25.436508 [debug] [Thread-1 (]: On test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _order
from `data-pipeline-project-474812`.`raw_data`.`shipments`
where _order is null



  
  
      
    ) dbt_internal_test
[0m15:43:25.440076 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:43:26.064391 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:516c8084-14b7-4c15-8359-63eddad138a6&page=queryresults
[0m15:43:26.072368 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:c2d53f90-e590-4c8e-969f-4762acd60c20&page=queryresults
[0m15:43:26.492948 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:9c448e80-1582-4fa3-867c-5e807f3f3fa0&page=queryresults
[0m15:43:26.731987 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:886d11d6-b1d0-42a7-9ce1-536c5b069f02&page=queryresults
[0m15:43:27.069633 [info ] [Thread-4 (]: 50 of 71 PASS source_not_null_raw_data_orders__user ............................ [[32mPASS[0m in 2.28s]
[0m15:43:27.071272 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f
[0m15:43:27.072087 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98
[0m15:43:27.073800 [info ] [Thread-4 (]: 53 of 71 START test source_not_null_raw_data_shipments__user ................... [RUN]
[0m15:43:27.076001 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f, now test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98)
[0m15:43:27.078204 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98
[0m15:43:27.084689 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98"
[0m15:43:27.088871 [info ] [Thread-2 (]: 49 of 71 PASS source_not_null_raw_data_orders__id .............................. [[32mPASS[0m in 2.39s]
[0m15:43:27.092155 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98
[0m15:43:27.099868 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98"
[0m15:43:27.094753 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878
[0m15:43:27.102567 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86
[0m15:43:27.103770 [info ] [Thread-2 (]: 54 of 71 START test source_not_null_raw_data_states__id ........................ [RUN]
[0m15:43:27.105599 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878, now test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86)
[0m15:43:27.107703 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86
[0m15:43:27.119253 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86"
[0m15:43:27.121114 [debug] [Thread-4 (]: On test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _user
from `data-pipeline-project-474812`.`raw_data`.`shipments`
where _user is null



  
  
      
    ) dbt_internal_test
[0m15:43:27.122708 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86
[0m15:43:27.126528 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m15:43:27.131801 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86"
[0m15:43:27.181163 [debug] [Thread-2 (]: On test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`states`
where _id is null



  
  
      
    ) dbt_internal_test
[0m15:43:27.184361 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m15:43:27.444428 [info ] [Thread-3 (]: 51 of 71 PASS source_not_null_raw_data_shipments__id ........................... [[32mPASS[0m in 2.25s]
[0m15:43:27.451730 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2
[0m15:43:27.453096 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a
[0m15:43:27.455049 [info ] [Thread-3 (]: 55 of 71 START test source_not_null_raw_data_subscriptions__id ................. [RUN]
[0m15:43:27.458882 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2, now test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a)
[0m15:43:27.461341 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a
[0m15:43:27.487187 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a"
[0m15:43:27.490554 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a
[0m15:43:27.498882 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a"
[0m15:43:27.500823 [debug] [Thread-3 (]: On test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`subscriptions`
where _id is null



  
  
      
    ) dbt_internal_test
[0m15:43:27.502116 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m15:43:27.702279 [info ] [Thread-1 (]: 52 of 71 PASS source_not_null_raw_data_shipments__order ........................ [[32mPASS[0m in 2.32s]
[0m15:43:27.704397 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1
[0m15:43:27.705580 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6
[0m15:43:27.706642 [info ] [Thread-1 (]: 56 of 71 START test source_not_null_raw_data_subscriptions__user ............... [RUN]
[0m15:43:27.708542 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1, now test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6)
[0m15:43:27.709428 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6
[0m15:43:27.715847 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6"
[0m15:43:27.716877 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6
[0m15:43:27.722403 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6"
[0m15:43:27.723507 [debug] [Thread-1 (]: On test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _user
from `data-pipeline-project-474812`.`raw_data`.`subscriptions`
where _user is null



  
  
      
    ) dbt_internal_test
[0m15:43:27.724333 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:43:28.389562 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:5e066972-26d8-4cc0-9536-2ec85bc1eac2&page=queryresults
[0m15:43:28.459243 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:359faf91-3d23-4453-8446-5289e6cf8173&page=queryresults
[0m15:43:28.666417 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:cc67eade-a0e2-4424-854e-47150fddcb6d&page=queryresults
[0m15:43:28.875819 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:a9344aa3-9450-4515-b179-d97e03945d24&page=queryresults
[0m15:43:29.359429 [info ] [Thread-2 (]: 54 of 71 PASS source_not_null_raw_data_states__id .............................. [[32mPASS[0m in 2.25s]
[0m15:43:29.361956 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86
[0m15:43:29.362863 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b
[0m15:43:29.363632 [info ] [Thread-2 (]: 57 of 71 START test source_not_null_raw_data_users__id ......................... [RUN]
[0m15:43:29.364836 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86, now test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b)
[0m15:43:29.366195 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b
[0m15:43:29.386394 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b"
[0m15:43:29.388857 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b
[0m15:43:29.395467 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b"
[0m15:43:29.397528 [debug] [Thread-2 (]: On test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`users`
where _id is null



  
  
      
    ) dbt_internal_test
[0m15:43:29.398398 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m15:43:29.461496 [info ] [Thread-4 (]: 53 of 71 PASS source_not_null_raw_data_shipments__user ......................... [[32mPASS[0m in 2.38s]
[0m15:43:29.465512 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98
[0m15:43:29.468852 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1
[0m15:43:29.470689 [info ] [Thread-4 (]: 58 of 71 START test unique_dim_customer_customer_id ............................ [RUN]
[0m15:43:29.477040 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98, now test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1)
[0m15:43:29.479344 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1
[0m15:43:29.504865 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1"
[0m15:43:29.506559 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1
[0m15:43:29.511831 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1"
[0m15:43:29.514384 [debug] [Thread-4 (]: On test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select customer_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_customer`
  where customer_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m15:43:29.517033 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m15:43:29.600597 [info ] [Thread-3 (]: 55 of 71 PASS source_not_null_raw_data_subscriptions__id ....................... [[32mPASS[0m in 2.14s]
[0m15:43:29.602274 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a
[0m15:43:29.604511 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469
[0m15:43:29.606483 [info ] [Thread-3 (]: 59 of 71 START test unique_dim_subscription_subscription_id .................... [RUN]
[0m15:43:29.609055 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a, now test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469)
[0m15:43:29.609845 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469
[0m15:43:29.620338 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469"
[0m15:43:29.622085 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469
[0m15:43:29.627004 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469"
[0m15:43:29.628479 [debug] [Thread-3 (]: On test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select subscription_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_subscription`
  where subscription_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m15:43:29.629668 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m15:43:29.817720 [info ] [Thread-1 (]: 56 of 71 PASS source_not_null_raw_data_subscriptions__user ..................... [[32mPASS[0m in 2.11s]
[0m15:43:29.820685 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6
[0m15:43:29.827730 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.unique_fct_marketing_spend_date.e93d255723
[0m15:43:29.830672 [info ] [Thread-1 (]: 60 of 71 START test unique_fct_marketing_spend_date ............................ [RUN]
[0m15:43:29.834014 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6, now test.data_pipeline_project.unique_fct_marketing_spend_date.e93d255723)
[0m15:43:29.835444 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.unique_fct_marketing_spend_date.e93d255723
[0m15:43:29.853485 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.unique_fct_marketing_spend_date.e93d255723"
[0m15:43:29.857715 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.unique_fct_marketing_spend_date.e93d255723
[0m15:43:29.877193 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.unique_fct_marketing_spend_date.e93d255723"
[0m15:43:29.878609 [debug] [Thread-1 (]: On test.data_pipeline_project.unique_fct_marketing_spend_date.e93d255723: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_fct_marketing_spend_date.e93d255723"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select date as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_marketing_spend`
  where date is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m15:43:29.879304 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:43:30.687133 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:f5022241-97df-4308-9d34-a37959579184&page=queryresults
[0m15:43:30.829808 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:5ea0b14b-383a-4629-9b1e-64326f9ae354&page=queryresults
[0m15:43:30.998662 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:76863012-e56c-4cb8-b7e3-2270a3ec1191&page=queryresults
[0m15:43:31.232128 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:5f8ca656-3261-446d-bb46-8f0e7b504386&page=queryresults
[0m15:43:31.646718 [info ] [Thread-2 (]: 57 of 71 PASS source_not_null_raw_data_users__id ............................... [[32mPASS[0m in 2.28s]
[0m15:43:31.647861 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b
[0m15:43:31.648665 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.unique_fct_order_order_id.653705d6a5
[0m15:43:31.649598 [info ] [Thread-2 (]: 61 of 71 START test unique_fct_order_order_id .................................. [RUN]
[0m15:43:31.650642 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b, now test.data_pipeline_project.unique_fct_order_order_id.653705d6a5)
[0m15:43:31.651612 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.unique_fct_order_order_id.653705d6a5
[0m15:43:31.658875 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.unique_fct_order_order_id.653705d6a5"
[0m15:43:31.660570 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.unique_fct_order_order_id.653705d6a5
[0m15:43:31.665290 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.unique_fct_order_order_id.653705d6a5"
[0m15:43:31.667451 [debug] [Thread-2 (]: On test.data_pipeline_project.unique_fct_order_order_id.653705d6a5: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_fct_order_order_id.653705d6a5"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select order_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_order`
  where order_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m15:43:31.668321 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m15:43:31.863533 [info ] [Thread-3 (]: 59 of 71 PASS unique_dim_subscription_subscription_id .......................... [[32mPASS[0m in 2.25s]
[0m15:43:31.864831 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469
[0m15:43:31.865519 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3
[0m15:43:31.866522 [info ] [Thread-3 (]: 62 of 71 START test unique_fct_shipment_shipment_id ............................ [RUN]
[0m15:43:31.867663 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469, now test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3)
[0m15:43:31.868108 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3
[0m15:43:31.874689 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3"
[0m15:43:31.876320 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3
[0m15:43:31.881295 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3"
[0m15:43:31.882713 [debug] [Thread-3 (]: On test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select shipment_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_shipment`
  where shipment_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m15:43:31.883717 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m15:43:32.112012 [error] [Thread-4 (]: 58 of 71 FAIL 16 unique_dim_customer_customer_id ............................... [[31mFAIL 16[0m in 2.64s]
[0m15:43:32.113731 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1
[0m15:43:32.115741 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd
[0m15:43:32.116600 [info ] [Thread-4 (]: 63 of 71 START test unique_mart_acquisition_efficiency_date .................... [RUN]
[0m15:43:32.118776 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1, now test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd)
[0m15:43:32.119838 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd
[0m15:43:32.128499 [error] [Thread-1 (]: 60 of 71 FAIL 8 unique_fct_marketing_spend_date ................................ [[31mFAIL 8[0m in 2.29s]
[0m15:43:32.130152 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.unique_fct_marketing_spend_date.e93d255723
[0m15:43:32.132370 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8
[0m15:43:32.135788 [info ] [Thread-1 (]: 64 of 71 START test unique_mart_revenue_daily_date ............................. [RUN]
[0m15:43:32.134563 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd"
[0m15:43:32.137966 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_fct_marketing_spend_date.e93d255723, now test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8)
[0m15:43:32.140008 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd
[0m15:43:32.144127 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8
[0m15:43:32.150569 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd"
[0m15:43:32.158921 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8"
[0m15:43:32.161442 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8
[0m15:43:32.162629 [debug] [Thread-4 (]: On test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select date as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_acquisition_efficiency`
  where date is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m15:43:32.166422 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8"
[0m15:43:32.168541 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m15:43:32.217668 [debug] [Thread-1 (]: On test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select date as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_revenue_daily`
  where date is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m15:43:32.220058 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:43:32.944594 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:b7eccc5b-129e-4a48-8cee-b79e41f36543&page=queryresults
[0m15:43:33.071128 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:22b53e94-5a8e-4948-a99e-d386f871c5f3&page=queryresults
[0m15:43:33.374942 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:b7dac066-ce19-4194-86f3-d813a0a5353b&page=queryresults
[0m15:43:33.456103 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:dd9047c1-88d0-44e5-b124-1040e0ef46a4&page=queryresults
[0m15:43:34.059812 [info ] [Thread-2 (]: 61 of 71 PASS unique_fct_order_order_id ........................................ [[32mPASS[0m in 2.41s]
[0m15:43:34.062125 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.unique_fct_order_order_id.653705d6a5
[0m15:43:34.070613 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c
[0m15:43:34.069689 [info ] [Thread-3 (]: 62 of 71 PASS unique_fct_shipment_shipment_id .................................. [[32mPASS[0m in 2.20s]
[0m15:43:34.072652 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3
[0m15:43:34.073659 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00
[0m15:43:34.071487 [info ] [Thread-2 (]: 65 of 71 START test unique_mart_subscription_daily_date ........................ [RUN]
[0m15:43:34.074236 [info ] [Thread-3 (]: 66 of 71 START test unique_stg_addresses_address_id ............................ [RUN]
[0m15:43:34.076724 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_fct_order_order_id.653705d6a5, now test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c)
[0m15:43:34.077882 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3, now test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00)
[0m15:43:34.078803 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c
[0m15:43:34.079835 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00
[0m15:43:34.100233 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00"
[0m15:43:34.096876 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c"
[0m15:43:34.104590 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c
[0m15:43:34.102649 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00
[0m15:43:34.111861 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00"
[0m15:43:34.107656 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c"
[0m15:43:34.114028 [debug] [Thread-2 (]: On test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select date as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily`
  where date is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m15:43:34.115304 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m15:43:34.119816 [debug] [Thread-3 (]: On test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select address_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_addresses`
  where address_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m15:43:34.120873 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m15:43:34.305500 [info ] [Thread-4 (]: 63 of 71 PASS unique_mart_acquisition_efficiency_date .......................... [[32mPASS[0m in 2.19s]
[0m15:43:34.307575 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd
[0m15:43:34.308905 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.unique_stg_countries_country_id.8679936442
[0m15:43:34.310238 [info ] [Thread-4 (]: 67 of 71 START test unique_stg_countries_country_id ............................ [RUN]
[0m15:43:34.311646 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd, now test.data_pipeline_project.unique_stg_countries_country_id.8679936442)
[0m15:43:34.313226 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.unique_stg_countries_country_id.8679936442
[0m15:43:34.323467 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.unique_stg_countries_country_id.8679936442"
[0m15:43:34.344174 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.unique_stg_countries_country_id.8679936442
[0m15:43:34.343068 [info ] [Thread-1 (]: 64 of 71 PASS unique_mart_revenue_daily_date ................................... [[32mPASS[0m in 2.20s]
[0m15:43:34.352212 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8
[0m15:43:34.353078 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a
[0m15:43:34.355628 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.unique_stg_countries_country_id.8679936442"
[0m15:43:34.356671 [debug] [Thread-4 (]: On test.data_pipeline_project.unique_stg_countries_country_id.8679936442: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_stg_countries_country_id.8679936442"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select country_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_countries`
  where country_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m15:43:34.357467 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m15:43:34.353930 [info ] [Thread-1 (]: 68 of 71 START test unique_stg_orders_order_id ................................. [RUN]
[0m15:43:34.449875 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8, now test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a)
[0m15:43:34.454121 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a
[0m15:43:34.470806 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a"
[0m15:43:34.474654 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a
[0m15:43:34.480124 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a"
[0m15:43:34.481540 [debug] [Thread-1 (]: On test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select order_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_orders`
  where order_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m15:43:34.484008 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:43:35.504462 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:07e69282-cfc7-4610-83f5-78c61d6e177c&page=queryresults
[0m15:43:35.521864 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:2b096c1d-eb33-43b3-aaa1-2d6e4538b24a&page=queryresults
[0m15:43:35.693586 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:9032ac56-2e00-454f-a3e4-d7d781977e92&page=queryresults
[0m15:43:35.720400 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:68ca3096-4aef-4269-898e-b4f5140022a2&page=queryresults
[0m15:43:36.396255 [info ] [Thread-2 (]: 65 of 71 PASS unique_mart_subscription_daily_date .............................. [[32mPASS[0m in 2.32s]
[0m15:43:36.398283 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c
[0m15:43:36.399568 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c
[0m15:43:36.400601 [info ] [Thread-2 (]: 69 of 71 START test unique_stg_shipments_shipment_id ........................... [RUN]
[0m15:43:36.403178 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c, now test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c)
[0m15:43:36.404380 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c
[0m15:43:36.415465 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c"
[0m15:43:36.417802 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c
[0m15:43:36.421658 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c"
[0m15:43:36.423129 [debug] [Thread-2 (]: On test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select shipment_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_shipments`
  where shipment_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m15:43:36.424174 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m15:43:36.488347 [info ] [Thread-3 (]: 66 of 71 PASS unique_stg_addresses_address_id .................................. [[32mPASS[0m in 2.41s]
[0m15:43:36.490549 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00
[0m15:43:36.491592 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m15:43:36.492716 [info ] [Thread-3 (]: 70 of 71 START test unique_stg_subscriptions_subscription_id ................... [RUN]
[0m15:43:36.494730 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00, now test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e)
[0m15:43:36.495804 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m15:43:36.511998 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e"
[0m15:43:36.516222 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m15:43:36.514754 [info ] [Thread-4 (]: 67 of 71 PASS unique_stg_countries_country_id .................................. [[32mPASS[0m in 2.20s]
[0m15:43:36.531130 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.unique_stg_countries_country_id.8679936442
[0m15:43:36.537010 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b
[0m15:43:36.535831 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e"
[0m15:43:36.539020 [debug] [Thread-3 (]: On test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select subscription_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_subscriptions`
  where subscription_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m15:43:36.537583 [info ] [Thread-4 (]: 71 of 71 START test unique_stg_users_user_id ................................... [RUN]
[0m15:43:36.542761 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_stg_countries_country_id.8679936442, now test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b)
[0m15:43:36.544612 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m15:43:36.555657 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b
[0m15:43:36.644061 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b"
[0m15:43:36.646561 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b
[0m15:43:36.653741 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b"
[0m15:43:36.656748 [debug] [Thread-4 (]: On test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select user_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
  where user_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m15:43:36.658058 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m15:43:36.788883 [info ] [Thread-1 (]: 68 of 71 PASS unique_stg_orders_order_id ....................................... [[32mPASS[0m in 2.33s]
[0m15:43:36.796591 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a
[0m15:43:37.798092 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:ac51e984-510c-4d5a-a700-631f89f6d0db&page=queryresults
[0m15:43:37.818978 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:6f7e646d-970d-48c0-920b-50b07f17431c&page=queryresults
[0m15:43:38.018641 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:5e34ed94-4dff-452c-934d-a53c8c4b1e57&page=queryresults
[0m15:43:38.728516 [info ] [Thread-3 (]: 70 of 71 PASS unique_stg_subscriptions_subscription_id ......................... [[32mPASS[0m in 2.23s]
[0m15:43:38.732645 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m15:43:38.736502 [info ] [Thread-2 (]: 69 of 71 PASS unique_stg_shipments_shipment_id ................................. [[32mPASS[0m in 2.33s]
[0m15:43:38.739547 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c
[0m15:43:39.055046 [info ] [Thread-4 (]: 71 of 71 PASS unique_stg_users_user_id ......................................... [[32mPASS[0m in 2.51s]
[0m15:43:39.058008 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b
[0m15:43:39.068505 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:43:39.076591 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:43:39.079291 [debug] [MainThread]: Connection 'test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c' was properly closed.
[0m15:43:39.080449 [debug] [MainThread]: Connection 'test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a' was properly closed.
[0m15:43:39.082297 [debug] [MainThread]: Connection 'test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e' was properly closed.
[0m15:43:39.082861 [debug] [MainThread]: Connection 'test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b' was properly closed.
[0m15:43:39.084341 [info ] [MainThread]: 
[0m15:43:39.085501 [info ] [MainThread]: Finished running 71 data tests in 0 hours 0 minutes and 44.07 seconds (44.07s).
[0m15:43:39.105847 [debug] [MainThread]: Command end result
[0m15:43:39.214702 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m15:43:39.220541 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m15:43:39.239516 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m15:43:39.241026 [info ] [MainThread]: 
[0m15:43:39.245489 [info ] [MainThread]: [31mCompleted with 3 errors, 0 partial successes, and 1 warning:[0m
[0m15:43:39.246094 [info ] [MainThread]: 
[0m15:43:39.247296 [error] [MainThread]: [31mFailure in test relationships_fct_shipment_order_id__order_id__ref_fct_order_ (models/intermediate/int_facts_marts_schema.yml)[0m
[0m15:43:39.248728 [error] [MainThread]:   Got 476 results, configured to fail if != 0
[0m15:43:39.249667 [info ] [MainThread]: 
[0m15:43:39.250613 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_project/models/intermediate/int_facts_marts_schema.yml/relationships_fct_shipment_order_id__order_id__ref_fct_order_.sql
[0m15:43:39.251315 [info ] [MainThread]: 
[0m15:43:39.252436 [error] [MainThread]: [31mFailure in test unique_dim_customer_customer_id (models/intermediate/int_facts_marts_schema.yml)[0m
[0m15:43:39.253109 [error] [MainThread]:   Got 16 results, configured to fail if != 0
[0m15:43:39.254371 [info ] [MainThread]: 
[0m15:43:39.255529 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_project/models/intermediate/int_facts_marts_schema.yml/unique_dim_customer_customer_id.sql
[0m15:43:39.256980 [info ] [MainThread]: 
[0m15:43:39.259119 [error] [MainThread]: [31mFailure in test unique_fct_marketing_spend_date (models/intermediate/int_facts_marts_schema.yml)[0m
[0m15:43:39.260472 [error] [MainThread]:   Got 8 results, configured to fail if != 0
[0m15:43:39.261004 [info ] [MainThread]: 
[0m15:43:39.261515 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_project/models/intermediate/int_facts_marts_schema.yml/unique_fct_marketing_spend_date.sql
[0m15:43:39.261962 [info ] [MainThread]: 
[0m15:43:39.263093 [warn ] [MainThread]: [33mWarning in test not_null_int_shipments_decomposed_delivered_at (models/staging/staging_schema.yml)[0m
[0m15:43:39.264929 [warn ] [MainThread]: Got 303 results, configured to warn if != 0
[0m15:43:39.266732 [info ] [MainThread]: 
[0m15:43:39.268923 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_project/models/staging/staging_schema.yml/not_null_int_shipments_decomposed_delivered_at.sql
[0m15:43:39.270539 [info ] [MainThread]: 
[0m15:43:39.271831 [info ] [MainThread]: Done. PASS=67 WARN=1 ERROR=3 SKIP=0 NO-OP=0 TOTAL=71
[0m15:43:39.276414 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 47.54072, "process_in_blocks": "0", "process_kernel_time": 5.126602, "process_mem_max_rss": "395444", "process_out_blocks": "5624", "process_user_time": 16.520794}
[0m15:43:39.277531 [debug] [MainThread]: Command `dbt test` failed at 15:43:39.277378 after 47.54 seconds
[0m15:43:39.278191 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79b87081cb00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79b84c4297c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79b84c2ffe30>]}
[0m15:43:39.279136 [debug] [MainThread]: Flushing usage events
[0m15:43:40.187830 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:54:39.923318 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76e27df6d6d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76e27cf23ec0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76e27df1c7a0>]}


============================== 15:54:39.928564 | 0e662e1b-0284-464f-a524-9681bc6ab4c5 ==============================
[0m15:54:39.928564 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m15:54:39.930010 [debug] [MainThread]: running dbt with arguments {'empty': 'None', 'log_format': 'default', 'version_check': 'True', 'indirect_selection': 'eager', 'cache_selected_only': 'False', 'no_print': 'None', 'quiet': 'False', 'send_anonymous_usage_stats': 'True', 'partial_parse': 'True', 'debug': 'False', 'target_path': 'None', 'fail_fast': 'False', 'write_json': 'True', 'introspect': 'True', 'invocation_command': 'dbt test --select stg_users', 'use_colors': 'True', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'use_experimental_parser': 'False', 'profiles_dir': '/home/ecem/.dbt', 'log_cache_events': 'False', 'warn_error': 'None'}
[0m15:54:42.224566 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0e662e1b-0284-464f-a524-9681bc6ab4c5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76e25dcc8bf0>]}
[0m15:54:42.302324 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '0e662e1b-0284-464f-a524-9681bc6ab4c5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76e2606177d0>]}
[0m15:54:42.303578 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m15:54:42.580067 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m15:54:42.850167 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:54:42.851706 [debug] [MainThread]: Partial parsing: updated file: data_pipeline_project://models/intermediate/int_facts_marts_schema.yml
[0m15:54:43.308710 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0e662e1b-0284-464f-a524-9681bc6ab4c5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76e25c8d7f50>]}
[0m15:54:43.436819 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m15:54:43.439206 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m15:54:43.475397 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0e662e1b-0284-464f-a524-9681bc6ab4c5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76e25d530d70>]}
[0m15:54:43.476352 [info ] [MainThread]: Found 21 models, 68 data tests, 10 sources, 624 macros
[0m15:54:43.477104 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0e662e1b-0284-464f-a524-9681bc6ab4c5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76e25d4ed700>]}
[0m15:54:43.480468 [info ] [MainThread]: 
[0m15:54:43.481172 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:54:43.481948 [info ] [MainThread]: 
[0m15:54:43.482790 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m15:54:43.489899 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_staging'
[0m15:54:43.491351 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:54:43.553651 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_marts'
[0m15:54:43.560906 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:54:44.782955 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0e662e1b-0284-464f-a524-9681bc6ab4c5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76e25d96c260>]}
[0m15:54:44.784905 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:54:44.816845 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25
[0m15:54:44.819482 [info ] [Thread-1 (]: 1 of 7 START test not_null_stg_users_created_at ................................ [RUN]
[0m15:54:44.828514 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781
[0m15:54:44.826127 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77
[0m15:54:44.832788 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25)
[0m15:54:44.841211 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25
[0m15:54:44.833913 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737
[0m15:54:44.835071 [info ] [Thread-3 (]: 3 of 7 START test relationships_stg_addresses_user_id__user_id__ref_stg_users_ . [RUN]
[0m15:54:44.837786 [info ] [Thread-2 (]: 2 of 7 START test not_null_stg_users_user_id ................................... [RUN]
[0m15:54:44.850970 [info ] [Thread-4 (]: 4 of 7 START test relationships_stg_orders_user_id__user_id__ref_stg_users_ .... [RUN]
[0m15:54:44.881650 [debug] [Thread-3 (]: Acquiring new bigquery connection 'test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781'
[0m15:54:44.894058 [debug] [Thread-4 (]: Acquiring new bigquery connection 'test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737'
[0m15:54:44.885670 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_marts, now test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77)
[0m15:54:44.920255 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25"
[0m15:54:44.922995 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781
[0m15:54:44.936960 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77
[0m15:54:44.930015 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737
[0m15:54:44.950677 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25
[0m15:54:45.025093 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737"
[0m15:54:45.038240 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77"
[0m15:54:45.040696 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781"
[0m15:54:45.065496 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737
[0m15:54:45.068319 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25"
[0m15:54:45.072471 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77
[0m15:54:45.078562 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737"
[0m15:54:45.083192 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781
[0m15:54:45.084806 [debug] [Thread-1 (]: On test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select created_at
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
where created_at is null



  
  
      
    ) dbt_internal_test
[0m15:54:45.099518 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77"
[0m15:54:45.113841 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:54:45.114917 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781"
[0m15:54:45.119276 [debug] [Thread-2 (]: On test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
where user_id is null



  
  
      
    ) dbt_internal_test
[0m15:54:45.120869 [debug] [Thread-4 (]: On test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select user_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_orders`
    where user_id is not null
),

parent as (
    select user_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m15:54:45.178502 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m15:54:45.182110 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m15:54:45.283734 [debug] [Thread-3 (]: On test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select user_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_addresses`
    where user_id is not null
),

parent as (
    select user_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m15:54:45.285565 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m15:54:46.619613 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:720128da-58be-4efc-84c8-af4ce7b40875&page=queryresults
[0m15:54:46.722650 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:c52e671d-5958-4126-8a28-523477fbbb40&page=queryresults
[0m15:54:46.764701 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:254edc4a-2ed5-443e-978d-f37908ef8e33&page=queryresults
[0m15:54:46.944593 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:0da98c29-1ec0-41bf-9da2-e64f89ecc25d&page=queryresults
[0m15:54:47.773002 [info ] [Thread-2 (]: 2 of 7 PASS not_null_stg_users_user_id ......................................... [[32mPASS[0m in 2.89s]
[0m15:54:47.796768 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77
[0m15:54:47.813005 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8
[0m15:54:47.808820 [info ] [Thread-1 (]: 1 of 7 PASS not_null_stg_users_created_at ...................................... [[32mPASS[0m in 2.97s]
[0m15:54:47.806724 [info ] [Thread-4 (]: 4 of 7 PASS relationships_stg_orders_user_id__user_id__ref_stg_users_ .......... [[32mPASS[0m in 2.89s]
[0m15:54:47.827998 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25
[0m15:54:47.824338 [info ] [Thread-2 (]: 5 of 7 START test relationships_stg_shipments_user_id__user_id__ref_stg_users_ . [RUN]
[0m15:54:47.846541 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737
[0m15:54:47.844114 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77, now test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8)
[0m15:54:47.839438 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m15:54:47.854827 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b
[0m15:54:47.863784 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8
[0m15:54:47.868819 [info ] [Thread-1 (]: 6 of 7 START test relationships_stg_subscriptions_user_id__user_id__ref_stg_users_  [RUN]
[0m15:54:47.898709 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25, now test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e)
[0m15:54:47.900020 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m15:54:47.874578 [info ] [Thread-4 (]: 7 of 7 START test unique_stg_users_user_id ..................................... [RUN]
[0m15:54:47.908703 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8"
[0m15:54:47.914752 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737, now test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b)
[0m15:54:47.913698 [info ] [Thread-3 (]: 3 of 7 PASS relationships_stg_addresses_user_id__user_id__ref_stg_users_ ....... [[32mPASS[0m in 3.03s]
[0m15:54:47.920405 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e"
[0m15:54:47.923869 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b
[0m15:54:47.927846 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8
[0m15:54:47.929694 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781
[0m15:54:47.955971 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m15:54:47.942265 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8"
[0m15:54:47.970360 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e"
[0m15:54:47.997037 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b"
[0m15:54:48.006917 [debug] [Thread-1 (]: On test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select user_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_subscriptions`
    where user_id is not null
),

parent as (
    select user_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m15:54:48.008752 [debug] [Thread-2 (]: On test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select user_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_shipments`
    where user_id is not null
),

parent as (
    select user_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m15:54:48.013485 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b
[0m15:54:48.016089 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:54:48.020640 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m15:54:48.047038 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b"
[0m15:54:48.163568 [debug] [Thread-4 (]: On test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select user_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
  where user_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m15:54:48.164660 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m15:54:49.524698 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:4f88b6de-8608-49f9-b758-c9adb3395492&page=queryresults
[0m15:54:49.526464 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:5c72af02-d7a5-46cf-ad34-69c31aca3c16&page=queryresults
[0m15:54:49.527756 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:34e4ce6e-6bf4-47a1-8e11-0279461622cf&page=queryresults
[0m15:54:50.395614 [info ] [Thread-4 (]: 7 of 7 PASS unique_stg_users_user_id ........................................... [[32mPASS[0m in 2.48s]
[0m15:54:50.396693 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b
[0m15:54:50.402552 [info ] [Thread-1 (]: 6 of 7 PASS relationships_stg_subscriptions_user_id__user_id__ref_stg_users_ ... [[32mPASS[0m in 2.51s]
[0m15:54:50.404654 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m15:54:50.451835 [info ] [Thread-2 (]: 5 of 7 PASS relationships_stg_shipments_user_id__user_id__ref_stg_users_ ....... [[32mPASS[0m in 2.61s]
[0m15:54:50.456920 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8
[0m15:54:50.469448 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:54:50.474372 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:54:50.474921 [debug] [MainThread]: Connection 'test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e' was properly closed.
[0m15:54:50.475310 [debug] [MainThread]: Connection 'test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8' was properly closed.
[0m15:54:50.475846 [debug] [MainThread]: Connection 'test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781' was properly closed.
[0m15:54:50.476604 [debug] [MainThread]: Connection 'test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b' was properly closed.
[0m15:54:50.477056 [info ] [MainThread]: 
[0m15:54:50.477659 [info ] [MainThread]: Finished running 7 data tests in 0 hours 0 minutes and 6.99 seconds (6.99s).
[0m15:54:50.480418 [debug] [MainThread]: Command end result
[0m15:54:50.553962 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m15:54:50.557735 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m15:54:50.571830 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m15:54:50.574766 [info ] [MainThread]: 
[0m15:54:50.579410 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:54:50.581471 [info ] [MainThread]: 
[0m15:54:50.584785 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=7
[0m15:54:50.591079 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": true, "command_wall_clock_time": 10.728216, "process_in_blocks": "0", "process_kernel_time": 1.655616, "process_mem_max_rss": "393904", "process_out_blocks": "5728", "process_user_time": 6.592482}
[0m15:54:50.593092 [debug] [MainThread]: Command `dbt test` succeeded at 15:54:50.592505 after 10.73 seconds
[0m15:54:50.594470 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76e27de29f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76e25c726480>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76e25c8544a0>]}
[0m15:54:50.595149 [debug] [MainThread]: Flushing usage events
[0m15:54:51.519909 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:56:00.024787 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74fcae3a79e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74fcae082db0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74fcade19310>]}


============================== 15:56:00.030379 | b592253d-d89e-46ae-bbd2-bb30f06bbd54 ==============================
[0m15:56:00.030379 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m15:56:00.031599 [debug] [MainThread]: running dbt with arguments {'log_format': 'default', 'invocation_command': 'dbt test --select source:users', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'profiles_dir': '/home/ecem/.dbt', 'quiet': 'False', 'log_cache_events': 'False', 'target_path': 'None', 'no_print': 'None', 'send_anonymous_usage_stats': 'True', 'empty': 'None', 'write_json': 'True', 'debug': 'False', 'use_colors': 'True', 'fail_fast': 'False', 'static_parser': 'True', 'version_check': 'True', 'introspect': 'True', 'warn_error': 'None', 'cache_selected_only': 'False', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'printer_width': '80', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'partial_parse': 'True'}
[0m15:56:02.400173 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b592253d-d89e-46ae-bbd2-bb30f06bbd54', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74fcafdaba10>]}
[0m15:56:02.469754 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b592253d-d89e-46ae-bbd2-bb30f06bbd54', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74fcad550b90>]}
[0m15:56:02.471562 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m15:56:02.718786 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m15:56:03.008286 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:56:03.008809 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:56:03.055198 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b592253d-d89e-46ae-bbd2-bb30f06bbd54', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74fcadcc1160>]}
[0m15:56:03.165630 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m15:56:03.167750 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m15:56:03.207389 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b592253d-d89e-46ae-bbd2-bb30f06bbd54', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74fc8d88f140>]}
[0m15:56:03.209059 [info ] [MainThread]: Found 21 models, 68 data tests, 10 sources, 624 macros
[0m15:56:03.209671 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b592253d-d89e-46ae-bbd2-bb30f06bbd54', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74fc8da7cf20>]}
[0m15:56:03.210535 [warn ] [MainThread]: The selection criterion 'source:users' does not match any enabled nodes
[0m15:56:03.212946 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m15:56:03.217476 [debug] [MainThread]: Command end result
[0m15:56:03.275266 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m15:56:03.282668 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m15:56:03.287372 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m15:56:03.288806 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": true, "command_wall_clock_time": 3.334803, "process_in_blocks": "0", "process_kernel_time": 0.904198, "process_mem_max_rss": "372620", "process_out_blocks": "3848", "process_user_time": 4.151596}
[0m15:56:03.289692 [debug] [MainThread]: Command `dbt test` succeeded at 15:56:03.289583 after 3.34 seconds
[0m15:56:03.290307 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74fcadc82390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74fc8de3c950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74fc8e17b530>]}
[0m15:56:03.290827 [debug] [MainThread]: Flushing usage events
[0m15:56:04.224292 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:56:22.840876 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74fb1a091d30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74fb19890bc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74fb1b45c0b0>]}


============================== 15:56:22.844384 | b6876180-fbbe-41d3-994d-7f47257524ea ==============================
[0m15:56:22.844384 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m15:56:22.845369 [debug] [MainThread]: running dbt with arguments {'fail_fast': 'False', 'use_colors': 'True', 'warn_error': 'None', 'version_check': 'True', 'log_format': 'default', 'send_anonymous_usage_stats': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'indirect_selection': 'eager', 'profiles_dir': '/home/ecem/.dbt', 'log_cache_events': 'False', 'no_print': 'None', 'cache_selected_only': 'False', 'static_parser': 'True', 'invocation_command': 'dbt test --select users', 'target_path': 'None', 'quiet': 'False', 'use_experimental_parser': 'False', 'empty': 'None', 'partial_parse': 'True', 'write_json': 'True', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'printer_width': '80', 'debug': 'False'}
[0m15:56:25.096140 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b6876180-fbbe-41d3-994d-7f47257524ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74fb1ba2be30>]}
[0m15:56:25.162820 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b6876180-fbbe-41d3-994d-7f47257524ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74fafb1a3c20>]}
[0m15:56:25.166686 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m15:56:25.451688 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m15:56:25.728157 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:56:25.730316 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:56:25.777588 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b6876180-fbbe-41d3-994d-7f47257524ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74faf9b839e0>]}
[0m15:56:25.893445 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m15:56:25.897324 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m15:56:25.928705 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b6876180-fbbe-41d3-994d-7f47257524ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74faf944a540>]}
[0m15:56:25.929736 [info ] [MainThread]: Found 21 models, 68 data tests, 10 sources, 624 macros
[0m15:56:25.930588 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b6876180-fbbe-41d3-994d-7f47257524ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74faf956b2c0>]}
[0m15:56:25.932035 [warn ] [MainThread]: The selection criterion 'users' does not match any enabled nodes
[0m15:56:25.933790 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m15:56:25.935760 [debug] [MainThread]: Command end result
[0m15:56:25.982085 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m15:56:25.986639 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m15:56:25.993673 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m15:56:25.996545 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": true, "command_wall_clock_time": 3.2252364, "process_in_blocks": "0", "process_kernel_time": 1.136997, "process_mem_max_rss": "373004", "process_out_blocks": "3848", "process_user_time": 3.861289}
[0m15:56:25.998864 [debug] [MainThread]: Command `dbt test` succeeded at 15:56:25.998487 after 3.23 seconds
[0m15:56:25.999975 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74fb1a131fd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74faf9ba23f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74fb1a0a9af0>]}
[0m15:56:26.000452 [debug] [MainThread]: Flushing usage events
[0m15:56:26.761731 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:00:39.056417 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d9f3f8b980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d9f3a03470>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d9f573f980>]}


============================== 16:00:39.059806 | 10eaa32f-9279-44b8-ba9a-55be639bbeb7 ==============================
[0m16:00:39.059806 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m16:00:39.060707 [debug] [MainThread]: running dbt with arguments {'debug': 'False', 'use_experimental_parser': 'False', 'static_parser': 'True', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'log_cache_events': 'False', 'log_format': 'default', 'send_anonymous_usage_stats': 'True', 'profiles_dir': '/home/ecem/.dbt', 'indirect_selection': 'eager', 'write_json': 'True', 'empty': 'None', 'invocation_command': 'dbt test --select dim_customer', 'warn_error': 'None', 'cache_selected_only': 'False', 'partial_parse': 'True', 'printer_width': '80', 'fail_fast': 'False', 'quiet': 'False', 'use_colors': 'True', 'target_path': 'None', 'version_check': 'True', 'introspect': 'True', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])'}
[0m16:00:41.385822 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '10eaa32f-9279-44b8-ba9a-55be639bbeb7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d9d41530b0>]}
[0m16:00:41.450800 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '10eaa32f-9279-44b8-ba9a-55be639bbeb7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d9f3bed040>]}
[0m16:00:41.451776 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m16:00:41.664990 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m16:00:41.943969 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m16:00:41.946355 [debug] [MainThread]: Partial parsing: updated file: data_pipeline_project://models/intermediate/int_facts_marts_schema.yml
[0m16:00:41.947014 [debug] [MainThread]: Partial parsing: updated file: data_pipeline_project://models/marts/mart_subscription_daily.sql
[0m16:00:42.447047 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '10eaa32f-9279-44b8-ba9a-55be639bbeb7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d9d2dfe870>]}
[0m16:00:42.570942 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m16:00:42.575482 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m16:00:42.609105 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '10eaa32f-9279-44b8-ba9a-55be639bbeb7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d9d2e21100>]}
[0m16:00:42.610126 [info ] [MainThread]: Found 21 models, 68 data tests, 10 sources, 624 macros
[0m16:00:42.610937 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '10eaa32f-9279-44b8-ba9a-55be639bbeb7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d9d39e52b0>]}
[0m16:00:42.613686 [info ] [MainThread]: 
[0m16:00:42.614572 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m16:00:42.614961 [info ] [MainThread]: 
[0m16:00:42.615747 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m16:00:42.622843 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_staging'
[0m16:00:42.624632 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_marts'
[0m16:00:42.625736 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:00:42.626820 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:00:43.797145 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '10eaa32f-9279-44b8-ba9a-55be639bbeb7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d9f5e02540>]}
[0m16:00:43.801587 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:00:43.812284 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc
[0m16:00:43.813201 [info ] [Thread-1 (]: 1 of 4 START test not_null_dim_customer_customer_id ............................ [RUN]
[0m16:00:43.814207 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_marts, now test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc)
[0m16:00:43.815516 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29
[0m16:00:43.816336 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc
[0m16:00:43.817720 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85
[0m16:00:43.824738 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1
[0m16:00:43.834571 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc"
[0m16:00:43.818399 [info ] [Thread-2 (]: 2 of 4 START test relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_  [RUN]
[0m16:00:43.841545 [info ] [Thread-3 (]: 3 of 4 START test relationships_fct_order_customer_id__customer_id__ref_dim_customer_  [RUN]
[0m16:00:43.844564 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29)
[0m16:00:43.846209 [debug] [Thread-3 (]: Acquiring new bigquery connection 'test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85'
[0m16:00:43.843140 [info ] [Thread-4 (]: 4 of 4 START test unique_dim_customer_customer_id .............................. [RUN]
[0m16:00:43.850192 [debug] [Thread-4 (]: Acquiring new bigquery connection 'test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1'
[0m16:00:43.848130 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29
[0m16:00:43.849187 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85
[0m16:00:43.850907 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1
[0m16:00:43.847288 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc
[0m16:00:43.880382 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29"
[0m16:00:43.899395 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1"
[0m16:00:43.898585 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85"
[0m16:00:43.920816 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1
[0m16:00:43.945787 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85
[0m16:00:43.952817 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29
[0m16:00:44.017784 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1"
[0m16:00:44.021240 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85"
[0m16:00:44.023193 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29"
[0m16:00:44.030902 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc"
[0m16:00:44.041149 [debug] [Thread-1 (]: On test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select customer_id
from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_customer`
where customer_id is null



  
  
      
    ) dbt_internal_test
[0m16:00:44.039626 [debug] [Thread-3 (]: On test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select customer_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_order`
    where customer_id is not null
),

parent as (
    select customer_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_customer`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m16:00:44.034987 [debug] [Thread-4 (]: On test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select customer_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_customer`
  where customer_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m16:00:44.042685 [debug] [Thread-2 (]: On test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select customer_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_subscription`
    where customer_id is not null
),

parent as (
    select customer_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_customer`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m16:00:44.044846 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m16:00:44.043533 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m16:00:44.044288 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m16:00:44.041985 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:00:45.391367 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:766b6619-d179-4a65-adb7-5caee3bea7cb&page=queryresults
[0m16:00:45.443448 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:dc13f6d1-5a3e-4d7d-bf70-513c40bf807f&page=queryresults
[0m16:00:45.446015 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:e791ab92-ab3c-4c9c-a90d-492dc145c213&page=queryresults
[0m16:00:45.790776 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:e9bc2844-58fb-4d35-83e9-f3f0ec453968&page=queryresults
[0m16:00:46.373859 [info ] [Thread-1 (]: 1 of 4 PASS not_null_dim_customer_customer_id .................................. [[32mPASS[0m in 2.56s]
[0m16:00:46.375826 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc
[0m16:00:46.390172 [error] [Thread-4 (]: 4 of 4 FAIL 16 unique_dim_customer_customer_id ................................. [[31mFAIL 16[0m in 2.54s]
[0m16:00:46.391746 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1
[0m16:00:46.407067 [info ] [Thread-3 (]: 3 of 4 PASS relationships_fct_order_customer_id__customer_id__ref_dim_customer_  [[32mPASS[0m in 2.56s]
[0m16:00:46.410359 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85
[0m16:00:46.726175 [info ] [Thread-2 (]: 2 of 4 PASS relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_  [[32mPASS[0m in 2.88s]
[0m16:00:46.731506 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29
[0m16:00:46.743156 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m16:00:46.746445 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:00:46.747202 [debug] [MainThread]: Connection 'test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29' was properly closed.
[0m16:00:46.747748 [debug] [MainThread]: Connection 'test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc' was properly closed.
[0m16:00:46.748318 [debug] [MainThread]: Connection 'test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85' was properly closed.
[0m16:00:46.749562 [debug] [MainThread]: Connection 'test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1' was properly closed.
[0m16:00:46.750257 [info ] [MainThread]: 
[0m16:00:46.751215 [info ] [MainThread]: Finished running 4 data tests in 0 hours 0 minutes and 4.13 seconds (4.13s).
[0m16:00:46.754581 [debug] [MainThread]: Command end result
[0m16:00:46.819218 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m16:00:46.828530 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m16:00:46.843585 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m16:00:46.846654 [info ] [MainThread]: 
[0m16:00:46.849230 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m16:00:46.852015 [info ] [MainThread]: 
[0m16:00:46.853325 [error] [MainThread]: [31mFailure in test unique_dim_customer_customer_id (models/intermediate/int_facts_marts_schema.yml)[0m
[0m16:00:46.855337 [error] [MainThread]:   Got 16 results, configured to fail if != 0
[0m16:00:46.856181 [info ] [MainThread]: 
[0m16:00:46.858041 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_project/models/intermediate/int_facts_marts_schema.yml/unique_dim_customer_customer_id.sql
[0m16:00:46.860438 [info ] [MainThread]: 
[0m16:00:46.862412 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=4
[0m16:00:46.865023 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 7.8676963, "process_in_blocks": "0", "process_kernel_time": 1.608721, "process_mem_max_rss": "393492", "process_out_blocks": "5648", "process_user_time": 5.613225}
[0m16:00:46.865632 [debug] [MainThread]: Command `dbt test` failed at 16:00:46.865527 after 7.87 seconds
[0m16:00:46.866131 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d9f4213950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d9d2d3ee70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d9d2d3f110>]}
[0m16:00:46.866693 [debug] [MainThread]: Flushing usage events
[0m16:00:47.870135 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:07:52.396674 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x707c18233cb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x707c1857ef30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x707c1857d1f0>]}


============================== 16:07:52.400816 | e2278c9a-f043-4ecd-b90c-fa566d63fdea ==============================
[0m16:07:52.400816 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m16:07:52.405466 [debug] [MainThread]: running dbt with arguments {'target_path': 'None', 'printer_width': '80', 'indirect_selection': 'eager', 'use_colors': 'True', 'profiles_dir': '/home/ecem/.dbt', 'send_anonymous_usage_stats': 'True', 'fail_fast': 'False', 'warn_error': 'None', 'version_check': 'True', 'write_json': 'True', 'empty': 'None', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'log_format': 'default', 'use_experimental_parser': 'False', 'no_print': 'None', 'log_cache_events': 'False', 'cache_selected_only': 'False', 'introspect': 'True', 'invocation_command': 'dbt clean', 'partial_parse': 'True', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'static_parser': 'True', 'debug': 'False'}
[0m16:07:52.555605 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e2278c9a-f043-4ecd-b90c-fa566d63fdea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x707c182334a0>]}
[0m16:07:52.619382 [debug] [MainThread]: Resource report: {"command_name": "clean", "command_success": true, "command_wall_clock_time": 0.27596962, "process_in_blocks": "0", "process_kernel_time": 0.307616, "process_mem_max_rss": "101648", "process_out_blocks": "8", "process_user_time": 1.466211}
[0m16:07:52.620961 [debug] [MainThread]: Command `dbt clean` succeeded at 16:07:52.620728 after 0.28 seconds
[0m16:07:52.621553 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x707c17e17e30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x707c17bcc5f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x707c1a856e10>]}
[0m16:07:52.622162 [debug] [MainThread]: Flushing usage events
[0m16:07:53.618473 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:08:11.814761 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7666b726d850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7666b7125fa0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7666b6b1b680>]}


============================== 16:08:11.819126 | 0fe4af1b-9a00-4b37-8d42-93da456e2c89 ==============================
[0m16:08:11.819126 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m16:08:11.820140 [debug] [MainThread]: running dbt with arguments {'log_cache_events': 'False', 'write_json': 'True', 'target_path': 'None', 'debug': 'False', 'profiles_dir': '/home/ecem/.dbt', 'use_colors': 'True', 'introspect': 'True', 'no_print': 'None', 'printer_width': '80', 'fail_fast': 'False', 'use_experimental_parser': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'log_format': 'default', 'version_check': 'True', 'invocation_command': 'dbt run --select dim_customer --full-refresh', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'empty': 'False', 'static_parser': 'True', 'quiet': 'False', 'send_anonymous_usage_stats': 'True', 'warn_error': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'indirect_selection': 'eager'}
[0m16:08:14.190630 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0fe4af1b-9a00-4b37-8d42-93da456e2c89', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x766697e59ac0>]}
[0m16:08:14.256041 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '0fe4af1b-9a00-4b37-8d42-93da456e2c89', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7666b6982750>]}
[0m16:08:14.257843 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m16:08:14.495198 [error] [MainThread]: Encountered an error:
Compilation Error
  dbt found 1 package(s) specified in packages.yml, but only 0 package(s) installed in dbt_packages. Run "dbt deps" to install package dependencies.
[0m16:08:14.497051 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 2.7424395, "process_in_blocks": "0", "process_kernel_time": 0.937386, "process_mem_max_rss": "364516", "process_out_blocks": "8", "process_user_time": 3.572449}
[0m16:08:14.498160 [debug] [MainThread]: Command `dbt run` failed at 16:08:14.498049 after 2.74 seconds
[0m16:08:14.498657 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7666b70a7890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7666b61cab10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7666b630c3e0>]}
[0m16:08:14.499131 [debug] [MainThread]: Flushing usage events
[0m16:08:15.242104 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:09:17.361551 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7bfd25054860>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7bfd24bcc2f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7bfd24554b00>]}


============================== 16:09:17.380377 | 0da873c0-260f-4637-b62d-d11dbe145458 ==============================
[0m16:09:17.380377 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m16:09:17.384351 [debug] [MainThread]: running dbt with arguments {'debug': 'False', 'partial_parse': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_colors': 'True', 'no_print': 'None', 'profiles_dir': '/home/ecem/.dbt', 'cache_selected_only': 'False', 'send_anonymous_usage_stats': 'True', 'log_format': 'default', 'static_parser': 'True', 'warn_error': 'None', 'introspect': 'True', 'printer_width': '80', 'log_cache_events': 'False', 'version_check': 'True', 'fail_fast': 'False', 'indirect_selection': 'eager', 'invocation_command': 'dbt deps', 'quiet': 'False', 'use_experimental_parser': 'False', 'write_json': 'True', 'empty': 'None', 'target_path': 'None', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs'}
[0m16:09:17.572599 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0da873c0-260f-4637-b62d-d11dbe145458', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7bfd25054860>]}
[0m16:09:17.594642 [debug] [MainThread]: Set downloads directory='/tmp/dbt-downloads-8g1lg0dx'
[0m16:09:17.595754 [debug] [MainThread]: Making package index registry request: GET https://hub.getdbt.com/api/v1/index.json
[0m16:09:18.058767 [debug] [MainThread]: Response from registry index: GET https://hub.getdbt.com/api/v1/index.json 200
[0m16:09:18.067641 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
[0m16:09:18.306544 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
[0m16:09:18.336453 [info ] [MainThread]: Installing dbt-labs/dbt_utils
[0m16:09:19.205987 [info ] [MainThread]: Installed from version 1.3.1
[0m16:09:19.209555 [info ] [MainThread]: Up to date!
[0m16:09:19.211074 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': '0da873c0-260f-4637-b62d-d11dbe145458', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7bfd23dde480>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7bfd23c490d0>]}
[0m16:09:19.220165 [debug] [MainThread]: Resource report: {"command_name": "deps", "command_success": true, "command_wall_clock_time": 1.9413484, "process_in_blocks": "0", "process_kernel_time": 0.410573, "process_mem_max_rss": "103496", "process_out_blocks": "2288", "process_user_time": 1.77082}
[0m16:09:19.226905 [debug] [MainThread]: Command `dbt deps` succeeded at 16:09:19.226567 after 1.95 seconds
[0m16:09:19.228994 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7bfd24329d00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7bfd23c55160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7bfd24669670>]}
[0m16:09:19.230728 [debug] [MainThread]: Flushing usage events
[0m16:09:20.421318 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:09:24.601680 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71cfaf310800>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71cfaf0d7800>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71cfaf1ef560>]}


============================== 16:09:24.608773 | 3033ff9a-dcac-4ed0-89dc-3dcb5143a7e9 ==============================
[0m16:09:24.608773 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m16:09:24.613558 [debug] [MainThread]: running dbt with arguments {'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'quiet': 'False', 'printer_width': '80', 'indirect_selection': 'eager', 'partial_parse': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'invocation_command': 'dbt run --select dim_customer --full-refresh', 'no_print': 'None', 'use_colors': 'True', 'target_path': 'None', 'version_check': 'True', 'write_json': 'True', 'fail_fast': 'False', 'log_cache_events': 'False', 'debug': 'False', 'send_anonymous_usage_stats': 'True', 'cache_selected_only': 'False', 'use_experimental_parser': 'False', 'static_parser': 'True', 'introspect': 'True', 'profiles_dir': '/home/ecem/.dbt', 'warn_error': 'None', 'empty': 'False', 'log_format': 'default'}
[0m16:09:26.937663 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3033ff9a-dcac-4ed0-89dc-3dcb5143a7e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71cf8f3345f0>]}
[0m16:09:27.052334 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '3033ff9a-dcac-4ed0-89dc-3dcb5143a7e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71cf92144dd0>]}
[0m16:09:27.053385 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m16:09:27.309251 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m16:09:27.310704 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m16:09:27.311272 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '3033ff9a-dcac-4ed0-89dc-3dcb5143a7e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71cfaea19460>]}
[0m16:09:28.964633 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `relationships` defined on 'stg_orders' in
package 'data_pipeline_project' (models/staging/staging_schema.yml). Arguments
to generic tests should be nested under the `arguments` property.
[0m16:09:28.965650 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '3033ff9a-dcac-4ed0-89dc-3dcb5143a7e9', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71cf8ee43e00>]}
[0m16:09:29.456212 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3033ff9a-dcac-4ed0-89dc-3dcb5143a7e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71cf8e4e7380>]}
[0m16:09:29.596704 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m16:09:29.598992 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m16:09:29.616070 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3033ff9a-dcac-4ed0-89dc-3dcb5143a7e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71cf8e25a3f0>]}
[0m16:09:29.616660 [info ] [MainThread]: Found 21 models, 68 data tests, 10 sources, 624 macros
[0m16:09:29.617247 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3033ff9a-dcac-4ed0-89dc-3dcb5143a7e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71cf8e288470>]}
[0m16:09:29.619470 [info ] [MainThread]: 
[0m16:09:29.620029 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m16:09:29.620540 [info ] [MainThread]: 
[0m16:09:29.621233 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m16:09:29.623438 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812'
[0m16:09:29.625412 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:09:30.792171 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812, now list_data-pipeline-project-474812_analytics_staging_marts)
[0m16:09:30.795214 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_staging'
[0m16:09:30.795997 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m16:09:30.797546 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:09:32.129327 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3033ff9a-dcac-4ed0-89dc-3dcb5143a7e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71cf8e34c260>]}
[0m16:09:32.130358 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:09:32.140250 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.dim_customer
[0m16:09:32.145100 [info ] [Thread-1 (]: 1 of 1 START sql table model analytics_staging_marts.dim_customer .............. [RUN]
[0m16:09:32.148410 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_marts, now model.data_pipeline_project.dim_customer)
[0m16:09:32.149079 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.dim_customer
[0m16:09:32.164588 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.dim_customer"
[0m16:09:32.167011 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.dim_customer
[0m16:09:32.187755 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:09:33.524160 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.dim_customer"
[0m16:09:33.528616 [debug] [Thread-1 (]: On model.data_pipeline_project.dim_customer: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.dim_customer"} */

  
    

    create or replace table `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_customer`
      
    
    

    
    OPTIONS()
    as (
      


with users as (
    -- Start with the clean user base (PK: user_id)
    select distinct -- ADDED: This guarantees only one record per user_id in the dimension table
        user_id as customer_id,
        created_at
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
),

addresses as (
    -- Link users to their primary address/location (FK: user_id)
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_addresses`
),

neighborhoods as (
    -- Descriptive geography (FK: neighborhood_id, city_id, country_id)
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_neighborhoods`
),

cities as (
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_cities`
),

states as (
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_states`
),

countries as (
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_countries`
),

-- Join everything into a single, comprehensive dimension
final as (
    select
        u.user_id as customer_id, -- Rename PK to customer_id for the dim table
        u.created_at as signup_date,
     
        -- Address and Invoice Details
        a.invoice_type,
        
        -- Neighborhood Details
        n.neighborhood_name as neighborhood, 
        
        -- City, State, Country Details (The full hierarchy)
        c.city_name as city,
        s.state_name as state,
        co.country_name as country,

        n.postal_code
        
    from users u
    -- Join to get the address link (assuming a user has one primary address record for simplicity)
    left join addresses a
        on u.user_id = a.user_id
    -- Join up the geography hierarchy
    left join neighborhoods n
        on a.neighborhood_id = n.neighborhood_id
    left join cities c
        on n.city_id = c.city_id
    left join states s
        on c.state_id = s.state_id
    left join countries co
        on c.country_id = co.country_id
)

select * from final
    );
  
[0m16:09:34.457487 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:e97afd37-023a-4c72-be79-828fd99d8a2b&page=queryresults
[0m16:09:34.785441 [error] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:e97afd37-023a-4c72-be79-828fd99d8a2b&page=queryresults
[0m16:09:34.796233 [debug] [Thread-1 (]: Database Error in model dim_customer (models/dims/dim_customer.sql)
  Name user_id not found inside u at [69:14]
  compiled code at target/run/data_pipeline_project/models/dims/dim_customer.sql
[0m16:09:34.800005 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3033ff9a-dcac-4ed0-89dc-3dcb5143a7e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71cf8f46b740>]}
[0m16:09:34.801767 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model analytics_staging_marts.dim_customer ..... [[31mERROR[0m in 2.65s]
[0m16:09:34.802990 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.dim_customer
[0m16:09:34.804725 [debug] [Thread-7 (]: Marking all children of 'model.data_pipeline_project.dim_customer' to be skipped because of status 'error'.  Reason: Database Error in model dim_customer (models/dims/dim_customer.sql)
  Name user_id not found inside u at [69:14]
  compiled code at target/run/data_pipeline_project/models/dims/dim_customer.sql.
[0m16:09:34.809625 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m16:09:34.812278 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:09:34.813460 [debug] [MainThread]: Connection 'model.data_pipeline_project.dim_customer' was properly closed.
[0m16:09:34.814401 [debug] [MainThread]: Connection 'list_data-pipeline-project-474812_analytics_staging_staging' was properly closed.
[0m16:09:34.815382 [info ] [MainThread]: 
[0m16:09:34.815974 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 5.19 seconds (5.19s).
[0m16:09:34.817366 [debug] [MainThread]: Command end result
[0m16:09:34.893907 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m16:09:34.897751 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m16:09:34.909459 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m16:09:34.910065 [info ] [MainThread]: 
[0m16:09:34.912142 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m16:09:34.913576 [info ] [MainThread]: 
[0m16:09:34.914970 [error] [MainThread]: [31mFailure in model dim_customer (models/dims/dim_customer.sql)[0m
[0m16:09:34.916177 [error] [MainThread]:   Database Error in model dim_customer (models/dims/dim_customer.sql)
  Name user_id not found inside u at [69:14]
  compiled code at target/run/data_pipeline_project/models/dims/dim_customer.sql
[0m16:09:34.918489 [info ] [MainThread]: 
[0m16:09:34.920258 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_project/models/dims/dim_customer.sql
[0m16:09:34.922026 [info ] [MainThread]: 
[0m16:09:34.923054 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m16:09:34.924971 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 18 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m16:09:34.930369 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 10.397625, "process_in_blocks": "0", "process_kernel_time": 1.470463, "process_mem_max_rss": "386468", "process_out_blocks": "5528", "process_user_time": 6.266142}
[0m16:09:34.933298 [debug] [MainThread]: Command `dbt run` failed at 16:09:34.933029 after 10.40 seconds
[0m16:09:34.935421 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71cfaf84f560>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71cfaf89cbf0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71cfaf89dbb0>]}
[0m16:09:34.936801 [debug] [MainThread]: Flushing usage events
[0m16:09:35.657068 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:13:59.399411 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7825a8a8b5f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7825a8eb37a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7825aa8d69c0>]}


============================== 16:13:59.402616 | 127b976c-96c8-4365-a29a-4c7eb86cde85 ==============================
[0m16:13:59.402616 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m16:13:59.403627 [debug] [MainThread]: running dbt with arguments {'log_cache_events': 'False', 'send_anonymous_usage_stats': 'True', 'warn_error': 'None', 'profiles_dir': '/home/ecem/.dbt', 'use_colors': 'True', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'partial_parse': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'invocation_command': 'dbt run --select dim_customer --full-refresh', 'no_print': 'None', 'target_path': 'None', 'cache_selected_only': 'False', 'version_check': 'True', 'printer_width': '80', 'fail_fast': 'False', 'static_parser': 'True', 'use_experimental_parser': 'False', 'write_json': 'True', 'debug': 'False', 'empty': 'False', 'introspect': 'True', 'log_format': 'default', 'quiet': 'False', 'indirect_selection': 'eager'}
[0m16:14:01.643428 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '127b976c-96c8-4365-a29a-4c7eb86cde85', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7825aa7cbf50>]}
[0m16:14:01.733428 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '127b976c-96c8-4365-a29a-4c7eb86cde85', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7825aa6a2750>]}
[0m16:14:01.735112 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m16:14:02.061803 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m16:14:02.331008 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m16:14:02.332240 [debug] [MainThread]: Partial parsing: updated file: data_pipeline_project://models/dims/dim_subscription.sql
[0m16:14:02.332960 [debug] [MainThread]: Partial parsing: updated file: data_pipeline_project://models/facts/fct_order.sql
[0m16:14:02.333523 [debug] [MainThread]: Partial parsing: updated file: data_pipeline_project://models/dims/dim_customer.sql
[0m16:14:02.851383 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '127b976c-96c8-4365-a29a-4c7eb86cde85', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7825879ebe60>]}
[0m16:14:02.978768 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m16:14:02.982312 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m16:14:03.004916 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '127b976c-96c8-4365-a29a-4c7eb86cde85', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78258794d0a0>]}
[0m16:14:03.005697 [info ] [MainThread]: Found 21 models, 68 data tests, 10 sources, 624 macros
[0m16:14:03.006458 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '127b976c-96c8-4365-a29a-4c7eb86cde85', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7825879ff9b0>]}
[0m16:14:03.009086 [info ] [MainThread]: 
[0m16:14:03.009953 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m16:14:03.010653 [info ] [MainThread]: 
[0m16:14:03.011575 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m16:14:03.013087 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812'
[0m16:14:03.013852 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:14:03.970040 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812, now list_data-pipeline-project-474812_analytics_staging_marts)
[0m16:14:03.970840 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m16:14:04.003286 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_staging'
[0m16:14:04.004272 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:14:04.868032 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '127b976c-96c8-4365-a29a-4c7eb86cde85', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7825879bac00>]}
[0m16:14:04.868850 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:14:04.876745 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.dim_customer
[0m16:14:04.877828 [info ] [Thread-1 (]: 1 of 1 START sql table model analytics_staging_marts.dim_customer .............. [RUN]
[0m16:14:04.878781 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now model.data_pipeline_project.dim_customer)
[0m16:14:04.879591 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.dim_customer
[0m16:14:04.889421 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.dim_customer"
[0m16:14:04.890596 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.dim_customer
[0m16:14:04.913023 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:14:05.873801 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.dim_customer"
[0m16:14:05.878411 [debug] [Thread-1 (]: On model.data_pipeline_project.dim_customer: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.dim_customer"} */

  
    

    create or replace table `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_customer`
      
    
    

    
    OPTIONS()
    as (
      


with users as (
    -- Start with the clean user base (PK: user_id)
    select distinct -- ADDED: This guarantees only one record per user_id in the dimension table
        user_id,
        created_at
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
),

addresses as (
    -- Link users to their primary address/location (FK: user_id)
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_addresses`
),

neighborhoods as (
    -- Descriptive geography (FK: neighborhood_id, city_id, country_id)
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_neighborhoods`
),

cities as (
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_cities`
),

states as (
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_states`
),

countries as (
    select * from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_countries`
),

-- Join everything into a single, comprehensive dimension
final as (
    select
        u.user_id as customer_id, -- Rename PK to customer_id for the dim table
        u.created_at as signup_date,
     
        -- Address and Invoice Details
        a.invoice_type,
        
        -- Neighborhood Details
        n.neighborhood_name as neighborhood, 
        
        -- City, State, Country Details (The full hierarchy)
        c.city_name as city,
        s.state_name as state,
        co.country_name as country,

        n.postal_code
        
    from users u
    -- Join to get the address link (assuming a user has one primary address record for simplicity)
    left join addresses a
        on u.user_id = a.user_id
    -- Join up the geography hierarchy
    left join neighborhoods n
        on a.neighborhood_id = n.neighborhood_id
    left join cities c
        on n.city_id = c.city_id
    left join states s
        on c.state_id = s.state_id
    left join countries co
        on c.country_id = co.country_id
)

select * from final
    );
  
[0m16:14:06.720894 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:a414cf83-97bc-4c93-8403-44add1e5560f&page=queryresults
[0m16:14:10.151615 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '127b976c-96c8-4365-a29a-4c7eb86cde85', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7825883f2d80>]}
[0m16:14:10.156748 [info ] [Thread-1 (]: 1 of 1 OK created sql table model analytics_staging_marts.dim_customer ......... [[32mCREATE TABLE (4.5k rows, 8.3 MiB processed)[0m in 5.27s]
[0m16:14:10.157725 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.dim_customer
[0m16:14:10.161419 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m16:14:10.167876 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:14:10.168540 [debug] [MainThread]: Connection 'list_data-pipeline-project-474812_analytics_staging_marts' was properly closed.
[0m16:14:10.168909 [debug] [MainThread]: Connection 'model.data_pipeline_project.dim_customer' was properly closed.
[0m16:14:10.169539 [info ] [MainThread]: 
[0m16:14:10.170227 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 7.16 seconds (7.16s).
[0m16:14:10.171081 [debug] [MainThread]: Command end result
[0m16:14:10.236082 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m16:14:10.239524 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m16:14:10.250244 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m16:14:10.250804 [info ] [MainThread]: 
[0m16:14:10.251699 [info ] [MainThread]: [32mCompleted successfully[0m
[0m16:14:10.252184 [info ] [MainThread]: 
[0m16:14:10.252878 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m16:14:10.254171 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 10.919706, "process_in_blocks": "0", "process_kernel_time": 1.270985, "process_mem_max_rss": "387580", "process_out_blocks": "5592", "process_user_time": 4.998553}
[0m16:14:10.254717 [debug] [MainThread]: Command `dbt run` succeeded at 16:14:10.254617 after 10.92 seconds
[0m16:14:10.255168 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7825aa8d69c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78258794e3f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7825879ce360>]}
[0m16:14:10.255619 [debug] [MainThread]: Flushing usage events
[0m16:14:11.218764 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:15:40.893258 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x706a50fc1160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x706a5127e570>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x706a50fc11f0>]}


============================== 16:15:40.898494 | 55e9a583-3171-4093-b666-4b55296e303b ==============================
[0m16:15:40.898494 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m16:15:40.899388 [debug] [MainThread]: running dbt with arguments {'log_cache_events': 'False', 'indirect_selection': 'eager', 'partial_parse': 'True', 'warn_error': 'None', 'fail_fast': 'False', 'send_anonymous_usage_stats': 'True', 'introspect': 'True', 'debug': 'False', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'empty': 'None', 'invocation_command': 'dbt deps', 'target_path': 'None', 'log_format': 'default', 'no_print': 'None', 'static_parser': 'True', 'write_json': 'True', 'cache_selected_only': 'False', 'printer_width': '80', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'profiles_dir': '/home/ecem/.dbt', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])'}
[0m16:15:41.060918 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '55e9a583-3171-4093-b666-4b55296e303b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x706a5081f470>]}
[0m16:15:41.088782 [debug] [MainThread]: Set downloads directory='/tmp/dbt-downloads-agwiar8h'
[0m16:15:41.089344 [debug] [MainThread]: Making package index registry request: GET https://hub.getdbt.com/api/v1/index.json
[0m16:15:41.598741 [debug] [MainThread]: Response from registry index: GET https://hub.getdbt.com/api/v1/index.json 200
[0m16:15:41.608043 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
[0m16:15:41.801263 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
[0m16:15:41.807739 [info ] [MainThread]: Installing dbt-labs/dbt_utils
[0m16:15:42.484103 [info ] [MainThread]: Installed from version 1.3.1
[0m16:15:42.485261 [info ] [MainThread]: Up to date!
[0m16:15:42.486433 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': '55e9a583-3171-4093-b666-4b55296e303b', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x706a5060ba70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x706a52fd33e0>]}
[0m16:15:42.487845 [debug] [MainThread]: Resource report: {"command_name": "deps", "command_success": true, "command_wall_clock_time": 1.655176, "process_in_blocks": "0", "process_kernel_time": 0.369487, "process_mem_max_rss": "103280", "process_out_blocks": "2288", "process_user_time": 1.614795}
[0m16:15:42.488362 [debug] [MainThread]: Command `dbt deps` succeeded at 16:15:42.488239 after 1.66 seconds
[0m16:15:42.489055 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x706a5172a510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x706a50f4fef0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x706a52fd33e0>]}
[0m16:15:42.489616 [debug] [MainThread]: Flushing usage events
[0m16:15:43.631152 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:16:10.322290 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e16759b9070>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e167574d280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e1674e30fb0>]}


============================== 16:16:10.325506 | fb1feb0e-1f5c-4047-aa50-cb12904c5a84 ==============================
[0m16:16:10.325506 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m16:16:10.329784 [debug] [MainThread]: running dbt with arguments {'quiet': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'send_anonymous_usage_stats': 'True', 'use_experimental_parser': 'False', 'log_cache_events': 'False', 'write_json': 'True', 'printer_width': '80', 'cache_selected_only': 'False', 'version_check': 'True', 'fail_fast': 'False', 'empty': 'None', 'invocation_command': 'dbt test', 'target_path': 'None', 'warn_error': 'None', 'log_format': 'default', 'use_colors': 'True', 'partial_parse': 'True', 'no_print': 'None', 'introspect': 'True', 'debug': 'False', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'profiles_dir': '/home/ecem/.dbt', 'static_parser': 'True', 'indirect_selection': 'eager'}
[0m16:16:12.635409 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'fb1feb0e-1f5c-4047-aa50-cb12904c5a84', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e16556ad6a0>]}
[0m16:16:12.727755 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'fb1feb0e-1f5c-4047-aa50-cb12904c5a84', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e1676f765d0>]}
[0m16:16:12.731497 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m16:16:12.985456 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m16:16:13.284438 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:16:13.285323 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:16:13.339530 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fb1feb0e-1f5c-4047-aa50-cb12904c5a84', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e1655109460>]}
[0m16:16:13.454004 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m16:16:13.456664 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m16:16:13.485642 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fb1feb0e-1f5c-4047-aa50-cb12904c5a84', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e1655155100>]}
[0m16:16:13.486442 [info ] [MainThread]: Found 21 models, 68 data tests, 10 sources, 624 macros
[0m16:16:13.487008 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fb1feb0e-1f5c-4047-aa50-cb12904c5a84', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e16552714f0>]}
[0m16:16:13.491676 [info ] [MainThread]: 
[0m16:16:13.492596 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m16:16:13.493983 [info ] [MainThread]: 
[0m16:16:13.495081 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m16:16:13.501654 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_marts'
[0m16:16:13.503347 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_staging'
[0m16:16:13.504414 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:16:13.505355 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:16:14.628759 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fb1feb0e-1f5c-4047-aa50-cb12904c5a84', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e1675775cd0>]}
[0m16:16:14.629868 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:16:14.640097 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.dbt_utils_accepted_range_fct_marketing_spend_spend_try__0.bfe523cb61
[0m16:16:14.642293 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.dbt_utils_accepted_range_fct_order_discount_total__0.5f31954cd6
[0m16:16:14.643713 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.dbt_utils_accepted_range_fct_order_net_revenue__0.cb846760ce
[0m16:16:14.640936 [info ] [Thread-1 (]: 1 of 68 START test dbt_utils_accepted_range_fct_marketing_spend_spend_try__0 ... [RUN]
[0m16:16:14.648078 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now test.data_pipeline_project.dbt_utils_accepted_range_fct_marketing_spend_spend_try__0.bfe523cb61)
[0m16:16:14.644340 [info ] [Thread-2 (]: 2 of 68 START test dbt_utils_accepted_range_fct_order_discount_total__0 ........ [RUN]
[0m16:16:14.646609 [info ] [Thread-3 (]: 3 of 68 START test dbt_utils_accepted_range_fct_order_net_revenue__0 ........... [RUN]
[0m16:16:14.645501 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.dbt_utils_accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0.be1f75eba9
[0m16:16:14.649449 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.dbt_utils_accepted_range_fct_marketing_spend_spend_try__0.bfe523cb61
[0m16:16:14.650506 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_marts, now test.data_pipeline_project.dbt_utils_accepted_range_fct_order_discount_total__0.5f31954cd6)
[0m16:16:14.651344 [debug] [Thread-3 (]: Acquiring new bigquery connection 'test.data_pipeline_project.dbt_utils_accepted_range_fct_order_net_revenue__0.cb846760ce'
[0m16:16:14.653647 [info ] [Thread-4 (]: 4 of 68 START test dbt_utils_accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0  [RUN]
[0m16:16:14.666913 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.dbt_utils_accepted_range_fct_order_discount_total__0.5f31954cd6
[0m16:16:14.676959 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.dbt_utils_accepted_range_fct_order_net_revenue__0.cb846760ce
[0m16:16:14.683165 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.dbt_utils_accepted_range_fct_marketing_spend_spend_try__0.bfe523cb61"
[0m16:16:14.685774 [debug] [Thread-4 (]: Acquiring new bigquery connection 'test.data_pipeline_project.dbt_utils_accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0.be1f75eba9'
[0m16:16:14.704474 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.dbt_utils_accepted_range_fct_order_discount_total__0.5f31954cd6"
[0m16:16:14.710149 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.dbt_utils_accepted_range_fct_order_net_revenue__0.cb846760ce"
[0m16:16:14.721533 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.dbt_utils_accepted_range_fct_order_net_revenue__0.cb846760ce
[0m16:16:14.718756 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.dbt_utils_accepted_range_fct_marketing_spend_spend_try__0.bfe523cb61
[0m16:16:14.714246 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.dbt_utils_accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0.be1f75eba9
[0m16:16:14.766028 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.dbt_utils_accepted_range_fct_order_discount_total__0.5f31954cd6
[0m16:16:14.793913 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.dbt_utils_accepted_range_fct_order_net_revenue__0.cb846760ce"
[0m16:16:14.800033 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.dbt_utils_accepted_range_fct_marketing_spend_spend_try__0.bfe523cb61"
[0m16:16:14.804177 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.dbt_utils_accepted_range_fct_order_discount_total__0.5f31954cd6"
[0m16:16:14.806965 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.dbt_utils_accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0.be1f75eba9"
[0m16:16:14.810012 [debug] [Thread-1 (]: On test.data_pipeline_project.dbt_utils_accepted_range_fct_marketing_spend_spend_try__0.bfe523cb61: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.dbt_utils_accepted_range_fct_marketing_spend_spend_try__0.bfe523cb61"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  

with meet_condition as(
  select *
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_marketing_spend`
),

validation_errors as (
  select *
  from meet_condition
  where
    -- never true, defaults to an empty result set. Exists to ensure any combo of the `or` clauses below succeeds
    1 = 2
    -- records with a value >= min_value are permitted. The `not` flips this to find records that don't meet the rule.
    or not spend_try >= 0
)

select *
from validation_errors


  
  
      
    ) dbt_internal_test
[0m16:16:14.810749 [debug] [Thread-3 (]: On test.data_pipeline_project.dbt_utils_accepted_range_fct_order_net_revenue__0.cb846760ce: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.dbt_utils_accepted_range_fct_order_net_revenue__0.cb846760ce"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  

with meet_condition as(
  select *
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_order`
),

validation_errors as (
  select *
  from meet_condition
  where
    -- never true, defaults to an empty result set. Exists to ensure any combo of the `or` clauses below succeeds
    1 = 2
    -- records with a value >= min_value are permitted. The `not` flips this to find records that don't meet the rule.
    or not net_revenue >= 0
)

select *
from validation_errors


  
  
      
    ) dbt_internal_test
[0m16:16:14.813105 [debug] [Thread-2 (]: On test.data_pipeline_project.dbt_utils_accepted_range_fct_order_discount_total__0.5f31954cd6: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.dbt_utils_accepted_range_fct_order_discount_total__0.5f31954cd6"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  

with meet_condition as(
  select *
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_order`
),

validation_errors as (
  select *
  from meet_condition
  where
    -- never true, defaults to an empty result set. Exists to ensure any combo of the `or` clauses below succeeds
    1 = 2
    -- records with a value >= min_value are permitted. The `not` flips this to find records that don't meet the rule.
    or not discount_total >= 0
)

select *
from validation_errors


  
  
      
    ) dbt_internal_test
[0m16:16:14.815844 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:16:14.817008 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.dbt_utils_accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0.be1f75eba9
[0m16:16:14.819291 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m16:16:14.822038 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m16:16:14.830065 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.dbt_utils_accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0.be1f75eba9"
[0m16:16:14.945885 [debug] [Thread-4 (]: On test.data_pipeline_project.dbt_utils_accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0.be1f75eba9: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.dbt_utils_accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0.be1f75eba9"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  

with meet_condition as(
  select *
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_acquisition_efficiency`
),

validation_errors as (
  select *
  from meet_condition
  where
    -- never true, defaults to an empty result set. Exists to ensure any combo of the `or` clauses below succeeds
    1 = 2
    -- records with a value >= min_value are permitted. The `not` flips this to find records that don't meet the rule.
    or not daily_customer_acquisition_cost >= 0
)

select *
from validation_errors


  
  
      
    ) dbt_internal_test
[0m16:16:14.946762 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m16:16:16.047334 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:21ddafda-70cc-4872-b92c-23bd9d941a3a&page=queryresults
[0m16:16:16.091699 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:332cbca6-c0c3-4229-86eb-2cc955b0c7fa&page=queryresults
[0m16:16:16.260659 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:ae5c808a-d54c-400d-a8bd-30194b5a5dd2&page=queryresults
[0m16:16:16.368543 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:64a01b37-a2bb-4d8c-8346-34e669dd0ddc&page=queryresults
[0m16:16:17.006430 [info ] [Thread-2 (]: 2 of 68 PASS dbt_utils_accepted_range_fct_order_discount_total__0 .............. [[32mPASS[0m in 2.35s]
[0m16:16:17.009818 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.dbt_utils_accepted_range_fct_order_discount_total__0.5f31954cd6
[0m16:16:17.011460 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_cac_payback_months__0.440e9fbb47
[0m16:16:17.012407 [info ] [Thread-2 (]: 5 of 68 START test dbt_utils_accepted_range_mart_kpis_latest_cac_payback_months__0  [RUN]
[0m16:16:17.015450 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.dbt_utils_accepted_range_fct_order_discount_total__0.5f31954cd6, now test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_cac_payback_months__0.440e9fbb47)
[0m16:16:17.016546 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_cac_payback_months__0.440e9fbb47
[0m16:16:17.021688 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_cac_payback_months__0.440e9fbb47"
[0m16:16:17.023522 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_cac_payback_months__0.440e9fbb47
[0m16:16:17.026690 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_cac_payback_months__0.440e9fbb47"
[0m16:16:17.028049 [debug] [Thread-2 (]: On test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_cac_payback_months__0.440e9fbb47: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_cac_payback_months__0.440e9fbb47"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  

with meet_condition as(
  select *
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_kpis_latest`
),

validation_errors as (
  select *
  from meet_condition
  where
    -- never true, defaults to an empty result set. Exists to ensure any combo of the `or` clauses below succeeds
    1 = 2
    -- records with a value >= min_value are permitted. The `not` flips this to find records that don't meet the rule.
    or not cac_payback_months >= 0
)

select *
from validation_errors


  
  
      
    ) dbt_internal_test
[0m16:16:17.031845 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m16:16:17.110091 [info ] [Thread-3 (]: 3 of 68 PASS dbt_utils_accepted_range_fct_order_net_revenue__0 ................. [[32mPASS[0m in 2.46s]
[0m16:16:17.111571 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.dbt_utils_accepted_range_fct_order_net_revenue__0.cb846760ce
[0m16:16:17.112483 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_monthly_recurring_revenue__0.e7fa0e4f46
[0m16:16:17.113412 [info ] [Thread-3 (]: 6 of 68 START test dbt_utils_accepted_range_mart_kpis_latest_monthly_recurring_revenue__0  [RUN]
[0m16:16:17.116866 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.dbt_utils_accepted_range_fct_order_net_revenue__0.cb846760ce, now test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_monthly_recurring_revenue__0.e7fa0e4f46)
[0m16:16:17.121169 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_monthly_recurring_revenue__0.e7fa0e4f46
[0m16:16:17.126535 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_monthly_recurring_revenue__0.e7fa0e4f46"
[0m16:16:17.127551 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_monthly_recurring_revenue__0.e7fa0e4f46
[0m16:16:17.131720 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_monthly_recurring_revenue__0.e7fa0e4f46"
[0m16:16:17.133106 [debug] [Thread-3 (]: On test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_monthly_recurring_revenue__0.e7fa0e4f46: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_monthly_recurring_revenue__0.e7fa0e4f46"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  

with meet_condition as(
  select *
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_kpis_latest`
),

validation_errors as (
  select *
  from meet_condition
  where
    -- never true, defaults to an empty result set. Exists to ensure any combo of the `or` clauses below succeeds
    1 = 2
    -- records with a value >= min_value are permitted. The `not` flips this to find records that don't meet the rule.
    or not monthly_recurring_revenue >= 0
)

select *
from validation_errors


  
  
      
    ) dbt_internal_test
[0m16:16:17.133720 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m16:16:17.219393 [info ] [Thread-1 (]: 1 of 68 PASS dbt_utils_accepted_range_fct_marketing_spend_spend_try__0 ......... [[32mPASS[0m in 2.57s]
[0m16:16:17.269776 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.dbt_utils_accepted_range_fct_marketing_spend_spend_try__0.bfe523cb61
[0m16:16:17.272614 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.dbt_utils_accepted_range_mart_revenue_daily_daily_revenue__0.611ebe5f4e
[0m16:16:17.274413 [info ] [Thread-1 (]: 7 of 68 START test dbt_utils_accepted_range_mart_revenue_daily_daily_revenue__0  [RUN]
[0m16:16:17.276545 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.dbt_utils_accepted_range_fct_marketing_spend_spend_try__0.bfe523cb61, now test.data_pipeline_project.dbt_utils_accepted_range_mart_revenue_daily_daily_revenue__0.611ebe5f4e)
[0m16:16:17.277376 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.dbt_utils_accepted_range_mart_revenue_daily_daily_revenue__0.611ebe5f4e
[0m16:16:17.282215 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.dbt_utils_accepted_range_mart_revenue_daily_daily_revenue__0.611ebe5f4e"
[0m16:16:17.285702 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.dbt_utils_accepted_range_mart_revenue_daily_daily_revenue__0.611ebe5f4e
[0m16:16:17.289839 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.dbt_utils_accepted_range_mart_revenue_daily_daily_revenue__0.611ebe5f4e"
[0m16:16:17.290784 [debug] [Thread-1 (]: On test.data_pipeline_project.dbt_utils_accepted_range_mart_revenue_daily_daily_revenue__0.611ebe5f4e: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.dbt_utils_accepted_range_mart_revenue_daily_daily_revenue__0.611ebe5f4e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  

with meet_condition as(
  select *
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_revenue_daily`
),

validation_errors as (
  select *
  from meet_condition
  where
    -- never true, defaults to an empty result set. Exists to ensure any combo of the `or` clauses below succeeds
    1 = 2
    -- records with a value >= min_value are permitted. The `not` flips this to find records that don't meet the rule.
    or not daily_revenue >= 0
)

select *
from validation_errors


  
  
      
    ) dbt_internal_test
[0m16:16:17.291390 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:16:17.522846 [info ] [Thread-4 (]: 4 of 68 PASS dbt_utils_accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0  [[32mPASS[0m in 2.84s]
[0m16:16:17.523832 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.dbt_utils_accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0.be1f75eba9
[0m16:16:17.525764 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_active_subscribers__0.74fd82653d
[0m16:16:17.532640 [info ] [Thread-4 (]: 8 of 68 START test dbt_utils_accepted_range_mart_subscription_daily_active_subscribers__0  [RUN]
[0m16:16:17.535265 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.dbt_utils_accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0.be1f75eba9, now test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_active_subscribers__0.74fd82653d)
[0m16:16:17.537565 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_active_subscribers__0.74fd82653d
[0m16:16:17.548175 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_active_subscribers__0.74fd82653d"
[0m16:16:17.549694 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_active_subscribers__0.74fd82653d
[0m16:16:17.556742 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_active_subscribers__0.74fd82653d"
[0m16:16:17.559896 [debug] [Thread-4 (]: On test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_active_subscribers__0.74fd82653d: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_active_subscribers__0.74fd82653d"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  

with meet_condition as(
  select *
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily`
),

validation_errors as (
  select *
  from meet_condition
  where
    -- never true, defaults to an empty result set. Exists to ensure any combo of the `or` clauses below succeeds
    1 = 2
    -- records with a value >= min_value are permitted. The `not` flips this to find records that don't meet the rule.
    or not active_subscribers >= 0
)

select *
from validation_errors


  
  
      
    ) dbt_internal_test
[0m16:16:17.560886 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m16:16:18.403154 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:f9000e62-7163-49d9-a8db-b014572b31de&page=queryresults
[0m16:16:18.410024 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:4a33f952-2371-4096-8bd8-6eff0b7846e0&page=queryresults
[0m16:16:18.669173 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:4c2af3cc-d9de-42bd-b6a3-89487fc53786&page=queryresults
[0m16:16:18.804753 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:e30be6e0-cdd6-4d77-a66f-aea721ffbdb9&page=queryresults
[0m16:16:19.281413 [info ] [Thread-3 (]: 6 of 68 PASS dbt_utils_accepted_range_mart_kpis_latest_monthly_recurring_revenue__0  [[32mPASS[0m in 2.16s]
[0m16:16:19.287106 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_monthly_recurring_revenue__0.e7fa0e4f46
[0m16:16:19.290273 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_new_subscribers__0.654a8ca7d1
[0m16:16:19.297827 [info ] [Thread-3 (]: 9 of 68 START test dbt_utils_accepted_range_mart_subscription_daily_new_subscribers__0  [RUN]
[0m16:16:19.298460 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_monthly_recurring_revenue__0.e7fa0e4f46, now test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_new_subscribers__0.654a8ca7d1)
[0m16:16:19.299482 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_new_subscribers__0.654a8ca7d1
[0m16:16:19.305940 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_new_subscribers__0.654a8ca7d1"
[0m16:16:19.309257 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_new_subscribers__0.654a8ca7d1
[0m16:16:19.315701 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_new_subscribers__0.654a8ca7d1"
[0m16:16:19.317313 [debug] [Thread-3 (]: On test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_new_subscribers__0.654a8ca7d1: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_new_subscribers__0.654a8ca7d1"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  

with meet_condition as(
  select *
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily`
),

validation_errors as (
  select *
  from meet_condition
  where
    -- never true, defaults to an empty result set. Exists to ensure any combo of the `or` clauses below succeeds
    1 = 2
    -- records with a value >= min_value are permitted. The `not` flips this to find records that don't meet the rule.
    or not new_subscribers >= 0
)

select *
from validation_errors


  
  
      
    ) dbt_internal_test
[0m16:16:19.318350 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m16:16:19.373896 [info ] [Thread-2 (]: 5 of 68 PASS dbt_utils_accepted_range_mart_kpis_latest_cac_payback_months__0 ... [[32mPASS[0m in 2.36s]
[0m16:16:19.374948 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_cac_payback_months__0.440e9fbb47
[0m16:16:19.376501 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc
[0m16:16:19.378593 [info ] [Thread-2 (]: 10 of 68 START test not_null_dim_customer_customer_id .......................... [RUN]
[0m16:16:19.380217 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_cac_payback_months__0.440e9fbb47, now test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc)
[0m16:16:19.382948 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc
[0m16:16:19.414105 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc"
[0m16:16:19.420884 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc
[0m16:16:19.425469 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc"
[0m16:16:19.427907 [debug] [Thread-2 (]: On test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select customer_id
from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_customer`
where customer_id is null



  
  
      
    ) dbt_internal_test
[0m16:16:19.428828 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m16:16:19.719363 [info ] [Thread-4 (]: 8 of 68 PASS dbt_utils_accepted_range_mart_subscription_daily_active_subscribers__0  [[32mPASS[0m in 2.18s]
[0m16:16:19.723196 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_active_subscribers__0.74fd82653d
[0m16:16:19.725762 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723
[0m16:16:19.728056 [info ] [Thread-4 (]: 11 of 68 START test not_null_dim_subscription_customer_id ...................... [RUN]
[0m16:16:19.731042 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_active_subscribers__0.74fd82653d, now test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723)
[0m16:16:19.732513 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723
[0m16:16:19.743513 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723"
[0m16:16:19.744741 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723
[0m16:16:19.750525 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723"
[0m16:16:19.751606 [debug] [Thread-4 (]: On test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select customer_id
from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_subscription`
where customer_id is null



  
  
      
    ) dbt_internal_test
[0m16:16:19.752424 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m16:16:19.864326 [info ] [Thread-1 (]: 7 of 68 PASS dbt_utils_accepted_range_mart_revenue_daily_daily_revenue__0 ...... [[32mPASS[0m in 2.58s]
[0m16:16:19.866068 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.dbt_utils_accepted_range_mart_revenue_daily_daily_revenue__0.611ebe5f4e
[0m16:16:19.869524 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6
[0m16:16:19.877439 [info ] [Thread-1 (]: 12 of 68 START test not_null_dim_subscription_subscription_id .................. [RUN]
[0m16:16:19.884941 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.dbt_utils_accepted_range_mart_revenue_daily_daily_revenue__0.611ebe5f4e, now test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6)
[0m16:16:19.886021 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6
[0m16:16:19.895139 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6"
[0m16:16:19.906199 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6
[0m16:16:19.915549 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6"
[0m16:16:19.917421 [debug] [Thread-1 (]: On test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select subscription_id
from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_subscription`
where subscription_id is null



  
  
      
    ) dbt_internal_test
[0m16:16:19.918031 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:16:20.508867 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:bcab758a-6f9f-4c5e-a2bb-7a205b90db7d&page=queryresults
[0m16:16:20.922001 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:1b263c38-6e6d-4ba6-92c5-078c1f0e0909&page=queryresults
[0m16:16:20.948914 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:1141dce9-203c-49c7-a1e0-dd32cd315a90&page=queryresults
[0m16:16:21.060805 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:7e7271f3-d707-4fae-891b-a867add6f7e6&page=queryresults
[0m16:16:21.498733 [info ] [Thread-3 (]: 9 of 68 PASS dbt_utils_accepted_range_mart_subscription_daily_new_subscribers__0  [[32mPASS[0m in 2.20s]
[0m16:16:21.503422 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_new_subscribers__0.654a8ca7d1
[0m16:16:21.506663 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719
[0m16:16:21.509393 [info ] [Thread-3 (]: 13 of 68 START test not_null_fct_marketing_spend_date .......................... [RUN]
[0m16:16:21.511711 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_new_subscribers__0.654a8ca7d1, now test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719)
[0m16:16:21.513276 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719
[0m16:16:21.524581 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719"
[0m16:16:21.527712 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719
[0m16:16:21.531072 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719"
[0m16:16:21.543519 [debug] [Thread-3 (]: On test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select date
from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_marketing_spend`
where date is null



  
  
      
    ) dbt_internal_test
[0m16:16:21.544392 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m16:16:21.839589 [info ] [Thread-4 (]: 11 of 68 PASS not_null_dim_subscription_customer_id ............................ [[32mPASS[0m in 2.11s]
[0m16:16:21.854239 [info ] [Thread-2 (]: 10 of 68 PASS not_null_dim_customer_customer_id ................................ [[32mPASS[0m in 2.47s]
[0m16:16:21.857066 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723
[0m16:16:21.859865 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc
[0m16:16:21.862673 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d
[0m16:16:21.864766 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff
[0m16:16:21.866707 [info ] [Thread-4 (]: 14 of 68 START test not_null_fct_marketing_spend_spend_try ..................... [RUN]
[0m16:16:21.873948 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723, now test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d)
[0m16:16:21.867830 [info ] [Thread-2 (]: 15 of 68 START test not_null_fct_order_customer_id ............................. [RUN]
[0m16:16:21.878969 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d
[0m16:16:21.879799 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc, now test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff)
[0m16:16:21.884856 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d"
[0m16:16:21.888635 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d
[0m16:16:21.886899 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff
[0m16:16:21.891827 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d"
[0m16:16:21.900571 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff"
[0m16:16:21.901372 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff
[0m16:16:21.905849 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff"
[0m16:16:21.907794 [debug] [Thread-4 (]: On test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select spend_try
from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_marketing_spend`
where spend_try is null



  
  
      
    ) dbt_internal_test
[0m16:16:21.908378 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m16:16:21.960553 [debug] [Thread-2 (]: On test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select customer_id
from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_order`
where customer_id is null



  
  
      
    ) dbt_internal_test
[0m16:16:21.962994 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m16:16:22.027396 [info ] [Thread-1 (]: 12 of 68 PASS not_null_dim_subscription_subscription_id ........................ [[32mPASS[0m in 2.14s]
[0m16:16:22.037125 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6
[0m16:16:22.041555 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea
[0m16:16:22.043827 [info ] [Thread-1 (]: 16 of 68 START test not_null_fct_order_discount_total .......................... [RUN]
[0m16:16:22.045715 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6, now test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea)
[0m16:16:22.046206 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea
[0m16:16:22.051598 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea"
[0m16:16:22.053597 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea
[0m16:16:22.056089 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea"
[0m16:16:22.056772 [debug] [Thread-1 (]: On test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select discount_total
from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_order`
where discount_total is null



  
  
      
    ) dbt_internal_test
[0m16:16:22.057295 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:16:22.985113 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:596608f0-2d09-4210-9367-0f77a7763508&page=queryresults
[0m16:16:23.223642 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:89c2f262-e316-45b7-b305-82728b221b1b&page=queryresults
[0m16:16:23.262225 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:af94dac1-bce8-43ea-8612-4302feb57fc9&page=queryresults
[0m16:16:23.416304 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:a9216c99-2e06-4fe0-a379-7ca523200523&page=queryresults
[0m16:16:23.914691 [info ] [Thread-3 (]: 13 of 68 PASS not_null_fct_marketing_spend_date ................................ [[32mPASS[0m in 2.40s]
[0m16:16:23.916039 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719
[0m16:16:23.916788 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e
[0m16:16:23.917325 [info ] [Thread-3 (]: 17 of 68 START test not_null_fct_order_net_revenue ............................. [RUN]
[0m16:16:23.918390 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719, now test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e)
[0m16:16:23.918896 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e
[0m16:16:23.923857 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e"
[0m16:16:23.925202 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e
[0m16:16:23.929022 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e"
[0m16:16:23.930431 [debug] [Thread-3 (]: On test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select net_revenue
from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_order`
where net_revenue is null



  
  
      
    ) dbt_internal_test
[0m16:16:23.931219 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m16:16:24.048271 [info ] [Thread-2 (]: 15 of 68 PASS not_null_fct_order_customer_id ................................... [[32mPASS[0m in 2.17s]
[0m16:16:24.049740 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff
[0m16:16:24.050338 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def
[0m16:16:24.052164 [info ] [Thread-2 (]: 18 of 68 START test not_null_fct_order_order_id ................................ [RUN]
[0m16:16:24.053441 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff, now test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def)
[0m16:16:24.053960 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def
[0m16:16:24.059792 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def"
[0m16:16:24.061249 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def
[0m16:16:24.064595 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def"
[0m16:16:24.065829 [debug] [Thread-2 (]: On test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select order_id
from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_order`
where order_id is null



  
  
      
    ) dbt_internal_test
[0m16:16:24.066573 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m16:16:24.146767 [info ] [Thread-1 (]: 16 of 68 PASS not_null_fct_order_discount_total ................................ [[32mPASS[0m in 2.10s]
[0m16:16:24.148650 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea
[0m16:16:24.149397 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d
[0m16:16:24.149908 [info ] [Thread-1 (]: 19 of 68 START test not_null_fct_shipment_shipment_id .......................... [RUN]
[0m16:16:24.150853 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea, now test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d)
[0m16:16:24.151352 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d
[0m16:16:24.160194 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d"
[0m16:16:24.162758 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d
[0m16:16:24.177167 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d"
[0m16:16:24.178851 [debug] [Thread-1 (]: On test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select shipment_id
from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_shipment`
where shipment_id is null



  
  
      
    ) dbt_internal_test
[0m16:16:24.180449 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:16:24.281035 [info ] [Thread-4 (]: 14 of 68 PASS not_null_fct_marketing_spend_spend_try ........................... [[32mPASS[0m in 2.41s]
[0m16:16:24.282341 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d
[0m16:16:24.283733 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b
[0m16:16:24.284795 [info ] [Thread-4 (]: 20 of 68 START test not_null_int_shipments_decomposed_delivered_at ............. [RUN]
[0m16:16:24.285724 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d, now test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b)
[0m16:16:24.286178 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b
[0m16:16:24.293102 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b"
[0m16:16:24.294374 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b
[0m16:16:24.297205 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b"
[0m16:16:24.298473 [debug] [Thread-4 (]: On test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select delivered_at
from `data-pipeline-project-474812`.`analytics_staging_staging`.`int_shipments_decomposed`
where delivered_at is null



  
  
      
    ) dbt_internal_test
[0m16:16:24.299279 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m16:16:25.270217 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:69593dd0-6a8b-459d-ae60-5a07d5350d46&page=queryresults
[0m16:16:25.323038 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:5d36f3a9-9d7f-494f-a250-b89f7897c974&page=queryresults
[0m16:16:25.465963 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:0cc41c76-d6ab-4e44-9384-d4d7f3be8acb&page=queryresults
[0m16:16:25.680919 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:74c2396d-f0d1-4f0f-b595-56764675e3f4&page=queryresults
[0m16:16:26.162128 [info ] [Thread-2 (]: 18 of 68 PASS not_null_fct_order_order_id ...................................... [[32mPASS[0m in 2.11s]
[0m16:16:26.163660 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def
[0m16:16:26.164542 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31
[0m16:16:26.165352 [info ] [Thread-2 (]: 21 of 68 START test not_null_int_shipments_decomposed_latest_status ............ [RUN]
[0m16:16:26.166550 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def, now test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31)
[0m16:16:26.167117 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31
[0m16:16:26.171717 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31"
[0m16:16:26.173827 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31
[0m16:16:26.177021 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31"
[0m16:16:26.178374 [debug] [Thread-2 (]: On test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select latest_status
from `data-pipeline-project-474812`.`analytics_staging_staging`.`int_shipments_decomposed`
where latest_status is null



  
  
      
    ) dbt_internal_test
[0m16:16:26.178967 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m16:16:26.252491 [info ] [Thread-1 (]: 19 of 68 PASS not_null_fct_shipment_shipment_id ................................ [[32mPASS[0m in 2.10s]
[0m16:16:26.256241 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d
[0m16:16:26.257354 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b
[0m16:16:26.258218 [info ] [Thread-1 (]: 22 of 68 START test not_null_mart_acquisition_efficiency_date .................. [RUN]
[0m16:16:26.260080 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d, now test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b)
[0m16:16:26.260878 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b
[0m16:16:26.265974 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b"
[0m16:16:26.267510 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b
[0m16:16:26.279301 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b"
[0m16:16:26.280685 [debug] [Thread-1 (]: On test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select date
from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_acquisition_efficiency`
where date is null



  
  
      
    ) dbt_internal_test
[0m16:16:26.281264 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:16:26.371920 [info ] [Thread-3 (]: 17 of 68 PASS not_null_fct_order_net_revenue ................................... [[32mPASS[0m in 2.45s]
[0m16:16:26.374640 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e
[0m16:16:26.379092 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23
[0m16:16:26.380543 [info ] [Thread-3 (]: 23 of 68 START test not_null_mart_revenue_daily_date ........................... [RUN]
[0m16:16:26.382420 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e, now test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23)
[0m16:16:26.382961 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23
[0m16:16:26.388788 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23"
[0m16:16:26.390334 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23
[0m16:16:26.395124 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23"
[0m16:16:26.397420 [debug] [Thread-3 (]: On test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select date
from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_revenue_daily`
where date is null



  
  
      
    ) dbt_internal_test
[0m16:16:26.398357 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m16:16:26.698228 [warn ] [Thread-4 (]: 20 of 68 WARN 303 not_null_int_shipments_decomposed_delivered_at ............... [[33mWARN 303[0m in 2.41s]
[0m16:16:26.704866 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b
[0m16:16:26.707899 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322
[0m16:16:26.709350 [info ] [Thread-4 (]: 24 of 68 START test not_null_mart_subscription_daily_date ...................... [RUN]
[0m16:16:26.711800 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b, now test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322)
[0m16:16:26.713491 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322
[0m16:16:26.727782 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322"
[0m16:16:26.728995 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322
[0m16:16:26.732916 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322"
[0m16:16:26.734808 [debug] [Thread-4 (]: On test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select date
from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily`
where date is null



  
  
      
    ) dbt_internal_test
[0m16:16:26.735894 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m16:16:27.514252 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:ece2bcc8-29bd-437c-a515-ec9b1dce7c29&page=queryresults
[0m16:16:27.654411 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:58b2cb4e-1234-451a-a63e-13ee03491a63&page=queryresults
[0m16:16:27.705185 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:7cbbbd1c-9655-4bad-b42c-332b96d8c039&page=queryresults
[0m16:16:27.894315 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:5ee80495-82f9-49a9-bff4-b2d0df947baa&page=queryresults
[0m16:16:28.462892 [info ] [Thread-1 (]: 22 of 68 PASS not_null_mart_acquisition_efficiency_date ........................ [[32mPASS[0m in 2.20s]
[0m16:16:28.464580 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b
[0m16:16:28.465667 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501
[0m16:16:28.467062 [info ] [Thread-1 (]: 25 of 68 START test not_null_stg_addresses_address_id .......................... [RUN]
[0m16:16:28.471080 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b, now test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501)
[0m16:16:28.471598 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501
[0m16:16:28.477260 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501"
[0m16:16:28.478906 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501
[0m16:16:28.486424 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501"
[0m16:16:28.487256 [debug] [Thread-1 (]: On test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select address_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_addresses`
where address_id is null



  
  
      
    ) dbt_internal_test
[0m16:16:28.488325 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:16:28.531461 [info ] [Thread-3 (]: 23 of 68 PASS not_null_mart_revenue_daily_date ................................. [[32mPASS[0m in 2.15s]
[0m16:16:28.533385 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23
[0m16:16:28.534734 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3
[0m16:16:28.537411 [info ] [Thread-3 (]: 26 of 68 START test not_null_stg_addresses_user_id ............................. [RUN]
[0m16:16:28.538707 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23, now test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3)
[0m16:16:28.539555 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3
[0m16:16:28.554135 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3"
[0m16:16:28.555481 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3
[0m16:16:28.563663 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3"
[0m16:16:28.566669 [debug] [Thread-3 (]: On test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_addresses`
where user_id is null



  
  
      
    ) dbt_internal_test
[0m16:16:28.567481 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m16:16:28.622004 [info ] [Thread-2 (]: 21 of 68 PASS not_null_int_shipments_decomposed_latest_status .................. [[32mPASS[0m in 2.45s]
[0m16:16:28.623099 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31
[0m16:16:28.623835 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2
[0m16:16:28.630868 [info ] [Thread-2 (]: 27 of 68 START test not_null_stg_countries_country_id .......................... [RUN]
[0m16:16:28.631711 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31, now test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2)
[0m16:16:28.632402 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2
[0m16:16:28.644760 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2"
[0m16:16:28.646614 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2
[0m16:16:28.653673 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2"
[0m16:16:28.654679 [debug] [Thread-2 (]: On test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select country_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_countries`
where country_id is null



  
  
      
    ) dbt_internal_test
[0m16:16:28.655412 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m16:16:28.731693 [info ] [Thread-4 (]: 24 of 68 PASS not_null_mart_subscription_daily_date ............................ [[32mPASS[0m in 2.02s]
[0m16:16:28.732924 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322
[0m16:16:28.734599 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64
[0m16:16:28.735458 [info ] [Thread-4 (]: 28 of 68 START test not_null_stg_orders_order_id ............................... [RUN]
[0m16:16:28.738097 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322, now test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64)
[0m16:16:28.738675 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64
[0m16:16:28.747949 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64"
[0m16:16:28.752980 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64
[0m16:16:28.759357 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64"
[0m16:16:28.762558 [debug] [Thread-4 (]: On test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select order_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_orders`
where order_id is null



  
  
      
    ) dbt_internal_test
[0m16:16:28.765922 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m16:16:29.810342 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:02588a4a-8ca9-48af-a273-1a6098e212b4&page=queryresults
[0m16:16:29.822312 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:5adc8132-6bf1-4eb9-9a03-a160e2cf8fad&page=queryresults
[0m16:16:30.031499 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:d3bc4321-652e-4cbd-abae-842ed2bc9c30&page=queryresults
[0m16:16:30.053921 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:48c524cc-f705-4676-a822-a13c483b23b5&page=queryresults
[0m16:16:30.720368 [info ] [Thread-1 (]: 25 of 68 PASS not_null_stg_addresses_address_id ................................ [[32mPASS[0m in 2.25s]
[0m16:16:30.721908 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501
[0m16:16:30.722701 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209
[0m16:16:30.724001 [info ] [Thread-1 (]: 29 of 68 START test not_null_stg_orders_user_id ................................ [RUN]
[0m16:16:30.725271 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501, now test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209)
[0m16:16:30.726280 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209
[0m16:16:30.731796 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209"
[0m16:16:30.733981 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209
[0m16:16:30.738955 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209"
[0m16:16:30.742659 [debug] [Thread-1 (]: On test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_orders`
where user_id is null



  
  
      
    ) dbt_internal_test
[0m16:16:30.745823 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:16:30.745182 [info ] [Thread-3 (]: 26 of 68 PASS not_null_stg_addresses_user_id ................................... [[32mPASS[0m in 2.21s]
[0m16:16:30.782764 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3
[0m16:16:30.785088 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380
[0m16:16:30.786521 [info ] [Thread-3 (]: 30 of 68 START test not_null_stg_shipments_shipment_id ......................... [RUN]
[0m16:16:30.789482 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3, now test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380)
[0m16:16:30.790476 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380
[0m16:16:30.799670 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380"
[0m16:16:30.801542 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380
[0m16:16:30.806716 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380"
[0m16:16:30.808283 [debug] [Thread-3 (]: On test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select shipment_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_shipments`
where shipment_id is null



  
  
      
    ) dbt_internal_test
[0m16:16:30.810992 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m16:16:30.903776 [info ] [Thread-2 (]: 27 of 68 PASS not_null_stg_countries_country_id ................................ [[32mPASS[0m in 2.27s]
[0m16:16:30.907171 [info ] [Thread-4 (]: 28 of 68 PASS not_null_stg_orders_order_id ..................................... [[32mPASS[0m in 2.17s]
[0m16:16:30.909312 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2
[0m16:16:30.912559 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852
[0m16:16:30.914085 [info ] [Thread-2 (]: 31 of 68 START test not_null_stg_shipments_user_id ............................. [RUN]
[0m16:16:30.910624 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64
[0m16:16:30.916106 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2, now test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852)
[0m16:16:30.917955 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8
[0m16:16:30.919593 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852
[0m16:16:30.926120 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852"
[0m16:16:30.928866 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852
[0m16:16:30.920464 [info ] [Thread-4 (]: 32 of 68 START test not_null_stg_subscriptions_start_date ...................... [RUN]
[0m16:16:30.940817 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852"
[0m16:16:30.941780 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64, now test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8)
[0m16:16:30.944002 [debug] [Thread-2 (]: On test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_shipments`
where user_id is null



  
  
      
    ) dbt_internal_test
[0m16:16:30.944767 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8
[0m16:16:30.946756 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m16:16:30.959024 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8"
[0m16:16:31.009039 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8
[0m16:16:31.025547 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8"
[0m16:16:31.030177 [debug] [Thread-4 (]: On test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select start_date
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_subscriptions`
where start_date is null



  
  
      
    ) dbt_internal_test
[0m16:16:31.033850 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m16:16:32.029910 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:8ff4650e-e506-467b-ac03-a7bfe682b648&page=queryresults
[0m16:16:32.212374 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:cbbdd551-fed6-488c-82e4-e074adfa2aa2&page=queryresults
[0m16:16:32.317627 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:148694ac-c181-49cc-9c56-65009edc8607&page=queryresults
[0m16:16:32.392985 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:12378404-a947-400f-ba8b-0085bcee5efb&page=queryresults
[0m16:16:32.858485 [info ] [Thread-1 (]: 29 of 68 PASS not_null_stg_orders_user_id ...................................... [[32mPASS[0m in 2.13s]
[0m16:16:32.860285 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209
[0m16:16:32.861319 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m16:16:32.862414 [info ] [Thread-1 (]: 33 of 68 START test not_null_stg_subscriptions_subscription_id ................. [RUN]
[0m16:16:32.863384 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209, now test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91)
[0m16:16:32.864077 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m16:16:32.870503 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91"
[0m16:16:32.882260 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m16:16:32.886023 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91"
[0m16:16:32.886919 [debug] [Thread-1 (]: On test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select subscription_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_subscriptions`
where subscription_id is null



  
  
      
    ) dbt_internal_test
[0m16:16:32.887579 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:16:33.079636 [info ] [Thread-3 (]: 30 of 68 PASS not_null_stg_shipments_shipment_id ............................... [[32mPASS[0m in 2.29s]
[0m16:16:33.082992 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380
[0m16:16:33.084585 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2
[0m16:16:33.085624 [info ] [Thread-3 (]: 34 of 68 START test not_null_stg_subscriptions_user_id ......................... [RUN]
[0m16:16:33.087188 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380, now test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2)
[0m16:16:33.088171 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2
[0m16:16:33.095876 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2"
[0m16:16:33.097049 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2
[0m16:16:33.100191 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2"
[0m16:16:33.101442 [debug] [Thread-3 (]: On test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_subscriptions`
where user_id is null



  
  
      
    ) dbt_internal_test
[0m16:16:33.102283 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m16:16:33.159380 [info ] [Thread-2 (]: 31 of 68 PASS not_null_stg_shipments_user_id ................................... [[32mPASS[0m in 2.24s]
[0m16:16:33.161731 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852
[0m16:16:33.163003 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25
[0m16:16:33.164063 [info ] [Thread-2 (]: 35 of 68 START test not_null_stg_users_created_at .............................. [RUN]
[0m16:16:33.165791 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852, now test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25)
[0m16:16:33.166765 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25
[0m16:16:33.179696 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25"
[0m16:16:33.185773 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25
[0m16:16:33.194541 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25"
[0m16:16:33.198980 [debug] [Thread-2 (]: On test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select created_at
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
where created_at is null



  
  
      
    ) dbt_internal_test
[0m16:16:33.200921 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m16:16:33.273246 [info ] [Thread-4 (]: 32 of 68 PASS not_null_stg_subscriptions_start_date ............................ [[32mPASS[0m in 2.33s]
[0m16:16:33.278390 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8
[0m16:16:33.279853 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77
[0m16:16:33.280507 [info ] [Thread-4 (]: 36 of 68 START test not_null_stg_users_user_id ................................. [RUN]
[0m16:16:33.283360 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8, now test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77)
[0m16:16:33.284085 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77
[0m16:16:33.290986 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77"
[0m16:16:33.291977 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77
[0m16:16:33.297152 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77"
[0m16:16:33.300985 [debug] [Thread-4 (]: On test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
where user_id is null



  
  
      
    ) dbt_internal_test
[0m16:16:33.302013 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m16:16:34.380456 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:61a60fd4-12a1-4cc0-9cc3-62f617126d8f&page=queryresults
[0m16:16:34.400255 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:fac4ccb0-de3c-45be-9598-bb8577e208a3&page=queryresults
[0m16:16:34.647463 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:aa928e7b-a84d-4613-b0e1-d7dddf7ef0a2&page=queryresults
[0m16:16:34.726411 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:3fa3050d-40e6-4f4c-863f-da8b1c2d4eed&page=queryresults
[0m16:16:35.285125 [info ] [Thread-3 (]: 34 of 68 PASS not_null_stg_subscriptions_user_id ............................... [[32mPASS[0m in 2.20s]
[0m16:16:35.288199 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2
[0m16:16:35.289998 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29
[0m16:16:35.292570 [info ] [Thread-3 (]: 37 of 68 START test relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_  [RUN]
[0m16:16:35.293616 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2, now test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29)
[0m16:16:35.294216 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29
[0m16:16:35.306348 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29"
[0m16:16:35.308050 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29
[0m16:16:35.311635 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29"
[0m16:16:35.312589 [debug] [Thread-3 (]: On test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select customer_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_subscription`
    where customer_id is not null
),

parent as (
    select customer_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_customer`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m16:16:35.313453 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m16:16:35.383339 [info ] [Thread-1 (]: 33 of 68 PASS not_null_stg_subscriptions_subscription_id ....................... [[32mPASS[0m in 2.52s]
[0m16:16:35.384401 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m16:16:35.384976 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85
[0m16:16:35.385471 [info ] [Thread-1 (]: 38 of 68 START test relationships_fct_order_customer_id__customer_id__ref_dim_customer_  [RUN]
[0m16:16:35.386286 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91, now test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85)
[0m16:16:35.386779 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85
[0m16:16:35.396298 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85"
[0m16:16:35.397282 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85
[0m16:16:35.400277 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85"
[0m16:16:35.402386 [debug] [Thread-1 (]: On test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select customer_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_order`
    where customer_id is not null
),

parent as (
    select customer_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_customer`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m16:16:35.403378 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:16:35.569140 [info ] [Thread-2 (]: 35 of 68 PASS not_null_stg_users_created_at .................................... [[32mPASS[0m in 2.40s]
[0m16:16:35.570900 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25
[0m16:16:35.573256 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781
[0m16:16:35.574713 [info ] [Thread-2 (]: 39 of 68 START test relationships_stg_addresses_user_id__user_id__ref_stg_users_  [RUN]
[0m16:16:35.575788 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25, now test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781)
[0m16:16:35.577372 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781
[0m16:16:35.591813 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781"
[0m16:16:35.592759 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781
[0m16:16:35.598146 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781"
[0m16:16:35.602996 [debug] [Thread-2 (]: On test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select user_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_addresses`
    where user_id is not null
),

parent as (
    select user_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m16:16:35.607742 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m16:16:35.671770 [info ] [Thread-4 (]: 36 of 68 PASS not_null_stg_users_user_id ....................................... [[32mPASS[0m in 2.39s]
[0m16:16:35.674728 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77
[0m16:16:35.677509 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737
[0m16:16:35.679917 [info ] [Thread-4 (]: 40 of 68 START test relationships_stg_orders_user_id__user_id__ref_stg_users_ .. [RUN]
[0m16:16:35.680951 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77, now test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737)
[0m16:16:35.682476 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737
[0m16:16:35.698436 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737"
[0m16:16:35.705928 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737
[0m16:16:35.708924 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737"
[0m16:16:35.711155 [debug] [Thread-4 (]: On test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select user_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_orders`
    where user_id is not null
),

parent as (
    select user_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m16:16:35.712025 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m16:16:36.704731 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:d9dd6fc5-cea4-4f0d-a66c-4316b582356e&page=queryresults
[0m16:16:36.722493 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:bb5e8c71-dc24-44e1-af33-9b87466fca58&page=queryresults
[0m16:16:37.026299 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:d38fd474-61ef-4163-952d-e976a965ee45&page=queryresults
[0m16:16:37.048829 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:85058481-417a-4c8e-9446-0980b836c700&page=queryresults
[0m16:16:37.592798 [info ] [Thread-3 (]: 37 of 68 PASS relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_  [[32mPASS[0m in 2.30s]
[0m16:16:37.596359 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29
[0m16:16:37.597835 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8
[0m16:16:37.599263 [info ] [Thread-3 (]: 41 of 68 START test relationships_stg_shipments_user_id__user_id__ref_stg_users_  [RUN]
[0m16:16:37.600672 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29, now test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8)
[0m16:16:37.601728 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8
[0m16:16:37.610919 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8"
[0m16:16:37.612126 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8
[0m16:16:37.615484 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8"
[0m16:16:37.616648 [debug] [Thread-3 (]: On test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select user_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_shipments`
    where user_id is not null
),

parent as (
    select user_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m16:16:37.617515 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m16:16:37.670845 [info ] [Thread-1 (]: 38 of 68 PASS relationships_fct_order_customer_id__customer_id__ref_dim_customer_  [[32mPASS[0m in 2.28s]
[0m16:16:37.674496 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85
[0m16:16:37.678628 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m16:16:37.680200 [info ] [Thread-1 (]: 42 of 68 START test relationships_stg_subscriptions_user_id__user_id__ref_stg_users_  [RUN]
[0m16:16:37.689553 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85, now test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e)
[0m16:16:37.692413 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m16:16:37.705734 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e"
[0m16:16:37.708089 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m16:16:37.714000 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e"
[0m16:16:37.715039 [debug] [Thread-1 (]: On test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select user_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_subscriptions`
    where user_id is not null
),

parent as (
    select user_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m16:16:37.715756 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:16:37.864290 [info ] [Thread-4 (]: 40 of 68 PASS relationships_stg_orders_user_id__user_id__ref_stg_users_ ........ [[32mPASS[0m in 2.18s]
[0m16:16:37.865173 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737
[0m16:16:37.865956 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6
[0m16:16:37.867517 [info ] [Thread-4 (]: 43 of 68 START test source_not_null_raw_data_addresses__id ..................... [RUN]
[0m16:16:37.868598 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737, now test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6)
[0m16:16:37.869267 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6
[0m16:16:37.874390 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6"
[0m16:16:37.875172 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6
[0m16:16:37.877453 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6"
[0m16:16:37.878074 [debug] [Thread-4 (]: On test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`addresses`
where _id is null



  
  
      
    ) dbt_internal_test
[0m16:16:37.878543 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m16:16:37.943866 [info ] [Thread-2 (]: 39 of 68 PASS relationships_stg_addresses_user_id__user_id__ref_stg_users_ ..... [[32mPASS[0m in 2.37s]
[0m16:16:37.944854 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781
[0m16:16:37.945576 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc
[0m16:16:37.946209 [info ] [Thread-2 (]: 44 of 68 START test source_not_null_raw_data_cities__id ........................ [RUN]
[0m16:16:37.947050 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781, now test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc)
[0m16:16:37.948029 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc
[0m16:16:37.955791 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc"
[0m16:16:37.957889 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc
[0m16:16:37.967285 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc"
[0m16:16:37.969852 [debug] [Thread-2 (]: On test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`cities`
where _id is null



  
  
      
    ) dbt_internal_test
[0m16:16:37.975044 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m16:16:38.928716 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:6644ea33-654f-4322-b7b3-f933d8298285&page=queryresults
[0m16:16:39.101856 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:8e770f90-0e45-4645-b3d2-2d8d28ceda6c&page=queryresults
[0m16:16:39.153938 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:0b61ca94-d9bd-410f-9a49-2c65d65ba6c5&page=queryresults
[0m16:16:39.168285 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:5a39bf16-3100-4911-85f3-c1eb951b7c78&page=queryresults
[0m16:16:39.858191 [info ] [Thread-4 (]: 43 of 68 PASS source_not_null_raw_data_addresses__id ........................... [[32mPASS[0m in 1.99s]
[0m16:16:39.869305 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6
[0m16:16:39.871308 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd
[0m16:16:39.872692 [info ] [Thread-4 (]: 45 of 68 START test source_not_null_raw_data_countries__id ..................... [RUN]
[0m16:16:39.875120 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6, now test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd)
[0m16:16:39.876116 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd
[0m16:16:39.883599 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd"
[0m16:16:39.890784 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd
[0m16:16:39.894502 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd"
[0m16:16:39.895476 [debug] [Thread-4 (]: On test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`countries`
where _id is null



  
  
      
    ) dbt_internal_test
[0m16:16:39.896336 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m16:16:40.137885 [info ] [Thread-1 (]: 42 of 68 PASS relationships_stg_subscriptions_user_id__user_id__ref_stg_users_ . [[32mPASS[0m in 2.45s]
[0m16:16:40.142690 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m16:16:40.144365 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be
[0m16:16:40.146864 [info ] [Thread-1 (]: 46 of 68 START test source_not_null_raw_data_neighborhoods__id ................. [RUN]
[0m16:16:40.149444 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e, now test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be)
[0m16:16:40.150521 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be
[0m16:16:40.160288 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be"
[0m16:16:40.161690 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be
[0m16:16:40.169567 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be"
[0m16:16:40.171236 [debug] [Thread-1 (]: On test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`neighborhoods`
where _id is null



  
  
      
    ) dbt_internal_test
[0m16:16:40.171814 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:16:40.229215 [info ] [Thread-3 (]: 41 of 68 PASS relationships_stg_shipments_user_id__user_id__ref_stg_users_ ..... [[32mPASS[0m in 2.63s]
[0m16:16:40.231384 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8
[0m16:16:40.231962 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878
[0m16:16:40.232871 [info ] [Thread-3 (]: 47 of 68 START test source_not_null_raw_data_orders__id ........................ [RUN]
[0m16:16:40.234001 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8, now test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878)
[0m16:16:40.234789 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878
[0m16:16:40.240519 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878"
[0m16:16:40.241986 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878
[0m16:16:40.246668 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878"
[0m16:16:40.251050 [debug] [Thread-3 (]: On test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`orders`
where _id is null



  
  
      
    ) dbt_internal_test
[0m16:16:40.252760 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m16:16:40.317738 [info ] [Thread-2 (]: 44 of 68 PASS source_not_null_raw_data_cities__id .............................. [[32mPASS[0m in 2.37s]
[0m16:16:40.329482 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc
[0m16:16:40.334652 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f
[0m16:16:40.336424 [info ] [Thread-2 (]: 48 of 68 START test source_not_null_raw_data_orders__user ...................... [RUN]
[0m16:16:40.339307 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc, now test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f)
[0m16:16:40.340177 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f
[0m16:16:40.360620 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f"
[0m16:16:40.362150 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f
[0m16:16:40.369392 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f"
[0m16:16:40.376582 [debug] [Thread-2 (]: On test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _user
from `data-pipeline-project-474812`.`raw_data`.`orders`
where _user is null



  
  
      
    ) dbt_internal_test
[0m16:16:40.380037 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m16:16:41.244413 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:c2a5abf5-7781-4c1a-965f-e4a70f8221cf&page=queryresults
[0m16:16:41.375082 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:c726613d-e124-4073-8a08-f040d740d8aa&page=queryresults
[0m16:16:41.542660 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:1920ea96-0bdb-4e89-8b05-6b8f8886b99c&page=queryresults
[0m16:16:41.547237 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:d47b9e36-c8af-4f5a-bd9e-d121f6437d2f&page=queryresults
[0m16:16:42.213906 [info ] [Thread-4 (]: 45 of 68 PASS source_not_null_raw_data_countries__id ........................... [[32mPASS[0m in 2.34s]
[0m16:16:42.219426 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd
[0m16:16:42.221649 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2
[0m16:16:42.223246 [info ] [Thread-4 (]: 49 of 68 START test source_not_null_raw_data_shipments__id ..................... [RUN]
[0m16:16:42.225488 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd, now test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2)
[0m16:16:42.226679 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2
[0m16:16:42.235635 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2"
[0m16:16:42.239521 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2
[0m16:16:42.244115 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2"
[0m16:16:42.249347 [info ] [Thread-1 (]: 46 of 68 PASS source_not_null_raw_data_neighborhoods__id ....................... [[32mPASS[0m in 2.10s]
[0m16:16:42.250931 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be
[0m16:16:42.251527 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1
[0m16:16:42.253527 [debug] [Thread-4 (]: On test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`shipments`
where _id is null



  
  
      
    ) dbt_internal_test
[0m16:16:42.254803 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m16:16:42.252348 [info ] [Thread-1 (]: 50 of 68 START test source_not_null_raw_data_shipments__order .................. [RUN]
[0m16:16:42.299909 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be, now test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1)
[0m16:16:42.306612 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1
[0m16:16:42.319400 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1"
[0m16:16:42.342415 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1
[0m16:16:42.349591 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1"
[0m16:16:42.351700 [debug] [Thread-1 (]: On test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _order
from `data-pipeline-project-474812`.`raw_data`.`shipments`
where _order is null



  
  
      
    ) dbt_internal_test
[0m16:16:42.352418 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:16:42.461295 [info ] [Thread-3 (]: 47 of 68 PASS source_not_null_raw_data_orders__id .............................. [[32mPASS[0m in 2.23s]
[0m16:16:42.463627 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878
[0m16:16:42.465015 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98
[0m16:16:42.465756 [info ] [Thread-3 (]: 51 of 68 START test source_not_null_raw_data_shipments__user ................... [RUN]
[0m16:16:42.466684 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878, now test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98)
[0m16:16:42.467463 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98
[0m16:16:42.474307 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98"
[0m16:16:42.476955 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98
[0m16:16:42.485808 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98"
[0m16:16:42.488001 [debug] [Thread-3 (]: On test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _user
from `data-pipeline-project-474812`.`raw_data`.`shipments`
where _user is null



  
  
      
    ) dbt_internal_test
[0m16:16:42.488753 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m16:16:42.621031 [info ] [Thread-2 (]: 48 of 68 PASS source_not_null_raw_data_orders__user ............................ [[32mPASS[0m in 2.28s]
[0m16:16:42.622683 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f
[0m16:16:42.623275 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86
[0m16:16:42.624299 [info ] [Thread-2 (]: 52 of 68 START test source_not_null_raw_data_states__id ........................ [RUN]
[0m16:16:42.625351 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f, now test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86)
[0m16:16:42.626084 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86
[0m16:16:42.633607 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86"
[0m16:16:42.634667 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86
[0m16:16:42.638210 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86"
[0m16:16:42.639014 [debug] [Thread-2 (]: On test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`states`
where _id is null



  
  
      
    ) dbt_internal_test
[0m16:16:42.639464 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m16:16:43.417444 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:adfd049d-c502-4a74-8ba4-b14b63d4dc25&page=queryresults
[0m16:16:43.533492 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:ffecc592-ec99-4724-9cac-f541f57e61b3&page=queryresults
[0m16:16:43.589196 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:cace6cab-b101-4174-9db8-a366b075bf9e&page=queryresults
[0m16:16:43.761524 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:d3faf5d5-eedd-4d62-91a3-3c1837727ae9&page=queryresults
[0m16:16:44.296810 [info ] [Thread-4 (]: 49 of 68 PASS source_not_null_raw_data_shipments__id ........................... [[32mPASS[0m in 2.07s]
[0m16:16:44.298056 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2
[0m16:16:44.299034 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a
[0m16:16:44.299791 [info ] [Thread-4 (]: 53 of 68 START test source_not_null_raw_data_subscriptions__id ................. [RUN]
[0m16:16:44.300740 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2, now test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a)
[0m16:16:44.301241 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a
[0m16:16:44.306316 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a"
[0m16:16:44.307528 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a
[0m16:16:44.311433 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a"
[0m16:16:44.313036 [debug] [Thread-4 (]: On test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`subscriptions`
where _id is null



  
  
      
    ) dbt_internal_test
[0m16:16:44.314350 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m16:16:44.394645 [info ] [Thread-3 (]: 51 of 68 PASS source_not_null_raw_data_shipments__user ......................... [[32mPASS[0m in 1.93s]
[0m16:16:44.396655 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98
[0m16:16:44.397613 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6
[0m16:16:44.398313 [info ] [Thread-3 (]: 54 of 68 START test source_not_null_raw_data_subscriptions__user ............... [RUN]
[0m16:16:44.398974 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98, now test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6)
[0m16:16:44.399809 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6
[0m16:16:44.411290 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6"
[0m16:16:44.413531 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6
[0m16:16:44.417508 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6"
[0m16:16:44.419575 [debug] [Thread-3 (]: On test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _user
from `data-pipeline-project-474812`.`raw_data`.`subscriptions`
where _user is null



  
  
      
    ) dbt_internal_test
[0m16:16:44.420319 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m16:16:44.601023 [info ] [Thread-1 (]: 50 of 68 PASS source_not_null_raw_data_shipments__order ........................ [[32mPASS[0m in 2.30s]
[0m16:16:44.602156 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1
[0m16:16:44.602873 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b
[0m16:16:44.603296 [info ] [Thread-1 (]: 55 of 68 START test source_not_null_raw_data_users__id ......................... [RUN]
[0m16:16:44.604560 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1, now test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b)
[0m16:16:44.605056 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b
[0m16:16:44.611137 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b"
[0m16:16:44.612128 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b
[0m16:16:44.616579 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b"
[0m16:16:44.618069 [debug] [Thread-1 (]: On test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`users`
where _id is null



  
  
      
    ) dbt_internal_test
[0m16:16:44.619635 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:16:44.669841 [info ] [Thread-2 (]: 52 of 68 PASS source_not_null_raw_data_states__id .............................. [[32mPASS[0m in 2.04s]
[0m16:16:44.673348 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86
[0m16:16:44.674374 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1
[0m16:16:44.675735 [info ] [Thread-2 (]: 56 of 68 START test unique_dim_customer_customer_id ............................ [RUN]
[0m16:16:44.679455 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86, now test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1)
[0m16:16:44.679990 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1
[0m16:16:44.687453 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1"
[0m16:16:44.689198 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1
[0m16:16:44.692959 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1"
[0m16:16:44.695341 [debug] [Thread-2 (]: On test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select customer_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_customer`
  where customer_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m16:16:44.696379 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m16:16:45.540235 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:e7797f52-5b84-4f4e-ba8a-5fe6b80c03b4&page=queryresults
[0m16:16:45.570632 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:a4b3fae3-2a5e-4d10-a947-178e20c9c448&page=queryresults
[0m16:16:45.642747 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:4ac1bd25-6912-463f-a626-08427ba5d7f4&page=queryresults
[0m16:16:45.887866 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:005be0dc-b685-4a41-b325-a2f2ac4060a9&page=queryresults
[0m16:16:46.399364 [info ] [Thread-4 (]: 53 of 68 PASS source_not_null_raw_data_subscriptions__id ....................... [[32mPASS[0m in 2.10s]
[0m16:16:46.404701 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a
[0m16:16:46.406961 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469
[0m16:16:46.408783 [info ] [Thread-4 (]: 57 of 68 START test unique_dim_subscription_subscription_id .................... [RUN]
[0m16:16:46.410691 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a, now test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469)
[0m16:16:46.412550 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469
[0m16:16:46.421466 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469"
[0m16:16:46.422715 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469
[0m16:16:46.426517 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469"
[0m16:16:46.428006 [debug] [Thread-4 (]: On test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select subscription_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_subscription`
  where subscription_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m16:16:46.428857 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m16:16:46.753392 [info ] [Thread-1 (]: 55 of 68 PASS source_not_null_raw_data_users__id ............................... [[32mPASS[0m in 2.15s]
[0m16:16:46.757456 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b
[0m16:16:46.758440 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.unique_fct_order_order_id.653705d6a5
[0m16:16:46.759659 [info ] [Thread-1 (]: 58 of 68 START test unique_fct_order_order_id .................................. [RUN]
[0m16:16:46.760416 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b, now test.data_pipeline_project.unique_fct_order_order_id.653705d6a5)
[0m16:16:46.760954 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.unique_fct_order_order_id.653705d6a5
[0m16:16:46.766706 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.unique_fct_order_order_id.653705d6a5"
[0m16:16:46.767786 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.unique_fct_order_order_id.653705d6a5
[0m16:16:46.771462 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.unique_fct_order_order_id.653705d6a5"
[0m16:16:46.774047 [debug] [Thread-1 (]: On test.data_pipeline_project.unique_fct_order_order_id.653705d6a5: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_fct_order_order_id.653705d6a5"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select order_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_order`
  where order_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m16:16:46.775378 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:16:46.825844 [error] [Thread-2 (]: 56 of 68 FAIL 16 unique_dim_customer_customer_id ............................... [[31mFAIL 16[0m in 2.14s]
[0m16:16:46.831140 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1
[0m16:16:46.832844 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3
[0m16:16:46.833781 [info ] [Thread-2 (]: 59 of 68 START test unique_fct_shipment_shipment_id ............................ [RUN]
[0m16:16:46.837920 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_dim_customer_customer_id.b42affccd1, now test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3)
[0m16:16:46.839027 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3
[0m16:16:46.851518 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3"
[0m16:16:46.856136 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3
[0m16:16:46.868812 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3"
[0m16:16:46.872611 [debug] [Thread-2 (]: On test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select shipment_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_shipment`
  where shipment_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m16:16:46.873684 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m16:16:46.943091 [info ] [Thread-3 (]: 54 of 68 PASS source_not_null_raw_data_subscriptions__user ..................... [[32mPASS[0m in 2.54s]
[0m16:16:46.949518 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6
[0m16:16:46.951276 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd
[0m16:16:46.956809 [info ] [Thread-3 (]: 60 of 68 START test unique_mart_acquisition_efficiency_date .................... [RUN]
[0m16:16:46.963381 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6, now test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd)
[0m16:16:46.964396 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd
[0m16:16:46.975301 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd"
[0m16:16:46.976178 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd
[0m16:16:46.978583 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd"
[0m16:16:46.979290 [debug] [Thread-3 (]: On test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select date as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_acquisition_efficiency`
  where date is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m16:16:46.979840 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m16:16:47.528103 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:acf728c8-b126-4ecc-a966-47a1bb776a48&page=queryresults
[0m16:16:48.028628 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:e83c9c6f-88b1-4f1f-953c-e69c0f7a64a5&page=queryresults
[0m16:16:48.106317 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:f78e0134-a234-47e7-9610-82295918c386&page=queryresults
[0m16:16:48.193573 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:950cd1c9-b7b1-450b-9b11-6cfdee83e13f&page=queryresults
[0m16:16:48.396502 [info ] [Thread-4 (]: 57 of 68 PASS unique_dim_subscription_subscription_id .......................... [[32mPASS[0m in 1.98s]
[0m16:16:48.400797 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469
[0m16:16:48.402885 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8
[0m16:16:48.405348 [info ] [Thread-4 (]: 61 of 68 START test unique_mart_revenue_daily_date ............................. [RUN]
[0m16:16:48.407658 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469, now test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8)
[0m16:16:48.409507 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8
[0m16:16:48.418971 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8"
[0m16:16:48.420623 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8
[0m16:16:48.426805 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8"
[0m16:16:48.428269 [debug] [Thread-4 (]: On test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select date as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_revenue_daily`
  where date is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m16:16:48.429570 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m16:16:48.841605 [info ] [Thread-2 (]: 59 of 68 PASS unique_fct_shipment_shipment_id .................................. [[32mPASS[0m in 2.00s]
[0m16:16:48.849164 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3
[0m16:16:48.855491 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c
[0m16:16:48.858757 [info ] [Thread-2 (]: 62 of 68 START test unique_mart_subscription_daily_date ........................ [RUN]
[0m16:16:48.860640 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3, now test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c)
[0m16:16:48.862737 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c
[0m16:16:48.874139 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c"
[0m16:16:48.879539 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c
[0m16:16:48.883574 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c"
[0m16:16:48.885679 [debug] [Thread-2 (]: On test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select date as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily`
  where date is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m16:16:48.886809 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m16:16:48.955650 [info ] [Thread-1 (]: 58 of 68 PASS unique_fct_order_order_id ........................................ [[32mPASS[0m in 2.19s]
[0m16:16:48.958784 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.unique_fct_order_order_id.653705d6a5
[0m16:16:48.959472 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00
[0m16:16:48.960040 [info ] [Thread-1 (]: 63 of 68 START test unique_stg_addresses_address_id ............................ [RUN]
[0m16:16:48.961383 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_fct_order_order_id.653705d6a5, now test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00)
[0m16:16:48.961989 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00
[0m16:16:48.975490 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00"
[0m16:16:48.978270 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00
[0m16:16:48.982788 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00"
[0m16:16:48.984493 [debug] [Thread-1 (]: On test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select address_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_addresses`
  where address_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m16:16:48.991789 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:16:49.083704 [info ] [Thread-3 (]: 60 of 68 PASS unique_mart_acquisition_efficiency_date .......................... [[32mPASS[0m in 2.12s]
[0m16:16:49.088881 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd
[0m16:16:49.090568 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.unique_stg_countries_country_id.8679936442
[0m16:16:49.091773 [info ] [Thread-3 (]: 64 of 68 START test unique_stg_countries_country_id ............................ [RUN]
[0m16:16:49.094643 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd, now test.data_pipeline_project.unique_stg_countries_country_id.8679936442)
[0m16:16:49.095577 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.unique_stg_countries_country_id.8679936442
[0m16:16:49.116890 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.unique_stg_countries_country_id.8679936442"
[0m16:16:49.121748 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.unique_stg_countries_country_id.8679936442
[0m16:16:49.125827 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.unique_stg_countries_country_id.8679936442"
[0m16:16:49.127999 [debug] [Thread-3 (]: On test.data_pipeline_project.unique_stg_countries_country_id.8679936442: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_stg_countries_country_id.8679936442"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select country_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_countries`
  where country_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m16:16:49.128982 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m16:16:49.599738 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:5c869453-9316-407e-b37e-9a1875dc82b5&page=queryresults
[0m16:16:50.042359 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:7ceb004d-3624-48e5-8b8f-7e5e0436c7ce&page=queryresults
[0m16:16:50.314780 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:606631fb-3b26-4f39-a06e-0dd934e8c075&page=queryresults
[0m16:16:50.317185 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:3a007b67-46f2-4ff8-b3ac-3ef19a51c9f7&page=queryresults
[0m16:16:50.441742 [info ] [Thread-4 (]: 61 of 68 PASS unique_mart_revenue_daily_date ................................... [[32mPASS[0m in 2.03s]
[0m16:16:50.445473 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8
[0m16:16:50.447784 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a
[0m16:16:50.450491 [info ] [Thread-4 (]: 65 of 68 START test unique_stg_orders_order_id ................................. [RUN]
[0m16:16:50.451830 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8, now test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a)
[0m16:16:50.452723 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a
[0m16:16:50.460750 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a"
[0m16:16:50.461813 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a
[0m16:16:50.465464 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a"
[0m16:16:50.466471 [debug] [Thread-4 (]: On test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select order_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_orders`
  where order_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m16:16:50.466983 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m16:16:50.863398 [info ] [Thread-2 (]: 62 of 68 PASS unique_mart_subscription_daily_date .............................. [[32mPASS[0m in 2.00s]
[0m16:16:50.870667 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c
[0m16:16:50.873202 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c
[0m16:16:50.874530 [info ] [Thread-2 (]: 66 of 68 START test unique_stg_shipments_shipment_id ........................... [RUN]
[0m16:16:50.877131 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c, now test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c)
[0m16:16:50.878324 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c
[0m16:16:50.893494 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c"
[0m16:16:50.898076 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c
[0m16:16:50.902770 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c"
[0m16:16:50.903521 [debug] [Thread-2 (]: On test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select shipment_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_shipments`
  where shipment_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m16:16:50.903969 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m16:16:51.171938 [info ] [Thread-1 (]: 63 of 68 PASS unique_stg_addresses_address_id .................................. [[32mPASS[0m in 2.21s]
[0m16:16:51.180265 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00
[0m16:16:51.183944 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m16:16:51.185669 [info ] [Thread-1 (]: 67 of 68 START test unique_stg_subscriptions_subscription_id ................... [RUN]
[0m16:16:51.187884 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00, now test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e)
[0m16:16:51.191594 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m16:16:51.203051 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e"
[0m16:16:51.204116 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m16:16:51.207951 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e"
[0m16:16:51.210084 [debug] [Thread-1 (]: On test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select subscription_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_subscriptions`
  where subscription_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m16:16:51.213124 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:16:51.251277 [info ] [Thread-3 (]: 64 of 68 PASS unique_stg_countries_country_id .................................. [[32mPASS[0m in 2.16s]
[0m16:16:51.254461 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.unique_stg_countries_country_id.8679936442
[0m16:16:51.255806 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b
[0m16:16:51.256903 [info ] [Thread-3 (]: 68 of 68 START test unique_stg_users_user_id ................................... [RUN]
[0m16:16:51.258279 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_stg_countries_country_id.8679936442, now test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b)
[0m16:16:51.259264 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b
[0m16:16:51.276833 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b"
[0m16:16:51.278008 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b
[0m16:16:51.282059 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b"
[0m16:16:51.285171 [debug] [Thread-3 (]: On test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select user_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
  where user_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m16:16:51.286678 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m16:16:51.775135 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:4dea9481-8aba-4f24-8f8d-72359a863fe3&page=queryresults
[0m16:16:52.526856 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:c08bfc5c-1336-4eb3-9c16-5bf55cd93e31&page=queryresults
[0m16:16:52.686397 [info ] [Thread-4 (]: 65 of 68 PASS unique_stg_orders_order_id ....................................... [[32mPASS[0m in 2.23s]
[0m16:16:52.687702 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a
[0m16:16:52.765496 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:e98fb4dc-7516-47ee-ae5e-7edccc4771f1&page=queryresults
[0m16:16:52.917213 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:2d9246aa-e061-48ae-99fc-b15671069ff8&page=queryresults
[0m16:16:53.365692 [info ] [Thread-2 (]: 66 of 68 PASS unique_stg_shipments_shipment_id ................................. [[32mPASS[0m in 2.49s]
[0m16:16:53.371896 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c
[0m16:16:53.563410 [info ] [Thread-1 (]: 67 of 68 PASS unique_stg_subscriptions_subscription_id ......................... [[32mPASS[0m in 2.37s]
[0m16:16:53.569351 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m16:16:53.921311 [info ] [Thread-3 (]: 68 of 68 PASS unique_stg_users_user_id ......................................... [[32mPASS[0m in 2.66s]
[0m16:16:53.922716 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b
[0m16:16:53.928390 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m16:16:53.930667 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:16:53.931470 [debug] [MainThread]: Connection 'test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c' was properly closed.
[0m16:16:53.931999 [debug] [MainThread]: Connection 'test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e' was properly closed.
[0m16:16:53.932493 [debug] [MainThread]: Connection 'test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b' was properly closed.
[0m16:16:53.933364 [debug] [MainThread]: Connection 'test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a' was properly closed.
[0m16:16:53.934344 [info ] [MainThread]: 
[0m16:16:53.935197 [info ] [MainThread]: Finished running 68 data tests in 0 hours 0 minutes and 40.44 seconds (40.44s).
[0m16:16:53.953932 [debug] [MainThread]: Command end result
[0m16:16:54.030537 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m16:16:54.034651 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m16:16:54.046642 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m16:16:54.047612 [info ] [MainThread]: 
[0m16:16:54.048546 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 1 warning:[0m
[0m16:16:54.049329 [info ] [MainThread]: 
[0m16:16:54.050077 [error] [MainThread]: [31mFailure in test unique_dim_customer_customer_id (models/intermediate/int_facts_marts_schema.yml)[0m
[0m16:16:54.050895 [error] [MainThread]:   Got 16 results, configured to fail if != 0
[0m16:16:54.051718 [info ] [MainThread]: 
[0m16:16:54.052485 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_project/models/intermediate/int_facts_marts_schema.yml/unique_dim_customer_customer_id.sql
[0m16:16:54.053412 [info ] [MainThread]: 
[0m16:16:54.054037 [warn ] [MainThread]: [33mWarning in test not_null_int_shipments_decomposed_delivered_at (models/staging/staging_schema.yml)[0m
[0m16:16:54.055510 [warn ] [MainThread]: Got 303 results, configured to warn if != 0
[0m16:16:54.056640 [info ] [MainThread]: 
[0m16:16:54.057490 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_project/models/staging/staging_schema.yml/not_null_int_shipments_decomposed_delivered_at.sql
[0m16:16:54.058336 [info ] [MainThread]: 
[0m16:16:54.059004 [info ] [MainThread]: Done. PASS=66 WARN=1 ERROR=1 SKIP=0 NO-OP=0 TOTAL=68
[0m16:16:54.063385 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 43.801476, "process_in_blocks": "0", "process_kernel_time": 4.059437, "process_mem_max_rss": "395356", "process_out_blocks": "5520", "process_user_time": 16.08281}
[0m16:16:54.064420 [debug] [MainThread]: Command `dbt test` failed at 16:16:54.064289 after 43.80 seconds
[0m16:16:54.065068 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e1674fb4470>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e164ccb3470>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e164cdae0f0>]}
[0m16:16:54.067374 [debug] [MainThread]: Flushing usage events
[0m16:16:54.981777 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:39:05.788147 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x729709ad8740>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x729709ad92e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72970a0838c0>]}


============================== 16:39:05.792027 | 269ef1de-2a9c-4278-bf96-1d6820c23a19 ==============================
[0m16:39:05.792027 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m16:39:05.792991 [debug] [MainThread]: running dbt with arguments {'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'quiet': 'False', 'version_check': 'True', 'static_parser': 'True', 'log_cache_events': 'False', 'debug': 'False', 'use_experimental_parser': 'False', 'log_format': 'default', 'cache_selected_only': 'False', 'profiles_dir': '/home/ecem/.dbt', 'indirect_selection': 'eager', 'printer_width': '80', 'empty': 'None', 'write_json': 'True', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'introspect': 'True', 'invocation_command': 'dbt test --select dim_customer', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'no_print': 'None', 'target_path': 'None'}
[0m16:39:08.197294 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '269ef1de-2a9c-4278-bf96-1d6820c23a19', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72970b946f90>]}
[0m16:39:08.262586 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '269ef1de-2a9c-4278-bf96-1d6820c23a19', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72970997d4c0>]}
[0m16:39:08.263727 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m16:39:08.477529 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m16:39:08.810641 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m16:39:08.812201 [debug] [MainThread]: Partial parsing: updated file: data_pipeline_project://models/dims/dim_subscription.sql
[0m16:39:08.813088 [debug] [MainThread]: Partial parsing: updated file: data_pipeline_project://models/facts/fct_order.sql
[0m16:39:08.813767 [debug] [MainThread]: Partial parsing: updated file: data_pipeline_project://models/intermediate/int_facts_marts_schema.yml
[0m16:39:09.342136 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '269ef1de-2a9c-4278-bf96-1d6820c23a19', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7296e8c42660>]}
[0m16:39:09.472063 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m16:39:09.475352 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m16:39:09.510168 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '269ef1de-2a9c-4278-bf96-1d6820c23a19', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7296e8a3a7e0>]}
[0m16:39:09.512192 [info ] [MainThread]: Found 21 models, 67 data tests, 10 sources, 624 macros
[0m16:39:09.512912 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '269ef1de-2a9c-4278-bf96-1d6820c23a19', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7296e97d7a10>]}
[0m16:39:09.515706 [info ] [MainThread]: 
[0m16:39:09.516427 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m16:39:09.517100 [info ] [MainThread]: 
[0m16:39:09.518047 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m16:39:09.524914 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_marts'
[0m16:39:09.525651 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:39:09.561841 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_staging'
[0m16:39:09.563627 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:39:10.905903 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '269ef1de-2a9c-4278-bf96-1d6820c23a19', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7296e8a12150>]}
[0m16:39:10.906897 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:39:10.916418 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc
[0m16:39:10.917589 [info ] [Thread-1 (]: 1 of 3 START test not_null_dim_customer_customer_id ............................ [RUN]
[0m16:39:10.919076 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29
[0m16:39:10.920031 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_marts, now test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc)
[0m16:39:10.920814 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85
[0m16:39:10.921770 [info ] [Thread-2 (]: 2 of 3 START test relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_  [RUN]
[0m16:39:10.922822 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc
[0m16:39:10.924573 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29)
[0m16:39:10.923856 [info ] [Thread-3 (]: 3 of 3 START test relationships_fct_order_customer_id__customer_id__ref_dim_customer_  [RUN]
[0m16:39:10.938863 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29
[0m16:39:10.949330 [debug] [Thread-3 (]: Acquiring new bigquery connection 'test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85'
[0m16:39:10.947999 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc"
[0m16:39:10.956653 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29"
[0m16:39:10.957602 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85
[0m16:39:10.958974 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29
[0m16:39:10.959970 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc
[0m16:39:10.966557 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85"
[0m16:39:11.040159 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc"
[0m16:39:11.037111 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29"
[0m16:39:11.042000 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85
[0m16:39:11.043574 [debug] [Thread-2 (]: On test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select customer_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_subscription`
    where customer_id is not null
),

parent as (
    select customer_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_customer`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m16:39:11.047112 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85"
[0m16:39:11.048750 [debug] [Thread-1 (]: On test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select customer_id
from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_customer`
where customer_id is null



  
  
      
    ) dbt_internal_test
[0m16:39:11.050112 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m16:39:11.053831 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:39:11.113643 [debug] [Thread-3 (]: On test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select customer_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_order`
    where customer_id is not null
),

parent as (
    select customer_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_customer`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m16:39:11.146826 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m16:39:12.483802 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:6a23fc13-7600-4e57-8939-0ccc45fecb67&page=queryresults
[0m16:39:12.488572 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:8499f1d3-3ebd-43f5-9843-c4a01cf9f9e0&page=queryresults
[0m16:39:12.490300 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:fe80d2fd-1f04-4432-b7a8-7fc07038b2c0&page=queryresults
[0m16:39:13.857849 [info ] [Thread-2 (]: 2 of 3 PASS relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_  [[32mPASS[0m in 2.93s]
[0m16:39:13.861504 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29
[0m16:39:13.873056 [info ] [Thread-1 (]: 1 of 3 PASS not_null_dim_customer_customer_id .................................. [[32mPASS[0m in 2.95s]
[0m16:39:13.875422 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc
[0m16:39:13.878321 [info ] [Thread-3 (]: 3 of 3 PASS relationships_fct_order_customer_id__customer_id__ref_dim_customer_  [[32mPASS[0m in 2.93s]
[0m16:39:13.879829 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85
[0m16:39:13.890133 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m16:39:13.892058 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:39:13.893794 [debug] [MainThread]: Connection 'test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc' was properly closed.
[0m16:39:13.895021 [debug] [MainThread]: Connection 'test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29' was properly closed.
[0m16:39:13.895932 [debug] [MainThread]: Connection 'test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85' was properly closed.
[0m16:39:13.897917 [info ] [MainThread]: 
[0m16:39:13.899118 [info ] [MainThread]: Finished running 3 data tests in 0 hours 0 minutes and 4.38 seconds (4.38s).
[0m16:39:13.901784 [debug] [MainThread]: Command end result
[0m16:39:13.983565 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m16:39:13.987885 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m16:39:13.996706 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m16:39:13.997220 [info ] [MainThread]: 
[0m16:39:13.997994 [info ] [MainThread]: [32mCompleted successfully[0m
[0m16:39:13.998624 [info ] [MainThread]: 
[0m16:39:13.999279 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=3
[0m16:39:14.000497 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": true, "command_wall_clock_time": 8.273726, "process_in_blocks": "0", "process_kernel_time": 1.267471, "process_mem_max_rss": "391136", "process_out_blocks": "5616", "process_user_time": 5.532484}
[0m16:39:14.001094 [debug] [MainThread]: Command `dbt test` succeeded at 16:39:14.000990 after 8.27 seconds
[0m16:39:14.001566 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7297097346e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7296e8b53d10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7296e8b50080>]}
[0m16:39:14.002072 [debug] [MainThread]: Flushing usage events
[0m16:39:19.850230 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m18:37:25.773673 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x713dda8f1520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x713dda23bb00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x713dda99b140>]}


============================== 18:37:25.778475 | 6db7e7f8-5767-4d4f-86c7-7f3b3383404d ==============================
[0m18:37:25.778475 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m18:37:25.779559 [debug] [MainThread]: running dbt with arguments {'write_json': 'True', 'printer_width': '80', 'use_experimental_parser': 'False', 'log_cache_events': 'False', 'indirect_selection': 'eager', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'send_anonymous_usage_stats': 'True', 'static_parser': 'True', 'log_format': 'default', 'empty': 'None', 'version_check': 'True', 'use_colors': 'True', 'debug': 'False', 'no_print': 'None', 'fail_fast': 'False', 'partial_parse': 'True', 'target_path': 'None', 'invocation_command': 'dbt docs generate', 'introspect': 'True', 'profiles_dir': '/home/ecem/.dbt', 'warn_error': 'None', 'cache_selected_only': 'False', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'quiet': 'False'}
[0m18:37:28.902137 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6db7e7f8-5767-4d4f-86c7-7f3b3383404d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x713dbacf0200>]}
[0m18:37:28.971285 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6db7e7f8-5767-4d4f-86c7-7f3b3383404d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x713ddc9162a0>]}
[0m18:37:28.972981 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m18:37:29.250195 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m18:37:29.582853 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 3 files added, 0 files changed.
[0m18:37:29.584119 [debug] [MainThread]: Partial parsing: added file: data_pipeline_project://analyses/CAC_CAC_payback_5_last_days.sql
[0m18:37:29.584966 [debug] [MainThread]: Partial parsing: added file: data_pipeline_project://analyses/top_10_cities_subscribers.sql
[0m18:37:29.585707 [debug] [MainThread]: Partial parsing: added file: data_pipeline_project://analyses/last_5_days_daily_revenue.sql
[0m18:37:30.068220 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6db7e7f8-5767-4d4f-86c7-7f3b3383404d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x713dba5a0320>]}
[0m18:37:30.139705 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6db7e7f8-5767-4d4f-86c7-7f3b3383404d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x713dba45d7c0>]}
[0m18:37:30.142954 [info ] [MainThread]: Found 21 models, 67 data tests, 3 analyses, 10 sources, 624 macros
[0m18:37:30.145341 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6db7e7f8-5767-4d4f-86c7-7f3b3383404d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x713dba5327e0>]}
[0m18:37:30.161786 [info ] [MainThread]: 
[0m18:37:30.164594 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m18:37:30.166753 [info ] [MainThread]: 
[0m18:37:30.168241 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m18:37:30.178320 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_marts'
[0m18:37:30.179270 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:37:30.180766 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_staging'
[0m18:37:30.243905 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:37:31.380219 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6db7e7f8-5767-4d4f-86c7-7f3b3383404d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x713dba5a0230>]}
[0m18:37:31.381707 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:37:31.412661 [debug] [Thread-2 (]: Began running node analysis.data_pipeline_project.last_5_days_daily_revenue
[0m18:37:31.422708 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_marts, now analysis.data_pipeline_project.last_5_days_daily_revenue)
[0m18:37:31.416766 [debug] [Thread-3 (]: Began running node analysis.data_pipeline_project.top_10_cities_subscribers
[0m18:37:31.407294 [debug] [Thread-1 (]: Began running node analysis.data_pipeline_project.CAC_CAC_payback_5_last_days
[0m18:37:31.425756 [debug] [Thread-2 (]: Began compiling node analysis.data_pipeline_project.last_5_days_daily_revenue
[0m18:37:31.427296 [debug] [Thread-4 (]: Began running node model.data_pipeline_project.stg_addresses
[0m18:37:31.431602 [debug] [Thread-3 (]: Acquiring new bigquery connection 'analysis.data_pipeline_project.top_10_cities_subscribers'
[0m18:37:31.433879 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now analysis.data_pipeline_project.CAC_CAC_payback_5_last_days)
[0m18:37:31.449287 [debug] [Thread-2 (]: Writing injected SQL for node "analysis.data_pipeline_project.last_5_days_daily_revenue"
[0m18:37:31.451944 [debug] [Thread-4 (]: Acquiring new bigquery connection 'model.data_pipeline_project.stg_addresses'
[0m18:37:31.453922 [debug] [Thread-3 (]: Began compiling node analysis.data_pipeline_project.top_10_cities_subscribers
[0m18:37:31.455678 [debug] [Thread-1 (]: Began compiling node analysis.data_pipeline_project.CAC_CAC_payback_5_last_days
[0m18:37:31.460732 [debug] [Thread-4 (]: Began compiling node model.data_pipeline_project.stg_addresses
[0m18:37:31.477063 [debug] [Thread-3 (]: Writing injected SQL for node "analysis.data_pipeline_project.top_10_cities_subscribers"
[0m18:37:31.480885 [debug] [Thread-2 (]: Began executing node analysis.data_pipeline_project.last_5_days_daily_revenue
[0m18:37:31.485164 [debug] [Thread-1 (]: Writing injected SQL for node "analysis.data_pipeline_project.CAC_CAC_payback_5_last_days"
[0m18:37:31.497027 [debug] [Thread-4 (]: Writing injected SQL for node "model.data_pipeline_project.stg_addresses"
[0m18:37:31.499819 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m18:37:31.503020 [debug] [Thread-3 (]: Began executing node analysis.data_pipeline_project.top_10_cities_subscribers
[0m18:37:31.506482 [debug] [Thread-1 (]: Began executing node analysis.data_pipeline_project.CAC_CAC_payback_5_last_days
[0m18:37:31.508935 [debug] [Thread-4 (]: Began executing node model.data_pipeline_project.stg_addresses
[0m18:37:31.514836 [debug] [Thread-2 (]: Finished running node analysis.data_pipeline_project.last_5_days_daily_revenue
[0m18:37:31.516198 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m18:37:31.518609 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m18:37:31.523617 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m18:37:31.525790 [debug] [Thread-2 (]: Began running node model.data_pipeline_project.stg_cities
[0m18:37:31.534411 [debug] [Thread-3 (]: Finished running node analysis.data_pipeline_project.top_10_cities_subscribers
[0m18:37:31.541100 [debug] [Thread-1 (]: Finished running node analysis.data_pipeline_project.CAC_CAC_payback_5_last_days
[0m18:37:31.546193 [debug] [Thread-4 (]: Finished running node model.data_pipeline_project.stg_addresses
[0m18:37:31.548278 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly analysis.data_pipeline_project.last_5_days_daily_revenue, now model.data_pipeline_project.stg_cities)
[0m18:37:31.560061 [debug] [Thread-2 (]: Began compiling node model.data_pipeline_project.stg_cities
[0m18:37:31.553751 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.stg_marketing_spend
[0m18:37:31.576299 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly analysis.data_pipeline_project.CAC_CAC_payback_5_last_days, now model.data_pipeline_project.stg_marketing_spend)
[0m18:37:31.550998 [debug] [Thread-3 (]: Began running node model.data_pipeline_project.stg_countries
[0m18:37:31.574524 [debug] [Thread-2 (]: Writing injected SQL for node "model.data_pipeline_project.stg_cities"
[0m18:37:31.557782 [debug] [Thread-4 (]: Began running node model.data_pipeline_project.stg_neighborhoods
[0m18:37:31.577827 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.stg_marketing_spend
[0m18:37:31.579831 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly analysis.data_pipeline_project.top_10_cities_subscribers, now model.data_pipeline_project.stg_countries)
[0m18:37:31.582326 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.data_pipeline_project.stg_addresses, now model.data_pipeline_project.stg_neighborhoods)
[0m18:37:31.584457 [debug] [Thread-2 (]: Began executing node model.data_pipeline_project.stg_cities
[0m18:37:31.595784 [debug] [Thread-3 (]: Began compiling node model.data_pipeline_project.stg_countries
[0m18:37:31.611441 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m18:37:31.603927 [debug] [Thread-4 (]: Began compiling node model.data_pipeline_project.stg_neighborhoods
[0m18:37:31.601859 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.stg_marketing_spend"
[0m18:37:31.630747 [debug] [Thread-2 (]: Finished running node model.data_pipeline_project.stg_cities
[0m18:37:31.636148 [debug] [Thread-3 (]: Writing injected SQL for node "model.data_pipeline_project.stg_countries"
[0m18:37:31.643283 [debug] [Thread-4 (]: Writing injected SQL for node "model.data_pipeline_project.stg_neighborhoods"
[0m18:37:31.648652 [debug] [Thread-2 (]: Began running node model.data_pipeline_project.stg_orders
[0m18:37:31.650697 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.stg_marketing_spend
[0m18:37:31.652701 [debug] [Thread-3 (]: Began executing node model.data_pipeline_project.stg_countries
[0m18:37:31.653896 [debug] [Thread-4 (]: Began executing node model.data_pipeline_project.stg_neighborhoods
[0m18:37:31.654990 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.data_pipeline_project.stg_cities, now model.data_pipeline_project.stg_orders)
[0m18:37:31.656077 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m18:37:31.657314 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m18:37:31.657977 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m18:37:31.659678 [debug] [Thread-2 (]: Began compiling node model.data_pipeline_project.stg_orders
[0m18:37:31.663340 [debug] [Thread-3 (]: Finished running node model.data_pipeline_project.stg_countries
[0m18:37:31.665910 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.stg_marketing_spend
[0m18:37:31.668117 [debug] [Thread-4 (]: Finished running node model.data_pipeline_project.stg_neighborhoods
[0m18:37:31.672036 [debug] [Thread-2 (]: Writing injected SQL for node "model.data_pipeline_project.stg_orders"
[0m18:37:31.673744 [debug] [Thread-3 (]: Began running node model.data_pipeline_project.stg_shipments
[0m18:37:31.674655 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.stg_states
[0m18:37:31.675527 [debug] [Thread-4 (]: Began running node model.data_pipeline_project.stg_subscriptions
[0m18:37:31.677974 [debug] [Thread-2 (]: Began executing node model.data_pipeline_project.stg_orders
[0m18:37:31.679025 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly model.data_pipeline_project.stg_countries, now model.data_pipeline_project.stg_shipments)
[0m18:37:31.680433 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline_project.stg_marketing_spend, now model.data_pipeline_project.stg_states)
[0m18:37:31.681287 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.data_pipeline_project.stg_neighborhoods, now model.data_pipeline_project.stg_subscriptions)
[0m18:37:31.682756 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m18:37:31.684271 [debug] [Thread-3 (]: Began compiling node model.data_pipeline_project.stg_shipments
[0m18:37:31.684930 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.stg_states
[0m18:37:31.685795 [debug] [Thread-4 (]: Began compiling node model.data_pipeline_project.stg_subscriptions
[0m18:37:31.688785 [debug] [Thread-2 (]: Finished running node model.data_pipeline_project.stg_orders
[0m18:37:31.694796 [debug] [Thread-3 (]: Writing injected SQL for node "model.data_pipeline_project.stg_shipments"
[0m18:37:31.699201 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.stg_states"
[0m18:37:31.702818 [debug] [Thread-4 (]: Writing injected SQL for node "model.data_pipeline_project.stg_subscriptions"
[0m18:37:31.703911 [debug] [Thread-2 (]: Began running node model.data_pipeline_project.stg_users
[0m18:37:31.708649 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.data_pipeline_project.stg_orders, now model.data_pipeline_project.stg_users)
[0m18:37:31.707826 [debug] [Thread-3 (]: Began executing node model.data_pipeline_project.stg_shipments
[0m18:37:31.709567 [debug] [Thread-4 (]: Began executing node model.data_pipeline_project.stg_subscriptions
[0m18:37:31.710111 [debug] [Thread-2 (]: Began compiling node model.data_pipeline_project.stg_users
[0m18:37:31.710928 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.stg_states
[0m18:37:31.711817 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m18:37:31.712572 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m18:37:31.716016 [debug] [Thread-2 (]: Writing injected SQL for node "model.data_pipeline_project.stg_users"
[0m18:37:31.717334 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m18:37:31.720949 [debug] [Thread-3 (]: Finished running node model.data_pipeline_project.stg_shipments
[0m18:37:31.723076 [debug] [Thread-4 (]: Finished running node model.data_pipeline_project.stg_subscriptions
[0m18:37:31.723872 [debug] [Thread-2 (]: Began executing node model.data_pipeline_project.stg_users
[0m18:37:31.726606 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.stg_states
[0m18:37:31.727716 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6
[0m18:37:31.728598 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc
[0m18:37:31.729822 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m18:37:31.731374 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd
[0m18:37:31.732367 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly model.data_pipeline_project.stg_shipments, now test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6)
[0m18:37:31.733090 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.data_pipeline_project.stg_subscriptions, now test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc)
[0m18:37:31.735341 [debug] [Thread-2 (]: Finished running node model.data_pipeline_project.stg_users
[0m18:37:31.736898 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline_project.stg_states, now test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd)
[0m18:37:31.739102 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6
[0m18:37:31.739921 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc
[0m18:37:31.740625 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be
[0m18:37:31.741690 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd
[0m18:37:31.757994 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.data_pipeline_project.stg_users, now test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be)
[0m18:37:31.763346 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6"
[0m18:37:31.762227 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc"
[0m18:37:31.769946 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd"
[0m18:37:31.770782 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be
[0m18:37:31.778846 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc
[0m18:37:31.779864 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be"
[0m18:37:31.780740 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6
[0m18:37:31.781491 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd
[0m18:37:31.782127 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m18:37:31.783103 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m18:37:31.784874 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be
[0m18:37:31.784110 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m18:37:31.787573 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6
[0m18:37:31.788553 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m18:37:31.791079 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc
[0m18:37:31.793113 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd
[0m18:37:31.794306 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878
[0m18:37:31.796658 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be
[0m18:37:31.797627 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f
[0m18:37:31.798478 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2
[0m18:37:31.799353 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6, now test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878)
[0m18:37:31.800031 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1
[0m18:37:31.800987 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc, now test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f)
[0m18:37:31.801881 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd, now test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2)
[0m18:37:31.802759 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878
[0m18:37:31.803401 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be, now test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1)
[0m18:37:31.804086 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f
[0m18:37:31.804933 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2
[0m18:37:31.810806 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878"
[0m18:37:31.811828 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1
[0m18:37:31.816279 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f"
[0m18:37:31.821050 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2"
[0m18:37:31.826482 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1"
[0m18:37:31.827224 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878
[0m18:37:31.829865 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f
[0m18:37:31.831461 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m18:37:31.830744 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2
[0m18:37:31.831992 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1
[0m18:37:31.832782 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m18:37:31.833577 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m18:37:31.835566 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878
[0m18:37:31.836537 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m18:37:31.840016 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f
[0m18:37:31.842468 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2
[0m18:37:31.843423 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98
[0m18:37:31.845714 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1
[0m18:37:31.846386 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86
[0m18:37:31.847902 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a
[0m18:37:31.848687 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878, now test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98)
[0m18:37:31.849647 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6
[0m18:37:31.850522 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f, now test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86)
[0m18:37:31.852289 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2, now test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a)
[0m18:37:31.853289 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98
[0m18:37:31.853990 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1, now test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6)
[0m18:37:31.854861 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86
[0m18:37:31.855742 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a
[0m18:37:31.861822 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98"
[0m18:37:31.862529 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6
[0m18:37:31.867741 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86"
[0m18:37:31.872541 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a"
[0m18:37:31.874849 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98
[0m18:37:31.881294 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6"
[0m18:37:31.883030 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m18:37:31.885596 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86
[0m18:37:31.886921 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a
[0m18:37:31.889935 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98
[0m18:37:31.890588 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6
[0m18:37:31.891461 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m18:37:31.892114 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m18:37:31.893028 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b
[0m18:37:31.893797 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m18:37:31.895980 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86
[0m18:37:31.898262 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a
[0m18:37:31.898903 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98, now test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b)
[0m18:37:31.901418 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6
[0m18:37:31.902186 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501
[0m18:37:31.903486 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3
[0m18:37:31.904519 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b
[0m18:37:31.905523 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00
[0m18:37:31.906494 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86, now test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501)
[0m18:37:31.907598 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a, now test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3)
[0m18:37:31.913350 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b"
[0m18:37:31.914511 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6, now test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00)
[0m18:37:31.915682 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501
[0m18:37:31.916699 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3
[0m18:37:31.918447 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00
[0m18:37:31.918983 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b
[0m18:37:31.922999 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501"
[0m18:37:31.927142 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3"
[0m18:37:31.933550 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00"
[0m18:37:31.934496 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m18:37:31.937690 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3
[0m18:37:31.940088 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00
[0m18:37:31.943699 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b
[0m18:37:31.944653 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501
[0m18:37:31.945735 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m18:37:31.946522 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m18:37:31.947543 [debug] [Thread-3 (]: Began running node model.data_pipeline_project.fct_marketing_spend
[0m18:37:31.948702 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m18:37:31.950692 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3
[0m18:37:31.953441 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00
[0m18:37:31.954340 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b, now model.data_pipeline_project.fct_marketing_spend)
[0m18:37:31.956764 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501
[0m18:37:31.958547 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2
[0m18:37:31.959466 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.unique_stg_countries_country_id.8679936442
[0m18:37:31.963948 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00, now test.data_pipeline_project.unique_stg_countries_country_id.8679936442)
[0m18:37:31.961058 [debug] [Thread-4 (]: Began running node model.data_pipeline_project.int_orders_decomposed
[0m18:37:31.962017 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3, now test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2)
[0m18:37:31.960398 [debug] [Thread-3 (]: Began compiling node model.data_pipeline_project.fct_marketing_spend
[0m18:37:31.964721 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.unique_stg_countries_country_id.8679936442
[0m18:37:31.965755 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501, now model.data_pipeline_project.int_orders_decomposed)
[0m18:37:31.966539 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2
[0m18:37:31.973067 [debug] [Thread-3 (]: Writing injected SQL for node "model.data_pipeline_project.fct_marketing_spend"
[0m18:37:31.978540 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.unique_stg_countries_country_id.8679936442"
[0m18:37:31.980064 [debug] [Thread-4 (]: Began compiling node model.data_pipeline_project.int_orders_decomposed
[0m18:37:31.985427 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2"
[0m18:37:31.987372 [debug] [Thread-3 (]: Began executing node model.data_pipeline_project.fct_marketing_spend
[0m18:37:31.991283 [debug] [Thread-4 (]: Writing injected SQL for node "model.data_pipeline_project.int_orders_decomposed"
[0m18:37:31.992706 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.unique_stg_countries_country_id.8679936442
[0m18:37:31.993880 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m18:37:31.994692 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2
[0m18:37:31.995677 [debug] [Thread-4 (]: Began executing node model.data_pipeline_project.int_orders_decomposed
[0m18:37:31.996489 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m18:37:31.998882 [debug] [Thread-3 (]: Finished running node model.data_pipeline_project.fct_marketing_spend
[0m18:37:31.999639 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m18:37:32.000298 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m18:37:32.002556 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.unique_stg_countries_country_id.8679936442
[0m18:37:32.003495 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64
[0m18:37:32.006021 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2
[0m18:37:32.008369 [debug] [Thread-4 (]: Finished running node model.data_pipeline_project.int_orders_decomposed
[0m18:37:32.009475 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209
[0m18:37:32.010385 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly model.data_pipeline_project.fct_marketing_spend, now test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64)
[0m18:37:32.011231 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a
[0m18:37:32.012644 [debug] [Thread-4 (]: Began running node model.data_pipeline_project.dim_subscription
[0m18:37:32.013535 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_stg_countries_country_id.8679936442, now test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209)
[0m18:37:32.014481 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64
[0m18:37:32.015144 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2, now test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a)
[0m18:37:32.016106 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.data_pipeline_project.int_orders_decomposed, now model.data_pipeline_project.dim_subscription)
[0m18:37:32.016930 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209
[0m18:37:32.022707 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64"
[0m18:37:32.024194 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a
[0m18:37:32.024863 [debug] [Thread-4 (]: Began compiling node model.data_pipeline_project.dim_subscription
[0m18:37:32.029545 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209"
[0m18:37:32.034753 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a"
[0m18:37:32.035389 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64
[0m18:37:32.039282 [debug] [Thread-4 (]: Writing injected SQL for node "model.data_pipeline_project.dim_subscription"
[0m18:37:32.040951 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m18:37:32.041938 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a
[0m18:37:32.042835 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209
[0m18:37:32.043715 [debug] [Thread-4 (]: Began executing node model.data_pipeline_project.dim_subscription
[0m18:37:32.046372 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64
[0m18:37:32.047114 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m18:37:32.047844 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m18:37:32.048979 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m18:37:32.058698 [debug] [Thread-4 (]: Finished running node model.data_pipeline_project.dim_subscription
[0m18:37:32.052809 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a
[0m18:37:32.056565 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209
[0m18:37:32.050396 [debug] [Thread-3 (]: Began running node model.data_pipeline_project.int_shipments_decomposed
[0m18:37:32.059508 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380
[0m18:37:32.061934 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852
[0m18:37:32.063431 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8
[0m18:37:32.064448 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64, now model.data_pipeline_project.int_shipments_decomposed)
[0m18:37:32.065460 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.data_pipeline_project.dim_subscription, now test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380)
[0m18:37:32.066189 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a, now test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852)
[0m18:37:32.067119 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209, now test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8)
[0m18:37:32.067950 [debug] [Thread-3 (]: Began compiling node model.data_pipeline_project.int_shipments_decomposed
[0m18:37:32.068752 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380
[0m18:37:32.069629 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852
[0m18:37:32.070834 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8
[0m18:37:32.074177 [debug] [Thread-3 (]: Writing injected SQL for node "model.data_pipeline_project.int_shipments_decomposed"
[0m18:37:32.085320 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852"
[0m18:37:32.087732 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380"
[0m18:37:32.098472 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8"
[0m18:37:32.101182 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380
[0m18:37:32.103835 [debug] [Thread-3 (]: Began executing node model.data_pipeline_project.int_shipments_decomposed
[0m18:37:32.104681 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8
[0m18:37:32.105972 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m18:37:32.106853 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m18:37:32.107665 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852
[0m18:37:32.108467 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m18:37:32.111808 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380
[0m18:37:32.114043 [debug] [Thread-3 (]: Finished running node model.data_pipeline_project.int_shipments_decomposed
[0m18:37:32.114913 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m18:37:32.117758 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8
[0m18:37:32.118565 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m18:37:32.119455 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2
[0m18:37:32.121911 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852
[0m18:37:32.122825 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c
[0m18:37:32.123696 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380, now test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91)
[0m18:37:32.124393 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly model.data_pipeline_project.int_shipments_decomposed, now test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2)
[0m18:37:32.125134 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m18:37:32.126020 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8, now test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c)
[0m18:37:32.127136 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m18:37:32.128006 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2
[0m18:37:32.128898 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852, now test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e)
[0m18:37:32.129659 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c
[0m18:37:32.135427 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91"
[0m18:37:32.140253 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2"
[0m18:37:32.141457 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m18:37:32.149801 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c"
[0m18:37:32.151200 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m18:37:32.152276 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2
[0m18:37:32.156938 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e"
[0m18:37:32.161516 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c
[0m18:37:32.159992 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m18:37:32.158461 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m18:37:32.162073 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m18:37:32.162905 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m18:37:32.164897 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2
[0m18:37:32.167672 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m18:37:32.168439 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m18:37:32.170735 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c
[0m18:37:32.171609 [debug] [Thread-3 (]: Began running node model.data_pipeline_project.dim_customer
[0m18:37:32.172917 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25
[0m18:37:32.176088 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m18:37:32.177359 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77
[0m18:37:32.178339 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2, now model.data_pipeline_project.dim_customer)
[0m18:37:32.179579 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91, now test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25)
[0m18:37:32.180453 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781
[0m18:37:32.181278 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c, now test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77)
[0m18:37:32.182110 [debug] [Thread-3 (]: Began compiling node model.data_pipeline_project.dim_customer
[0m18:37:32.183392 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25
[0m18:37:32.184463 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e, now test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781)
[0m18:37:32.185116 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77
[0m18:37:32.189858 [debug] [Thread-3 (]: Writing injected SQL for node "model.data_pipeline_project.dim_customer"
[0m18:37:32.195299 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25"
[0m18:37:32.196118 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781
[0m18:37:32.201329 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77"
[0m18:37:32.203220 [debug] [Thread-3 (]: Began executing node model.data_pipeline_project.dim_customer
[0m18:37:32.212084 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781"
[0m18:37:32.214628 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77
[0m18:37:32.213898 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m18:37:32.216431 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m18:37:32.217115 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781
[0m18:37:32.217770 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25
[0m18:37:32.220532 [debug] [Thread-3 (]: Finished running node model.data_pipeline_project.dim_customer
[0m18:37:32.223808 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77
[0m18:37:32.224652 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m18:37:32.225942 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m18:37:32.226826 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737
[0m18:37:32.227826 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8
[0m18:37:32.230687 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781
[0m18:37:32.233743 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25
[0m18:37:32.234725 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly model.data_pipeline_project.dim_customer, now test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737)
[0m18:37:32.235603 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77, now test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8)
[0m18:37:32.236501 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m18:37:32.237414 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b
[0m18:37:32.238076 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737
[0m18:37:32.239266 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8
[0m18:37:32.239979 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781, now test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e)
[0m18:37:32.240804 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25, now test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b)
[0m18:37:32.247304 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737"
[0m18:37:32.255212 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8"
[0m18:37:32.256733 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m18:37:32.259695 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b
[0m18:37:32.267904 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e"
[0m18:37:32.268915 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8
[0m18:37:32.269937 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737
[0m18:37:32.274865 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b"
[0m18:37:32.276298 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m18:37:32.277470 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m18:37:32.278574 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m18:37:32.279986 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b
[0m18:37:32.280958 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m18:37:32.283844 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8
[0m18:37:32.286351 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737
[0m18:37:32.287330 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m18:37:32.289950 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m18:37:32.291357 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.dbt_utils_accepted_range_fct_marketing_spend_spend_try__0.bfe523cb61
[0m18:37:32.292324 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719
[0m18:37:32.294865 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b
[0m18:37:32.295882 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d
[0m18:37:32.296604 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8, now test.data_pipeline_project.dbt_utils_accepted_range_fct_marketing_spend_spend_try__0.bfe523cb61)
[0m18:37:32.301018 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.dbt_utils_accepted_range_fct_marketing_spend_spend_try__0.bfe523cb61
[0m18:37:32.299437 [debug] [Thread-4 (]: Began running node model.data_pipeline_project.fct_order
[0m18:37:32.300164 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e, now test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d)
[0m18:37:32.298459 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737, now test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719)
[0m18:37:32.309623 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.dbt_utils_accepted_range_fct_marketing_spend_spend_try__0.bfe523cb61"
[0m18:37:32.310603 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b, now model.data_pipeline_project.fct_order)
[0m18:37:32.311970 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d
[0m18:37:32.312836 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719
[0m18:37:32.314392 [debug] [Thread-4 (]: Began compiling node model.data_pipeline_project.fct_order
[0m18:37:32.319057 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d"
[0m18:37:32.319687 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.dbt_utils_accepted_range_fct_marketing_spend_spend_try__0.bfe523cb61
[0m18:37:32.324084 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719"
[0m18:37:32.327806 [debug] [Thread-4 (]: Writing injected SQL for node "model.data_pipeline_project.fct_order"
[0m18:37:32.329380 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m18:37:32.329983 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d
[0m18:37:32.331372 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719
[0m18:37:32.334509 [debug] [Thread-4 (]: Began executing node model.data_pipeline_project.fct_order
[0m18:37:32.333885 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.dbt_utils_accepted_range_fct_marketing_spend_spend_try__0.bfe523cb61
[0m18:37:32.335210 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m18:37:32.336209 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m18:37:32.337276 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m18:37:32.343923 [debug] [Thread-4 (]: Finished running node model.data_pipeline_project.fct_order
[0m18:37:32.339941 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d
[0m18:37:32.342210 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719
[0m18:37:32.346450 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469
[0m18:37:32.344659 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723
[0m18:37:32.345591 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6
[0m18:37:32.337811 [debug] [Thread-2 (]: Began running node model.data_pipeline_project.mart_subscription_daily
[0m18:37:32.346980 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719, now test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469)
[0m18:37:32.347865 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.data_pipeline_project.fct_order, now test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723)
[0m18:37:32.348658 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d, now test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6)
[0m18:37:32.349356 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.dbt_utils_accepted_range_fct_marketing_spend_spend_try__0.bfe523cb61, now model.data_pipeline_project.mart_subscription_daily)
[0m18:37:32.350182 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469
[0m18:37:32.351061 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723
[0m18:37:32.351927 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6
[0m18:37:32.352798 [debug] [Thread-2 (]: Began compiling node model.data_pipeline_project.mart_subscription_daily
[0m18:37:32.358586 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469"
[0m18:37:32.364362 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723"
[0m18:37:32.373887 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6"
[0m18:37:32.377034 [debug] [Thread-2 (]: Writing injected SQL for node "model.data_pipeline_project.mart_subscription_daily"
[0m18:37:32.379744 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469
[0m18:37:32.380614 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723
[0m18:37:32.382071 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6
[0m18:37:32.383667 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m18:37:32.384623 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m18:37:32.385286 [debug] [Thread-2 (]: Began executing node model.data_pipeline_project.mart_subscription_daily
[0m18:37:32.386443 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m18:37:32.390058 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469
[0m18:37:32.392209 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723
[0m18:37:32.392933 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m18:37:32.395743 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6
[0m18:37:32.396532 [debug] [Thread-3 (]: Began running node model.data_pipeline_project.fct_shipment
[0m18:37:32.398368 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b
[0m18:37:32.399477 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31
[0m18:37:32.401711 [debug] [Thread-2 (]: Finished running node model.data_pipeline_project.mart_subscription_daily
[0m18:37:32.402652 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469, now model.data_pipeline_project.fct_shipment)
[0m18:37:32.403725 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723, now test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b)
[0m18:37:32.404436 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6, now test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31)
[0m18:37:32.405060 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc
[0m18:37:32.405980 [debug] [Thread-3 (]: Began compiling node model.data_pipeline_project.fct_shipment
[0m18:37:32.412898 [debug] [Thread-3 (]: Writing injected SQL for node "model.data_pipeline_project.fct_shipment"
[0m18:37:32.408560 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31
[0m18:37:32.409651 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.data_pipeline_project.mart_subscription_daily, now test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc)
[0m18:37:32.407662 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b
[0m18:37:32.414306 [debug] [Thread-3 (]: Began executing node model.data_pipeline_project.fct_shipment
[0m18:37:32.419681 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31"
[0m18:37:32.420632 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc
[0m18:37:32.425844 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b"
[0m18:37:32.426649 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m18:37:32.431351 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc"
[0m18:37:32.432510 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31
[0m18:37:32.435398 [debug] [Thread-3 (]: Finished running node model.data_pipeline_project.fct_shipment
[0m18:37:32.436715 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m18:37:32.437522 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b
[0m18:37:32.438739 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29
[0m18:37:32.439333 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc
[0m18:37:32.441546 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31
[0m18:37:32.442992 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m18:37:32.443869 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly model.data_pipeline_project.fct_shipment, now test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29)
[0m18:37:32.444773 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m18:37:32.445855 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.dbt_utils_accepted_range_fct_order_discount_total__0.5f31954cd6
[0m18:37:32.448133 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b
[0m18:37:32.448849 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29
[0m18:37:32.451277 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc
[0m18:37:32.452239 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31, now test.data_pipeline_project.dbt_utils_accepted_range_fct_order_discount_total__0.5f31954cd6)
[0m18:37:32.453064 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.dbt_utils_accepted_range_fct_order_net_revenue__0.cb846760ce
[0m18:37:32.460435 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29"
[0m18:37:32.462795 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff
[0m18:37:32.464206 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.dbt_utils_accepted_range_fct_order_discount_total__0.5f31954cd6
[0m18:37:32.466384 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b, now test.data_pipeline_project.dbt_utils_accepted_range_fct_order_net_revenue__0.cb846760ce)
[0m18:37:32.467494 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc, now test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff)
[0m18:37:32.472231 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.dbt_utils_accepted_range_fct_order_discount_total__0.5f31954cd6"
[0m18:37:32.473422 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.dbt_utils_accepted_range_fct_order_net_revenue__0.cb846760ce
[0m18:37:32.474791 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff
[0m18:37:32.475819 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29
[0m18:37:32.477504 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.dbt_utils_accepted_range_fct_order_discount_total__0.5f31954cd6
[0m18:37:32.484407 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.dbt_utils_accepted_range_fct_order_net_revenue__0.cb846760ce"
[0m18:37:32.488767 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff"
[0m18:37:32.489630 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m18:37:32.490642 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m18:37:32.492904 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.dbt_utils_accepted_range_fct_order_net_revenue__0.cb846760ce
[0m18:37:32.493630 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff
[0m18:37:32.497233 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29
[0m18:37:32.500103 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.dbt_utils_accepted_range_fct_order_discount_total__0.5f31954cd6
[0m18:37:32.500702 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m18:37:32.501653 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m18:37:32.502563 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea
[0m18:37:32.503363 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e
[0m18:37:32.506488 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.dbt_utils_accepted_range_fct_order_net_revenue__0.cb846760ce
[0m18:37:32.509726 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff
[0m18:37:32.510706 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29, now test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea)
[0m18:37:32.511586 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.dbt_utils_accepted_range_fct_order_discount_total__0.5f31954cd6, now test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e)
[0m18:37:32.512448 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def
[0m18:37:32.513153 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85
[0m18:37:32.514847 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea
[0m18:37:32.515582 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e
[0m18:37:32.516567 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.dbt_utils_accepted_range_fct_order_net_revenue__0.cb846760ce, now test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def)
[0m18:37:32.518111 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff, now test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85)
[0m18:37:32.523072 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea"
[0m18:37:32.527575 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e"
[0m18:37:32.528864 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def
[0m18:37:32.529825 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85
[0m18:37:32.534714 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def"
[0m18:37:32.539835 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85"
[0m18:37:32.542413 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea
[0m18:37:32.543335 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e
[0m18:37:32.544366 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m18:37:32.545532 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m18:37:32.546341 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85
[0m18:37:32.548632 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea
[0m18:37:32.550862 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e
[0m18:37:32.551604 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m18:37:32.552181 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def
[0m18:37:32.553032 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.unique_fct_order_order_id.653705d6a5
[0m18:37:32.554350 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.mart_acquisition_efficiency
[0m18:37:32.556786 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85
[0m18:37:32.558103 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m18:37:32.558993 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea, now test.data_pipeline_project.unique_fct_order_order_id.653705d6a5)
[0m18:37:32.559982 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e, now model.data_pipeline_project.mart_acquisition_efficiency)
[0m18:37:32.560926 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_active_subscribers__0.74fd82653d
[0m18:37:32.564502 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def
[0m18:37:32.565317 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.unique_fct_order_order_id.653705d6a5
[0m18:37:32.566410 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.mart_acquisition_efficiency
[0m18:37:32.567288 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85, now test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_active_subscribers__0.74fd82653d)
[0m18:37:32.568130 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_new_subscribers__0.654a8ca7d1
[0m18:37:32.576841 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.unique_fct_order_order_id.653705d6a5"
[0m18:37:32.584560 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_active_subscribers__0.74fd82653d
[0m18:37:32.583508 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.mart_acquisition_efficiency"
[0m18:37:32.595231 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.mart_acquisition_efficiency
[0m18:37:32.586961 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.unique_fct_order_order_id.653705d6a5
[0m18:37:32.594254 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_active_subscribers__0.74fd82653d"
[0m18:37:32.585769 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def, now test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_new_subscribers__0.654a8ca7d1)
[0m18:37:32.596568 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m18:37:32.597788 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m18:37:32.599345 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_new_subscribers__0.654a8ca7d1
[0m18:37:32.600732 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_active_subscribers__0.74fd82653d
[0m18:37:32.604029 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.mart_acquisition_efficiency
[0m18:37:32.609772 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_new_subscribers__0.654a8ca7d1"
[0m18:37:32.611099 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m18:37:32.614105 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.unique_fct_order_order_id.653705d6a5
[0m18:37:32.614870 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322
[0m18:37:32.618414 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_active_subscribers__0.74fd82653d
[0m18:37:32.619463 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c
[0m18:37:32.620606 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_new_subscribers__0.654a8ca7d1
[0m18:37:32.621695 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline_project.mart_acquisition_efficiency, now test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322)
[0m18:37:32.622724 [debug] [Thread-2 (]: Began running node model.data_pipeline_project.mart_revenue_daily
[0m18:37:32.623805 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_fct_order_order_id.653705d6a5, now test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c)
[0m18:37:32.624574 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m18:37:32.625492 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322
[0m18:37:32.626151 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_active_subscribers__0.74fd82653d, now model.data_pipeline_project.mart_revenue_daily)
[0m18:37:32.626926 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c
[0m18:37:32.629159 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_new_subscribers__0.654a8ca7d1
[0m18:37:32.633861 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322"
[0m18:37:32.634866 [debug] [Thread-2 (]: Began compiling node model.data_pipeline_project.mart_revenue_daily
[0m18:37:32.639651 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c"
[0m18:37:32.640670 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d
[0m18:37:32.644273 [debug] [Thread-2 (]: Writing injected SQL for node "model.data_pipeline_project.mart_revenue_daily"
[0m18:37:32.645238 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322
[0m18:37:32.646811 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_new_subscribers__0.654a8ca7d1, now test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d)
[0m18:37:32.647783 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c
[0m18:37:32.648848 [debug] [Thread-2 (]: Began executing node model.data_pipeline_project.mart_revenue_daily
[0m18:37:32.649714 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m18:37:32.650572 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d
[0m18:37:32.651435 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m18:37:32.652162 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m18:37:32.654738 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322
[0m18:37:32.658682 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d"
[0m18:37:32.661222 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c
[0m18:37:32.663568 [debug] [Thread-2 (]: Finished running node model.data_pipeline_project.mart_revenue_daily
[0m18:37:32.664525 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3
[0m18:37:32.665844 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.dbt_utils_accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0.be1f75eba9
[0m18:37:32.667037 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b
[0m18:37:32.668068 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d
[0m18:37:32.672879 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m18:37:32.669729 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322, now test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3)
[0m18:37:32.672114 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.data_pipeline_project.mart_revenue_daily, now test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b)
[0m18:37:32.670964 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c, now test.data_pipeline_project.dbt_utils_accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0.be1f75eba9)
[0m18:37:32.675841 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d
[0m18:37:32.676763 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3
[0m18:37:32.678459 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b
[0m18:37:32.679439 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.dbt_utils_accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0.be1f75eba9
[0m18:37:32.680961 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd
[0m18:37:32.690038 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3"
[0m18:37:32.695585 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b"
[0m18:37:32.701916 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.dbt_utils_accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0.be1f75eba9"
[0m18:37:32.702784 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d, now test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd)
[0m18:37:32.708432 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd
[0m18:37:32.713078 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd"
[0m18:37:32.714393 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd
[0m18:37:32.715067 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m18:37:32.718014 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd
[0m18:37:32.719712 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b
[0m18:37:32.721965 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3
[0m18:37:32.725149 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m18:37:32.723693 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.dbt_utils_accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0.be1f75eba9
[0m18:37:32.724489 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m18:37:32.723018 [debug] [Thread-4 (]: Began running node model.data_pipeline_project.mart_kpis_latest
[0m18:37:32.727273 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3
[0m18:37:32.728424 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m18:37:32.730839 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b
[0m18:37:32.731609 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd, now model.data_pipeline_project.mart_kpis_latest)
[0m18:37:32.732763 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.dbt_utils_accepted_range_mart_revenue_daily_daily_revenue__0.611ebe5f4e
[0m18:37:32.734885 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.dbt_utils_accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0.be1f75eba9
[0m18:37:32.735881 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23
[0m18:37:32.736723 [debug] [Thread-4 (]: Began compiling node model.data_pipeline_project.mart_kpis_latest
[0m18:37:32.737990 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3, now test.data_pipeline_project.dbt_utils_accepted_range_mart_revenue_daily_daily_revenue__0.611ebe5f4e)
[0m18:37:32.738772 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8
[0m18:37:32.739524 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b, now test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23)
[0m18:37:32.744603 [debug] [Thread-4 (]: Writing injected SQL for node "model.data_pipeline_project.mart_kpis_latest"
[0m18:37:32.745754 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.dbt_utils_accepted_range_mart_revenue_daily_daily_revenue__0.611ebe5f4e
[0m18:37:32.746811 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.dbt_utils_accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0.be1f75eba9, now test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8)
[0m18:37:32.747846 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23
[0m18:37:32.748970 [debug] [Thread-4 (]: Began executing node model.data_pipeline_project.mart_kpis_latest
[0m18:37:32.753370 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.dbt_utils_accepted_range_mart_revenue_daily_daily_revenue__0.611ebe5f4e"
[0m18:37:32.754466 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8
[0m18:37:32.759109 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23"
[0m18:37:32.759917 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m18:37:32.764959 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8"
[0m18:37:32.766572 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.dbt_utils_accepted_range_mart_revenue_daily_daily_revenue__0.611ebe5f4e
[0m18:37:32.767712 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23
[0m18:37:32.768753 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m18:37:32.769377 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8
[0m18:37:32.775621 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m18:37:32.774616 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m18:37:32.773557 [debug] [Thread-4 (]: Finished running node model.data_pipeline_project.mart_kpis_latest
[0m18:37:32.778945 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8
[0m18:37:32.781932 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.dbt_utils_accepted_range_mart_revenue_daily_daily_revenue__0.611ebe5f4e
[0m18:37:32.784340 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23
[0m18:37:32.787257 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_cac_payback_months__0.440e9fbb47
[0m18:37:32.787995 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.data_pipeline_project.mart_kpis_latest, now test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_cac_payback_months__0.440e9fbb47)
[0m18:37:32.788538 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_cac_payback_months__0.440e9fbb47
[0m18:37:32.797291 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_monthly_recurring_revenue__0.e7fa0e4f46
[0m18:37:32.798540 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8, now test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_monthly_recurring_revenue__0.e7fa0e4f46)
[0m18:37:32.799061 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_monthly_recurring_revenue__0.e7fa0e4f46
[0m18:37:32.803774 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_monthly_recurring_revenue__0.e7fa0e4f46"
[0m18:37:32.804622 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_cac_payback_months__0.440e9fbb47"
[0m18:37:32.806520 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_monthly_recurring_revenue__0.e7fa0e4f46
[0m18:37:32.808684 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_cac_payback_months__0.440e9fbb47
[0m18:37:32.809564 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m18:37:32.810291 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m18:37:32.812624 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_monthly_recurring_revenue__0.e7fa0e4f46
[0m18:37:32.814837 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_cac_payback_months__0.440e9fbb47
[0m18:37:32.819758 [debug] [MainThread]: Connection 'master' was properly closed.
[0m18:37:32.820431 [debug] [MainThread]: Connection 'test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23' was properly closed.
[0m18:37:32.820914 [debug] [MainThread]: Connection 'test.data_pipeline_project.dbt_utils_accepted_range_mart_revenue_daily_daily_revenue__0.611ebe5f4e' was properly closed.
[0m18:37:32.821414 [debug] [MainThread]: Connection 'test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_monthly_recurring_revenue__0.e7fa0e4f46' was properly closed.
[0m18:37:32.822455 [debug] [MainThread]: Connection 'test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_cac_payback_months__0.440e9fbb47' was properly closed.
[0m18:37:32.835938 [debug] [MainThread]: Command end result
[0m18:37:32.974780 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m18:37:32.978390 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m18:37:32.992721 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m18:37:33.021393 [debug] [MainThread]: Acquiring new bigquery connection 'generate_catalog'
[0m18:37:33.022029 [info ] [MainThread]: Building catalog
[0m18:37:33.031685 [debug] [ThreadPool]: Acquiring new bigquery connection 'data-pipeline-project-474812.information_schema'
[0m18:37:33.033514 [debug] [ThreadPool]: Acquiring new bigquery connection 'data-pipeline-project-474812.information_schema'
[0m18:37:33.035346 [debug] [ThreadPool]: Acquiring new bigquery connection 'data-pipeline-project-474812.information_schema'
[0m18:37:33.085052 [debug] [ThreadPool]: On data-pipeline-project-474812.information_schema: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "connection_name": "data-pipeline-project-474812.information_schema"} */

    with
                table_shards_stage as (
    select
        tables.project_id as table_catalog,
        tables.dataset_id as table_schema,
        coalesce(REGEXP_EXTRACT(tables.table_id, '^(.+)[0-9]{8}$'), tables.table_id) as table_name,
        tables.table_id as shard_name,
        REGEXP_EXTRACT(tables.table_id, '^.+([0-9]{8})$') as shard_index,
        REGEXP_CONTAINS(tables.table_id, '^.+[0-9]{8}$') and tables.type = 1 as is_date_shard,
        case
            when materialized_views.table_name is not null then 'materialized view'
            when tables.type = 1 then 'table'
            when tables.type = 2 then 'view'
            else 'external'
        end as table_type,
        tables.type = 1 as is_table,
        JSON_VALUE(table_description.option_value) as table_comment,
        tables.size_bytes,
        tables.row_count
    from `data-pipeline-project-474812`.`raw_data`.__TABLES__ tables
    left join `data-pipeline-project-474812`.`raw_data`.INFORMATION_SCHEMA.MATERIALIZED_VIEWS materialized_views
        on materialized_views.table_catalog = tables.project_id
        and materialized_views.table_schema = tables.dataset_id
        and materialized_views.table_name = tables.table_id
    left join `data-pipeline-project-474812`.`raw_data`.INFORMATION_SCHEMA.TABLE_OPTIONS table_description
        on table_description.table_catalog = tables.project_id
        and table_description.table_schema = tables.dataset_id
        and table_description.table_name = tables.table_id
        and table_description.option_name = 'description'
),
                table_shards as (
                    select * from table_shards_stage
                    where ((
                                upper(table_schema) = upper('raw_data')
                            and upper(table_name) = upper('shipments')
                            ) or (
                                upper(table_schema) = upper('raw_data')
                            and upper(table_name) = upper('addresses')
                            ) or (
                                upper(table_schema) = upper('raw_data')
                            and upper(table_name) = upper('neighborhoods')
                            ) or (
                                upper(table_schema) = upper('raw_data')
                            and upper(table_name) = upper('cities')
                            ) or (
                                upper(table_schema) = upper('raw_data')
                            and upper(table_name) = upper('countries')
                            ) or (
                                upper(table_schema) = upper('raw_data')
                            and upper(table_name) = upper('orders')
                            ) or (
                                upper(table_schema) = upper('raw_data')
                            and upper(table_name) = upper('subscriptions')
                            ) or (
                                upper(table_schema) = upper('raw_data')
                            and upper(table_name) = upper('states')
                            ) or (
                                upper(table_schema) = upper('raw_data')
                            and upper(table_name) = upper('users')
                            ) or (
                                upper(table_schema) = upper('raw_data')
                            and upper(table_name) = upper('marketing_spend')
                            ))
                ),
                tables as (
    select distinct
        table_catalog,
        table_schema,
        table_name,
        is_date_shard,
        table_type,
        is_table,
        table_comment
    from table_shards
),
                table_stats as (
    select
        table_catalog,
        table_schema,
        table_name,
        max(shard_name) as latest_shard_name,
        min(shard_index) as shard_min,
        max(shard_index) as shard_max,
        count(shard_index) as shard_count,
        sum(size_bytes) as size_bytes,
        sum(row_count) as row_count
    from table_shards
    group by 1, 2, 3
),

                columns as (
    select
        columns.table_catalog,
        columns.table_schema,
        columns.table_name as shard_name,
        coalesce(paths.field_path, '<unknown>') as column_name,
        -- invent a row number to account for nested fields
        -- BQ does not treat these nested properties as independent fields
        row_number() over (
            partition by
                columns.table_catalog,
                columns.table_schema,
                columns.table_name
            order by
                columns.ordinal_position,
                paths.field_path
        ) as column_index,
        coalesce(paths.data_type, '<unknown>') as column_type,
        paths.description as column_comment,
        case when columns.is_partitioning_column = 'YES' then 1 else 0 end as is_partitioning_column,
        case when columns.is_partitioning_column = 'YES' then paths.field_path end as partition_column,
        case when columns.clustering_ordinal_position is not null then 1 else 0 end as is_clustering_column,
        case when columns.clustering_ordinal_position is not null then paths.field_path end as cluster_column,
        columns.clustering_ordinal_position
    from `data-pipeline-project-474812`.`raw_data`.INFORMATION_SCHEMA.COLUMNS columns
    join `data-pipeline-project-474812`.`raw_data`.INFORMATION_SCHEMA.COLUMN_FIELD_PATHS paths
        on paths.table_catalog = columns.table_catalog
        and paths.table_schema = columns.table_schema
        and paths.table_name = columns.table_name
        and paths.column_name = columns.column_name
    where columns.ordinal_position is not null
),
                column_stats as (
    select
        table_catalog,
        table_schema,
        shard_name,
        max(is_partitioning_column) = 1 as is_partitioned,
        max(partition_column) as partition_column,
        max(is_clustering_column) = 1 as is_clustered,
        array_to_string(
            array_agg(
                cluster_column ignore nulls
                order by clustering_ordinal_position
            ), ', '
        ) as clustering_columns
    from columns
    group by 1, 2, 3
)

            
    select
        tables.table_catalog as table_database,
        tables.table_schema,
        case
            when tables.is_date_shard then concat(tables.table_name, '*')
            else tables.table_name
        end as table_name,
        tables.table_type,
        tables.table_comment,
        -- coalesce column metadata fields to ensure they are non-null for catalog generation
        -- external table columns are not present in COLUMN_FIELD_PATHS
        coalesce(columns.column_name, '<unknown>') as column_name,
        coalesce(columns.column_index, 1) as column_index,
        coalesce(columns.column_type, '<unknown>') as column_type,
        coalesce(columns.column_comment, '') as column_comment,

        'Shard count' as `stats__date_shards__label`,
        table_stats.shard_count as `stats__date_shards__value`,
        'The number of date shards in this table' as `stats__date_shards__description`,
        tables.is_date_shard as `stats__date_shards__include`,

        'Shard (min)' as `stats__date_shard_min__label`,
        table_stats.shard_min as `stats__date_shard_min__value`,
        'The first date shard in this table' as `stats__date_shard_min__description`,
        tables.is_date_shard as `stats__date_shard_min__include`,

        'Shard (max)' as `stats__date_shard_max__label`,
        table_stats.shard_max as `stats__date_shard_max__value`,
        'The last date shard in this table' as `stats__date_shard_max__description`,
        tables.is_date_shard as `stats__date_shard_max__include`,

        '# Rows' as `stats__num_rows__label`,
        table_stats.row_count as `stats__num_rows__value`,
        'Approximate count of rows in this table' as `stats__num_rows__description`,
        tables.is_table as `stats__num_rows__include`,

        'Approximate Size' as `stats__num_bytes__label`,
        table_stats.size_bytes as `stats__num_bytes__value`,
        'Approximate size of table as reported by BigQuery' as `stats__num_bytes__description`,
        tables.is_table as `stats__num_bytes__include`,

        'Partitioned By' as `stats__partitioning_type__label`,
        column_stats.partition_column as `stats__partitioning_type__value`,
        'The partitioning column for this table' as `stats__partitioning_type__description`,
        column_stats.is_partitioned as `stats__partitioning_type__include`,

        'Clustered By' as `stats__clustering_fields__label`,
        column_stats.clustering_columns as `stats__clustering_fields__value`,
        'The clustering columns for this table' as `stats__clustering_fields__description`,
        column_stats.is_clustered as `stats__clustering_fields__include`

    from tables
    join table_stats
        on table_stats.table_catalog = tables.table_catalog
        and table_stats.table_schema = tables.table_schema
        and table_stats.table_name = tables.table_name
    left join column_stats
        on column_stats.table_catalog = tables.table_catalog
        and column_stats.table_schema = tables.table_schema
        and column_stats.shard_name = table_stats.latest_shard_name
    left join columns
        on columns.table_catalog = tables.table_catalog
        and columns.table_schema = tables.table_schema
        and columns.shard_name = table_stats.latest_shard_name

  
[0m18:37:33.082911 [debug] [ThreadPool]: On data-pipeline-project-474812.information_schema: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "connection_name": "data-pipeline-project-474812.information_schema"} */

    with
                table_shards_stage as (
    select
        tables.project_id as table_catalog,
        tables.dataset_id as table_schema,
        coalesce(REGEXP_EXTRACT(tables.table_id, '^(.+)[0-9]{8}$'), tables.table_id) as table_name,
        tables.table_id as shard_name,
        REGEXP_EXTRACT(tables.table_id, '^.+([0-9]{8})$') as shard_index,
        REGEXP_CONTAINS(tables.table_id, '^.+[0-9]{8}$') and tables.type = 1 as is_date_shard,
        case
            when materialized_views.table_name is not null then 'materialized view'
            when tables.type = 1 then 'table'
            when tables.type = 2 then 'view'
            else 'external'
        end as table_type,
        tables.type = 1 as is_table,
        JSON_VALUE(table_description.option_value) as table_comment,
        tables.size_bytes,
        tables.row_count
    from `data-pipeline-project-474812`.`analytics_staging_marts`.__TABLES__ tables
    left join `data-pipeline-project-474812`.`analytics_staging_marts`.INFORMATION_SCHEMA.MATERIALIZED_VIEWS materialized_views
        on materialized_views.table_catalog = tables.project_id
        and materialized_views.table_schema = tables.dataset_id
        and materialized_views.table_name = tables.table_id
    left join `data-pipeline-project-474812`.`analytics_staging_marts`.INFORMATION_SCHEMA.TABLE_OPTIONS table_description
        on table_description.table_catalog = tables.project_id
        and table_description.table_schema = tables.dataset_id
        and table_description.table_name = tables.table_id
        and table_description.option_name = 'description'
),
                table_shards as (
                    select * from table_shards_stage
                    where ((
                                upper(table_schema) = upper('analytics_staging_marts')
                            and upper(table_name) = upper('fct_shipment')
                            ) or (
                                upper(table_schema) = upper('analytics_staging_marts')
                            and upper(table_name) = upper('fct_order')
                            ) or (
                                upper(table_schema) = upper('analytics_staging_marts')
                            and upper(table_name) = upper('mart_revenue_daily')
                            ) or (
                                upper(table_schema) = upper('analytics_staging_marts')
                            and upper(table_name) = upper('dim_subscription')
                            ) or (
                                upper(table_schema) = upper('analytics_staging_marts')
                            and upper(table_name) = upper('dim_customer')
                            ) or (
                                upper(table_schema) = upper('analytics_staging_marts')
                            and upper(table_name) = upper('mart_subscription_daily')
                            ) or (
                                upper(table_schema) = upper('analytics_staging_marts')
                            and upper(table_name) = upper('fct_marketing_spend')
                            ) or (
                                upper(table_schema) = upper('analytics_staging_marts')
                            and upper(table_name) = upper('mart_acquisition_efficiency')
                            ) or (
                                upper(table_schema) = upper('analytics_staging_marts')
                            and upper(table_name) = upper('mart_kpis_latest')
                            ))
                ),
                tables as (
    select distinct
        table_catalog,
        table_schema,
        table_name,
        is_date_shard,
        table_type,
        is_table,
        table_comment
    from table_shards
),
                table_stats as (
    select
        table_catalog,
        table_schema,
        table_name,
        max(shard_name) as latest_shard_name,
        min(shard_index) as shard_min,
        max(shard_index) as shard_max,
        count(shard_index) as shard_count,
        sum(size_bytes) as size_bytes,
        sum(row_count) as row_count
    from table_shards
    group by 1, 2, 3
),

                columns as (
    select
        columns.table_catalog,
        columns.table_schema,
        columns.table_name as shard_name,
        coalesce(paths.field_path, '<unknown>') as column_name,
        -- invent a row number to account for nested fields
        -- BQ does not treat these nested properties as independent fields
        row_number() over (
            partition by
                columns.table_catalog,
                columns.table_schema,
                columns.table_name
            order by
                columns.ordinal_position,
                paths.field_path
        ) as column_index,
        coalesce(paths.data_type, '<unknown>') as column_type,
        paths.description as column_comment,
        case when columns.is_partitioning_column = 'YES' then 1 else 0 end as is_partitioning_column,
        case when columns.is_partitioning_column = 'YES' then paths.field_path end as partition_column,
        case when columns.clustering_ordinal_position is not null then 1 else 0 end as is_clustering_column,
        case when columns.clustering_ordinal_position is not null then paths.field_path end as cluster_column,
        columns.clustering_ordinal_position
    from `data-pipeline-project-474812`.`analytics_staging_marts`.INFORMATION_SCHEMA.COLUMNS columns
    join `data-pipeline-project-474812`.`analytics_staging_marts`.INFORMATION_SCHEMA.COLUMN_FIELD_PATHS paths
        on paths.table_catalog = columns.table_catalog
        and paths.table_schema = columns.table_schema
        and paths.table_name = columns.table_name
        and paths.column_name = columns.column_name
    where columns.ordinal_position is not null
),
                column_stats as (
    select
        table_catalog,
        table_schema,
        shard_name,
        max(is_partitioning_column) = 1 as is_partitioned,
        max(partition_column) as partition_column,
        max(is_clustering_column) = 1 as is_clustered,
        array_to_string(
            array_agg(
                cluster_column ignore nulls
                order by clustering_ordinal_position
            ), ', '
        ) as clustering_columns
    from columns
    group by 1, 2, 3
)

            
    select
        tables.table_catalog as table_database,
        tables.table_schema,
        case
            when tables.is_date_shard then concat(tables.table_name, '*')
            else tables.table_name
        end as table_name,
        tables.table_type,
        tables.table_comment,
        -- coalesce column metadata fields to ensure they are non-null for catalog generation
        -- external table columns are not present in COLUMN_FIELD_PATHS
        coalesce(columns.column_name, '<unknown>') as column_name,
        coalesce(columns.column_index, 1) as column_index,
        coalesce(columns.column_type, '<unknown>') as column_type,
        coalesce(columns.column_comment, '') as column_comment,

        'Shard count' as `stats__date_shards__label`,
        table_stats.shard_count as `stats__date_shards__value`,
        'The number of date shards in this table' as `stats__date_shards__description`,
        tables.is_date_shard as `stats__date_shards__include`,

        'Shard (min)' as `stats__date_shard_min__label`,
        table_stats.shard_min as `stats__date_shard_min__value`,
        'The first date shard in this table' as `stats__date_shard_min__description`,
        tables.is_date_shard as `stats__date_shard_min__include`,

        'Shard (max)' as `stats__date_shard_max__label`,
        table_stats.shard_max as `stats__date_shard_max__value`,
        'The last date shard in this table' as `stats__date_shard_max__description`,
        tables.is_date_shard as `stats__date_shard_max__include`,

        '# Rows' as `stats__num_rows__label`,
        table_stats.row_count as `stats__num_rows__value`,
        'Approximate count of rows in this table' as `stats__num_rows__description`,
        tables.is_table as `stats__num_rows__include`,

        'Approximate Size' as `stats__num_bytes__label`,
        table_stats.size_bytes as `stats__num_bytes__value`,
        'Approximate size of table as reported by BigQuery' as `stats__num_bytes__description`,
        tables.is_table as `stats__num_bytes__include`,

        'Partitioned By' as `stats__partitioning_type__label`,
        column_stats.partition_column as `stats__partitioning_type__value`,
        'The partitioning column for this table' as `stats__partitioning_type__description`,
        column_stats.is_partitioned as `stats__partitioning_type__include`,

        'Clustered By' as `stats__clustering_fields__label`,
        column_stats.clustering_columns as `stats__clustering_fields__value`,
        'The clustering columns for this table' as `stats__clustering_fields__description`,
        column_stats.is_clustered as `stats__clustering_fields__include`

    from tables
    join table_stats
        on table_stats.table_catalog = tables.table_catalog
        and table_stats.table_schema = tables.table_schema
        and table_stats.table_name = tables.table_name
    left join column_stats
        on column_stats.table_catalog = tables.table_catalog
        and column_stats.table_schema = tables.table_schema
        and column_stats.shard_name = table_stats.latest_shard_name
    left join columns
        on columns.table_catalog = tables.table_catalog
        and columns.table_schema = tables.table_schema
        and columns.shard_name = table_stats.latest_shard_name

  
[0m18:37:33.089318 [debug] [ThreadPool]: On data-pipeline-project-474812.information_schema: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "connection_name": "data-pipeline-project-474812.information_schema"} */

    with
                table_shards_stage as (
    select
        tables.project_id as table_catalog,
        tables.dataset_id as table_schema,
        coalesce(REGEXP_EXTRACT(tables.table_id, '^(.+)[0-9]{8}$'), tables.table_id) as table_name,
        tables.table_id as shard_name,
        REGEXP_EXTRACT(tables.table_id, '^.+([0-9]{8})$') as shard_index,
        REGEXP_CONTAINS(tables.table_id, '^.+[0-9]{8}$') and tables.type = 1 as is_date_shard,
        case
            when materialized_views.table_name is not null then 'materialized view'
            when tables.type = 1 then 'table'
            when tables.type = 2 then 'view'
            else 'external'
        end as table_type,
        tables.type = 1 as is_table,
        JSON_VALUE(table_description.option_value) as table_comment,
        tables.size_bytes,
        tables.row_count
    from `data-pipeline-project-474812`.`analytics_staging_staging`.__TABLES__ tables
    left join `data-pipeline-project-474812`.`analytics_staging_staging`.INFORMATION_SCHEMA.MATERIALIZED_VIEWS materialized_views
        on materialized_views.table_catalog = tables.project_id
        and materialized_views.table_schema = tables.dataset_id
        and materialized_views.table_name = tables.table_id
    left join `data-pipeline-project-474812`.`analytics_staging_staging`.INFORMATION_SCHEMA.TABLE_OPTIONS table_description
        on table_description.table_catalog = tables.project_id
        and table_description.table_schema = tables.dataset_id
        and table_description.table_name = tables.table_id
        and table_description.option_name = 'description'
),
                table_shards as (
                    select * from table_shards_stage
                    where ((
                                upper(table_schema) = upper('analytics_staging_staging')
                            and upper(table_name) = upper('stg_orders')
                            ) or (
                                upper(table_schema) = upper('analytics_staging_staging')
                            and upper(table_name) = upper('stg_users')
                            ) or (
                                upper(table_schema) = upper('analytics_staging_staging')
                            and upper(table_name) = upper('stg_neighborhoods')
                            ) or (
                                upper(table_schema) = upper('analytics_staging_staging')
                            and upper(table_name) = upper('stg_addresses')
                            ) or (
                                upper(table_schema) = upper('analytics_staging_staging')
                            and upper(table_name) = upper('stg_marketing_spend')
                            ) or (
                                upper(table_schema) = upper('analytics_staging_staging')
                            and upper(table_name) = upper('stg_states')
                            ) or (
                                upper(table_schema) = upper('analytics_staging_staging')
                            and upper(table_name) = upper('stg_cities')
                            ) or (
                                upper(table_schema) = upper('analytics_staging_staging')
                            and upper(table_name) = upper('stg_subscriptions')
                            ) or (
                                upper(table_schema) = upper('analytics_staging_staging')
                            and upper(table_name) = upper('stg_countries')
                            ) or (
                                upper(table_schema) = upper('analytics_staging_staging')
                            and upper(table_name) = upper('int_shipments_decomposed')
                            ) or (
                                upper(table_schema) = upper('analytics_staging_staging')
                            and upper(table_name) = upper('int_orders_decomposed')
                            ) or (
                                upper(table_schema) = upper('analytics_staging_staging')
                            and upper(table_name) = upper('stg_shipments')
                            ))
                ),
                tables as (
    select distinct
        table_catalog,
        table_schema,
        table_name,
        is_date_shard,
        table_type,
        is_table,
        table_comment
    from table_shards
),
                table_stats as (
    select
        table_catalog,
        table_schema,
        table_name,
        max(shard_name) as latest_shard_name,
        min(shard_index) as shard_min,
        max(shard_index) as shard_max,
        count(shard_index) as shard_count,
        sum(size_bytes) as size_bytes,
        sum(row_count) as row_count
    from table_shards
    group by 1, 2, 3
),

                columns as (
    select
        columns.table_catalog,
        columns.table_schema,
        columns.table_name as shard_name,
        coalesce(paths.field_path, '<unknown>') as column_name,
        -- invent a row number to account for nested fields
        -- BQ does not treat these nested properties as independent fields
        row_number() over (
            partition by
                columns.table_catalog,
                columns.table_schema,
                columns.table_name
            order by
                columns.ordinal_position,
                paths.field_path
        ) as column_index,
        coalesce(paths.data_type, '<unknown>') as column_type,
        paths.description as column_comment,
        case when columns.is_partitioning_column = 'YES' then 1 else 0 end as is_partitioning_column,
        case when columns.is_partitioning_column = 'YES' then paths.field_path end as partition_column,
        case when columns.clustering_ordinal_position is not null then 1 else 0 end as is_clustering_column,
        case when columns.clustering_ordinal_position is not null then paths.field_path end as cluster_column,
        columns.clustering_ordinal_position
    from `data-pipeline-project-474812`.`analytics_staging_staging`.INFORMATION_SCHEMA.COLUMNS columns
    join `data-pipeline-project-474812`.`analytics_staging_staging`.INFORMATION_SCHEMA.COLUMN_FIELD_PATHS paths
        on paths.table_catalog = columns.table_catalog
        and paths.table_schema = columns.table_schema
        and paths.table_name = columns.table_name
        and paths.column_name = columns.column_name
    where columns.ordinal_position is not null
),
                column_stats as (
    select
        table_catalog,
        table_schema,
        shard_name,
        max(is_partitioning_column) = 1 as is_partitioned,
        max(partition_column) as partition_column,
        max(is_clustering_column) = 1 as is_clustered,
        array_to_string(
            array_agg(
                cluster_column ignore nulls
                order by clustering_ordinal_position
            ), ', '
        ) as clustering_columns
    from columns
    group by 1, 2, 3
)

            
    select
        tables.table_catalog as table_database,
        tables.table_schema,
        case
            when tables.is_date_shard then concat(tables.table_name, '*')
            else tables.table_name
        end as table_name,
        tables.table_type,
        tables.table_comment,
        -- coalesce column metadata fields to ensure they are non-null for catalog generation
        -- external table columns are not present in COLUMN_FIELD_PATHS
        coalesce(columns.column_name, '<unknown>') as column_name,
        coalesce(columns.column_index, 1) as column_index,
        coalesce(columns.column_type, '<unknown>') as column_type,
        coalesce(columns.column_comment, '') as column_comment,

        'Shard count' as `stats__date_shards__label`,
        table_stats.shard_count as `stats__date_shards__value`,
        'The number of date shards in this table' as `stats__date_shards__description`,
        tables.is_date_shard as `stats__date_shards__include`,

        'Shard (min)' as `stats__date_shard_min__label`,
        table_stats.shard_min as `stats__date_shard_min__value`,
        'The first date shard in this table' as `stats__date_shard_min__description`,
        tables.is_date_shard as `stats__date_shard_min__include`,

        'Shard (max)' as `stats__date_shard_max__label`,
        table_stats.shard_max as `stats__date_shard_max__value`,
        'The last date shard in this table' as `stats__date_shard_max__description`,
        tables.is_date_shard as `stats__date_shard_max__include`,

        '# Rows' as `stats__num_rows__label`,
        table_stats.row_count as `stats__num_rows__value`,
        'Approximate count of rows in this table' as `stats__num_rows__description`,
        tables.is_table as `stats__num_rows__include`,

        'Approximate Size' as `stats__num_bytes__label`,
        table_stats.size_bytes as `stats__num_bytes__value`,
        'Approximate size of table as reported by BigQuery' as `stats__num_bytes__description`,
        tables.is_table as `stats__num_bytes__include`,

        'Partitioned By' as `stats__partitioning_type__label`,
        column_stats.partition_column as `stats__partitioning_type__value`,
        'The partitioning column for this table' as `stats__partitioning_type__description`,
        column_stats.is_partitioned as `stats__partitioning_type__include`,

        'Clustered By' as `stats__clustering_fields__label`,
        column_stats.clustering_columns as `stats__clustering_fields__value`,
        'The clustering columns for this table' as `stats__clustering_fields__description`,
        column_stats.is_clustered as `stats__clustering_fields__include`

    from tables
    join table_stats
        on table_stats.table_catalog = tables.table_catalog
        and table_stats.table_schema = tables.table_schema
        and table_stats.table_name = tables.table_name
    left join column_stats
        on column_stats.table_catalog = tables.table_catalog
        and column_stats.table_schema = tables.table_schema
        and column_stats.shard_name = table_stats.latest_shard_name
    left join columns
        on columns.table_catalog = tables.table_catalog
        and columns.table_schema = tables.table_schema
        and columns.shard_name = table_stats.latest_shard_name

  
[0m18:37:33.095783 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:37:33.092020 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:37:33.091340 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:37:34.434191 [debug] [ThreadPool]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:ca291b7b-ec5d-476e-9b0c-1cd95ad8357d&page=queryresults
[0m18:37:34.499204 [debug] [ThreadPool]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:c99c76f7-b623-4477-a239-bc0f1982ca8e&page=queryresults
[0m18:37:34.503854 [debug] [ThreadPool]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:7444c3f0-0892-41af-b42c-0d4958a48fee&page=queryresults
[0m18:37:36.905049 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:37:36.939558 [debug] [MainThread]: Wrote artifact CatalogArtifact to /home/ecem/Desktop/data-pipeline-project/dbt/target/catalog.json
[0m18:37:36.992786 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m18:37:36.997561 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m18:37:36.998658 [info ] [MainThread]: Catalog written to /home/ecem/Desktop/data-pipeline-project/dbt/target/catalog.json
[0m18:37:37.009845 [debug] [MainThread]: Resource report: {"command_name": "generate", "command_success": true, "command_wall_clock_time": 11.305161, "process_in_blocks": "3408", "process_kernel_time": 2.079615, "process_mem_max_rss": "389220", "process_out_blocks": "10448", "process_user_time": 7.655803}
[0m18:37:37.010753 [debug] [MainThread]: Command `dbt docs generate` succeeded at 18:37:37.010631 after 11.31 seconds
[0m18:37:37.011472 [debug] [MainThread]: Connection 'generate_catalog' was properly closed.
[0m18:37:37.011930 [debug] [MainThread]: Connection 'data-pipeline-project-474812.information_schema' was properly closed.
[0m18:37:37.012342 [debug] [MainThread]: Connection 'data-pipeline-project-474812.information_schema' was properly closed.
[0m18:37:37.012978 [debug] [MainThread]: Connection 'data-pipeline-project-474812.information_schema' was properly closed.
[0m18:37:37.013582 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x713dda2101d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x713db832fe30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x713db82306b0>]}
[0m18:37:37.014372 [debug] [MainThread]: Flushing usage events
[0m18:37:38.103061 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m18:38:32.821439 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7dbb620539e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7dbb620d4950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7dbb622435c0>]}


============================== 18:38:32.825094 | 1cf6d909-e5c0-496f-b8f3-5c2034774f8f ==============================
[0m18:38:32.825094 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m18:38:32.832371 [debug] [MainThread]: running dbt with arguments {'quiet': 'False', 'fail_fast': 'False', 'log_cache_events': 'False', 'profiles_dir': '/home/ecem/.dbt', 'debug': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'send_anonymous_usage_stats': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'use_experimental_parser': 'False', 'log_format': 'default', 'static_parser': 'True', 'partial_parse': 'True', 'indirect_selection': 'eager', 'target_path': 'None', 'printer_width': '80', 'invocation_command': 'dbt docs serve', 'empty': 'None', 'introspect': 'True', 'write_json': 'True', 'use_colors': 'True', 'no_print': 'None'}
[0m18:38:35.160699 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1cf6d909-e5c0-496f-b8f3-5c2034774f8f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7dbb421306b0>]}
[0m18:38:35.224382 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '1cf6d909-e5c0-496f-b8f3-5c2034774f8f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7dbb425c2b40>]}
[0m18:45:19.049550 [error] [MainThread]: Encountered an error:

[0m18:45:19.090785 [error] [MainThread]: Traceback (most recent call last):
  File "/home/ecem/Desktop/data-pipeline-project/venv/lib/python3.12/site-packages/dbt/cli/requires.py", line 178, in wrapper
    result, success = func(*args, **kwargs)
                      ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ecem/Desktop/data-pipeline-project/venv/lib/python3.12/site-packages/dbt/cli/requires.py", line 128, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ecem/Desktop/data-pipeline-project/venv/lib/python3.12/site-packages/dbt/cli/requires.py", line 272, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ecem/Desktop/data-pipeline-project/venv/lib/python3.12/site-packages/dbt/cli/requires.py", line 303, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ecem/Desktop/data-pipeline-project/venv/lib/python3.12/site-packages/dbt/cli/requires.py", line 350, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ecem/Desktop/data-pipeline-project/venv/lib/python3.12/site-packages/dbt/cli/main.py", line 307, in docs_serve
    results = task.run()
              ^^^^^^^^^^
  File "/home/ecem/Desktop/data-pipeline-project/venv/lib/python3.12/site-packages/dbt/task/docs/serve.py", line 29, in run
    httpd.serve_forever()
  File "/usr/lib/python3.12/socketserver.py", line 235, in serve_forever
    ready = selector.select(poll_interval)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/selectors.py", line 415, in select
    fd_event_list = self._selector.poll(timeout)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt

[0m18:45:19.094829 [debug] [MainThread]: Resource report: {"command_name": "serve", "command_success": false, "command_wall_clock_time": 406.3275, "process_in_blocks": "3696", "process_kernel_time": 1.175938, "process_mem_max_rss": "363656", "process_out_blocks": "3368", "process_user_time": 3.3889}
[0m18:45:19.096350 [debug] [MainThread]: Command `dbt docs serve` failed at 18:45:19.096226 after 406.33 seconds
[0m18:45:19.096998 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7dbb62659bb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7dbb420d6510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7dbb42132600>]}
[0m18:45:19.097521 [debug] [MainThread]: Flushing usage events
[0m18:45:20.119217 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:44:00.282038 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x768cc736faa0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x768cc66ea8a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x768cc736e690>]}


============================== 12:44:00.291659 | a57e840e-18e0-4074-9ce3-eb4a4955d85f ==============================
[0m12:44:00.291659 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m12:44:00.293374 [debug] [MainThread]: running dbt with arguments {'no_print': 'None', 'send_anonymous_usage_stats': 'True', 'fail_fast': 'False', 'use_experimental_parser': 'False', 'version_check': 'True', 'quiet': 'False', 'printer_width': '80', 'log_cache_events': 'False', 'target_path': 'None', 'empty': 'None', 'use_colors': 'True', 'indirect_selection': 'eager', 'static_parser': 'True', 'debug': 'False', 'partial_parse': 'True', 'invocation_command': 'dbt test --select models/staging/staging_schema.yml', 'introspect': 'True', 'profiles_dir': '/home/ecem/.dbt', 'cache_selected_only': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'warn_error': 'None', 'write_json': 'True', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'log_format': 'default'}
[0m12:44:06.458601 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a57e840e-18e0-4074-9ce3-eb4a4955d85f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x768ca6024830>]}
[0m12:44:06.549017 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a57e840e-18e0-4074-9ce3-eb4a4955d85f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x768ca62149e0>]}
[0m12:44:06.549982 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m12:44:07.065056 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m12:44:07.613529 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m12:44:07.614086 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m12:44:07.722480 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a57e840e-18e0-4074-9ce3-eb4a4955d85f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x768cc6e42d20>]}
[0m12:44:07.895093 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m12:44:07.900621 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m12:44:07.950659 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a57e840e-18e0-4074-9ce3-eb4a4955d85f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x768ca5488860>]}
[0m12:44:07.951670 [info ] [MainThread]: Found 21 models, 67 data tests, 3 analyses, 10 sources, 624 macros
[0m12:44:07.952607 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a57e840e-18e0-4074-9ce3-eb4a4955d85f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x768ca6091700>]}
[0m12:44:07.962010 [info ] [MainThread]: 
[0m12:44:07.963066 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m12:44:07.964461 [info ] [MainThread]: 
[0m12:44:07.966788 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m12:44:07.977220 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_marts'
[0m12:44:07.980510 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_staging'
[0m12:44:07.994248 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:44:07.984972 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:44:09.265671 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a57e840e-18e0-4074-9ce3-eb4a4955d85f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x768ca5fdf710>]}
[0m12:44:09.266458 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:44:09.312265 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b
[0m12:44:09.313898 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31
[0m12:44:09.314763 [info ] [Thread-1 (]: 1 of 24 START test not_null_int_shipments_decomposed_delivered_at .............. [RUN]
[0m12:44:09.323177 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b)
[0m12:44:09.316083 [info ] [Thread-2 (]: 2 of 24 START test not_null_int_shipments_decomposed_latest_status ............. [RUN]
[0m12:44:09.317200 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501
[0m12:44:09.324120 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b
[0m12:44:09.324967 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3
[0m12:44:09.325758 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_marts, now test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31)
[0m12:44:09.326507 [info ] [Thread-3 (]: 3 of 24 START test not_null_stg_addresses_address_id ........................... [RUN]
[0m12:44:09.368377 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31
[0m12:44:09.352824 [info ] [Thread-4 (]: 4 of 24 START test not_null_stg_addresses_user_id .............................. [RUN]
[0m12:44:09.384111 [debug] [Thread-3 (]: Acquiring new bigquery connection 'test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501'
[0m12:44:09.388550 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b"
[0m12:44:09.393787 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31"
[0m12:44:09.399521 [debug] [Thread-4 (]: Acquiring new bigquery connection 'test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3'
[0m12:44:09.403845 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501
[0m12:44:09.408132 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3
[0m12:44:09.409862 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31
[0m12:44:09.411048 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b
[0m12:44:09.430414 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501"
[0m12:44:09.444370 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3"
[0m12:44:09.501894 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b"
[0m12:44:09.515887 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31"
[0m12:44:09.518707 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3
[0m12:44:09.520925 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501
[0m12:44:09.523099 [debug] [Thread-1 (]: On test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select delivered_at
from `data-pipeline-project-474812`.`analytics_staging_staging`.`int_shipments_decomposed`
where delivered_at is null



  
  
      
    ) dbt_internal_test
[0m12:44:09.535919 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:44:09.528595 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3"
[0m12:44:09.533351 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501"
[0m12:44:09.529750 [debug] [Thread-2 (]: On test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select latest_status
from `data-pipeline-project-474812`.`analytics_staging_staging`.`int_shipments_decomposed`
where latest_status is null



  
  
      
    ) dbt_internal_test
[0m12:44:09.611415 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m12:44:09.664322 [debug] [Thread-3 (]: On test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select address_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_addresses`
where address_id is null



  
  
      
    ) dbt_internal_test
[0m12:44:09.665681 [debug] [Thread-4 (]: On test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_addresses`
where user_id is null



  
  
      
    ) dbt_internal_test
[0m12:44:09.667415 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m12:44:09.670794 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m12:44:11.022714 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:241eb17b-0374-4f09-983a-2a1ee827892b&page=queryresults
[0m12:44:11.053800 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:1edd170d-0ea5-472a-af39-299e9a07be57&page=queryresults
[0m12:44:11.106450 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:7171b305-086c-4d5e-854f-4403eb2798bd&page=queryresults
[0m12:44:11.251899 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:09a957d7-2c29-4474-85eb-3b29b6a75e9b&page=queryresults
[0m12:44:12.001549 [info ] [Thread-2 (]: 2 of 24 PASS not_null_int_shipments_decomposed_latest_status ................... [[32mPASS[0m in 2.67s]
[0m12:44:12.020606 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31
[0m12:44:12.015357 [warn ] [Thread-1 (]: 1 of 24 WARN 303 not_null_int_shipments_decomposed_delivered_at ................ [[33mWARN 303[0m in 2.68s]
[0m12:44:12.034617 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b
[0m12:44:12.029550 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2
[0m12:44:12.035334 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64
[0m12:44:12.027127 [info ] [Thread-4 (]: 4 of 24 PASS not_null_stg_addresses_user_id .................................... [[32mPASS[0m in 2.63s]
[0m12:44:12.036462 [info ] [Thread-2 (]: 5 of 24 START test not_null_stg_countries_country_id ........................... [RUN]
[0m12:44:12.040309 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3
[0m12:44:12.039485 [info ] [Thread-1 (]: 6 of 24 START test not_null_stg_orders_order_id ................................ [RUN]
[0m12:44:12.041343 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_int_shipments_decomposed_latest_status.19d7e94b31, now test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2)
[0m12:44:12.042487 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209
[0m12:44:12.051473 [info ] [Thread-4 (]: 7 of 24 START test not_null_stg_orders_user_id ................................. [RUN]
[0m12:44:12.048698 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2
[0m12:44:12.052209 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_addresses_user_id.a16f71a0d3, now test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209)
[0m12:44:12.047240 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_int_shipments_decomposed_delivered_at.8b5499f25b, now test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64)
[0m12:44:12.072694 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64
[0m12:44:12.068572 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2"
[0m12:44:12.065664 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209
[0m12:44:12.095053 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64"
[0m12:44:12.098552 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209"
[0m12:44:12.106778 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64
[0m12:44:12.108087 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209
[0m12:44:12.110004 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2
[0m12:44:12.116367 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64"
[0m12:44:12.124491 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209"
[0m12:44:12.133527 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2"
[0m12:44:12.145786 [debug] [Thread-4 (]: On test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_orders`
where user_id is null



  
  
      
    ) dbt_internal_test
[0m12:44:12.147368 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m12:44:12.205133 [debug] [Thread-1 (]: On test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select order_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_orders`
where order_id is null



  
  
      
    ) dbt_internal_test
[0m12:44:12.207274 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:44:12.209314 [debug] [Thread-2 (]: On test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select country_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_countries`
where country_id is null



  
  
      
    ) dbt_internal_test
[0m12:44:12.276045 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m12:44:12.357661 [info ] [Thread-3 (]: 3 of 24 PASS not_null_stg_addresses_address_id ................................. [[32mPASS[0m in 2.97s]
[0m12:44:12.361727 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501
[0m12:44:12.362318 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380
[0m12:44:12.362826 [info ] [Thread-3 (]: 8 of 24 START test not_null_stg_shipments_shipment_id .......................... [RUN]
[0m12:44:12.364808 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_addresses_address_id.eabbd5d501, now test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380)
[0m12:44:12.365813 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380
[0m12:44:12.373477 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380"
[0m12:44:12.376462 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380
[0m12:44:12.382206 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380"
[0m12:44:12.393155 [debug] [Thread-3 (]: On test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select shipment_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_shipments`
where shipment_id is null



  
  
      
    ) dbt_internal_test
[0m12:44:12.394533 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m12:44:13.618974 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:0fccdda8-395b-411e-aa22-2aaec9e3590f&page=queryresults
[0m12:44:13.722548 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:c98c205c-1bee-44b7-a606-f2cd646433c9&page=queryresults
[0m12:44:13.752437 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:c6a60194-e412-4290-abfe-a478e6242d13&page=queryresults
[0m12:44:13.806940 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:f0717a5d-8ba1-4288-aca4-030bc54bf10b&page=queryresults
[0m12:44:14.491003 [info ] [Thread-2 (]: 5 of 24 PASS not_null_stg_countries_country_id ................................. [[32mPASS[0m in 2.45s]
[0m12:44:14.494568 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2
[0m12:44:14.496065 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852
[0m12:44:14.497556 [info ] [Thread-2 (]: 9 of 24 START test not_null_stg_shipments_user_id .............................. [RUN]
[0m12:44:14.500228 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_countries_country_id.acacf2eaa2, now test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852)
[0m12:44:14.500861 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852
[0m12:44:14.506332 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852"
[0m12:44:14.509045 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852
[0m12:44:14.518459 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852"
[0m12:44:14.521525 [debug] [Thread-2 (]: On test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_shipments`
where user_id is null



  
  
      
    ) dbt_internal_test
[0m12:44:14.522036 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m12:44:14.606734 [info ] [Thread-1 (]: 6 of 24 PASS not_null_stg_orders_order_id ...................................... [[32mPASS[0m in 2.56s]
[0m12:44:14.610293 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64
[0m12:44:14.617403 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8
[0m12:44:14.618108 [info ] [Thread-1 (]: 10 of 24 START test not_null_stg_subscriptions_start_date ...................... [RUN]
[0m12:44:14.620842 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_orders_order_id.81cfe2fe64, now test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8)
[0m12:44:14.624622 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8
[0m12:44:14.654803 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8"
[0m12:44:14.660910 [info ] [Thread-4 (]: 7 of 24 PASS not_null_stg_orders_user_id ....................................... [[32mPASS[0m in 2.61s]
[0m12:44:14.666490 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8
[0m12:44:14.669085 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209
[0m12:44:14.677200 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m12:44:14.677756 [info ] [Thread-4 (]: 11 of 24 START test not_null_stg_subscriptions_subscription_id ................. [RUN]
[0m12:44:14.678898 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_orders_user_id.7d129d0209, now test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91)
[0m12:44:14.686136 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8"
[0m12:44:14.690542 [debug] [Thread-1 (]: On test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select start_date
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_subscriptions`
where start_date is null



  
  
      
    ) dbt_internal_test
[0m12:44:14.689089 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m12:44:14.694289 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:44:14.758303 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91"
[0m12:44:14.759738 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m12:44:14.770983 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91"
[0m12:44:14.772527 [debug] [Thread-4 (]: On test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select subscription_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_subscriptions`
where subscription_id is null



  
  
      
    ) dbt_internal_test
[0m12:44:14.774713 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m12:44:14.875543 [info ] [Thread-3 (]: 8 of 24 PASS not_null_stg_shipments_shipment_id ................................ [[32mPASS[0m in 2.50s]
[0m12:44:14.878755 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380
[0m12:44:14.879691 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2
[0m12:44:14.898067 [info ] [Thread-3 (]: 12 of 24 START test not_null_stg_subscriptions_user_id ......................... [RUN]
[0m12:44:14.899451 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_shipments_shipment_id.13d2043380, now test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2)
[0m12:44:14.900578 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2
[0m12:44:14.909085 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2"
[0m12:44:14.910599 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2
[0m12:44:14.917120 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2"
[0m12:44:14.918902 [debug] [Thread-3 (]: On test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_subscriptions`
where user_id is null



  
  
      
    ) dbt_internal_test
[0m12:44:14.919623 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m12:44:16.236760 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:688791ee-284a-40ee-b21b-7d7d1f4731cb&page=queryresults
[0m12:44:16.323218 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:26fc4538-6be7-4273-9d8f-cec0207d1851&page=queryresults
[0m12:44:16.438891 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:1617dc25-9d2a-49d2-8c0d-33829094b0ec&page=queryresults
[0m12:44:16.951671 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:7bce321f-9ea8-4932-ae75-13ed9359349d&page=queryresults
[0m12:44:17.117800 [info ] [Thread-1 (]: 10 of 24 PASS not_null_stg_subscriptions_start_date ............................ [[32mPASS[0m in 2.50s]
[0m12:44:17.123971 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8
[0m12:44:17.124623 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25
[0m12:44:17.125275 [info ] [Thread-1 (]: 13 of 24 START test not_null_stg_users_created_at .............................. [RUN]
[0m12:44:17.129779 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_subscriptions_start_date.aae12535b8, now test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25)
[0m12:44:17.130336 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25
[0m12:44:17.138182 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25"
[0m12:44:17.142266 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25
[0m12:44:17.147088 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25"
[0m12:44:17.149414 [debug] [Thread-1 (]: On test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select created_at
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
where created_at is null



  
  
      
    ) dbt_internal_test
[0m12:44:17.154888 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:44:17.232777 [info ] [Thread-3 (]: 12 of 24 PASS not_null_stg_subscriptions_user_id ............................... [[32mPASS[0m in 2.33s]
[0m12:44:17.234169 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2
[0m12:44:17.235181 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77
[0m12:44:17.235876 [info ] [Thread-3 (]: 14 of 24 START test not_null_stg_users_user_id ................................. [RUN]
[0m12:44:17.237120 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_subscriptions_user_id.3c73aaf4e2, now test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77)
[0m12:44:17.237796 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77
[0m12:44:17.244674 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77"
[0m12:44:17.247294 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77
[0m12:44:17.254044 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77"
[0m12:44:17.256405 [debug] [Thread-3 (]: On test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
where user_id is null



  
  
      
    ) dbt_internal_test
[0m12:44:17.256931 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m12:44:17.350572 [info ] [Thread-2 (]: 9 of 24 PASS not_null_stg_shipments_user_id .................................... [[32mPASS[0m in 2.85s]
[0m12:44:17.352417 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852
[0m12:44:17.353161 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781
[0m12:44:17.355426 [info ] [Thread-2 (]: 15 of 24 START test relationships_stg_addresses_user_id__user_id__ref_stg_users_  [RUN]
[0m12:44:17.356230 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_shipments_user_id.c5a36d6852, now test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781)
[0m12:44:17.356768 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781
[0m12:44:17.376683 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781"
[0m12:44:17.381813 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781
[0m12:44:17.392184 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781"
[0m12:44:17.394806 [debug] [Thread-2 (]: On test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select user_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_addresses`
    where user_id is not null
),

parent as (
    select user_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m12:44:17.396003 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m12:44:17.858498 [info ] [Thread-4 (]: 11 of 24 PASS not_null_stg_subscriptions_subscription_id ....................... [[32mPASS[0m in 3.17s]
[0m12:44:17.860084 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m12:44:17.861807 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737
[0m12:44:17.862979 [info ] [Thread-4 (]: 16 of 24 START test relationships_stg_orders_user_id__user_id__ref_stg_users_ .. [RUN]
[0m12:44:17.864596 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_subscriptions_subscription_id.68a3edda91, now test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737)
[0m12:44:17.866005 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737
[0m12:44:17.878921 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737"
[0m12:44:17.885588 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737
[0m12:44:17.907209 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737"
[0m12:44:17.919631 [debug] [Thread-4 (]: On test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select user_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_orders`
    where user_id is not null
),

parent as (
    select user_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m12:44:17.925668 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m12:44:18.726357 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:05273560-f8f4-494e-bbb5-332f5c0deff1&page=queryresults
[0m12:44:18.829701 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:49f43135-7d84-463a-b9f3-3889e8ad93d3&page=queryresults
[0m12:44:18.841341 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:aa1ec985-6bde-4fc2-b8cf-e5dc200ebb54&page=queryresults
[0m12:44:19.249232 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:2fc79283-2bdb-4b3d-9d72-c91acddc2fe3&page=queryresults
[0m12:44:19.634098 [info ] [Thread-3 (]: 14 of 24 PASS not_null_stg_users_user_id ....................................... [[32mPASS[0m in 2.40s]
[0m12:44:19.636088 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77
[0m12:44:19.637460 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8
[0m12:44:19.638500 [info ] [Thread-3 (]: 17 of 24 START test relationships_stg_shipments_user_id__user_id__ref_stg_users_  [RUN]
[0m12:44:19.639859 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_users_user_id.980dfc1b77, now test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8)
[0m12:44:19.640705 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8
[0m12:44:19.648214 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8"
[0m12:44:19.650096 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8
[0m12:44:19.653937 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8"
[0m12:44:19.655060 [debug] [Thread-3 (]: On test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select user_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_shipments`
    where user_id is not null
),

parent as (
    select user_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m12:44:19.655637 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m12:44:19.739996 [info ] [Thread-1 (]: 13 of 24 PASS not_null_stg_users_created_at .................................... [[32mPASS[0m in 2.61s]
[0m12:44:19.748635 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25
[0m12:44:19.749465 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m12:44:19.752105 [info ] [Thread-1 (]: 18 of 24 START test relationships_stg_subscriptions_user_id__user_id__ref_stg_users_  [RUN]
[0m12:44:19.753343 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_stg_users_created_at.7be3c50b25, now test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e)
[0m12:44:19.753803 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m12:44:19.759828 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e"
[0m12:44:19.760821 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m12:44:19.784594 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e"
[0m12:44:19.794178 [info ] [Thread-2 (]: 15 of 24 PASS relationships_stg_addresses_user_id__user_id__ref_stg_users_ ..... [[32mPASS[0m in 2.44s]
[0m12:44:19.800693 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781
[0m12:44:19.801714 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00
[0m12:44:19.809179 [debug] [Thread-1 (]: On test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select user_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_subscriptions`
    where user_id is not null
),

parent as (
    select user_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m12:44:19.812471 [info ] [Thread-2 (]: 19 of 24 START test unique_stg_addresses_address_id ............................ [RUN]
[0m12:44:19.813638 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:44:19.814684 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_stg_addresses_user_id__user_id__ref_stg_users_.7fba020781, now test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00)
[0m12:44:19.935283 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00
[0m12:44:19.953161 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00"
[0m12:44:19.955060 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00
[0m12:44:19.968749 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00"
[0m12:44:19.973717 [debug] [Thread-2 (]: On test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select address_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_addresses`
  where address_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m12:44:19.975770 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m12:44:20.386761 [info ] [Thread-4 (]: 16 of 24 PASS relationships_stg_orders_user_id__user_id__ref_stg_users_ ........ [[32mPASS[0m in 2.52s]
[0m12:44:20.388856 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737
[0m12:44:20.389728 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.unique_stg_countries_country_id.8679936442
[0m12:44:20.391108 [info ] [Thread-4 (]: 20 of 24 START test unique_stg_countries_country_id ............................ [RUN]
[0m12:44:20.392121 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_stg_orders_user_id__user_id__ref_stg_users_.42a5ade737, now test.data_pipeline_project.unique_stg_countries_country_id.8679936442)
[0m12:44:20.392807 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.unique_stg_countries_country_id.8679936442
[0m12:44:20.398170 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.unique_stg_countries_country_id.8679936442"
[0m12:44:20.399830 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.unique_stg_countries_country_id.8679936442
[0m12:44:20.403393 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.unique_stg_countries_country_id.8679936442"
[0m12:44:20.408748 [debug] [Thread-4 (]: On test.data_pipeline_project.unique_stg_countries_country_id.8679936442: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_stg_countries_country_id.8679936442"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select country_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_countries`
  where country_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m12:44:20.410285 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m12:44:21.239801 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:c2dd1159-b9e5-4539-9077-e2672cbfe475&page=queryresults
[0m12:44:21.534113 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:04499843-eb0c-4421-b49c-f07707ec8d96&page=queryresults
[0m12:44:21.537194 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:58c877d0-ef97-4a9d-8f46-2909225e2d2d&page=queryresults
[0m12:44:21.922132 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:7ff12824-3953-47ad-967f-7c24708a49a2&page=queryresults
[0m12:44:22.137560 [info ] [Thread-3 (]: 17 of 24 PASS relationships_stg_shipments_user_id__user_id__ref_stg_users_ ..... [[32mPASS[0m in 2.50s]
[0m12:44:22.142357 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8
[0m12:44:22.143252 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a
[0m12:44:22.145779 [info ] [Thread-3 (]: 21 of 24 START test unique_stg_orders_order_id ................................. [RUN]
[0m12:44:22.147822 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_stg_shipments_user_id__user_id__ref_stg_users_.dae05025e8, now test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a)
[0m12:44:22.148883 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a
[0m12:44:22.163325 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a"
[0m12:44:22.165789 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a
[0m12:44:22.171078 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a"
[0m12:44:22.175270 [debug] [Thread-3 (]: On test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select order_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_orders`
  where order_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m12:44:22.176937 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m12:44:22.422806 [info ] [Thread-2 (]: 19 of 24 PASS unique_stg_addresses_address_id .................................. [[32mPASS[0m in 2.61s]
[0m12:44:22.425945 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00
[0m12:44:22.428798 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c
[0m12:44:22.432123 [info ] [Thread-2 (]: 22 of 24 START test unique_stg_shipments_shipment_id ........................... [RUN]
[0m12:44:22.435696 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_stg_addresses_address_id.d15360bb00, now test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c)
[0m12:44:22.436536 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c
[0m12:44:22.450799 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c"
[0m12:44:22.452087 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c
[0m12:44:22.461923 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c"
[0m12:44:22.466012 [debug] [Thread-2 (]: On test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select shipment_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_shipments`
  where shipment_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m12:44:22.474397 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m12:44:22.571846 [info ] [Thread-1 (]: 18 of 24 PASS relationships_stg_subscriptions_user_id__user_id__ref_stg_users_ . [[32mPASS[0m in 2.82s]
[0m12:44:22.581119 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m12:44:22.584084 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m12:44:22.589444 [info ] [Thread-1 (]: 23 of 24 START test unique_stg_subscriptions_subscription_id ................... [RUN]
[0m12:44:22.592731 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e, now test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e)
[0m12:44:22.593280 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m12:44:22.606537 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e"
[0m12:44:22.607588 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m12:44:22.615185 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e"
[0m12:44:22.617558 [debug] [Thread-1 (]: On test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select subscription_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_subscriptions`
  where subscription_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m12:44:22.618416 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:44:22.856141 [info ] [Thread-4 (]: 20 of 24 PASS unique_stg_countries_country_id .................................. [[32mPASS[0m in 2.46s]
[0m12:44:22.861360 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.unique_stg_countries_country_id.8679936442
[0m12:44:22.867035 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b
[0m12:44:22.869348 [info ] [Thread-4 (]: 24 of 24 START test unique_stg_users_user_id ................................... [RUN]
[0m12:44:22.896367 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_stg_countries_country_id.8679936442, now test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b)
[0m12:44:22.897405 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b
[0m12:44:22.910004 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b"
[0m12:44:22.926360 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b
[0m12:44:22.930780 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b"
[0m12:44:22.949762 [debug] [Thread-4 (]: On test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select user_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
  where user_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m12:44:22.954109 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m12:44:23.592613 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:2c90a9ff-036c-4507-88e6-243e32e4c29f&page=queryresults
[0m12:44:24.003039 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:2a32d262-fd1c-43b7-b91d-9f748d65e2fa&page=queryresults
[0m12:44:24.104022 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:0d1d758f-375a-4ab5-8964-9f14122fef1a&page=queryresults
[0m12:44:24.167744 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:a3e0d1cc-2116-484e-b819-b0c8337e76ba&page=queryresults
[0m12:44:24.468308 [info ] [Thread-3 (]: 21 of 24 PASS unique_stg_orders_order_id ....................................... [[32mPASS[0m in 2.32s]
[0m12:44:24.469542 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a
[0m12:44:24.882493 [info ] [Thread-2 (]: 22 of 24 PASS unique_stg_shipments_shipment_id ................................. [[32mPASS[0m in 2.45s]
[0m12:44:24.886569 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c
[0m12:44:25.023447 [info ] [Thread-4 (]: 24 of 24 PASS unique_stg_users_user_id ......................................... [[32mPASS[0m in 2.12s]
[0m12:44:25.026486 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b
[0m12:44:25.169624 [info ] [Thread-1 (]: 23 of 24 PASS unique_stg_subscriptions_subscription_id ......................... [[32mPASS[0m in 2.58s]
[0m12:44:25.170734 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m12:44:25.174848 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m12:44:25.224197 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:44:25.225959 [debug] [MainThread]: Connection 'test.data_pipeline_project.unique_stg_shipments_shipment_id.61ce1cfa9c' was properly closed.
[0m12:44:25.226669 [debug] [MainThread]: Connection 'test.data_pipeline_project.unique_stg_subscriptions_subscription_id.d45e893a6e' was properly closed.
[0m12:44:25.227297 [debug] [MainThread]: Connection 'test.data_pipeline_project.unique_stg_orders_order_id.e3b841c71a' was properly closed.
[0m12:44:25.228697 [debug] [MainThread]: Connection 'test.data_pipeline_project.unique_stg_users_user_id.c2ff477e6b' was properly closed.
[0m12:44:25.230696 [info ] [MainThread]: 
[0m12:44:25.232239 [info ] [MainThread]: Finished running 24 data tests in 0 hours 0 minutes and 17.26 seconds (17.26s).
[0m12:44:25.239303 [debug] [MainThread]: Command end result
[0m12:44:25.304412 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m12:44:25.348116 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m12:44:25.362006 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m12:44:25.362897 [info ] [MainThread]: 
[0m12:44:25.364715 [info ] [MainThread]: [33mCompleted with 1 warning:[0m
[0m12:44:25.368128 [info ] [MainThread]: 
[0m12:44:25.369476 [warn ] [MainThread]: [33mWarning in test not_null_int_shipments_decomposed_delivered_at (models/staging/staging_schema.yml)[0m
[0m12:44:25.372547 [warn ] [MainThread]: Got 303 results, configured to warn if != 0
[0m12:44:25.373384 [info ] [MainThread]: 
[0m12:44:25.374364 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_project/models/staging/staging_schema.yml/not_null_int_shipments_decomposed_delivered_at.sql
[0m12:44:25.376054 [info ] [MainThread]: 
[0m12:44:25.377031 [info ] [MainThread]: Done. PASS=23 WARN=1 ERROR=0 SKIP=0 NO-OP=0 TOTAL=24
[0m12:44:25.379129 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": true, "command_wall_clock_time": 25.2224, "process_in_blocks": "162640", "process_kernel_time": 3.272541, "process_mem_max_rss": "414276", "process_out_blocks": "4488", "process_user_time": 9.679221}
[0m12:44:25.380098 [debug] [MainThread]: Command `dbt test` succeeded at 12:44:25.379981 after 25.22 seconds
[0m12:44:25.381402 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x768cc88c7260>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x768ca56f3530>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x768cc83ade20>]}
[0m12:44:25.382666 [debug] [MainThread]: Flushing usage events
[0m12:44:31.721219 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:56:32.337263 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x740de787b230>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x740de996a5a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x740de97e8410>]}


============================== 12:56:32.341261 | 3182db31-18fc-4a3d-933b-1c6ea549790b ==============================
[0m12:56:32.341261 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m12:56:32.342431 [debug] [MainThread]: running dbt with arguments {'log_cache_events': 'False', 'indirect_selection': 'eager', 'send_anonymous_usage_stats': 'True', 'profiles_dir': '/home/ecem/.dbt', 'quiet': 'False', 'partial_parse': 'True', 'introspect': 'True', 'target_path': 'None', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'warn_error': 'None', 'use_experimental_parser': 'False', 'empty': 'None', 'log_format': 'default', 'no_print': 'None', 'debug': 'False', 'write_json': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'invocation_command': 'dbt test --select models/intermediate/int_facts_marts_schema.yml', 'static_parser': 'True', 'use_colors': 'True', 'printer_width': '80', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])'}
[0m12:56:36.873962 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3182db31-18fc-4a3d-933b-1c6ea549790b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x740dc68bd430>]}
[0m12:56:36.951886 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '3182db31-18fc-4a3d-933b-1c6ea549790b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x740dc674e300>]}
[0m12:56:36.953261 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m12:56:37.268946 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m12:56:37.592066 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m12:56:37.592933 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m12:56:37.669304 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3182db31-18fc-4a3d-933b-1c6ea549790b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x740de6bde870>]}
[0m12:56:37.804466 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m12:56:37.834514 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m12:56:37.906922 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3182db31-18fc-4a3d-933b-1c6ea549790b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x740dc5b70dd0>]}
[0m12:56:37.907881 [info ] [MainThread]: Found 21 models, 67 data tests, 3 analyses, 10 sources, 624 macros
[0m12:56:37.909127 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3182db31-18fc-4a3d-933b-1c6ea549790b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x740de706f710>]}
[0m12:56:37.916263 [info ] [MainThread]: 
[0m12:56:37.916873 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m12:56:37.917545 [info ] [MainThread]: 
[0m12:56:37.918438 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m12:56:37.926788 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_marts'
[0m12:56:37.928473 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_staging'
[0m12:56:37.930658 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:56:37.929731 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:56:39.076509 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3182db31-18fc-4a3d-933b-1c6ea549790b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x740dc65320c0>]}
[0m12:56:39.077506 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:56:39.155605 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.dbt_utils_accepted_range_fct_marketing_spend_spend_try__0.bfe523cb61
[0m12:56:39.158692 [info ] [Thread-1 (]: 1 of 30 START test dbt_utils_accepted_range_fct_marketing_spend_spend_try__0 ... [RUN]
[0m12:56:39.157805 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.dbt_utils_accepted_range_fct_order_discount_total__0.5f31954cd6
[0m12:56:39.160037 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_marts, now test.data_pipeline_project.dbt_utils_accepted_range_fct_marketing_spend_spend_try__0.bfe523cb61)
[0m12:56:39.161117 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.dbt_utils_accepted_range_fct_order_net_revenue__0.cb846760ce
[0m12:56:39.162154 [info ] [Thread-2 (]: 2 of 30 START test dbt_utils_accepted_range_fct_order_discount_total__0 ........ [RUN]
[0m12:56:39.163406 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.dbt_utils_accepted_range_fct_marketing_spend_spend_try__0.bfe523cb61
[0m12:56:39.164058 [info ] [Thread-3 (]: 3 of 30 START test dbt_utils_accepted_range_fct_order_net_revenue__0 ........... [RUN]
[0m12:56:39.165186 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now test.data_pipeline_project.dbt_utils_accepted_range_fct_order_discount_total__0.5f31954cd6)
[0m12:56:39.173956 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.dbt_utils_accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0.be1f75eba9
[0m12:56:39.175757 [debug] [Thread-3 (]: Acquiring new bigquery connection 'test.data_pipeline_project.dbt_utils_accepted_range_fct_order_net_revenue__0.cb846760ce'
[0m12:56:39.193386 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.dbt_utils_accepted_range_fct_order_discount_total__0.5f31954cd6
[0m12:56:39.198949 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.dbt_utils_accepted_range_fct_marketing_spend_spend_try__0.bfe523cb61"
[0m12:56:39.202338 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.dbt_utils_accepted_range_fct_order_net_revenue__0.cb846760ce
[0m12:56:39.207243 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.dbt_utils_accepted_range_fct_order_discount_total__0.5f31954cd6"
[0m12:56:39.201245 [info ] [Thread-4 (]: 4 of 30 START test dbt_utils_accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0  [RUN]
[0m12:56:39.216023 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.dbt_utils_accepted_range_fct_order_net_revenue__0.cb846760ce"
[0m12:56:39.216794 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.dbt_utils_accepted_range_fct_marketing_spend_spend_try__0.bfe523cb61
[0m12:56:39.218277 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.dbt_utils_accepted_range_fct_order_discount_total__0.5f31954cd6
[0m12:56:39.219295 [debug] [Thread-4 (]: Acquiring new bigquery connection 'test.data_pipeline_project.dbt_utils_accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0.be1f75eba9'
[0m12:56:39.228030 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.dbt_utils_accepted_range_fct_order_net_revenue__0.cb846760ce
[0m12:56:39.243415 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.dbt_utils_accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0.be1f75eba9
[0m12:56:39.296775 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.dbt_utils_accepted_range_fct_marketing_spend_spend_try__0.bfe523cb61"
[0m12:56:39.302690 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.dbt_utils_accepted_range_fct_order_discount_total__0.5f31954cd6"
[0m12:56:39.310914 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.dbt_utils_accepted_range_fct_order_net_revenue__0.cb846760ce"
[0m12:56:39.341528 [debug] [Thread-3 (]: On test.data_pipeline_project.dbt_utils_accepted_range_fct_order_net_revenue__0.cb846760ce: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.dbt_utils_accepted_range_fct_order_net_revenue__0.cb846760ce"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  

with meet_condition as(
  select *
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_order`
),

validation_errors as (
  select *
  from meet_condition
  where
    -- never true, defaults to an empty result set. Exists to ensure any combo of the `or` clauses below succeeds
    1 = 2
    -- records with a value >= min_value are permitted. The `not` flips this to find records that don't meet the rule.
    or not net_revenue >= 0
)

select *
from validation_errors


  
  
      
    ) dbt_internal_test
[0m12:56:39.339891 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.dbt_utils_accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0.be1f75eba9"
[0m12:56:39.344796 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.dbt_utils_accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0.be1f75eba9
[0m12:56:39.342456 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m12:56:39.332997 [debug] [Thread-1 (]: On test.data_pipeline_project.dbt_utils_accepted_range_fct_marketing_spend_spend_try__0.bfe523cb61: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.dbt_utils_accepted_range_fct_marketing_spend_spend_try__0.bfe523cb61"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  

with meet_condition as(
  select *
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_marketing_spend`
),

validation_errors as (
  select *
  from meet_condition
  where
    -- never true, defaults to an empty result set. Exists to ensure any combo of the `or` clauses below succeeds
    1 = 2
    -- records with a value >= min_value are permitted. The `not` flips this to find records that don't meet the rule.
    or not spend_try >= 0
)

select *
from validation_errors


  
  
      
    ) dbt_internal_test
[0m12:56:39.345663 [debug] [Thread-2 (]: On test.data_pipeline_project.dbt_utils_accepted_range_fct_order_discount_total__0.5f31954cd6: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.dbt_utils_accepted_range_fct_order_discount_total__0.5f31954cd6"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  

with meet_condition as(
  select *
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_order`
),

validation_errors as (
  select *
  from meet_condition
  where
    -- never true, defaults to an empty result set. Exists to ensure any combo of the `or` clauses below succeeds
    1 = 2
    -- records with a value >= min_value are permitted. The `not` flips this to find records that don't meet the rule.
    or not discount_total >= 0
)

select *
from validation_errors


  
  
      
    ) dbt_internal_test
[0m12:56:39.351721 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:56:39.350768 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.dbt_utils_accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0.be1f75eba9"
[0m12:56:39.409401 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m12:56:39.463865 [debug] [Thread-4 (]: On test.data_pipeline_project.dbt_utils_accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0.be1f75eba9: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.dbt_utils_accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0.be1f75eba9"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  

with meet_condition as(
  select *
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_acquisition_efficiency`
),

validation_errors as (
  select *
  from meet_condition
  where
    -- never true, defaults to an empty result set. Exists to ensure any combo of the `or` clauses below succeeds
    1 = 2
    -- records with a value >= min_value are permitted. The `not` flips this to find records that don't meet the rule.
    or not daily_customer_acquisition_cost >= 0
)

select *
from validation_errors


  
  
      
    ) dbt_internal_test
[0m12:56:39.468257 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m12:56:40.789483 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:8b3787d5-a695-4ecb-8021-a5513a97e75f&page=queryresults
[0m12:56:41.005990 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:f7568d42-4c8e-413f-a724-f0d4722f969d&page=queryresults
[0m12:56:41.092186 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:6811edab-17e2-4bb6-a5b8-dd74d2d63e7a&page=queryresults
[0m12:56:41.207368 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:ba9f17a8-ddcf-4b33-9953-816de4e4d5fc&page=queryresults
[0m12:56:41.972983 [info ] [Thread-2 (]: 2 of 30 PASS dbt_utils_accepted_range_fct_order_discount_total__0 .............. [[32mPASS[0m in 2.81s]
[0m12:56:41.974623 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.dbt_utils_accepted_range_fct_order_discount_total__0.5f31954cd6
[0m12:56:41.975461 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_cac_payback_months__0.440e9fbb47
[0m12:56:41.976469 [info ] [Thread-2 (]: 5 of 30 START test dbt_utils_accepted_range_mart_kpis_latest_cac_payback_months__0  [RUN]
[0m12:56:41.977719 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.dbt_utils_accepted_range_fct_order_discount_total__0.5f31954cd6, now test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_cac_payback_months__0.440e9fbb47)
[0m12:56:41.978679 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_cac_payback_months__0.440e9fbb47
[0m12:56:41.984172 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_cac_payback_months__0.440e9fbb47"
[0m12:56:41.987051 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_cac_payback_months__0.440e9fbb47
[0m12:56:41.991967 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_cac_payback_months__0.440e9fbb47"
[0m12:56:41.993635 [debug] [Thread-2 (]: On test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_cac_payback_months__0.440e9fbb47: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_cac_payback_months__0.440e9fbb47"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  

with meet_condition as(
  select *
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_kpis_latest`
),

validation_errors as (
  select *
  from meet_condition
  where
    -- never true, defaults to an empty result set. Exists to ensure any combo of the `or` clauses below succeeds
    1 = 2
    -- records with a value >= min_value are permitted. The `not` flips this to find records that don't meet the rule.
    or not cac_payback_months >= 0
)

select *
from validation_errors


  
  
      
    ) dbt_internal_test
[0m12:56:41.994672 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m12:56:42.127232 [info ] [Thread-4 (]: 4 of 30 PASS dbt_utils_accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0  [[32mPASS[0m in 2.91s]
[0m12:56:42.129287 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.dbt_utils_accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0.be1f75eba9
[0m12:56:42.129971 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_monthly_recurring_revenue__0.e7fa0e4f46
[0m12:56:42.130923 [info ] [Thread-4 (]: 6 of 30 START test dbt_utils_accepted_range_mart_kpis_latest_monthly_recurring_revenue__0  [RUN]
[0m12:56:42.132334 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.dbt_utils_accepted_range_mart_acquisition_efficiency_daily_customer_acquisition_cost__0.be1f75eba9, now test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_monthly_recurring_revenue__0.e7fa0e4f46)
[0m12:56:42.133059 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_monthly_recurring_revenue__0.e7fa0e4f46
[0m12:56:42.143005 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_monthly_recurring_revenue__0.e7fa0e4f46"
[0m12:56:42.156401 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_monthly_recurring_revenue__0.e7fa0e4f46
[0m12:56:42.168508 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_monthly_recurring_revenue__0.e7fa0e4f46"
[0m12:56:42.173126 [debug] [Thread-4 (]: On test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_monthly_recurring_revenue__0.e7fa0e4f46: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_monthly_recurring_revenue__0.e7fa0e4f46"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  

with meet_condition as(
  select *
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_kpis_latest`
),

validation_errors as (
  select *
  from meet_condition
  where
    -- never true, defaults to an empty result set. Exists to ensure any combo of the `or` clauses below succeeds
    1 = 2
    -- records with a value >= min_value are permitted. The `not` flips this to find records that don't meet the rule.
    or not monthly_recurring_revenue >= 0
)

select *
from validation_errors


  
  
      
    ) dbt_internal_test
[0m12:56:42.177400 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m12:56:42.255523 [info ] [Thread-3 (]: 3 of 30 PASS dbt_utils_accepted_range_fct_order_net_revenue__0 ................. [[32mPASS[0m in 3.08s]
[0m12:56:42.262675 [info ] [Thread-1 (]: 1 of 30 PASS dbt_utils_accepted_range_fct_marketing_spend_spend_try__0 ......... [[32mPASS[0m in 3.10s]
[0m12:56:42.271192 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.dbt_utils_accepted_range_fct_order_net_revenue__0.cb846760ce
[0m12:56:42.274042 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.dbt_utils_accepted_range_fct_marketing_spend_spend_try__0.bfe523cb61
[0m12:56:42.275162 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.dbt_utils_accepted_range_mart_revenue_daily_daily_revenue__0.611ebe5f4e
[0m12:56:42.276942 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_active_subscribers__0.74fd82653d
[0m12:56:42.279080 [info ] [Thread-3 (]: 7 of 30 START test dbt_utils_accepted_range_mart_revenue_daily_daily_revenue__0  [RUN]
[0m12:56:42.282293 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.dbt_utils_accepted_range_fct_order_net_revenue__0.cb846760ce, now test.data_pipeline_project.dbt_utils_accepted_range_mart_revenue_daily_daily_revenue__0.611ebe5f4e)
[0m12:56:42.283161 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.dbt_utils_accepted_range_mart_revenue_daily_daily_revenue__0.611ebe5f4e
[0m12:56:42.280546 [info ] [Thread-1 (]: 8 of 30 START test dbt_utils_accepted_range_mart_subscription_daily_active_subscribers__0  [RUN]
[0m12:56:42.296891 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.dbt_utils_accepted_range_mart_revenue_daily_daily_revenue__0.611ebe5f4e"
[0m12:56:42.299767 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.dbt_utils_accepted_range_fct_marketing_spend_spend_try__0.bfe523cb61, now test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_active_subscribers__0.74fd82653d)
[0m12:56:42.305428 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.dbt_utils_accepted_range_mart_revenue_daily_daily_revenue__0.611ebe5f4e
[0m12:56:42.307685 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_active_subscribers__0.74fd82653d
[0m12:56:42.325215 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.dbt_utils_accepted_range_mart_revenue_daily_daily_revenue__0.611ebe5f4e"
[0m12:56:42.333551 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_active_subscribers__0.74fd82653d"
[0m12:56:42.344305 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_active_subscribers__0.74fd82653d
[0m12:56:42.351523 [debug] [Thread-3 (]: On test.data_pipeline_project.dbt_utils_accepted_range_mart_revenue_daily_daily_revenue__0.611ebe5f4e: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.dbt_utils_accepted_range_mart_revenue_daily_daily_revenue__0.611ebe5f4e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  

with meet_condition as(
  select *
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_revenue_daily`
),

validation_errors as (
  select *
  from meet_condition
  where
    -- never true, defaults to an empty result set. Exists to ensure any combo of the `or` clauses below succeeds
    1 = 2
    -- records with a value >= min_value are permitted. The `not` flips this to find records that don't meet the rule.
    or not daily_revenue >= 0
)

select *
from validation_errors


  
  
      
    ) dbt_internal_test
[0m12:56:42.356745 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_active_subscribers__0.74fd82653d"
[0m12:56:42.365767 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m12:56:42.447198 [debug] [Thread-1 (]: On test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_active_subscribers__0.74fd82653d: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_active_subscribers__0.74fd82653d"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  

with meet_condition as(
  select *
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily`
),

validation_errors as (
  select *
  from meet_condition
  where
    -- never true, defaults to an empty result set. Exists to ensure any combo of the `or` clauses below succeeds
    1 = 2
    -- records with a value >= min_value are permitted. The `not` flips this to find records that don't meet the rule.
    or not active_subscribers >= 0
)

select *
from validation_errors


  
  
      
    ) dbt_internal_test
[0m12:56:42.456754 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:56:43.645028 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:ae27342d-1bd9-408a-bf5b-2b029df764a6&page=queryresults
[0m12:56:43.955807 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:64debac7-ba76-4a5f-931b-e4fd661ee8ee&page=queryresults
[0m12:56:43.959486 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:8220dcbf-942a-4120-b99e-2c0f9a4a857c&page=queryresults
[0m12:56:44.025124 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:3de6182e-9d2d-4c1f-ad67-a89d453b3628&page=queryresults
[0m12:56:44.557452 [info ] [Thread-2 (]: 5 of 30 PASS dbt_utils_accepted_range_mart_kpis_latest_cac_payback_months__0 ... [[32mPASS[0m in 2.58s]
[0m12:56:44.558806 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_cac_payback_months__0.440e9fbb47
[0m12:56:44.559737 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_new_subscribers__0.654a8ca7d1
[0m12:56:44.560460 [info ] [Thread-2 (]: 9 of 30 START test dbt_utils_accepted_range_mart_subscription_daily_new_subscribers__0  [RUN]
[0m12:56:44.567109 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_cac_payback_months__0.440e9fbb47, now test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_new_subscribers__0.654a8ca7d1)
[0m12:56:44.568070 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_new_subscribers__0.654a8ca7d1
[0m12:56:44.574802 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_new_subscribers__0.654a8ca7d1"
[0m12:56:44.578922 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_new_subscribers__0.654a8ca7d1
[0m12:56:44.583158 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_new_subscribers__0.654a8ca7d1"
[0m12:56:44.584795 [debug] [Thread-2 (]: On test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_new_subscribers__0.654a8ca7d1: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_new_subscribers__0.654a8ca7d1"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  

with meet_condition as(
  select *
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily`
),

validation_errors as (
  select *
  from meet_condition
  where
    -- never true, defaults to an empty result set. Exists to ensure any combo of the `or` clauses below succeeds
    1 = 2
    -- records with a value >= min_value are permitted. The `not` flips this to find records that don't meet the rule.
    or not new_subscribers >= 0
)

select *
from validation_errors


  
  
      
    ) dbt_internal_test
[0m12:56:44.585859 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m12:56:44.848318 [info ] [Thread-3 (]: 7 of 30 PASS dbt_utils_accepted_range_mart_revenue_daily_daily_revenue__0 ...... [[32mPASS[0m in 2.57s]
[0m12:56:44.849936 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.dbt_utils_accepted_range_mart_revenue_daily_daily_revenue__0.611ebe5f4e
[0m12:56:44.855998 [info ] [Thread-4 (]: 6 of 30 PASS dbt_utils_accepted_range_mart_kpis_latest_monthly_recurring_revenue__0  [[32mPASS[0m in 2.72s]
[0m12:56:44.857314 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc
[0m12:56:44.860887 [info ] [Thread-3 (]: 10 of 30 START test not_null_dim_customer_customer_id .......................... [RUN]
[0m12:56:44.860010 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_monthly_recurring_revenue__0.e7fa0e4f46
[0m12:56:44.862173 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.dbt_utils_accepted_range_mart_revenue_daily_daily_revenue__0.611ebe5f4e, now test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc)
[0m12:56:44.863715 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723
[0m12:56:44.869359 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc
[0m12:56:44.870655 [info ] [Thread-4 (]: 11 of 30 START test not_null_dim_subscription_customer_id ...................... [RUN]
[0m12:56:44.886886 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.dbt_utils_accepted_range_mart_kpis_latest_monthly_recurring_revenue__0.e7fa0e4f46, now test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723)
[0m12:56:44.897353 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc"
[0m12:56:44.900729 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723
[0m12:56:44.909145 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723"
[0m12:56:44.918655 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc
[0m12:56:44.933122 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723
[0m12:56:44.935340 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc"
[0m12:56:44.955403 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723"
[0m12:56:44.958633 [debug] [Thread-3 (]: On test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select customer_id
from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_customer`
where customer_id is null



  
  
      
    ) dbt_internal_test
[0m12:56:44.962492 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m12:56:44.967539 [debug] [Thread-4 (]: On test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select customer_id
from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_subscription`
where customer_id is null



  
  
      
    ) dbt_internal_test
[0m12:56:45.057144 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m12:56:45.055792 [info ] [Thread-1 (]: 8 of 30 PASS dbt_utils_accepted_range_mart_subscription_daily_active_subscribers__0  [[32mPASS[0m in 2.76s]
[0m12:56:45.133448 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_active_subscribers__0.74fd82653d
[0m12:56:45.136569 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6
[0m12:56:45.138421 [info ] [Thread-1 (]: 12 of 30 START test not_null_dim_subscription_subscription_id .................. [RUN]
[0m12:56:45.141162 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_active_subscribers__0.74fd82653d, now test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6)
[0m12:56:45.142370 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6
[0m12:56:45.169212 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6"
[0m12:56:45.180713 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6
[0m12:56:45.208233 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6"
[0m12:56:45.224490 [debug] [Thread-1 (]: On test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select subscription_id
from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_subscription`
where subscription_id is null



  
  
      
    ) dbt_internal_test
[0m12:56:45.229957 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:56:46.187376 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:c0e9e6e7-00de-447a-8992-dd1951165784&page=queryresults
[0m12:56:46.291545 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:d78aa598-5dc7-40bd-b8de-03dae7bef31e&page=queryresults
[0m12:56:46.334529 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:6409782d-abf5-4a3d-bb9c-5da47ee36532&page=queryresults
[0m12:56:46.416729 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:9404cd0a-3ace-4b7e-8c21-48c828046768&page=queryresults
[0m12:56:47.133495 [info ] [Thread-3 (]: 10 of 30 PASS not_null_dim_customer_customer_id ................................ [[32mPASS[0m in 2.27s]
[0m12:56:47.135004 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc
[0m12:56:47.136019 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719
[0m12:56:47.137384 [info ] [Thread-3 (]: 13 of 30 START test not_null_fct_marketing_spend_date .......................... [RUN]
[0m12:56:47.138637 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_dim_customer_customer_id.9775012dfc, now test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719)
[0m12:56:47.139935 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719
[0m12:56:47.147385 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719"
[0m12:56:47.148646 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719
[0m12:56:47.157439 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719"
[0m12:56:47.158992 [debug] [Thread-3 (]: On test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select date
from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_marketing_spend`
where date is null



  
  
      
    ) dbt_internal_test
[0m12:56:47.159697 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m12:56:47.208309 [info ] [Thread-2 (]: 9 of 30 PASS dbt_utils_accepted_range_mart_subscription_daily_new_subscribers__0  [[32mPASS[0m in 2.64s]
[0m12:56:47.211518 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_new_subscribers__0.654a8ca7d1
[0m12:56:47.212345 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d
[0m12:56:47.212888 [info ] [Thread-2 (]: 14 of 30 START test not_null_fct_marketing_spend_spend_try ..................... [RUN]
[0m12:56:47.213885 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.dbt_utils_accepted_range_mart_subscription_daily_new_subscribers__0.654a8ca7d1, now test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d)
[0m12:56:47.214304 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d
[0m12:56:47.218431 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d"
[0m12:56:47.223032 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d
[0m12:56:47.228644 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d"
[0m12:56:47.230905 [debug] [Thread-2 (]: On test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select spend_try
from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_marketing_spend`
where spend_try is null



  
  
      
    ) dbt_internal_test
[0m12:56:47.231792 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m12:56:47.328337 [info ] [Thread-1 (]: 12 of 30 PASS not_null_dim_subscription_subscription_id ........................ [[32mPASS[0m in 2.19s]
[0m12:56:47.333650 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6
[0m12:56:47.334373 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff
[0m12:56:47.335589 [info ] [Thread-1 (]: 15 of 30 START test not_null_fct_order_customer_id ............................. [RUN]
[0m12:56:47.336913 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_dim_subscription_subscription_id.6cbf78dfa6, now test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff)
[0m12:56:47.337711 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff
[0m12:56:47.356058 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff"
[0m12:56:47.360448 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff
[0m12:56:47.379655 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff"
[0m12:56:47.381582 [debug] [Thread-1 (]: On test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select customer_id
from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_order`
where customer_id is null



  
  
      
    ) dbt_internal_test
[0m12:56:47.382621 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:56:47.471004 [info ] [Thread-4 (]: 11 of 30 PASS not_null_dim_subscription_customer_id ............................ [[32mPASS[0m in 2.58s]
[0m12:56:47.477357 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723
[0m12:56:47.481324 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea
[0m12:56:47.485879 [info ] [Thread-4 (]: 16 of 30 START test not_null_fct_order_discount_total .......................... [RUN]
[0m12:56:47.488896 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_dim_subscription_customer_id.6599c52723, now test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea)
[0m12:56:47.492123 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea
[0m12:56:47.515471 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea"
[0m12:56:47.522063 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea
[0m12:56:47.536414 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea"
[0m12:56:47.541810 [debug] [Thread-4 (]: On test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select discount_total
from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_order`
where discount_total is null



  
  
      
    ) dbt_internal_test
[0m12:56:47.542875 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m12:56:48.690994 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:d50eb5d8-6916-4468-930f-bfb92cfc0659&page=queryresults
[0m12:56:48.786722 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:731d23f6-52db-4be7-b683-527090b7599c&page=queryresults
[0m12:56:48.819036 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:74f37eb6-2949-4aee-ad62-7f593a2a9d33&page=queryresults
[0m12:56:48.957899 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:50aed385-0b20-4343-a04d-8434974723ab&page=queryresults
[0m12:56:49.669353 [info ] [Thread-1 (]: 15 of 30 PASS not_null_fct_order_customer_id ................................... [[32mPASS[0m in 2.33s]
[0m12:56:49.671630 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff
[0m12:56:49.672963 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e
[0m12:56:49.673888 [info ] [Thread-1 (]: 17 of 30 START test not_null_fct_order_net_revenue ............................. [RUN]
[0m12:56:49.675802 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_fct_order_customer_id.195e7badff, now test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e)
[0m12:56:49.676899 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e
[0m12:56:49.684894 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e"
[0m12:56:49.685961 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e
[0m12:56:49.690684 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e"
[0m12:56:49.697491 [debug] [Thread-1 (]: On test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select net_revenue
from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_order`
where net_revenue is null



  
  
      
    ) dbt_internal_test
[0m12:56:49.701665 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:56:49.750884 [info ] [Thread-2 (]: 14 of 30 PASS not_null_fct_marketing_spend_spend_try ........................... [[32mPASS[0m in 2.54s]
[0m12:56:49.755440 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d
[0m12:56:49.756109 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def
[0m12:56:49.756916 [info ] [Thread-2 (]: 18 of 30 START test not_null_fct_order_order_id ................................ [RUN]
[0m12:56:49.761464 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_fct_marketing_spend_spend_try.164ea9797d, now test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def)
[0m12:56:49.762253 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def
[0m12:56:49.768319 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def"
[0m12:56:49.769995 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def
[0m12:56:49.785072 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def"
[0m12:56:49.792904 [debug] [Thread-2 (]: On test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select order_id
from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_order`
where order_id is null



  
  
      
    ) dbt_internal_test
[0m12:56:49.794376 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m12:56:50.000488 [info ] [Thread-4 (]: 16 of 30 PASS not_null_fct_order_discount_total ................................ [[32mPASS[0m in 2.51s]
[0m12:56:50.018284 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea
[0m12:56:50.019913 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d
[0m12:56:50.017190 [info ] [Thread-3 (]: 13 of 30 PASS not_null_fct_marketing_spend_date ................................ [[32mPASS[0m in 2.88s]
[0m12:56:50.026313 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719
[0m12:56:50.027324 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b
[0m12:56:50.028810 [info ] [Thread-3 (]: 20 of 30 START test not_null_mart_acquisition_efficiency_date .................. [RUN]
[0m12:56:50.024824 [info ] [Thread-4 (]: 19 of 30 START test not_null_fct_shipment_shipment_id .......................... [RUN]
[0m12:56:50.030029 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_fct_marketing_spend_date.e20a13e719, now test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b)
[0m12:56:50.032872 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b
[0m12:56:50.031856 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_fct_order_discount_total.cfcdfc50ea, now test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d)
[0m12:56:50.044265 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d
[0m12:56:50.059742 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b"
[0m12:56:50.072641 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d"
[0m12:56:50.079025 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b
[0m12:56:50.085744 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b"
[0m12:56:50.091003 [debug] [Thread-3 (]: On test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select date
from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_acquisition_efficiency`
where date is null



  
  
      
    ) dbt_internal_test
[0m12:56:50.092265 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m12:56:50.147474 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d
[0m12:56:50.166098 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d"
[0m12:56:50.181032 [debug] [Thread-4 (]: On test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select shipment_id
from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_shipment`
where shipment_id is null



  
  
      
    ) dbt_internal_test
[0m12:56:50.182533 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m12:56:51.173622 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:a2e39b5b-6d34-4942-bce9-fc525ea5ee7b&page=queryresults
[0m12:56:51.388383 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:5fbea977-06bd-4abb-ab53-b246d0c6008a&page=queryresults
[0m12:56:51.423564 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:bf5830a0-09a9-4595-b62d-f0f0e6f198d2&page=queryresults
[0m12:56:51.453329 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:5d53b2b3-efab-4084-80f2-5efe7471e60c&page=queryresults
[0m12:56:52.028743 [info ] [Thread-1 (]: 17 of 30 PASS not_null_fct_order_net_revenue ................................... [[32mPASS[0m in 2.35s]
[0m12:56:52.030314 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e
[0m12:56:52.031459 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23
[0m12:56:52.032394 [info ] [Thread-1 (]: 21 of 30 START test not_null_mart_revenue_daily_date ........................... [RUN]
[0m12:56:52.033073 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_fct_order_net_revenue.739a986c4e, now test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23)
[0m12:56:52.033564 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23
[0m12:56:52.040684 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23"
[0m12:56:52.042123 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23
[0m12:56:52.047053 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23"
[0m12:56:52.049286 [debug] [Thread-1 (]: On test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select date
from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_revenue_daily`
where date is null



  
  
      
    ) dbt_internal_test
[0m12:56:52.060830 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:56:52.270472 [info ] [Thread-3 (]: 20 of 30 PASS not_null_mart_acquisition_efficiency_date ........................ [[32mPASS[0m in 2.24s]
[0m12:56:52.271855 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b
[0m12:56:52.273191 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322
[0m12:56:52.274170 [info ] [Thread-3 (]: 22 of 30 START test not_null_mart_subscription_daily_date ...................... [RUN]
[0m12:56:52.275581 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_mart_acquisition_efficiency_date.466595db2b, now test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322)
[0m12:56:52.276242 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322
[0m12:56:52.280847 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322"
[0m12:56:52.293166 [info ] [Thread-2 (]: 18 of 30 PASS not_null_fct_order_order_id ...................................... [[32mPASS[0m in 2.53s]
[0m12:56:52.296122 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322
[0m12:56:52.297647 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def
[0m12:56:52.300879 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322"
[0m12:56:52.301821 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29
[0m12:56:52.308600 [debug] [Thread-3 (]: On test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select date
from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily`
where date is null



  
  
      
    ) dbt_internal_test
[0m12:56:52.309518 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m12:56:52.304746 [info ] [Thread-2 (]: 23 of 30 START test relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_  [RUN]
[0m12:56:52.355489 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_fct_order_order_id.51e29a4def, now test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29)
[0m12:56:52.357324 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29
[0m12:56:52.390589 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29"
[0m12:56:52.394781 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29
[0m12:56:52.400598 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29"
[0m12:56:52.403438 [debug] [Thread-2 (]: On test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select customer_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_subscription`
    where customer_id is not null
),

parent as (
    select customer_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_customer`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m12:56:52.404400 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m12:56:52.515435 [info ] [Thread-4 (]: 19 of 30 PASS not_null_fct_shipment_shipment_id ................................ [[32mPASS[0m in 2.48s]
[0m12:56:52.520723 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d
[0m12:56:52.521684 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85
[0m12:56:52.526272 [info ] [Thread-4 (]: 24 of 30 START test relationships_fct_order_customer_id__customer_id__ref_dim_customer_  [RUN]
[0m12:56:52.531491 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_fct_shipment_shipment_id.a1fffbed8d, now test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85)
[0m12:56:52.532647 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85
[0m12:56:52.550438 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85"
[0m12:56:52.554273 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85
[0m12:56:52.558878 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85"
[0m12:56:52.560855 [debug] [Thread-4 (]: On test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select customer_id as from_field
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_order`
    where customer_id is not null
),

parent as (
    select customer_id as to_field
    from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_customer`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m12:56:52.565884 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m12:56:53.300213 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:728febac-9219-4a3f-8340-f6437a6cea12&page=queryresults
[0m12:56:53.496431 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:928a35c9-9a56-43c6-9728-f774e8c3bb2c&page=queryresults
[0m12:56:53.604202 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:460748dd-4232-4c9a-8594-fae865b94530&page=queryresults
[0m12:56:53.656415 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:58098729-f291-4b30-863e-f16baec04ec2&page=queryresults
[0m12:56:54.164786 [info ] [Thread-1 (]: 21 of 30 PASS not_null_mart_revenue_daily_date ................................. [[32mPASS[0m in 2.13s]
[0m12:56:54.166332 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23
[0m12:56:54.167163 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469
[0m12:56:54.167884 [info ] [Thread-1 (]: 25 of 30 START test unique_dim_subscription_subscription_id .................... [RUN]
[0m12:56:54.168789 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_mart_revenue_daily_date.606cbdcc23, now test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469)
[0m12:56:54.169723 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469
[0m12:56:54.183172 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469"
[0m12:56:54.185830 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469
[0m12:56:54.189499 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469"
[0m12:56:54.190788 [debug] [Thread-1 (]: On test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select subscription_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`dim_subscription`
  where subscription_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m12:56:54.191318 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:56:54.367075 [info ] [Thread-3 (]: 22 of 30 PASS not_null_mart_subscription_daily_date ............................ [[32mPASS[0m in 2.09s]
[0m12:56:54.369204 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322
[0m12:56:54.371412 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.unique_fct_order_order_id.653705d6a5
[0m12:56:54.373006 [info ] [Thread-3 (]: 26 of 30 START test unique_fct_order_order_id .................................. [RUN]
[0m12:56:54.374620 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.not_null_mart_subscription_daily_date.3cc363c322, now test.data_pipeline_project.unique_fct_order_order_id.653705d6a5)
[0m12:56:54.375511 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.unique_fct_order_order_id.653705d6a5
[0m12:56:54.383790 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.unique_fct_order_order_id.653705d6a5"
[0m12:56:54.387447 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.unique_fct_order_order_id.653705d6a5
[0m12:56:54.391910 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.unique_fct_order_order_id.653705d6a5"
[0m12:56:54.393340 [debug] [Thread-3 (]: On test.data_pipeline_project.unique_fct_order_order_id.653705d6a5: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_fct_order_order_id.653705d6a5"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select order_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_order`
  where order_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m12:56:54.394280 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m12:56:54.532206 [info ] [Thread-4 (]: 24 of 30 PASS relationships_fct_order_customer_id__customer_id__ref_dim_customer_  [[32mPASS[0m in 2.00s]
[0m12:56:54.545837 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85
[0m12:56:54.558882 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3
[0m12:56:54.577211 [info ] [Thread-4 (]: 27 of 30 START test unique_fct_shipment_shipment_id ............................ [RUN]
[0m12:56:54.580362 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_fct_order_customer_id__customer_id__ref_dim_customer_.580cf63d85, now test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3)
[0m12:56:54.595421 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3
[0m12:56:54.594134 [info ] [Thread-2 (]: 23 of 30 PASS relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_  [[32mPASS[0m in 2.24s]
[0m12:56:54.612999 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29
[0m12:56:54.623422 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd
[0m12:56:54.624837 [info ] [Thread-2 (]: 28 of 30 START test unique_mart_acquisition_efficiency_date .................... [RUN]
[0m12:56:54.628624 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.relationships_dim_subscription_customer_id__customer_id__ref_dim_customer_.5ad2e9ca29, now test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd)
[0m12:56:54.618193 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3"
[0m12:56:54.633362 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd
[0m12:56:54.637715 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3
[0m12:56:54.664649 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3"
[0m12:56:54.661503 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd"
[0m12:56:54.669816 [debug] [Thread-4 (]: On test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select shipment_id as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`fct_shipment`
  where shipment_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m12:56:54.672907 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m12:56:54.676825 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd
[0m12:56:54.797990 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd"
[0m12:56:54.814020 [debug] [Thread-2 (]: On test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select date as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_acquisition_efficiency`
  where date is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m12:56:54.820677 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m12:56:55.600947 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:b0427af2-dc19-4bd3-a768-62b0c61c2fe7&page=queryresults
[0m12:56:55.708012 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:4ced2e6f-b988-4522-bf35-3a1c2997d4c6&page=queryresults
[0m12:56:55.973512 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:43cc5be6-99f6-44d5-94f9-f63883746b93&page=queryresults
[0m12:56:55.993274 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:f5cd93d4-6224-455b-aff4-dc7badecf488&page=queryresults
[0m12:56:56.520204 [info ] [Thread-1 (]: 25 of 30 PASS unique_dim_subscription_subscription_id .......................... [[32mPASS[0m in 2.35s]
[0m12:56:56.521603 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469
[0m12:56:56.522454 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8
[0m12:56:56.523307 [info ] [Thread-1 (]: 29 of 30 START test unique_mart_revenue_daily_date ............................. [RUN]
[0m12:56:56.524041 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_dim_subscription_subscription_id.af781f2469, now test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8)
[0m12:56:56.524834 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8
[0m12:56:56.532389 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8"
[0m12:56:56.533565 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8
[0m12:56:56.541636 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8"
[0m12:56:56.543861 [debug] [Thread-1 (]: On test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select date as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_revenue_daily`
  where date is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m12:56:56.544658 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:56:56.617490 [info ] [Thread-3 (]: 26 of 30 PASS unique_fct_order_order_id ........................................ [[32mPASS[0m in 2.24s]
[0m12:56:56.620151 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.unique_fct_order_order_id.653705d6a5
[0m12:56:56.623178 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c
[0m12:56:56.625679 [info ] [Thread-3 (]: 30 of 30 START test unique_mart_subscription_daily_date ........................ [RUN]
[0m12:56:56.630372 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.unique_fct_order_order_id.653705d6a5, now test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c)
[0m12:56:56.632095 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c
[0m12:56:56.638087 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c"
[0m12:56:56.644029 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c
[0m12:56:56.648793 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c"
[0m12:56:56.652649 [debug] [Thread-3 (]: On test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select date as unique_field
  from `data-pipeline-project-474812`.`analytics_staging_marts`.`mart_subscription_daily`
  where date is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m12:56:56.654567 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m12:56:56.887381 [info ] [Thread-2 (]: 28 of 30 PASS unique_mart_acquisition_efficiency_date .......................... [[32mPASS[0m in 2.26s]
[0m12:56:56.889782 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd
[0m12:56:56.894856 [info ] [Thread-4 (]: 27 of 30 PASS unique_fct_shipment_shipment_id .................................. [[32mPASS[0m in 2.31s]
[0m12:56:56.897571 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3
[0m12:56:57.729096 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:07477eae-2f5c-47a4-8f2a-dad69e1d08bc&page=queryresults
[0m12:56:57.835996 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:fca241fb-fb00-4b6f-9a06-73ae82fcb071&page=queryresults
[0m12:56:58.627002 [info ] [Thread-3 (]: 30 of 30 PASS unique_mart_subscription_daily_date .............................. [[32mPASS[0m in 2.00s]
[0m12:56:58.630972 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c
[0m12:56:58.675889 [info ] [Thread-1 (]: 29 of 30 PASS unique_mart_revenue_daily_date ................................... [[32mPASS[0m in 2.15s]
[0m12:56:58.677220 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8
[0m12:56:58.683512 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m12:56:58.723745 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:56:58.724468 [debug] [MainThread]: Connection 'test.data_pipeline_project.unique_mart_revenue_daily_date.301e6020b8' was properly closed.
[0m12:56:58.724928 [debug] [MainThread]: Connection 'test.data_pipeline_project.unique_mart_acquisition_efficiency_date.788b6197fd' was properly closed.
[0m12:56:58.725360 [debug] [MainThread]: Connection 'test.data_pipeline_project.unique_mart_subscription_daily_date.4c4dec233c' was properly closed.
[0m12:56:58.726715 [debug] [MainThread]: Connection 'test.data_pipeline_project.unique_fct_shipment_shipment_id.5f49d8f7f3' was properly closed.
[0m12:56:58.727555 [info ] [MainThread]: 
[0m12:56:58.728278 [info ] [MainThread]: Finished running 30 data tests in 0 hours 0 minutes and 20.81 seconds (20.81s).
[0m12:56:58.739352 [debug] [MainThread]: Command end result
[0m12:56:58.818197 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m12:56:58.856866 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m12:56:58.871142 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m12:56:58.871889 [info ] [MainThread]: 
[0m12:56:58.872923 [info ] [MainThread]: [32mCompleted successfully[0m
[0m12:56:58.873708 [info ] [MainThread]: 
[0m12:56:58.874413 [info ] [MainThread]: Done. PASS=30 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=30
[0m12:56:58.878283 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": true, "command_wall_clock_time": 26.606325, "process_in_blocks": "131560", "process_kernel_time": 3.495689, "process_mem_max_rss": "414996", "process_out_blocks": "4632", "process_user_time": 9.894707}
[0m12:56:58.878904 [debug] [MainThread]: Command `dbt test` succeeded at 12:56:58.878800 after 26.61 seconds
[0m12:56:58.879326 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x740de6c81280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x740dc5ca6cf0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x740de75c43e0>]}
[0m12:56:58.880435 [debug] [MainThread]: Flushing usage events
[0m12:57:00.316851 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:09:43.030206 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b4647c713a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b464858dbe0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b464a93e510>]}


============================== 20:09:43.040342 | 7b25eaac-a3f6-4dcb-8464-5f6f4e50ec78 ==============================
[0m20:09:43.040342 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m20:09:43.048333 [debug] [MainThread]: running dbt with arguments {'invocation_command': 'dbt test --select path:models/staging/raw_sources.yml --profiles-dir .', 'printer_width': '80', 'log_cache_events': 'False', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'profiles_dir': '.', 'debug': 'False', 'use_colors': 'True', 'introspect': 'True', 'indirect_selection': 'eager', 'partial_parse': 'True', 'use_experimental_parser': 'False', 'warn_error': 'None', 'write_json': 'True', 'cache_selected_only': 'False', 'empty': 'None', 'quiet': 'False', 'version_check': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'no_print': 'None', 'target_path': 'None', 'fail_fast': 'False', 'send_anonymous_usage_stats': 'True', 'log_format': 'default'}
[0m20:09:43.058278 [error] [MainThread]: Encountered an error:
Runtime Error
  Could not find profile named 'data_pipeline_project'
[0m20:09:43.061619 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 0.16954865, "process_in_blocks": "24592", "process_kernel_time": 1.306746, "process_mem_max_rss": "102864", "process_out_blocks": "8", "process_user_time": 2.150016}
[0m20:09:43.063564 [debug] [MainThread]: Command `dbt test` failed at 20:09:43.062989 after 0.17 seconds
[0m20:09:43.065379 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b4648ff8560>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b4647bd7ec0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b4647bd7e90>]}
[0m20:09:43.066232 [debug] [MainThread]: Flushing usage events
[0m20:09:44.103154 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:09:51.513815 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x779ab5bdae70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x779ab5d3c260>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x779ab5e8fa40>]}


============================== 20:09:51.522817 | 2a5ff224-a68c-4f4e-ad39-0b87128f2df6 ==============================
[0m20:09:51.522817 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m20:09:51.524183 [debug] [MainThread]: running dbt with arguments {'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'profiles_dir': '.', 'write_json': 'True', 'target_path': 'None', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'debug': 'False', 'fail_fast': 'False', 'version_check': 'True', 'printer_width': '80', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'introspect': 'True', 'cache_selected_only': 'False', 'no_print': 'None', 'warn_error': 'None', 'use_colors': 'True', 'empty': 'False', 'invocation_command': 'dbt run --select path:models/staging/*.sql --profiles-dir .', 'log_format': 'default', 'quiet': 'False', 'log_cache_events': 'False'}
[0m20:09:51.531552 [error] [MainThread]: Encountered an error:
Runtime Error
  Could not find profile named 'data_pipeline_project'
[0m20:09:51.543733 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.18462576, "process_in_blocks": "168", "process_kernel_time": 0.886864, "process_mem_max_rss": "102712", "process_out_blocks": "8", "process_user_time": 2.758103}
[0m20:09:51.545519 [debug] [MainThread]: Command `dbt run` failed at 20:09:51.545224 after 0.19 seconds
[0m20:09:51.550549 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x779ab61bf950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x779ab52f7dd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x779ab591f5f0>]}
[0m20:09:51.552926 [debug] [MainThread]: Flushing usage events
[0m20:09:52.429356 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:15:00.847700 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7aad36cf91f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7aad34ffe5a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7aad36e6e7b0>]}


============================== 20:15:00.859498 | f2a9c47d-2f00-4ba7-becd-4afd8f51f596 ==============================
[0m20:15:00.859498 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m20:15:00.863376 [debug] [MainThread]: running dbt with arguments {'indirect_selection': 'eager', 'invocation_command': 'dbt run --select path:models/staging/*.sql --profiles-dir .', 'cache_selected_only': 'False', 'static_parser': 'True', 'warn_error': 'None', 'write_json': 'True', 'log_cache_events': 'False', 'no_print': 'None', 'use_experimental_parser': 'False', 'empty': 'False', 'fail_fast': 'False', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'version_check': 'True', 'profiles_dir': '.', 'quiet': 'False', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'introspect': 'True', 'debug': 'False', 'send_anonymous_usage_stats': 'True', 'partial_parse': 'True', 'log_format': 'default', 'use_colors': 'True', 'printer_width': '80'}
[0m20:15:00.875816 [error] [MainThread]: Encountered an error:
Runtime Error
  Could not find profile named 'data_pipeline_project'
[0m20:15:00.884893 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.22384088, "process_in_blocks": "0", "process_kernel_time": 0.93959, "process_mem_max_rss": "102956", "process_out_blocks": "8", "process_user_time": 2.910288}
[0m20:15:00.888202 [debug] [MainThread]: Command `dbt run` failed at 20:15:00.887858 after 0.23 seconds
[0m20:15:00.890034 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7aad350ef0e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7aad340f7f20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7aad340f7f50>]}
[0m20:15:00.891134 [debug] [MainThread]: Flushing usage events
[0m20:15:03.688330 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:50:02.483517 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b67059059a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b67060a7da0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b6706306ae0>]}


============================== 20:50:02.493913 | 77a7a439-1234-4c3d-a043-ccef3e05e74b ==============================
[0m20:50:02.493913 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m20:50:02.496369 [debug] [MainThread]: running dbt with arguments {'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'profiles_dir': '.', 'quiet': 'False', 'printer_width': '80', 'partial_parse': 'True', 'debug': 'False', 'warn_error': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True', 'log_cache_events': 'False', 'cache_selected_only': 'False', 'invocation_command': 'dbt test --select path:models/staging/raw_sources.yml --profiles-dir .', 'version_check': 'True', 'no_print': 'None', 'use_experimental_parser': 'False', 'use_colors': 'True', 'write_json': 'True', 'indirect_selection': 'eager', 'fail_fast': 'False', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'empty': 'None'}
[0m20:50:02.500792 [error] [MainThread]: Encountered an error:
Runtime Error
  Could not find profile named 'data_pipeline_project'
[0m20:50:02.503875 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 0.123979114, "process_in_blocks": "272", "process_kernel_time": 0.7602, "process_mem_max_rss": "103008", "process_out_blocks": "16", "process_user_time": 1.816407}
[0m20:50:02.505164 [debug] [MainThread]: Command `dbt test` failed at 20:50:02.504927 after 0.13 seconds
[0m20:50:02.506381 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b6706053710>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b67051bbdd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b67051bbda0>]}
[0m20:50:02.507621 [debug] [MainThread]: Flushing usage events
[0m20:50:04.268307 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:50:13.624898 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x785c2b7d2540>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x785c2b7ff230>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x785c2b7d25d0>]}


============================== 20:50:13.634771 | 87486901-7e77-44cd-a09f-fa1520820f08 ==============================
[0m20:50:13.634771 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m20:50:13.635993 [debug] [MainThread]: running dbt with arguments {'indirect_selection': 'eager', 'warn_error': 'None', 'fail_fast': 'False', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'invocation_command': 'dbt run --select path:models/staging/*.sql --profiles-dir .', 'empty': 'False', 'profiles_dir': '.', 'write_json': 'True', 'send_anonymous_usage_stats': 'True', 'introspect': 'True', 'log_format': 'default', 'use_experimental_parser': 'False', 'cache_selected_only': 'False', 'printer_width': '80', 'partial_parse': 'True', 'no_print': 'None', 'use_colors': 'True', 'static_parser': 'True', 'log_cache_events': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'version_check': 'True', 'target_path': 'None', 'debug': 'False', 'quiet': 'False'}
[0m20:50:13.639399 [error] [MainThread]: Encountered an error:
Runtime Error
  Could not find profile named 'data_pipeline_project'
[0m20:50:13.642313 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.22170755, "process_in_blocks": "0", "process_kernel_time": 0.877579, "process_mem_max_rss": "102596", "process_out_blocks": "0", "process_user_time": 2.246776}
[0m20:50:13.644528 [debug] [MainThread]: Command `dbt run` failed at 20:50:13.643774 after 0.22 seconds
[0m20:50:13.645591 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x785c2af0fb00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x785c2a7c3ec0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x785c2a7c3e90>]}
[0m20:50:13.646618 [debug] [MainThread]: Flushing usage events
[0m20:50:14.494887 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:55:21.184574 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79633f940050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79634032cef0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79633f942f00>]}


============================== 20:55:21.200014 | fa5e4af2-402d-4aa1-9a3b-43d9db9e0a89 ==============================
[0m20:55:21.200014 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m20:55:21.201426 [debug] [MainThread]: running dbt with arguments {'use_experimental_parser': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'partial_parse': 'True', 'introspect': 'True', 'empty': 'False', 'debug': 'False', 'target_path': 'None', 'no_print': 'None', 'log_cache_events': 'False', 'indirect_selection': 'eager', 'invocation_command': 'dbt run --select path:models/staging/*.sql --profiles-dir .', 'static_parser': 'True', 'log_format': 'default', 'send_anonymous_usage_stats': 'True', 'printer_width': '80', 'write_json': 'True', 'cache_selected_only': 'False', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'use_colors': 'True', 'version_check': 'True', 'quiet': 'False', 'profiles_dir': '.', 'fail_fast': 'False', 'warn_error': 'None'}
[0m20:55:21.205915 [error] [MainThread]: Encountered an error:
Runtime Error
  Could not find profile named 'data_pipeline_project'
[0m20:55:21.207481 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.091636404, "process_in_blocks": "25152", "process_kernel_time": 0.581264, "process_mem_max_rss": "102724", "process_out_blocks": "8", "process_user_time": 1.809458}
[0m20:55:21.208527 [debug] [MainThread]: Command `dbt run` failed at 20:55:21.208391 after 0.09 seconds
[0m20:55:21.209012 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7963401c6930>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79633f1bbec0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79633f1bbe90>]}
[0m20:55:21.209839 [debug] [MainThread]: Flushing usage events
[0m20:55:22.106171 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:55:27.984792 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x709a24fe2f60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x709a26029010>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x709a23feeed0>]}


============================== 20:55:28.010978 | 04a46b56-526e-4978-a89f-075f8e41e4ba ==============================
[0m20:55:28.010978 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m20:55:28.012929 [debug] [MainThread]: running dbt with arguments {'no_print': 'None', 'debug': 'False', 'version_check': 'True', 'introspect': 'True', 'warn_error': 'None', 'fail_fast': 'False', 'empty': 'None', 'profiles_dir': '/home/ecem/.dbt', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_cache_events': 'False', 'target_path': 'None', 'printer_width': '80', 'quiet': 'False', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True', 'use_experimental_parser': 'False', 'write_json': 'True', 'use_colors': 'True', 'cache_selected_only': 'False', 'partial_parse': 'True', 'log_format': 'default', 'indirect_selection': 'eager', 'invocation_command': 'dbt test --select models/staging/raw_sources.yml', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs'}
[0m20:55:35.065732 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '04a46b56-526e-4978-a89f-075f8e41e4ba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x709a037772f0>]}
[0m20:55:35.156256 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '04a46b56-526e-4978-a89f-075f8e41e4ba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x709a2490dee0>]}
[0m20:55:35.157299 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m20:55:35.598469 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m20:55:35.976720 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:55:35.978595 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:55:36.050542 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '04a46b56-526e-4978-a89f-075f8e41e4ba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x709a03659f70>]}
[0m20:55:36.179306 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m20:55:36.207164 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m20:55:36.274816 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '04a46b56-526e-4978-a89f-075f8e41e4ba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x709a02c64c50>]}
[0m20:55:36.275996 [info ] [MainThread]: Found 21 models, 67 data tests, 3 analyses, 10 sources, 624 macros
[0m20:55:36.276917 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '04a46b56-526e-4978-a89f-075f8e41e4ba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x709a24210440>]}
[0m20:55:36.285228 [info ] [MainThread]: 
[0m20:55:36.286522 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m20:55:36.287225 [info ] [MainThread]: 
[0m20:55:36.288480 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m20:55:36.296828 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_staging'
[0m20:55:36.298562 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_marts'
[0m20:55:36.299332 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:55:36.300280 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:55:37.395982 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '04a46b56-526e-4978-a89f-075f8e41e4ba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x709a046f0a70>]}
[0m20:55:37.396955 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:55:37.449167 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6
[0m20:55:37.450912 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc
[0m20:55:37.452505 [info ] [Thread-1 (]: 1 of 13 START test source_not_null_raw_data_addresses__id ...................... [RUN]
[0m20:55:37.453947 [info ] [Thread-2 (]: 2 of 13 START test source_not_null_raw_data_cities__id ......................... [RUN]
[0m20:55:37.455167 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6)
[0m20:55:37.457315 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd
[0m20:55:37.458406 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_marts, now test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc)
[0m20:55:37.464273 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc
[0m20:55:37.459798 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6
[0m20:55:37.461084 [info ] [Thread-3 (]: 3 of 13 START test source_not_null_raw_data_countries__id ...................... [RUN]
[0m20:55:37.459076 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be
[0m20:55:37.501534 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc"
[0m20:55:37.502499 [debug] [Thread-3 (]: Acquiring new bigquery connection 'test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd'
[0m20:55:37.505403 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6"
[0m20:55:37.507684 [info ] [Thread-4 (]: 4 of 13 START test source_not_null_raw_data_neighborhoods__id .................. [RUN]
[0m20:55:37.510437 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd
[0m20:55:37.512245 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6
[0m20:55:37.515757 [debug] [Thread-4 (]: Acquiring new bigquery connection 'test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be'
[0m20:55:37.513191 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc
[0m20:55:37.521507 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd"
[0m20:55:37.541996 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be
[0m20:55:37.567299 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6"
[0m20:55:37.580698 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc"
[0m20:55:37.597652 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd
[0m20:55:37.603173 [debug] [Thread-1 (]: On test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`addresses`
where _id is null



  
  
      
    ) dbt_internal_test
[0m20:55:37.621005 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m20:55:37.614318 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd"
[0m20:55:37.616863 [debug] [Thread-2 (]: On test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`cities`
where _id is null



  
  
      
    ) dbt_internal_test
[0m20:55:37.605707 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be"
[0m20:55:37.689654 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m20:55:37.701708 [debug] [Thread-3 (]: On test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`countries`
where _id is null



  
  
      
    ) dbt_internal_test
[0m20:55:37.704401 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m20:55:37.705669 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be
[0m20:55:37.801259 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be"
[0m20:55:37.806050 [debug] [Thread-4 (]: On test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`neighborhoods`
where _id is null



  
  
      
    ) dbt_internal_test
[0m20:55:37.806880 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m20:55:38.964463 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:34f599d1-39ef-4a62-b482-f2d061e023dd&page=queryresults
[0m20:55:38.995992 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:d90e8357-d123-430e-8a05-bc7c1fdd29af&page=queryresults
[0m20:55:39.034422 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:9eb65214-f874-4281-9382-c35c19244f7c&page=queryresults
[0m20:55:39.035545 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:8d91a82f-e35b-4581-b9cb-35fcf014e4cf&page=queryresults
[0m20:55:39.972114 [info ] [Thread-4 (]: 4 of 13 PASS source_not_null_raw_data_neighborhoods__id ........................ [[32mPASS[0m in 2.46s]
[0m20:55:39.976342 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be
[0m20:55:39.986628 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878
[0m20:55:39.980705 [info ] [Thread-3 (]: 3 of 13 PASS source_not_null_raw_data_countries__id ............................ [[32mPASS[0m in 2.48s]
[0m20:55:39.995843 [info ] [Thread-2 (]: 2 of 13 PASS source_not_null_raw_data_cities__id ............................... [[32mPASS[0m in 2.54s]
[0m20:55:40.002065 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd
[0m20:55:40.007751 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f
[0m20:55:40.000878 [info ] [Thread-1 (]: 1 of 13 PASS source_not_null_raw_data_addresses__id ............................ [[32mPASS[0m in 2.54s]
[0m20:55:39.997945 [info ] [Thread-4 (]: 5 of 13 START test source_not_null_raw_data_orders__id ......................... [RUN]
[0m20:55:40.006106 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc
[0m20:55:40.010729 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6
[0m20:55:40.011851 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_neighborhoods__id.d85fab20be, now test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878)
[0m20:55:40.013533 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2
[0m20:55:40.009011 [info ] [Thread-3 (]: 6 of 13 START test source_not_null_raw_data_orders__user ....................... [RUN]
[0m20:55:40.015482 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1
[0m20:55:40.017611 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878
[0m20:55:40.022160 [info ] [Thread-2 (]: 7 of 13 START test source_not_null_raw_data_shipments__id ...................... [RUN]
[0m20:55:40.023923 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_countries__id.0ab796c5dd, now test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f)
[0m20:55:40.025484 [info ] [Thread-1 (]: 8 of 13 START test source_not_null_raw_data_shipments__order ................... [RUN]
[0m20:55:40.042674 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_addresses__id.16cce229d6, now test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1)
[0m20:55:40.037929 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878"
[0m20:55:40.041884 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f
[0m20:55:40.043391 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1
[0m20:55:40.040695 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_cities__id.f31c177ccc, now test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2)
[0m20:55:40.045605 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878
[0m20:55:40.070546 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1"
[0m20:55:40.076991 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f"
[0m20:55:40.078540 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2
[0m20:55:40.081664 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878"
[0m20:55:40.082889 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1
[0m20:55:40.107019 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1"
[0m20:55:40.113505 [debug] [Thread-4 (]: On test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`orders`
where _id is null



  
  
      
    ) dbt_internal_test
[0m20:55:40.115121 [debug] [Thread-1 (]: On test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _order
from `data-pipeline-project-474812`.`raw_data`.`shipments`
where _order is null



  
  
      
    ) dbt_internal_test
[0m20:55:40.139277 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m20:55:40.118592 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m20:55:40.130312 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2"
[0m20:55:40.133168 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f
[0m20:55:40.307027 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f"
[0m20:55:40.313726 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2
[0m20:55:40.319377 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2"
[0m20:55:40.331226 [debug] [Thread-2 (]: On test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`shipments`
where _id is null



  
  
      
    ) dbt_internal_test
[0m20:55:40.326958 [debug] [Thread-3 (]: On test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _user
from `data-pipeline-project-474812`.`raw_data`.`orders`
where _user is null



  
  
      
    ) dbt_internal_test
[0m20:55:40.333720 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m20:55:40.334888 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m20:55:41.522753 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:7932c9f7-1446-4209-90ac-b5a5392eaea7&page=queryresults
[0m20:55:41.583005 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:e77542bc-5940-4270-aab0-16ebccb3c493&page=queryresults
[0m20:55:41.596459 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:97fce64e-d5bd-49db-bb0f-ad60400f8295&page=queryresults
[0m20:55:41.608714 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:7e8f981f-5dce-4e00-87fc-4342aeafc5a3&page=queryresults
[0m20:55:42.455688 [info ] [Thread-1 (]: 8 of 13 PASS source_not_null_raw_data_shipments__order ......................... [[32mPASS[0m in 2.41s]
[0m20:55:42.456805 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1
[0m20:55:42.458090 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98
[0m20:55:42.458882 [info ] [Thread-1 (]: 9 of 13 START test source_not_null_raw_data_shipments__user .................... [RUN]
[0m20:55:42.459800 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_shipments__order.e49b9d52c1, now test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98)
[0m20:55:42.460531 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98
[0m20:55:42.467247 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98"
[0m20:55:42.468743 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98
[0m20:55:42.472029 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98"
[0m20:55:42.473654 [debug] [Thread-1 (]: On test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _user
from `data-pipeline-project-474812`.`raw_data`.`shipments`
where _user is null



  
  
      
    ) dbt_internal_test
[0m20:55:42.477738 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m20:55:42.538869 [info ] [Thread-4 (]: 5 of 13 PASS source_not_null_raw_data_orders__id ............................... [[32mPASS[0m in 2.53s]
[0m20:55:42.539925 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878
[0m20:55:42.540817 [debug] [Thread-4 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86
[0m20:55:42.543556 [info ] [Thread-4 (]: 10 of 13 START test source_not_null_raw_data_states__id ........................ [RUN]
[0m20:55:42.545454 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_orders__id.b842d85878, now test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86)
[0m20:55:42.547114 [debug] [Thread-4 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86
[0m20:55:42.556742 [debug] [Thread-4 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86"
[0m20:55:42.559982 [debug] [Thread-4 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86
[0m20:55:42.572889 [debug] [Thread-4 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86"
[0m20:55:42.575611 [debug] [Thread-4 (]: On test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`states`
where _id is null



  
  
      
    ) dbt_internal_test
[0m20:55:42.577205 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m20:55:42.660861 [info ] [Thread-3 (]: 6 of 13 PASS source_not_null_raw_data_orders__user ............................. [[32mPASS[0m in 2.64s]
[0m20:55:42.678692 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f
[0m20:55:42.682345 [debug] [Thread-3 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a
[0m20:55:42.686997 [info ] [Thread-3 (]: 11 of 13 START test source_not_null_raw_data_subscriptions__id ................. [RUN]
[0m20:55:42.698745 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_orders__user.50a49afd3f, now test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a)
[0m20:55:42.699384 [debug] [Thread-3 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a
[0m20:55:42.696807 [info ] [Thread-2 (]: 7 of 13 PASS source_not_null_raw_data_shipments__id ............................ [[32mPASS[0m in 2.66s]
[0m20:55:42.719350 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2
[0m20:55:42.720382 [debug] [Thread-2 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6
[0m20:55:42.721184 [debug] [Thread-3 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a"
[0m20:55:42.724105 [info ] [Thread-2 (]: 12 of 13 START test source_not_null_raw_data_subscriptions__user ............... [RUN]
[0m20:55:42.725553 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_shipments__id.fae188d4a2, now test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6)
[0m20:55:42.727824 [debug] [Thread-2 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6
[0m20:55:42.737470 [debug] [Thread-2 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6"
[0m20:55:42.738304 [debug] [Thread-3 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a
[0m20:55:42.739436 [debug] [Thread-2 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6
[0m20:55:42.743482 [debug] [Thread-3 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a"
[0m20:55:42.747755 [debug] [Thread-2 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6"
[0m20:55:42.751655 [debug] [Thread-3 (]: On test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`subscriptions`
where _id is null



  
  
      
    ) dbt_internal_test
[0m20:55:42.756684 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m20:55:42.822290 [debug] [Thread-2 (]: On test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _user
from `data-pipeline-project-474812`.`raw_data`.`subscriptions`
where _user is null



  
  
      
    ) dbt_internal_test
[0m20:55:42.831628 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m20:55:43.768548 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:7a0719a8-ddf0-4d4f-bb0c-692b3918deb6&page=queryresults
[0m20:55:43.943839 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:c70130e6-b5d8-491c-88d5-d900802ad4ab&page=queryresults
[0m20:55:44.555967 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:9bd2aba9-0802-4230-90e8-db5434ae6147&page=queryresults
[0m20:55:44.581358 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:ff4425e1-9085-402a-87b3-e413f9d4030c&page=queryresults
[0m20:55:44.621410 [info ] [Thread-1 (]: 9 of 13 PASS source_not_null_raw_data_shipments__user .......................... [[32mPASS[0m in 2.16s]
[0m20:55:44.623761 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98
[0m20:55:44.624696 [debug] [Thread-1 (]: Began running node test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b
[0m20:55:44.625465 [info ] [Thread-1 (]: 13 of 13 START test source_not_null_raw_data_users__id ......................... [RUN]
[0m20:55:44.628180 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_project.source_not_null_raw_data_shipments__user.e06c92ea98, now test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b)
[0m20:55:44.632280 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b
[0m20:55:44.643927 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b"
[0m20:55:44.647919 [debug] [Thread-1 (]: Began executing node test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b
[0m20:55:44.653957 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b"
[0m20:55:44.664824 [debug] [Thread-1 (]: On test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select _id
from `data-pipeline-project-474812`.`raw_data`.`users`
where _id is null



  
  
      
    ) dbt_internal_test
[0m20:55:44.665614 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m20:55:44.942924 [info ] [Thread-4 (]: 10 of 13 PASS source_not_null_raw_data_states__id .............................. [[32mPASS[0m in 2.40s]
[0m20:55:44.945038 [debug] [Thread-4 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86
[0m20:55:45.452899 [info ] [Thread-3 (]: 11 of 13 PASS source_not_null_raw_data_subscriptions__id ....................... [[32mPASS[0m in 2.75s]
[0m20:55:45.454292 [debug] [Thread-3 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a
[0m20:55:45.465300 [info ] [Thread-2 (]: 12 of 13 PASS source_not_null_raw_data_subscriptions__user ..................... [[32mPASS[0m in 2.74s]
[0m20:55:45.470028 [debug] [Thread-2 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6
[0m20:55:46.087853 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:6ab6f3ef-9df5-48ec-b05b-50b2085341d3&page=queryresults
[0m20:55:46.975499 [info ] [Thread-1 (]: 13 of 13 PASS source_not_null_raw_data_users__id ............................... [[32mPASS[0m in 2.35s]
[0m20:55:46.977772 [debug] [Thread-1 (]: Finished running node test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b
[0m20:55:46.990977 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m20:55:47.061599 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:55:47.065652 [debug] [MainThread]: Connection 'test.data_pipeline_project.source_not_null_raw_data_users__id.bb993f435b' was properly closed.
[0m20:55:47.071998 [debug] [MainThread]: Connection 'test.data_pipeline_project.source_not_null_raw_data_subscriptions__user.25659372f6' was properly closed.
[0m20:55:47.072678 [debug] [MainThread]: Connection 'test.data_pipeline_project.source_not_null_raw_data_subscriptions__id.abcaddcd5a' was properly closed.
[0m20:55:47.073119 [debug] [MainThread]: Connection 'test.data_pipeline_project.source_not_null_raw_data_states__id.f3af611d86' was properly closed.
[0m20:55:47.073725 [info ] [MainThread]: 
[0m20:55:47.074665 [info ] [MainThread]: Finished running 13 data tests in 0 hours 0 minutes and 10.79 seconds (10.79s).
[0m20:55:47.077567 [debug] [MainThread]: Command end result
[0m20:55:47.167429 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m20:55:47.172527 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m20:55:47.186335 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m20:55:47.192782 [info ] [MainThread]: 
[0m20:55:47.200483 [info ] [MainThread]: [32mCompleted successfully[0m
[0m20:55:47.207930 [info ] [MainThread]: 
[0m20:55:47.211779 [info ] [MainThread]: Done. PASS=13 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=13
[0m20:55:47.246274 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": true, "command_wall_clock_time": 19.315739, "process_in_blocks": "192536", "process_kernel_time": 4.089322, "process_mem_max_rss": "412336", "process_out_blocks": "4192", "process_user_time": 6.498379}
[0m20:55:47.247851 [debug] [MainThread]: Command `dbt test` succeeded at 20:55:47.247462 after 19.32 seconds
[0m20:55:47.260518 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x709a24bfa5a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x709a02d78620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x709a02cdf2c0>]}
[0m20:55:47.264508 [debug] [MainThread]: Flushing usage events
[0m20:55:48.237541 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m21:06:13.902489 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x701d217f0d10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x701d2174c500>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x701d21013f80>]}


============================== 21:06:13.907086 | fc3556c0-b62c-456e-8c91-aaa13c39f302 ==============================
[0m21:06:13.907086 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m21:06:13.908465 [debug] [MainThread]: running dbt with arguments {'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'cache_selected_only': 'False', 'fail_fast': 'False', 'write_json': 'True', 'partial_parse': 'True', 'log_format': 'default', 'version_check': 'True', 'indirect_selection': 'eager', 'introspect': 'True', 'no_print': 'None', 'profiles_dir': '.', 'use_experimental_parser': 'False', 'log_cache_events': 'False', 'send_anonymous_usage_stats': 'True', 'use_colors': 'True', 'invocation_command': 'dbt run --select path:models/staging/*.sql --profiles-dir .', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'empty': 'False', 'debug': 'False', 'warn_error': 'None', 'quiet': 'False', 'target_path': 'None', 'printer_width': '80'}
[0m21:06:13.912141 [error] [MainThread]: Encountered an error:
Runtime Error
  Could not find profile named 'data_pipeline_project'
[0m21:06:13.913770 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.087773755, "process_in_blocks": "0", "process_kernel_time": 0.6127, "process_mem_max_rss": "102328", "process_out_blocks": "16", "process_user_time": 1.642747}
[0m21:06:13.914652 [debug] [MainThread]: Command `dbt run` failed at 21:06:13.914502 after 0.09 seconds
[0m21:06:13.915233 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x701d1f48ae10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x701d20932960>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x701d1ebe3e60>]}
[0m21:06:13.915838 [debug] [MainThread]: Flushing usage events
[0m21:06:19.757960 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m21:09:54.322294 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x707d405239b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x707d3fd6cc50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x707d40522ed0>]}


============================== 21:09:54.327978 | d12c1f10-b09f-4d61-af93-85337b5d6fa3 ==============================
[0m21:09:54.327978 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m21:09:54.331307 [debug] [MainThread]: running dbt with arguments {'invocation_command': 'dbt run --select tag:staging', 'target_path': 'None', 'log_cache_events': 'False', 'version_check': 'True', 'write_json': 'True', 'use_experimental_parser': 'False', 'printer_width': '80', 'no_print': 'None', 'partial_parse': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'quiet': 'False', 'profiles_dir': '/home/ecem/.dbt', 'empty': 'False', 'send_anonymous_usage_stats': 'True', 'introspect': 'True', 'indirect_selection': 'eager', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'log_format': 'default'}
[0m21:09:59.464016 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd12c1f10-b09f-4d61-af93-85337b5d6fa3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x707d1f784830>]}
[0m21:09:59.668521 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd12c1f10-b09f-4d61-af93-85337b5d6fa3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x707d3fbaef30>]}
[0m21:09:59.670607 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m21:10:00.777727 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m21:10:01.004761 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:10:01.005480 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:10:01.142587 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd12c1f10-b09f-4d61-af93-85337b5d6fa3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x707d1f5680e0>]}
[0m21:10:01.367232 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m21:10:01.371314 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m21:10:01.400943 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd12c1f10-b09f-4d61-af93-85337b5d6fa3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x707d1ecda720>]}
[0m21:10:01.401773 [info ] [MainThread]: Found 21 models, 67 data tests, 3 analyses, 10 sources, 624 macros
[0m21:10:01.402507 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd12c1f10-b09f-4d61-af93-85337b5d6fa3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x707d1ed287a0>]}
[0m21:10:01.404720 [warn ] [MainThread]: The selection criterion 'tag:staging' does not match any enabled nodes
[0m21:10:01.408010 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m21:10:01.411185 [debug] [MainThread]: Command end result
[0m21:10:01.472501 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m21:10:01.476874 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m21:10:01.482297 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m21:10:01.484249 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 7.2815547, "process_in_blocks": "8", "process_kernel_time": 1.768952, "process_mem_max_rss": "394416", "process_out_blocks": "3888", "process_user_time": 5.811174}
[0m21:10:01.485579 [debug] [MainThread]: Command `dbt run` succeeded at 21:10:01.485424 after 7.28 seconds
[0m21:10:01.486685 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x707d3fc75370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x707d3fb76a20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x707d1f428380>]}
[0m21:10:01.487514 [debug] [MainThread]: Flushing usage events
[0m21:10:02.530323 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m21:10:23.739494 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7113a486e930>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7113a1b8d400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7113a2529a00>]}


============================== 21:10:23.743059 | 9cf0d075-08b7-4c99-9c79-2f9a1d01b47f ==============================
[0m21:10:23.743059 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m21:10:23.744675 [debug] [MainThread]: running dbt with arguments {'warn_error': 'None', 'cache_selected_only': 'False', 'no_print': 'None', 'use_experimental_parser': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_colors': 'True', 'log_path': '/home/ecem/Desktop/data-pipeline-project/dbt/logs', 'write_json': 'True', 'debug': 'False', 'invocation_command': 'dbt run --select staging', 'indirect_selection': 'eager', 'version_check': 'True', 'profiles_dir': '/home/ecem/.dbt', 'static_parser': 'True', 'empty': 'False', 'printer_width': '80', 'target_path': 'None', 'log_cache_events': 'False', 'introspect': 'True', 'quiet': 'False', 'log_format': 'default', 'fail_fast': 'False', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True'}
[0m21:10:27.831417 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9cf0d075-08b7-4c99-9c79-2f9a1d01b47f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7113a2998800>]}
[0m21:10:27.921367 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9cf0d075-08b7-4c99-9c79-2f9a1d01b47f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71138143b9e0>]}
[0m21:10:27.922908 [info ] [MainThread]: Registered adapter: bigquery=1.10.2
[0m21:10:28.342135 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m21:10:28.614716 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:10:28.615399 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:10:28.727556 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9cf0d075-08b7-4c99-9c79-2f9a1d01b47f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x711381454620>]}
[0m21:10:28.892388 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m21:10:28.895693 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m21:10:28.927128 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9cf0d075-08b7-4c99-9c79-2f9a1d01b47f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x711380c06570>]}
[0m21:10:28.928487 [info ] [MainThread]: Found 21 models, 67 data tests, 3 analyses, 10 sources, 624 macros
[0m21:10:28.929606 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9cf0d075-08b7-4c99-9c79-2f9a1d01b47f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71138135bdd0>]}
[0m21:10:28.934054 [info ] [MainThread]: 
[0m21:10:28.935010 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m21:10:28.935608 [info ] [MainThread]: 
[0m21:10:28.937236 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m21:10:28.944806 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812'
[0m21:10:28.946116 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:10:30.158825 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812, now list_data-pipeline-project-474812_analytics_staging_marts)
[0m21:10:30.176251 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:10:30.327927 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_data-pipeline-project-474812_analytics_staging_staging'
[0m21:10:30.350907 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:10:31.429749 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9cf0d075-08b7-4c99-9c79-2f9a1d01b47f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71138155f6e0>]}
[0m21:10:31.430961 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:10:31.547149 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.stg_addresses
[0m21:10:31.549841 [debug] [Thread-2 (]: Began running node model.data_pipeline_project.stg_cities
[0m21:10:31.554274 [info ] [Thread-1 (]: 1 of 10 START sql view model analytics_staging_staging.stg_addresses ........... [RUN]
[0m21:10:31.555706 [info ] [Thread-2 (]: 2 of 10 START sql view model analytics_staging_staging.stg_cities .............. [RUN]
[0m21:10:31.569730 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_marts, now model.data_pipeline_project.stg_cities)
[0m21:10:31.563254 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_data-pipeline-project-474812_analytics_staging_staging, now model.data_pipeline_project.stg_addresses)
[0m21:10:31.566285 [debug] [Thread-4 (]: Began running node model.data_pipeline_project.stg_marketing_spend
[0m21:10:31.575091 [info ] [Thread-4 (]: 4 of 10 START sql view model analytics_staging_staging.stg_marketing_spend ..... [RUN]
[0m21:10:31.570779 [debug] [Thread-2 (]: Began compiling node model.data_pipeline_project.stg_cities
[0m21:10:31.572428 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.stg_addresses
[0m21:10:31.576933 [debug] [Thread-4 (]: Acquiring new bigquery connection 'model.data_pipeline_project.stg_marketing_spend'
[0m21:10:31.627822 [debug] [Thread-4 (]: Began compiling node model.data_pipeline_project.stg_marketing_spend
[0m21:10:31.562097 [debug] [Thread-3 (]: Began running node model.data_pipeline_project.stg_countries
[0m21:10:31.643017 [debug] [Thread-2 (]: Writing injected SQL for node "model.data_pipeline_project.stg_cities"
[0m21:10:31.645500 [debug] [Thread-4 (]: Writing injected SQL for node "model.data_pipeline_project.stg_marketing_spend"
[0m21:10:31.683018 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.stg_addresses"
[0m21:10:31.658189 [info ] [Thread-3 (]: 3 of 10 START sql view model analytics_staging_staging.stg_countries ........... [RUN]
[0m21:10:31.693256 [debug] [Thread-2 (]: Began executing node model.data_pipeline_project.stg_cities
[0m21:10:31.699928 [debug] [Thread-4 (]: Began executing node model.data_pipeline_project.stg_marketing_spend
[0m21:10:31.701450 [debug] [Thread-3 (]: Acquiring new bigquery connection 'model.data_pipeline_project.stg_countries'
[0m21:10:31.771608 [debug] [Thread-3 (]: Began compiling node model.data_pipeline_project.stg_countries
[0m21:10:31.806233 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.stg_addresses
[0m21:10:31.892626 [debug] [Thread-3 (]: Writing injected SQL for node "model.data_pipeline_project.stg_countries"
[0m21:10:31.920138 [debug] [Thread-2 (]: Writing runtime sql for node "model.data_pipeline_project.stg_cities"
[0m21:10:31.941200 [debug] [Thread-2 (]: On model.data_pipeline_project.stg_cities: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.stg_cities"} */


  create or replace view `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_cities`
  OPTIONS()
  as with source as (

    select * from `data-pipeline-project-474812`.`raw_data`.`cities`

)

select distinct
    -- Primary Key
    _id as city_id,
    
    -- Foreign Keys
    _state as state_id,
    _country as country_id,
    
    -- Attributes
    name as city_name

from source
where _id is not null;


[0m21:10:31.937021 [debug] [Thread-4 (]: Writing runtime sql for node "model.data_pipeline_project.stg_marketing_spend"
[0m21:10:31.922889 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.stg_addresses"
[0m21:10:31.952204 [debug] [Thread-1 (]: On model.data_pipeline_project.stg_addresses: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.stg_addresses"} */


  create or replace view `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_addresses`
  OPTIONS()
  as with source as (

    select * from `data-pipeline-project-474812`.`raw_data`.`addresses`

)

select distinct
    -- Primary Key
    _id as address_id,
    
    -- Foreign Keys (Geographical and User Links)
    _city as city_id,
    _state as state_id,
    _country as country_id,
    _user as user_id, 
    _neighborhood as neighborhood_id, -- Maps to the neighborhood table ID
    
    -- Attributes
    invoiceType as invoice_type

from source
where _id is not null;


[0m21:10:31.947192 [debug] [Thread-3 (]: Began executing node model.data_pipeline_project.stg_countries
[0m21:10:31.949863 [debug] [Thread-4 (]: On model.data_pipeline_project.stg_marketing_spend: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.stg_marketing_spend"} */


  create or replace view `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_marketing_spend`
  OPTIONS()
  as with source as (

    select * from `data-pipeline-project-474812`.`raw_data`.`marketing_spend`

),

unpivot_spend as (

    -- Manual UNPIVOT for each date column
    select
        channel as marketing_channel,
        '2025-09-20' as spend_date_raw,
        cast(replace(`2025-09-20`, ',', '') as numeric) as spend_amount_try
    from source
    where `2025-09-20` is not null and cast(replace(`2025-09-20`, ',', '') as numeric) > 0

    union all

    select
        channel as marketing_channel,
        '2025-09-21' as spend_date_raw,
        cast(replace(`2025-09-21`, ',', '') as numeric) as spend_amount_try
    from source
    where `2025-09-21` is not null and cast(replace(`2025-09-21`, ',', '') as numeric) > 0

    union all

    select
        channel as marketing_channel,
        '2025-09-22' as spend_date_raw,
        cast(replace(`2025-09-22`, ',', '') as numeric) as spend_amount_try
    from source
    where `2025-09-22` is not null and cast(replace(`2025-09-22`, ',', '') as numeric) > 0

    union all

    select
        channel as marketing_channel,
        '2025-09-23' as spend_date_raw,
        cast(replace(`2025-09-23`, ',', '') as numeric) as spend_amount_try
    from source
    where `2025-09-23` is not null and cast(replace(`2025-09-23`, ',', '') as numeric) > 0

    union all

    select
        channel as marketing_channel,
        '2025-09-24' as spend_date_raw,
        cast(replace(`2025-09-24`, ',', '') as numeric) as spend_amount_try
    from source
    where `2025-09-24` is not null and cast(replace(`2025-09-24`, ',', '') as numeric) > 0

    union all

    select
        channel as marketing_channel,
        '2025-09-25' as spend_date_raw,
        cast(replace(`2025-09-25`, ',', '') as numeric) as spend_amount_try
    from source
    where `2025-09-25` is not null and cast(replace(`2025-09-25`, ',', '') as numeric) > 0

    union all

    select
        channel as marketing_channel,
        '2025-09-26' as spend_date_raw,
        cast(replace(`2025-09-26`, ',', '') as numeric) as spend_amount_try
    from source
    where `2025-09-26` is not null and cast(replace(`2025-09-26`, ',', '') as numeric) > 0

    union all

    select
        channel as marketing_channel,
        '2025-09-27' as spend_date_raw,
        cast(replace(`2025-09-27`, ',', '') as numeric) as spend_amount_try
    from source
    where `2025-09-27` is not null and cast(replace(`2025-09-27`, ',', '') as numeric) > 0

),

final_conversion as (

    select
        parse_date('%Y-%m-%d', spend_date_raw) as spend_date,
        marketing_channel,
        spend_amount_try
    
    from unpivot_spend

)

select * from final_conversion;


[0m21:10:31.942472 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m21:10:31.956923 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:10:31.963614 [debug] [Thread-3 (]: Writing runtime sql for node "model.data_pipeline_project.stg_countries"
[0m21:10:31.971218 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m21:10:32.308104 [debug] [Thread-3 (]: On model.data_pipeline_project.stg_countries: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.stg_countries"} */


  create or replace view `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_countries`
  OPTIONS()
  as with source as (

    select * from `data-pipeline-project-474812`.`raw_data`.`countries`

)

select distinct
    -- Primary Key: Standardize to country_id
    _id as country_id,
    
    -- Attributes
    name as country_name

from source
where _id is not null;


[0m21:10:32.321233 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m21:10:33.968358 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:58fae8f8-35d1-4f0d-b253-53381cd06f9b&page=queryresults
[0m21:10:33.987060 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:39d3126d-8bc5-43fd-9c8f-eae92ebf7c8c&page=queryresults
[0m21:10:34.028718 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:29df5d1c-0688-4935-a028-f11aed16156e&page=queryresults
[0m21:10:34.147765 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:9edac689-189e-4611-9546-07447c2b344e&page=queryresults
[0m21:10:34.795162 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9cf0d075-08b7-4c99-9c79-2f9a1d01b47f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7113809ddb50>]}
[0m21:10:34.796718 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9cf0d075-08b7-4c99-9c79-2f9a1d01b47f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7113a29449e0>]}
[0m21:10:34.799857 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9cf0d075-08b7-4c99-9c79-2f9a1d01b47f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7113809dd040>]}
[0m21:10:34.801375 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9cf0d075-08b7-4c99-9c79-2f9a1d01b47f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7113809abe90>]}
[0m21:10:34.803391 [info ] [Thread-2 (]: 2 of 10 OK created sql view model analytics_staging_staging.stg_cities ......... [[32mCREATE VIEW (0 processed)[0m in 3.18s]
[0m21:10:34.808106 [info ] [Thread-1 (]: 1 of 10 OK created sql view model analytics_staging_staging.stg_addresses ...... [[32mCREATE VIEW (0 processed)[0m in 3.18s]
[0m21:10:34.824201 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.stg_addresses
[0m21:10:34.816413 [info ] [Thread-3 (]: 3 of 10 OK created sql view model analytics_staging_staging.stg_countries ...... [[32mCREATE VIEW (0 processed)[0m in 3.07s]
[0m21:10:34.821082 [debug] [Thread-2 (]: Finished running node model.data_pipeline_project.stg_cities
[0m21:10:34.825939 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.stg_neighborhoods
[0m21:10:34.818951 [info ] [Thread-4 (]: 4 of 10 OK created sql view model analytics_staging_staging.stg_marketing_spend  [[32mCREATE VIEW (0 processed)[0m in 3.19s]
[0m21:10:34.831632 [debug] [Thread-3 (]: Finished running node model.data_pipeline_project.stg_countries
[0m21:10:34.839926 [debug] [Thread-3 (]: Began running node model.data_pipeline_project.stg_shipments
[0m21:10:34.833012 [debug] [Thread-2 (]: Began running node model.data_pipeline_project.stg_orders
[0m21:10:34.835408 [info ] [Thread-1 (]: 5 of 10 START sql view model analytics_staging_staging.stg_neighborhoods ....... [RUN]
[0m21:10:34.837274 [debug] [Thread-4 (]: Finished running node model.data_pipeline_project.stg_marketing_spend
[0m21:10:34.842579 [info ] [Thread-3 (]: 7 of 10 START sql view model analytics_staging_staging.stg_shipments ........... [RUN]
[0m21:10:34.852255 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly model.data_pipeline_project.stg_countries, now model.data_pipeline_project.stg_shipments)
[0m21:10:34.850017 [debug] [Thread-4 (]: Began running node model.data_pipeline_project.stg_states
[0m21:10:34.847481 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline_project.stg_addresses, now model.data_pipeline_project.stg_neighborhoods)
[0m21:10:34.853065 [debug] [Thread-3 (]: Began compiling node model.data_pipeline_project.stg_shipments
[0m21:10:34.844741 [info ] [Thread-2 (]: 6 of 10 START sql view model analytics_staging_staging.stg_orders .............. [RUN]
[0m21:10:34.891545 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.data_pipeline_project.stg_cities, now model.data_pipeline_project.stg_orders)
[0m21:10:34.857530 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.stg_neighborhoods
[0m21:10:34.855092 [info ] [Thread-4 (]: 8 of 10 START sql view model analytics_staging_staging.stg_states .............. [RUN]
[0m21:10:34.915524 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.data_pipeline_project.stg_marketing_spend, now model.data_pipeline_project.stg_states)
[0m21:10:34.892771 [debug] [Thread-2 (]: Began compiling node model.data_pipeline_project.stg_orders
[0m21:10:34.887491 [debug] [Thread-3 (]: Writing injected SQL for node "model.data_pipeline_project.stg_shipments"
[0m21:10:34.918715 [debug] [Thread-4 (]: Began compiling node model.data_pipeline_project.stg_states
[0m21:10:34.920994 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.stg_neighborhoods"
[0m21:10:34.937412 [debug] [Thread-4 (]: Writing injected SQL for node "model.data_pipeline_project.stg_states"
[0m21:10:34.940434 [debug] [Thread-2 (]: Writing injected SQL for node "model.data_pipeline_project.stg_orders"
[0m21:10:34.945005 [debug] [Thread-4 (]: Began executing node model.data_pipeline_project.stg_states
[0m21:10:34.948930 [debug] [Thread-3 (]: Began executing node model.data_pipeline_project.stg_shipments
[0m21:10:34.960226 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.stg_neighborhoods
[0m21:10:34.965267 [debug] [Thread-4 (]: Writing runtime sql for node "model.data_pipeline_project.stg_states"
[0m21:10:34.968624 [debug] [Thread-2 (]: Began executing node model.data_pipeline_project.stg_orders
[0m21:10:34.978288 [debug] [Thread-3 (]: Writing runtime sql for node "model.data_pipeline_project.stg_shipments"
[0m21:10:34.992511 [debug] [Thread-4 (]: On model.data_pipeline_project.stg_states: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.stg_states"} */


  create or replace view `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_states`
  OPTIONS()
  as with source as (

    select * from `data-pipeline-project-474812`.`raw_data`.`states`

)

select distinct
    -- Primary Key
    _id as state_id,
    
    -- Foreign Key (to countries)
    _country as country_id,
    
    -- Attributes
    name as state_name

from source
where _id is not null;


[0m21:10:34.995837 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.stg_neighborhoods"
[0m21:10:35.012493 [debug] [Thread-2 (]: Writing runtime sql for node "model.data_pipeline_project.stg_orders"
[0m21:10:35.014518 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m21:10:35.023460 [debug] [Thread-2 (]: On model.data_pipeline_project.stg_orders: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.stg_orders"} */


  create or replace view `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_orders`
  OPTIONS()
  as with source as (

    select * from `data-pipeline-project-474812`.`raw_data`.`orders`

),

renamed_and_cleaned as (

    select

        -- Deduplication logic: Prioritize the first record if an ID appears more than once
        row_number() over (partition by _id order by createdAt asc) as rn,

        -- Primary Key
        _id as order_id,

        -- Foreign Keys
        _user as user_id,
        _deliveryAddress as delivery_address_id,
        _invoiceAddress as invoice_address_id,

        -- Standardized Timestamps
        cast(createdAt as timestamp) as created_at,

        -- Metrics: JSON Extraction
        -- Extract the final charged amount (in TRY, as confirmed)
        cast(json_extract_scalar(price, '$.chargedAmount') as numeric) as charged_amount_try,
        
        -- Attributes
        status as order_status,

        -- Complex/JSON Fields (Selected as raw strings)
        price as price_data,
        oneTimePurchase as one_time_purchase_data,
        subscriptions as subscription_data

    from source
    where _id is not null
    -- Simple cleaning: Filter out records where we cannot extract a charged amount
    and json_extract_scalar(price, '$.chargedAmount') is not null
    and cast(json_extract_scalar(price, '$.chargedAmount') as numeric) > 0

)

select 
    order_id,
    user_id,
    delivery_address_id,
    invoice_address_id,
    created_at,
    charged_amount_try,
    order_status,
    price_data,
    one_time_purchase_data,
    subscription_data
from renamed_and_cleaned
-- Final selection: ONLY select the single, canonical record (rn = 1)
where rn = 1;


[0m21:10:35.017898 [debug] [Thread-1 (]: On model.data_pipeline_project.stg_neighborhoods: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.stg_neighborhoods"} */


  create or replace view `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_neighborhoods`
  OPTIONS()
  as with source as (

    select * from `data-pipeline-project-474812`.`raw_data`.`neighborhoods`

)

select distinct
    -- Primary Key
    _id as neighborhood_id,
    
    -- Foreign Keys
    _city as city_id,
    _country as country_id,
    
    -- Attributes
    name as neighborhood_name,
    postalCode as postal_code

from source
where _id is not null;


[0m21:10:35.015956 [debug] [Thread-3 (]: On model.data_pipeline_project.stg_shipments: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.stg_shipments"} */


  create or replace view `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_shipments`
  OPTIONS()
  as with source as (

    select * from `data-pipeline-project-474812`.`raw_data`.`shipments`

),

renamed_and_cleaned as (

    select

        -- Deduplication logic: Prioritize the latest record (newest created_at, if available, otherwise just use a distinct ID)
        -- Since this table is likely event-based, we prioritize the latest row for any given ID.
        row_number() over (partition by _id order by collectDate desc) as rn,

        -- Primary Key
        _id as shipment_id,

        -- Foreign Keys
        _order as order_id,
        _user as user_id,

        -- Standardized Timestamps
        -- Assuming 'createdAt' still exists as a standard column for the record's creation time
        cast(collectDate as timestamp) as collected_at,

        -- Complex/JSON Fields (Selected as raw strings for later parsing in Intermediate models)
        details as details_json,
        label as label_json

    from source
    where _id is not null
    and _order is not null -- Ensure the shipment links to a valid order
)

select
    shipment_id,
    order_id,
    user_id,
    collected_at,
    details_json,
    label_json
from renamed_and_cleaned
-- Final selection: only include the latest, unique instance
where rn = 1;


[0m21:10:35.118217 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m21:10:35.131036 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:10:35.134911 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m21:10:36.513665 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:7827784f-025d-4708-83d9-ebdd31c3469c&page=queryresults
[0m21:10:36.923718 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:b104c448-835b-4ccc-85b7-b6530b4d52b1&page=queryresults
[0m21:10:36.958329 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:d6dbc447-fa37-4761-aca2-4bab762e6031&page=queryresults
[0m21:10:37.128487 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9cf0d075-08b7-4c99-9c79-2f9a1d01b47f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71137cd7b440>]}
[0m21:10:37.138076 [info ] [Thread-4 (]: 8 of 10 OK created sql view model analytics_staging_staging.stg_states ......... [[32mCREATE VIEW (0 processed)[0m in 2.21s]
[0m21:10:37.139833 [debug] [Thread-4 (]: Finished running node model.data_pipeline_project.stg_states
[0m21:10:37.141488 [debug] [Thread-4 (]: Began running node model.data_pipeline_project.stg_subscriptions
[0m21:10:37.144697 [info ] [Thread-4 (]: 9 of 10 START sql view model analytics_staging_staging.stg_subscriptions ....... [RUN]
[0m21:10:37.146635 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.data_pipeline_project.stg_states, now model.data_pipeline_project.stg_subscriptions)
[0m21:10:37.149057 [debug] [Thread-4 (]: Began compiling node model.data_pipeline_project.stg_subscriptions
[0m21:10:37.158925 [debug] [Thread-4 (]: Writing injected SQL for node "model.data_pipeline_project.stg_subscriptions"
[0m21:10:37.160960 [debug] [Thread-4 (]: Began executing node model.data_pipeline_project.stg_subscriptions
[0m21:10:37.166939 [debug] [Thread-4 (]: Writing runtime sql for node "model.data_pipeline_project.stg_subscriptions"
[0m21:10:37.172027 [debug] [Thread-4 (]: On model.data_pipeline_project.stg_subscriptions: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.stg_subscriptions"} */


  create or replace view `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_subscriptions`
  OPTIONS()
  as with source as (

    select * from `data-pipeline-project-474812`.`raw_data`.`subscriptions`

),

renamed_and_cleaned as (

    select

        -- Deduplication logic: Prioritize the latest record (newest created_at)
        row_number() over (partition by _id order by createdAt desc) as rn,

        -- Primary Key
        _id as subscription_id,

        -- Foreign Key
        _user as user_id,

        -- Standardized Timestamps
        cast(createdAt as timestamp) as created_at,
        cast(nextOrderDate as timestamp) as next_order_date,
        cast(startDate as timestamp) as start_date,

        -- Metrics
        cast(totalQuantity as integer) as total_quantity,

        -- Flags (Casting to BOOLEAN for clean usage)
        cast(isActive as boolean) as is_active,
        cast(isSkip as boolean) as is_skip,

        -- Complex/JSON Field (Selected as raw string for later parsing)
        products as products_data

    from source
    where _id is not null
    -- Simple cleaning: Ensure start_date and quantity are valid
    and startDate is not null
    and totalQuantity is not null and totalQuantity >= 0
)

select
    subscription_id,
    user_id,
    created_at,
    next_order_date,
    start_date,
    total_quantity,
    is_active,
    is_skip,
    products_data
from renamed_and_cleaned
-- Final selection: only include the latest, unique instance
where rn = 1;


[0m21:10:37.172884 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m21:10:37.452174 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:e7005ad7-3d3b-4024-8128-1d6955291359&page=queryresults
[0m21:10:37.808778 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9cf0d075-08b7-4c99-9c79-2f9a1d01b47f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7113809c7050>]}
[0m21:10:37.823582 [info ] [Thread-1 (]: 5 of 10 OK created sql view model analytics_staging_staging.stg_neighborhoods .. [[32mCREATE VIEW (0 processed)[0m in 2.96s]
[0m21:10:37.855587 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.stg_neighborhoods
[0m21:10:37.856357 [debug] [Thread-1 (]: Began running node model.data_pipeline_project.stg_users
[0m21:10:37.864600 [info ] [Thread-1 (]: 10 of 10 START sql view model analytics_staging_staging.stg_users .............. [RUN]
[0m21:10:37.869373 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline_project.stg_neighborhoods, now model.data_pipeline_project.stg_users)
[0m21:10:37.873529 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_project.stg_users
[0m21:10:37.889457 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_project.stg_users"
[0m21:10:37.913366 [debug] [Thread-1 (]: Began executing node model.data_pipeline_project.stg_users
[0m21:10:37.935354 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9cf0d075-08b7-4c99-9c79-2f9a1d01b47f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71137cd61400>]}
[0m21:10:37.947310 [info ] [Thread-2 (]: 6 of 10 OK created sql view model analytics_staging_staging.stg_orders ......... [[32mCREATE VIEW (0 processed)[0m in 3.04s]
[0m21:10:37.950210 [debug] [Thread-2 (]: Finished running node model.data_pipeline_project.stg_orders
[0m21:10:37.964549 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_project.stg_users"
[0m21:10:37.977952 [debug] [Thread-1 (]: On model.data_pipeline_project.stg_users: /* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "data_pipeline_project", "target_name": "dev", "node_id": "model.data_pipeline_project.stg_users"} */


  create or replace view `data-pipeline-project-474812`.`analytics_staging_staging`.`stg_users`
  OPTIONS()
  as with source as (

    select * from `data-pipeline-project-474812`.`raw_data`.`users`

),

-- Get the earliest subscription date for each user
subscription_dates as (
    select
        _user as user_id,
        min(cast(createdAt as timestamp)) as first_subscription_date
    from `data-pipeline-project-474812`.`raw_data`.`subscriptions`
    where _user is not null
    group by 1
),

renamed_and_cleaned as (

    select

        -- Primary Key: Standardized to user_id
        _id as user_id,

        -- SMART TIMESTAMP LOGIC: Use subscription date as fallback for missing created_at
        cast(
            case 
                when u.createdAt is not null then u.createdAt
                else s.first_subscription_date  -- Fallback to first subscription date
            end as timestamp
        ) as created_at,
        
        -- Deduplication logic: Assign a row number based on the creation time
        row_number() over (partition by _id order by 
            case 
                when u.createdAt is not null then u.createdAt
                else s.first_subscription_date 
            end desc
        ) as rn

    from source u
    left join subscription_dates s on u._id = s.user_id
    -- Simple cleaning: ensure that _id column is not null
    where _id is not null
)

select
    user_id,
    created_at

from renamed_and_cleaned
-- Final selection: only include the latest, unique instance of a user_id
where rn = 1;


[0m21:10:37.982464 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:10:38.312639 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9cf0d075-08b7-4c99-9c79-2f9a1d01b47f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71137cdc7260>]}
[0m21:10:38.344282 [info ] [Thread-3 (]: 7 of 10 OK created sql view model analytics_staging_staging.stg_shipments ...... [[32mCREATE VIEW (0 processed)[0m in 3.46s]
[0m21:10:38.384509 [debug] [Thread-3 (]: Finished running node model.data_pipeline_project.stg_shipments
[0m21:10:39.301616 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:0436094a-e027-4c7c-92f1-e5122400c76c&page=queryresults
[0m21:10:40.022500 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=data-pipeline-project-474812&j=bq:US:c5838133-f788-4609-ae58-2e35ec966e41&page=queryresults
[0m21:10:40.032501 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9cf0d075-08b7-4c99-9c79-2f9a1d01b47f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71137cdc7a70>]}
[0m21:10:40.061229 [info ] [Thread-4 (]: 9 of 10 OK created sql view model analytics_staging_staging.stg_subscriptions .. [[32mCREATE VIEW (0 processed)[0m in 2.84s]
[0m21:10:40.078103 [debug] [Thread-4 (]: Finished running node model.data_pipeline_project.stg_subscriptions
[0m21:10:40.625422 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9cf0d075-08b7-4c99-9c79-2f9a1d01b47f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71137cdb6c60>]}
[0m21:10:40.630373 [info ] [Thread-1 (]: 10 of 10 OK created sql view model analytics_staging_staging.stg_users ......... [[32mCREATE VIEW (0 processed)[0m in 2.76s]
[0m21:10:40.640971 [debug] [Thread-1 (]: Finished running node model.data_pipeline_project.stg_users
[0m21:10:40.693565 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:10:40.949312 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:10:40.954415 [debug] [MainThread]: Connection 'model.data_pipeline_project.stg_orders' was properly closed.
[0m21:10:40.957611 [debug] [MainThread]: Connection 'model.data_pipeline_project.stg_users' was properly closed.
[0m21:10:40.960143 [debug] [MainThread]: Connection 'model.data_pipeline_project.stg_subscriptions' was properly closed.
[0m21:10:40.960876 [debug] [MainThread]: Connection 'model.data_pipeline_project.stg_shipments' was properly closed.
[0m21:10:40.961965 [info ] [MainThread]: 
[0m21:10:40.962605 [info ] [MainThread]: Finished running 10 view models in 0 hours 0 minutes and 12.03 seconds (12.03s).
[0m21:10:40.971481 [debug] [MainThread]: Command end result
[0m21:10:41.298412 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/manifest.json
[0m21:10:41.305450 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ecem/Desktop/data-pipeline-project/dbt/target/semantic_manifest.json
[0m21:10:41.326710 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ecem/Desktop/data-pipeline-project/dbt/target/run_results.json
[0m21:10:41.327717 [info ] [MainThread]: 
[0m21:10:41.328763 [info ] [MainThread]: [32mCompleted successfully[0m
[0m21:10:41.329494 [info ] [MainThread]: 
[0m21:10:41.330660 [info ] [MainThread]: Done. PASS=10 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=10
[0m21:10:41.332445 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 17.680702, "process_in_blocks": "8", "process_kernel_time": 3.086409, "process_mem_max_rss": "411368", "process_out_blocks": "4176", "process_user_time": 8.962761}
[0m21:10:41.333568 [debug] [MainThread]: Command `dbt run` succeeded at 21:10:41.333413 after 17.68 seconds
[0m21:10:41.334456 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7113a2529f40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7113a20cc8f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7113a1a92ba0>]}
[0m21:10:41.335282 [debug] [MainThread]: Flushing usage events
[0m21:10:42.289360 [debug] [MainThread]: An error was encountered while trying to flush usage events
